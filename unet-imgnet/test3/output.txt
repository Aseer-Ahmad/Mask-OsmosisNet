CONFIG : 
{'EPOCHS': 10, 'RESUME_CHECKPOINT': None, 'SAVE_EVERY': 10, 'VAL_EVERY': 10, 'OUTPUT_DIR': 'unet-imgnet', 'EXP_NAME': 'test3', 'TRAIN_FILENAME': 'iids_train.txt', 'TEST_FILENAME': 'iids_test.txt', 'ROOT_DIR': 'dataset\\imagenet\\images\\', 'IMG_SIZE': 128, 'INP_CHANNELS': 1, 'OUT_CHANNELS': 1, 'LR': 0.001, 'WEIGHT_DECAY': 0.0, 'MOMENTUM': 0.9, 'OPT': 'Adam', 'SCHEDL': 'lambdaLR', 'TRAIN_BATCH': 8, 'TEST_BATCH': 8, 'ALPHA': 0.001, 'MASK_DEN': 0.1, 'BIN_METH': 'QUANT', 'OFFSET': None, 'TAU': None, 'ITERATIONS': None}

train test dataset loaded
train size : 100000
test  size  : 1000
model loaded
model summary
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
??DoubleConv: 1-1                        --
|    ??Sequential: 2-1                   --
|    |    ??Conv2d: 3-1                  576
|    |    ??BatchNorm2d: 3-2             128
|    |    ??ReLU: 3-3                    --
|    |    ??Conv2d: 3-4                  36,864
|    |    ??BatchNorm2d: 3-5             128
|    |    ??ReLU: 3-6                    --
??Down: 1-2                              --
|    ??Sequential: 2-2                   --
|    |    ??MaxPool2d: 3-7               --
|    |    ??DoubleConv: 3-8              221,696
??Down: 1-3                              --
|    ??Sequential: 2-3                   --
|    |    ??MaxPool2d: 3-9               --
|    |    ??DoubleConv: 3-10             885,760
??Up: 1-4                                --
|    ??ConvTranspose2d: 2-4              131,200
|    ??DoubleConv: 2-5                   --
|    |    ??Sequential: 3-11             442,880
??Up: 1-5                                --
|    ??ConvTranspose2d: 2-6              32,832
|    ??DoubleConv: 2-7                   --
|    |    ??Sequential: 3-12             110,848
??OutConv: 1-6                           --
|    ??Conv2d: 2-8                       65
=================================================================
Total params: 1,862,977
Trainable params: 1,862,977
Non-trainable params: 0
=================================================================
device : cuda
trainer configurations set
train and test dataloaders created
total train batches  : 12500
total test  batches  : 125
optimizer : Adam, scheduler : lambdaLR loaded
optimizer : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
scheduler : <torch.optim.lr_scheduler.LambdaLR object at 0x000002223E725940>
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization

beginning training ...
Epoch 0/10 , batch 1/12500 
ITERATION : 1, loss : 0.025693786493536756ITERATION : 2, loss : 0.027257541236732675ITERATION : 3, loss : 0.02899498768517225ITERATION : 4, loss : 0.03041810871570069ITERATION : 5, loss : 0.03149988271049576ITERATION : 6, loss : 0.03229730293626829ITERATION : 7, loss : 0.03287578184171988ITERATION : 8, loss : 0.03329149714230196ITERATION : 9, loss : 0.03358847380306992ITERATION : 10, loss : 0.03379979770585823ITERATION : 11, loss : 0.03394977475054439ITERATION : 12, loss : 0.034056019814904234ITERATION : 13, loss : 0.03413118908331708ITERATION : 14, loss : 0.03418432430384303ITERATION : 15, loss : 0.03422186065185465ITERATION : 16, loss : 0.034248365394687036ITERATION : 17, loss : 0.0342670747660598ITERATION : 18, loss : 0.034280278389284276ITERATION : 19, loss : 0.03428959510796402ITERATION : 20, loss : 0.0342961683422547ITERATION : 21, loss : 0.03430080557851372ITERATION : 22, loss : 0.03430407683848789ITERATION : 23, loss : 0.03430638438119025ITERATION : 24, loss : 0.03430801209706723ITERATION : 25, loss : 0.03430916021133096ITERATION : 26, loss : 0.03430997006680152ITERATION : 27, loss : 0.03431054120889065ITERATION : 28, loss : 0.034310944083744487ITERATION : 29, loss : 0.03431122831267409ITERATION : 30, loss : 0.03431142868577313ITERATION : 31, loss : 0.0343115701166012ITERATION : 32, loss : 0.034311669790034104ITERATION : 33, loss : 0.034311740192099816ITERATION : 34, loss : 0.03431178975685597ITERATION : 35, loss : 0.03431182474380068ITERATION : 36, loss : 0.0343118494528781ITERATION : 37, loss : 0.03431186689959123ITERATION : 38, loss : 0.034311879213322787ITERATION : 39, loss : 0.03431188790957359ITERATION : 40, loss : 0.03431189406166988ITERATION : 41, loss : 0.034311898410411636ITERATION : 42, loss : 0.034311901452567475ITERATION : 43, loss : 0.03431190360455204ITERATION : 44, loss : 0.03431190511804102ITERATION : 45, loss : 0.03431190619604331ITERATION : 46, loss : 0.03431190696329148ITERATION : 47, loss : 0.0343119075130129ITERATION : 48, loss : 0.03431190789031866ITERATION : 49, loss : 0.03431190813205562ITERATION : 50, loss : 0.034311908316273676ITERATION : 51, loss : 0.03431190842068811ITERATION : 52, loss : 0.03431190852754192ITERATION : 53, loss : 0.03431190853117294ITERATION : 54, loss : 0.03431190853117294ITERATION : 55, loss : 0.03431190853117294ITERATION : 56, loss : 0.03431190853117294ITERATION : 57, loss : 0.03431190853117294ITERATION : 58, loss : 0.03431190853117294ITERATION : 59, loss : 0.03431190853117294ITERATION : 60, loss : 0.03431190853117294ITERATION : 61, loss : 0.03431190853117294ITERATION : 62, loss : 0.03431190853117294ITERATION : 63, loss : 0.03431190853117294ITERATION : 64, loss : 0.03431190853117294ITERATION : 65, loss : 0.03431190853117294ITERATION : 66, loss : 0.03431190853117294ITERATION : 67, loss : 0.03431190853117294ITERATION : 68, loss : 0.03431190853117294ITERATION : 69, loss : 0.03431190853117294ITERATION : 70, loss : 0.03431190853117294ITERATION : 71, loss : 0.03431190853117294ITERATION : 72, loss : 0.03431190853117294ITERATION : 73, loss : 0.03431190853117294ITERATION : 74, loss : 0.03431190853117294ITERATION : 75, loss : 0.03431190853117294ITERATION : 76, loss : 0.03431190853117294ITERATION : 77, loss : 0.03431190853117294ITERATION : 78, loss : 0.03431190853117294ITERATION : 79, loss : 0.03431190853117294ITERATION : 80, loss : 0.03431190853117294ITERATION : 81, loss : 0.03431190853117294ITERATION : 82, loss : 0.03431190853117294ITERATION : 83, loss : 0.03431190853117294ITERATION : 84, loss : 0.03431190853117294ITERATION : 85, loss : 0.03431190853117294ITERATION : 86, loss : 0.03431190853117294ITERATION : 87, loss : 0.03431190853117294ITERATION : 88, loss : 0.03431190853117294ITERATION : 89, loss : 0.03431190853117294ITERATION : 90, loss : 0.03431190853117294ITERATION : 91, loss : 0.03431190853117294ITERATION : 92, loss : 0.03431190853117294ITERATION : 93, loss : 0.03431190853117294ITERATION : 94, loss : 0.03431190853117294ITERATION : 95, loss : 0.03431190853117294ITERATION : 96, loss : 0.03431190853117294ITERATION : 97, loss : 0.03431190853117294ITERATION : 98, loss : 0.03431190853117294ITERATION : 99, loss : 0.03431190853117294ITERATION : 100, loss : 0.03431190853117294
ITERATION : 1, loss : 0.049930707233580764ITERATION : 2, loss : 0.05614690492067015ITERATION : 3, loss : 0.06160263452271259ITERATION : 4, loss : 0.06590477477589017ITERATION : 5, loss : 0.069170812388944ITERATION : 6, loss : 0.07159913397693665ITERATION : 7, loss : 0.07338163568806846ITERATION : 8, loss : 0.07467967452768343ITERATION : 9, loss : 0.0756201873722331ITERATION : 10, loss : 0.07629946405386182ITERATION : 11, loss : 0.0767890367784131ITERATION : 12, loss : 0.07714138925586823ITERATION : 13, loss : 0.07739473834265397ITERATION : 14, loss : 0.07757678005354822ITERATION : 15, loss : 0.07770752231511986ITERATION : 16, loss : 0.07780138911665245ITERATION : 17, loss : 0.0778687641484668ITERATION : 18, loss : 0.07791711534540183ITERATION : 19, loss : 0.07795180952289231ITERATION : 20, loss : 0.07797670155486615ITERATION : 21, loss : 0.07799455938115026ITERATION : 22, loss : 0.07800737019113903ITERATION : 23, loss : 0.07801655997538214ITERATION : 24, loss : 0.07802315193672428ITERATION : 25, loss : 0.07802788025871524ITERATION : 26, loss : 0.0780312719329134ITERATION : 27, loss : 0.07803370464139799ITERATION : 28, loss : 0.07803544946122383ITERATION : 29, loss : 0.07803670099731999ITERATION : 30, loss : 0.07803759877629178ITERATION : 31, loss : 0.07803824272635701ITERATION : 32, loss : 0.07803870464157477ITERATION : 33, loss : 0.07803903594927114ITERATION : 34, loss : 0.0780392736132035ITERATION : 35, loss : 0.07803944407090993ITERATION : 36, loss : 0.07803956634575906ITERATION : 37, loss : 0.07803965403102207ITERATION : 38, loss : 0.07803971693384533ITERATION : 39, loss : 0.07803976202991912ITERATION : 40, loss : 0.07803979436395095ITERATION : 41, loss : 0.07803981753982275ITERATION : 42, loss : 0.07803983415742612ITERATION : 43, loss : 0.07803984606957583ITERATION : 44, loss : 0.0780398546371944ITERATION : 45, loss : 0.07803986078344564ITERATION : 46, loss : 0.07803986521732627ITERATION : 47, loss : 0.07803986840268551ITERATION : 48, loss : 0.07803987066880183ITERATION : 49, loss : 0.07803987220781983ITERATION : 50, loss : 0.07803987338381789ITERATION : 51, loss : 0.07803987419438811ITERATION : 52, loss : 0.0780398747772398ITERATION : 53, loss : 0.07803987513388422ITERATION : 54, loss : 0.07803987537457767ITERATION : 55, loss : 0.07803987537758661ITERATION : 56, loss : 0.07803987537758661ITERATION : 57, loss : 0.07803987537758661ITERATION : 58, loss : 0.07803987537758661ITERATION : 59, loss : 0.07803987537758661ITERATION : 60, loss : 0.07803987537758661ITERATION : 61, loss : 0.07803987537758661ITERATION : 62, loss : 0.07803987537758661ITERATION : 63, loss : 0.07803987537758661ITERATION : 64, loss : 0.07803987537758661ITERATION : 65, loss : 0.07803987537758661ITERATION : 66, loss : 0.07803987537758661ITERATION : 67, loss : 0.07803987537758661ITERATION : 68, loss : 0.07803987537758661ITERATION : 69, loss : 0.07803987537758661ITERATION : 70, loss : 0.07803987537758661ITERATION : 71, loss : 0.07803987537758661ITERATION : 72, loss : 0.07803987537758661ITERATION : 73, loss : 0.07803987537758661ITERATION : 74, loss : 0.07803987537758661ITERATION : 75, loss : 0.07803987537758661ITERATION : 76, loss : 0.07803987537758661ITERATION : 77, loss : 0.07803987537758661ITERATION : 78, loss : 0.07803987537758661ITERATION : 79, loss : 0.07803987537758661ITERATION : 80, loss : 0.07803987537758661ITERATION : 81, loss : 0.07803987537758661ITERATION : 82, loss : 0.07803987537758661ITERATION : 83, loss : 0.07803987537758661ITERATION : 84, loss : 0.07803987537758661ITERATION : 85, loss : 0.07803987537758661ITERATION : 86, loss : 0.07803987537758661ITERATION : 87, loss : 0.07803987537758661ITERATION : 88, loss : 0.07803987537758661ITERATION : 89, loss : 0.07803987537758661ITERATION : 90, loss : 0.07803987537758661ITERATION : 91, loss : 0.07803987537758661ITERATION : 92, loss : 0.07803987537758661ITERATION : 93, loss : 0.07803987537758661ITERATION : 94, loss : 0.07803987537758661ITERATION : 95, loss : 0.07803987537758661ITERATION : 96, loss : 0.07803987537758661ITERATION : 97, loss : 0.07803987537758661ITERATION : 98, loss : 0.07803987537758661ITERATION : 99, loss : 0.07803987537758661ITERATION : 100, loss : 0.07803987537758661
ITERATION : 1, loss : 0.11578865247728536ITERATION : 2, loss : 0.11476375465895598ITERATION : 3, loss : 0.1161358311535153ITERATION : 4, loss : 0.1176738382959057ITERATION : 5, loss : 0.11896837531292218ITERATION : 6, loss : 0.119965329399389ITERATION : 7, loss : 0.12070280250088687ITERATION : 8, loss : 0.12123697753537904ITERATION : 9, loss : 0.12161934988463682ITERATION : 10, loss : 0.12189115439728321ITERATION : 11, loss : 0.12208353667963537ITERATION : 12, loss : 0.12221933541474223ITERATION : 13, loss : 0.12231502297390509ITERATION : 14, loss : 0.12238236616240472ITERATION : 15, loss : 0.12242972326091929ITERATION : 16, loss : 0.1224630066352238ITERATION : 17, loss : 0.12248638946516888ITERATION : 18, loss : 0.12250281220739807ITERATION : 19, loss : 0.12251434438837358ITERATION : 20, loss : 0.12252244075980723ITERATION : 21, loss : 0.12252812402222982ITERATION : 22, loss : 0.12253211328397515ITERATION : 23, loss : 0.12253491344182336ITERATION : 24, loss : 0.12253687849185088ITERATION : 25, loss : 0.12253825754839986ITERATION : 26, loss : 0.12253922514980073ITERATION : 27, loss : 0.12253990422673984ITERATION : 28, loss : 0.12254038084353716ITERATION : 29, loss : 0.12254071546386222ITERATION : 30, loss : 0.12254095017805143ITERATION : 31, loss : 0.12254111503538649ITERATION : 32, loss : 0.12254123057611056ITERATION : 33, loss : 0.12254131164256689ITERATION : 34, loss : 0.12254136865323755ITERATION : 35, loss : 0.1225414084264465ITERATION : 36, loss : 0.12254143657986938ITERATION : 37, loss : 0.12254145603896507ITERATION : 38, loss : 0.12254146998576435ITERATION : 39, loss : 0.12254147949795212ITERATION : 40, loss : 0.12254148627430901ITERATION : 41, loss : 0.12254149098955389ITERATION : 42, loss : 0.122541493877322ITERATION : 43, loss : 0.12254149625246838ITERATION : 44, loss : 0.12254149806416255ITERATION : 45, loss : 0.12254149891527899ITERATION : 46, loss : 0.122541499829836ITERATION : 47, loss : 0.12254149986911313ITERATION : 48, loss : 0.12254149986911313ITERATION : 49, loss : 0.12254149986911313ITERATION : 50, loss : 0.12254149986911313ITERATION : 51, loss : 0.12254149986911313ITERATION : 52, loss : 0.12254149986911313ITERATION : 53, loss : 0.12254149986911313ITERATION : 54, loss : 0.12254149986911313ITERATION : 55, loss : 0.12254149986911313ITERATION : 56, loss : 0.12254149986911313ITERATION : 57, loss : 0.12254149986911313ITERATION : 58, loss : 0.12254149986911313ITERATION : 59, loss : 0.12254149986911313ITERATION : 60, loss : 0.12254149986911313ITERATION : 61, loss : 0.12254149986911313ITERATION : 62, loss : 0.12254149986911313ITERATION : 63, loss : 0.12254149986911313ITERATION : 64, loss : 0.12254149986911313ITERATION : 65, loss : 0.12254149986911313ITERATION : 66, loss : 0.12254149986911313ITERATION : 67, loss : 0.12254149986911313ITERATION : 68, loss : 0.12254149986911313ITERATION : 69, loss : 0.12254149986911313ITERATION : 70, loss : 0.12254149986911313ITERATION : 71, loss : 0.12254149986911313ITERATION : 72, loss : 0.12254149986911313ITERATION : 73, loss : 0.12254149986911313ITERATION : 74, loss : 0.12254149986911313ITERATION : 75, loss : 0.12254149986911313ITERATION : 76, loss : 0.12254149986911313ITERATION : 77, loss : 0.12254149986911313ITERATION : 78, loss : 0.12254149986911313ITERATION : 79, loss : 0.12254149986911313ITERATION : 80, loss : 0.12254149986911313ITERATION : 81, loss : 0.12254149986911313ITERATION : 82, loss : 0.12254149986911313ITERATION : 83, loss : 0.12254149986911313ITERATION : 84, loss : 0.12254149986911313ITERATION : 85, loss : 0.12254149986911313ITERATION : 86, loss : 0.12254149986911313ITERATION : 87, loss : 0.12254149986911313ITERATION : 88, loss : 0.12254149986911313ITERATION : 89, loss : 0.12254149986911313ITERATION : 90, loss : 0.12254149986911313ITERATION : 91, loss : 0.12254149986911313ITERATION : 92, loss : 0.12254149986911313ITERATION : 93, loss : 0.12254149986911313ITERATION : 94, loss : 0.12254149986911313ITERATION : 95, loss : 0.12254149986911313ITERATION : 96, loss : 0.12254149986911313ITERATION : 97, loss : 0.12254149986911313ITERATION : 98, loss : 0.12254149986911313ITERATION : 99, loss : 0.12254149986911313ITERATION : 100, loss : 0.12254149986911313
ITERATION : 1, loss : 0.09610320983591564ITERATION : 2, loss : 0.09412183801459667ITERATION : 3, loss : 0.09355672637469302ITERATION : 4, loss : 0.09331513731505788ITERATION : 5, loss : 0.09318797120754092ITERATION : 6, loss : 0.0931128371536705ITERATION : 7, loss : 0.09295287372411982ITERATION : 8, loss : 0.09280916429807143ITERATION : 9, loss : 0.09270936318953707ITERATION : 10, loss : 0.09263978669328662ITERATION : 11, loss : 0.09259115821490599ITERATION : 12, loss : 0.09255711045668381ITERATION : 13, loss : 0.09253324116916825ITERATION : 14, loss : 0.09251649201314167ITERATION : 15, loss : 0.09250473099585828ITERATION : 16, loss : 0.0924964684082642ITERATION : 17, loss : 0.09249066152650025ITERATION : 18, loss : 0.09248657921057453ITERATION : 19, loss : 0.09248370859569698ITERATION : 20, loss : 0.09248168995257838ITERATION : 21, loss : 0.09248027008693264ITERATION : 22, loss : 0.09247927132410597ITERATION : 23, loss : 0.09247856878111024ITERATION : 24, loss : 0.09247807451844196ITERATION : 25, loss : 0.09247772680888164ITERATION : 26, loss : 0.09247748223478038ITERATION : 27, loss : 0.09247731024138618ITERATION : 28, loss : 0.09247718918052404ITERATION : 29, loss : 0.09247710397779689ITERATION : 30, loss : 0.09247704401935064ITERATION : 31, loss : 0.09247700180590272ITERATION : 32, loss : 0.09247697209188849ITERATION : 33, loss : 0.09247695117105498ITERATION : 34, loss : 0.09247693648876874ITERATION : 35, loss : 0.09247692610281119ITERATION : 36, loss : 0.09247691883822283ITERATION : 37, loss : 0.09247691369671393ITERATION : 38, loss : 0.09247691002848371ITERATION : 39, loss : 0.09247690754900352ITERATION : 40, loss : 0.09247690571221202ITERATION : 41, loss : 0.09247690447502217ITERATION : 42, loss : 0.09247690363292253ITERATION : 43, loss : 0.09247690299019322ITERATION : 44, loss : 0.09247690261916985ITERATION : 45, loss : 0.09247690232968295ITERATION : 46, loss : 0.09247690225762571ITERATION : 47, loss : 0.09247690225627646ITERATION : 48, loss : 0.09247690225627646ITERATION : 49, loss : 0.09247690225627646ITERATION : 50, loss : 0.09247690225627646ITERATION : 51, loss : 0.09247690225627646ITERATION : 52, loss : 0.09247690225627646ITERATION : 53, loss : 0.09247690225627646ITERATION : 54, loss : 0.09247690225627646ITERATION : 55, loss : 0.09247690225627646ITERATION : 56, loss : 0.09247690225627646ITERATION : 57, loss : 0.09247690225627646ITERATION : 58, loss : 0.09247690225627646ITERATION : 59, loss : 0.09247690225627646ITERATION : 60, loss : 0.09247690225627646ITERATION : 61, loss : 0.09247690225627646ITERATION : 62, loss : 0.09247690225627646ITERATION : 63, loss : 0.09247690225627646ITERATION : 64, loss : 0.09247690225627646ITERATION : 65, loss : 0.09247690225627646ITERATION : 66, loss : 0.09247690225627646ITERATION : 67, loss : 0.09247690225627646ITERATION : 68, loss : 0.09247690225627646ITERATION : 69, loss : 0.09247690225627646ITERATION : 70, loss : 0.09247690225627646ITERATION : 71, loss : 0.09247690225627646ITERATION : 72, loss : 0.09247690225627646ITERATION : 73, loss : 0.09247690225627646ITERATION : 74, loss : 0.09247690225627646ITERATION : 75, loss : 0.09247690225627646ITERATION : 76, loss : 0.09247690225627646ITERATION : 77, loss : 0.09247690225627646ITERATION : 78, loss : 0.09247690225627646ITERATION : 79, loss : 0.09247690225627646ITERATION : 80, loss : 0.09247690225627646ITERATION : 81, loss : 0.09247690225627646ITERATION : 82, loss : 0.09247690225627646ITERATION : 83, loss : 0.09247690225627646ITERATION : 84, loss : 0.09247690225627646ITERATION : 85, loss : 0.09247690225627646ITERATION : 86, loss : 0.09247690225627646ITERATION : 87, loss : 0.09247690225627646ITERATION : 88, loss : 0.09247690225627646ITERATION : 89, loss : 0.09247690225627646ITERATION : 90, loss : 0.09247690225627646ITERATION : 91, loss : 0.09247690225627646ITERATION : 92, loss : 0.09247690225627646ITERATION : 93, loss : 0.09247690225627646ITERATION : 94, loss : 0.09247690225627646ITERATION : 95, loss : 0.09247690225627646ITERATION : 96, loss : 0.09247690225627646ITERATION : 97, loss : 0.09247690225627646ITERATION : 98, loss : 0.09247690225627646ITERATION : 99, loss : 0.09247690225627646ITERATION : 100, loss : 0.09247690225627646
ITERATION : 1, loss : 0.06579607045971364ITERATION : 2, loss : 0.07494187225073931ITERATION : 3, loss : 0.08231645054612577ITERATION : 4, loss : 0.08793306122313536ITERATION : 5, loss : 0.08959725153978892ITERATION : 6, loss : 0.08967249114687417ITERATION : 7, loss : 0.08972731443655113ITERATION : 8, loss : 0.08976698369991837ITERATION : 9, loss : 0.08979532158010493ITERATION : 10, loss : 0.08981530932640017ITERATION : 11, loss : 0.08982925593424014ITERATION : 12, loss : 0.08983890281537137ITERATION : 13, loss : 0.08984552974947121ITERATION : 14, loss : 0.0898500571846047ITERATION : 15, loss : 0.08985313644895374ITERATION : 16, loss : 0.08985522301084922ITERATION : 17, loss : 0.08985663238145067ITERATION : 18, loss : 0.08985758144628017ITERATION : 19, loss : 0.08985821882131484ITERATION : 20, loss : 0.08985864567398029ITERATION : 21, loss : 0.08985893082036187ITERATION : 22, loss : 0.08985912069812736ITERATION : 23, loss : 0.08985924678502602ITERATION : 24, loss : 0.08985933018580843ITERATION : 25, loss : 0.08985938519678481ITERATION : 26, loss : 0.08985942126200654ITERATION : 27, loss : 0.08985944484740398ITERATION : 28, loss : 0.08985946010670241ITERATION : 29, loss : 0.08985946994247539ITERATION : 30, loss : 0.0898594763241045ITERATION : 31, loss : 0.08985948043772142ITERATION : 32, loss : 0.08985948288562161ITERATION : 33, loss : 0.08985948450681681ITERATION : 34, loss : 0.08985948549397214ITERATION : 35, loss : 0.08985948598051328ITERATION : 36, loss : 0.08985948637125697ITERATION : 37, loss : 0.08985948637472156ITERATION : 38, loss : 0.08985948654488599ITERATION : 39, loss : 0.0898594864190733ITERATION : 40, loss : 0.0898594865179091ITERATION : 41, loss : 0.08985948635856766ITERATION : 42, loss : 0.08985948639989796ITERATION : 43, loss : 0.089859486303996ITERATION : 44, loss : 0.08985948634836641ITERATION : 45, loss : 0.08985948623845397ITERATION : 46, loss : 0.08985948635966363ITERATION : 47, loss : 0.08985948626317586ITERATION : 48, loss : 0.08985948623236778ITERATION : 49, loss : 0.0898594862404731ITERATION : 50, loss : 0.0898594862404731ITERATION : 51, loss : 0.0898594862404731ITERATION : 52, loss : 0.0898594862404731ITERATION : 53, loss : 0.0898594862404731ITERATION : 54, loss : 0.0898594862404731ITERATION : 55, loss : 0.0898594862404731ITERATION : 56, loss : 0.0898594862404731ITERATION : 57, loss : 0.0898594862404731ITERATION : 58, loss : 0.0898594862404731ITERATION : 59, loss : 0.0898594862404731ITERATION : 60, loss : 0.0898594862404731ITERATION : 61, loss : 0.0898594862404731ITERATION : 62, loss : 0.0898594862404731ITERATION : 63, loss : 0.0898594862404731ITERATION : 64, loss : 0.0898594862404731ITERATION : 65, loss : 0.0898594862404731ITERATION : 66, loss : 0.0898594862404731ITERATION : 67, loss : 0.0898594862404731ITERATION : 68, loss : 0.0898594862404731ITERATION : 69, loss : 0.0898594862404731ITERATION : 70, loss : 0.0898594862404731ITERATION : 71, loss : 0.0898594862404731ITERATION : 72, loss : 0.0898594862404731ITERATION : 73, loss : 0.0898594862404731ITERATION : 74, loss : 0.0898594862404731ITERATION : 75, loss : 0.0898594862404731ITERATION : 76, loss : 0.0898594862404731ITERATION : 77, loss : 0.0898594862404731ITERATION : 78, loss : 0.0898594862404731ITERATION : 79, loss : 0.0898594862404731ITERATION : 80, loss : 0.0898594862404731ITERATION : 81, loss : 0.0898594862404731ITERATION : 82, loss : 0.0898594862404731ITERATION : 83, loss : 0.0898594862404731ITERATION : 84, loss : 0.0898594862404731ITERATION : 85, loss : 0.0898594862404731ITERATION : 86, loss : 0.0898594862404731ITERATION : 87, loss : 0.0898594862404731ITERATION : 88, loss : 0.0898594862404731ITERATION : 89, loss : 0.0898594862404731ITERATION : 90, loss : 0.0898594862404731ITERATION : 91, loss : 0.0898594862404731ITERATION : 92, loss : 0.0898594862404731ITERATION : 93, loss : 0.0898594862404731ITERATION : 94, loss : 0.0898594862404731ITERATION : 95, loss : 0.0898594862404731ITERATION : 96, loss : 0.0898594862404731ITERATION : 97, loss : 0.0898594862404731ITERATION : 98, loss : 0.0898594862404731ITERATION : 99, loss : 0.0898594862404731ITERATION : 100, loss : 0.0898594862404731
ITERATION : 1, loss : 0.026534062019607223ITERATION : 2, loss : 0.03538320578515841ITERATION : 3, loss : 0.04060311950955089ITERATION : 4, loss : 0.041947468475787064ITERATION : 5, loss : 0.04281719597979929ITERATION : 6, loss : 0.04340448526943923ITERATION : 7, loss : 0.043809533499248286ITERATION : 8, loss : 0.04409193604450341ITERATION : 9, loss : 0.04428998950450846ITERATION : 10, loss : 0.04442935574992967ITERATION : 11, loss : 0.04452762409699771ITERATION : 12, loss : 0.04459700236266968ITERATION : 13, loss : 0.04464602503375685ITERATION : 14, loss : 0.04468068402957654ITERATION : 15, loss : 0.04470519759757896ITERATION : 16, loss : 0.04472254067594716ITERATION : 17, loss : 0.04473481289255109ITERATION : 18, loss : 0.04474349837457889ITERATION : 19, loss : 0.044749645806722534ITERATION : 20, loss : 0.044753997311361995ITERATION : 21, loss : 0.044757077824064725ITERATION : 22, loss : 0.04475925868532592ITERATION : 23, loss : 0.04476080260667927ITERATION : 24, loss : 0.0447618956450991ITERATION : 25, loss : 0.044762669425059305ITERATION : 26, loss : 0.044763217272462345ITERATION : 27, loss : 0.04476360509403463ITERATION : 28, loss : 0.044763879625025454ITERATION : 29, loss : 0.044764073911488185ITERATION : 30, loss : 0.0447642115583305ITERATION : 31, loss : 0.044764308956409475ITERATION : 32, loss : 0.04476437782903639ITERATION : 33, loss : 0.04476442659610957ITERATION : 34, loss : 0.04476446113232241ITERATION : 35, loss : 0.044764485628901965ITERATION : 36, loss : 0.04476450294383182ITERATION : 37, loss : 0.04476451527714282ITERATION : 38, loss : 0.044764523943437266ITERATION : 39, loss : 0.04476453014491075ITERATION : 40, loss : 0.04476453446152039ITERATION : 41, loss : 0.044764537603433116ITERATION : 42, loss : 0.04476453976618878ITERATION : 43, loss : 0.04476454132280427ITERATION : 44, loss : 0.04476454241190992ITERATION : 45, loss : 0.0447645432119344ITERATION : 46, loss : 0.044764543702012494ITERATION : 47, loss : 0.04476454415292654ITERATION : 48, loss : 0.04476454432966622ITERATION : 49, loss : 0.04476454459115045ITERATION : 50, loss : 0.044764544624109476ITERATION : 51, loss : 0.04476454473599747ITERATION : 52, loss : 0.044764544725911994ITERATION : 53, loss : 0.04476454473936688ITERATION : 54, loss : 0.04476454473936688ITERATION : 55, loss : 0.04476454473936688ITERATION : 56, loss : 0.04476454473936688ITERATION : 57, loss : 0.04476454473936688ITERATION : 58, loss : 0.04476454473936688ITERATION : 59, loss : 0.04476454473936688ITERATION : 60, loss : 0.04476454473936688ITERATION : 61, loss : 0.04476454473936688ITERATION : 62, loss : 0.04476454473936688ITERATION : 63, loss : 0.04476454473936688ITERATION : 64, loss : 0.04476454473936688ITERATION : 65, loss : 0.04476454473936688ITERATION : 66, loss : 0.04476454473936688ITERATION : 67, loss : 0.04476454473936688ITERATION : 68, loss : 0.04476454473936688ITERATION : 69, loss : 0.04476454473936688ITERATION : 70, loss : 0.04476454473936688ITERATION : 71, loss : 0.04476454473936688ITERATION : 72, loss : 0.04476454473936688ITERATION : 73, loss : 0.04476454473936688ITERATION : 74, loss : 0.04476454473936688ITERATION : 75, loss : 0.04476454473936688ITERATION : 76, loss : 0.04476454473936688ITERATION : 77, loss : 0.04476454473936688ITERATION : 78, loss : 0.04476454473936688ITERATION : 79, loss : 0.04476454473936688ITERATION : 80, loss : 0.04476454473936688ITERATION : 81, loss : 0.04476454473936688ITERATION : 82, loss : 0.04476454473936688ITERATION : 83, loss : 0.04476454473936688ITERATION : 84, loss : 0.04476454473936688ITERATION : 85, loss : 0.04476454473936688ITERATION : 86, loss : 0.04476454473936688ITERATION : 87, loss : 0.04476454473936688ITERATION : 88, loss : 0.04476454473936688ITERATION : 89, loss : 0.04476454473936688ITERATION : 90, loss : 0.04476454473936688ITERATION : 91, loss : 0.04476454473936688ITERATION : 92, loss : 0.04476454473936688ITERATION : 93, loss : 0.04476454473936688ITERATION : 94, loss : 0.04476454473936688ITERATION : 95, loss : 0.04476454473936688ITERATION : 96, loss : 0.04476454473936688ITERATION : 97, loss : 0.04476454473936688ITERATION : 98, loss : 0.04476454473936688ITERATION : 99, loss : 0.04476454473936688ITERATION : 100, loss : 0.04476454473936688
ITERATION : 1, loss : 0.06694831355582363ITERATION : 2, loss : 0.0713125769251967ITERATION : 3, loss : 0.0746921371866099ITERATION : 4, loss : 0.07703628133891643ITERATION : 5, loss : 0.07866376036843775ITERATION : 6, loss : 0.07979821070462363ITERATION : 7, loss : 0.08059061734644922ITERATION : 8, loss : 0.08114459392105917ITERATION : 9, loss : 0.0815319950621504ITERATION : 10, loss : 0.0818029105995324ITERATION : 11, loss : 0.08199234276185796ITERATION : 12, loss : 0.08212477595342325ITERATION : 13, loss : 0.08221734328883151ITERATION : 14, loss : 0.08228203342639694ITERATION : 15, loss : 0.08232723410277927ITERATION : 16, loss : 0.08235881211343649ITERATION : 17, loss : 0.08238087005821484ITERATION : 18, loss : 0.08239627609965836ITERATION : 19, loss : 0.08240703493955082ITERATION : 20, loss : 0.08241454797046142ITERATION : 21, loss : 0.08241979380680137ITERATION : 22, loss : 0.0824234563294679ITERATION : 23, loss : 0.08242601329914201ITERATION : 24, loss : 0.0824277982405327ITERATION : 25, loss : 0.08242904419900153ITERATION : 26, loss : 0.08242991388330406ITERATION : 27, loss : 0.08243052092783841ITERATION : 28, loss : 0.08243094451028367ITERATION : 29, loss : 0.08243124009983624ITERATION : 30, loss : 0.08243144641129777ITERATION : 31, loss : 0.08243159036484289ITERATION : 32, loss : 0.08243169089292406ITERATION : 33, loss : 0.08243176104617962ITERATION : 34, loss : 0.08243180992049004ITERATION : 35, loss : 0.08243184408767268ITERATION : 36, loss : 0.08243186790870809ITERATION : 37, loss : 0.08243188453348847ITERATION : 38, loss : 0.08243189606726067ITERATION : 39, loss : 0.0824319041235669ITERATION : 40, loss : 0.08243190967655982ITERATION : 41, loss : 0.08243191358197538ITERATION : 42, loss : 0.08243191626409752ITERATION : 43, loss : 0.0824319181628738ITERATION : 44, loss : 0.0824319194360969ITERATION : 45, loss : 0.08243192036551206ITERATION : 46, loss : 0.0824319209768358ITERATION : 47, loss : 0.08243192143929892ITERATION : 48, loss : 0.08243192165438702ITERATION : 49, loss : 0.08243192187595695ITERATION : 50, loss : 0.08243192206243431ITERATION : 51, loss : 0.08243192209707872ITERATION : 52, loss : 0.08243192220263676ITERATION : 53, loss : 0.0824319222022478ITERATION : 54, loss : 0.0824319222022478ITERATION : 55, loss : 0.0824319222022478ITERATION : 56, loss : 0.0824319222022478ITERATION : 57, loss : 0.0824319222022478ITERATION : 58, loss : 0.0824319222022478ITERATION : 59, loss : 0.0824319222022478ITERATION : 60, loss : 0.0824319222022478ITERATION : 61, loss : 0.0824319222022478ITERATION : 62, loss : 0.0824319222022478ITERATION : 63, loss : 0.0824319222022478ITERATION : 64, loss : 0.0824319222022478ITERATION : 65, loss : 0.0824319222022478ITERATION : 66, loss : 0.0824319222022478ITERATION : 67, loss : 0.0824319222022478ITERATION : 68, loss : 0.0824319222022478ITERATION : 69, loss : 0.0824319222022478ITERATION : 70, loss : 0.0824319222022478ITERATION : 71, loss : 0.0824319222022478ITERATION : 72, loss : 0.0824319222022478ITERATION : 73, loss : 0.0824319222022478ITERATION : 74, loss : 0.0824319222022478ITERATION : 75, loss : 0.0824319222022478ITERATION : 76, loss : 0.0824319222022478ITERATION : 77, loss : 0.0824319222022478ITERATION : 78, loss : 0.0824319222022478ITERATION : 79, loss : 0.0824319222022478ITERATION : 80, loss : 0.0824319222022478ITERATION : 81, loss : 0.0824319222022478ITERATION : 82, loss : 0.0824319222022478ITERATION : 83, loss : 0.0824319222022478ITERATION : 84, loss : 0.0824319222022478ITERATION : 85, loss : 0.0824319222022478ITERATION : 86, loss : 0.0824319222022478ITERATION : 87, loss : 0.0824319222022478ITERATION : 88, loss : 0.0824319222022478ITERATION : 89, loss : 0.0824319222022478ITERATION : 90, loss : 0.0824319222022478ITERATION : 91, loss : 0.0824319222022478ITERATION : 92, loss : 0.0824319222022478ITERATION : 93, loss : 0.0824319222022478ITERATION : 94, loss : 0.0824319222022478ITERATION : 95, loss : 0.0824319222022478ITERATION : 96, loss : 0.0824319222022478ITERATION : 97, loss : 0.0824319222022478ITERATION : 98, loss : 0.0824319222022478ITERATION : 99, loss : 0.0824319222022478ITERATION : 100, loss : 0.0824319222022478
ITERATION : 1, loss : 0.07325928411200877ITERATION : 2, loss : 0.06869277574906657ITERATION : 3, loss : 0.06676721506671905ITERATION : 4, loss : 0.06600782580467611ITERATION : 5, loss : 0.06570871076603009ITERATION : 6, loss : 0.06558998914374017ITERATION : 7, loss : 0.06554209794898087ITERATION : 8, loss : 0.06552204889362527ITERATION : 9, loss : 0.0655129892384769ITERATION : 10, loss : 0.06550834941132708ITERATION : 11, loss : 0.06550558737593475ITERATION : 12, loss : 0.06550372025840517ITERATION : 13, loss : 0.065502357560336ITERATION : 14, loss : 0.06550132936156412ITERATION : 15, loss : 0.06550054777174373ITERATION : 16, loss : 0.06549995600650335ITERATION : 17, loss : 0.06549951127259991ITERATION : 18, loss : 0.06549917994487374ITERATION : 19, loss : 0.06549893513911058ITERATION : 20, loss : 0.06549875570789569ITERATION : 21, loss : 0.06549862490774472ITERATION : 22, loss : 0.06549853006155222ITERATION : 23, loss : 0.06549846142987889ITERATION : 24, loss : 0.06549841202139303ITERATION : 25, loss : 0.06549837659284795ITERATION : 26, loss : 0.06549835122985023ITERATION : 27, loss : 0.06549833308830431ITERATION : 28, loss : 0.06549832013026098ITERATION : 29, loss : 0.06549831089378377ITERATION : 30, loss : 0.06549830430487728ITERATION : 31, loss : 0.06549829960530675ITERATION : 32, loss : 0.06549829627438801ITERATION : 33, loss : 0.06549829391160238ITERATION : 34, loss : 0.06549829220800658ITERATION : 35, loss : 0.06549829097030666ITERATION : 36, loss : 0.06549829014363974ITERATION : 37, loss : 0.06549828947045562ITERATION : 38, loss : 0.0654982890589441ITERATION : 39, loss : 0.06549828871489072ITERATION : 40, loss : 0.06549828853630117ITERATION : 41, loss : 0.06549828832719999ITERATION : 42, loss : 0.06549828821990884ITERATION : 43, loss : 0.06549828814806657ITERATION : 44, loss : 0.06549828806657565ITERATION : 45, loss : 0.06549828809088108ITERATION : 46, loss : 0.06549828802162877ITERATION : 47, loss : 0.06549828794173615ITERATION : 48, loss : 0.06549828799549322ITERATION : 49, loss : 0.0654982879955213ITERATION : 50, loss : 0.0654982879955213ITERATION : 51, loss : 0.0654982879955213ITERATION : 52, loss : 0.0654982879955213ITERATION : 53, loss : 0.0654982879955213ITERATION : 54, loss : 0.0654982879955213ITERATION : 55, loss : 0.0654982879955213ITERATION : 56, loss : 0.0654982879955213ITERATION : 57, loss : 0.0654982879955213ITERATION : 58, loss : 0.0654982879955213ITERATION : 59, loss : 0.0654982879955213ITERATION : 60, loss : 0.0654982879955213ITERATION : 61, loss : 0.0654982879955213ITERATION : 62, loss : 0.0654982879955213ITERATION : 63, loss : 0.0654982879955213ITERATION : 64, loss : 0.0654982879955213ITERATION : 65, loss : 0.0654982879955213ITERATION : 66, loss : 0.0654982879955213ITERATION : 67, loss : 0.0654982879955213ITERATION : 68, loss : 0.0654982879955213ITERATION : 69, loss : 0.0654982879955213ITERATION : 70, loss : 0.0654982879955213ITERATION : 71, loss : 0.0654982879955213ITERATION : 72, loss : 0.0654982879955213ITERATION : 73, loss : 0.0654982879955213ITERATION : 74, loss : 0.0654982879955213ITERATION : 75, loss : 0.0654982879955213ITERATION : 76, loss : 0.0654982879955213ITERATION : 77, loss : 0.0654982879955213ITERATION : 78, loss : 0.0654982879955213ITERATION : 79, loss : 0.0654982879955213ITERATION : 80, loss : 0.0654982879955213ITERATION : 81, loss : 0.0654982879955213ITERATION : 82, loss : 0.0654982879955213ITERATION : 83, loss : 0.0654982879955213ITERATION : 84, loss : 0.0654982879955213ITERATION : 85, loss : 0.0654982879955213ITERATION : 86, loss : 0.0654982879955213ITERATION : 87, loss : 0.0654982879955213ITERATION : 88, loss : 0.0654982879955213ITERATION : 89, loss : 0.0654982879955213ITERATION : 90, loss : 0.0654982879955213ITERATION : 91, loss : 0.0654982879955213ITERATION : 92, loss : 0.0654982879955213ITERATION : 93, loss : 0.0654982879955213ITERATION : 94, loss : 0.0654982879955213ITERATION : 95, loss : 0.0654982879955213ITERATION : 96, loss : 0.0654982879955213ITERATION : 97, loss : 0.0654982879955213ITERATION : 98, loss : 0.0654982879955213ITERATION : 99, loss : 0.0654982879955213ITERATION : 100, loss : 0.0654982879955213
gradient norm in None layer : 1.2419419724957854
gradient norm in None layer : 0.022682533485805734
gradient norm in None layer : 0.0182523501684627
gradient norm in None layer : 0.374770384482716
gradient norm in None layer : 0.015353721168865186
gradient norm in None layer : 0.016190671296817005
gradient norm in None layer : 0.1008181425314887
gradient norm in None layer : 0.003607462550637761
gradient norm in None layer : 0.0033884226377805795
gradient norm in None layer : 0.08267645550449809
gradient norm in None layer : 0.0031045869945488143
gradient norm in None layer : 0.002757787794535239
gradient norm in None layer : 0.033594888652668735
gradient norm in None layer : 0.0008292034610087845
gradient norm in None layer : 0.0007844347349963045
gradient norm in None layer : 0.02873709382481522
gradient norm in None layer : 0.0007920666984874218
gradient norm in None layer : 0.0008296662135644197
gradient norm in None layer : 0.033091715463087396
gradient norm in None layer : 0.0008833564540363796
gradient norm in None layer : 0.07204155170596745
gradient norm in None layer : 0.00258380690618867
gradient norm in None layer : 0.002418789195124799
gradient norm in None layer : 0.06851062682699818
gradient norm in None layer : 0.004122703255452122
gradient norm in None layer : 0.004860412598981406
gradient norm in None layer : 0.10998388119607246
gradient norm in None layer : 0.002197503088518763
gradient norm in None layer : 0.26011259796878505
gradient norm in None layer : 0.017099647389538924
gradient norm in None layer : 0.014412952005142294
gradient norm in None layer : 0.25318046769027175
gradient norm in None layer : 0.07287927160071121
gradient norm in None layer : 0.08674931848939411
gradient norm in None layer : 0.04494825477343419
gradient norm in None layer : 0.015024025397077412
Total gradient norm: 1.368828922775328
invariance loss : 40.88702362763361, avg_den : 0.0415802001953125, density loss : -0.049611288273938785, mse loss : 0.07624055340146978, solver time : 87.7701575756073 sec , total loss : 0.11707796574082946, running loss : 0.11707796574082946
Epoch 0/10 , batch 2/12500 
ITERATION : 1, loss : 0.03492878935279682ITERATION : 2, loss : 0.040620163925400715ITERATION : 3, loss : 0.045717453002005616ITERATION : 4, loss : 0.0497439065518426ITERATION : 5, loss : 0.052805443108175046ITERATION : 6, loss : 0.05507603241638214ITERATION : 7, loss : 0.05672932287054246ITERATION : 8, loss : 0.05791658827460161ITERATION : 9, loss : 0.058760347334217485ITERATION : 10, loss : 0.05935531288956315ITERATION : 11, loss : 0.05977240104945549ITERATION : 12, loss : 0.060063523660840765ITERATION : 13, loss : 0.06026606905999449ITERATION : 14, loss : 0.06040665112860789ITERATION : 15, loss : 0.06050405259610485ITERATION : 16, loss : 0.06057144746666188ITERATION : 17, loss : 0.06061803392170396ITERATION : 18, loss : 0.06065021253516187ITERATION : 19, loss : 0.060672426597235825ITERATION : 20, loss : 0.06068775546238815ITERATION : 21, loss : 0.0606983295818953ITERATION : 22, loss : 0.060705621771843285ITERATION : 23, loss : 0.060710649498975255ITERATION : 24, loss : 0.06071411555012059ITERATION : 25, loss : 0.060716504501140627ITERATION : 26, loss : 0.06071815090941543ITERATION : 27, loss : 0.06071928533166781ITERATION : 28, loss : 0.06072006705511584ITERATION : 29, loss : 0.06072060557605777ITERATION : 30, loss : 0.0607209765292686ITERATION : 31, loss : 0.0607212320419452ITERATION : 32, loss : 0.0607214080385591ITERATION : 33, loss : 0.060721529253429135ITERATION : 34, loss : 0.06072161274873528ITERATION : 35, loss : 0.0607216701967058ITERATION : 36, loss : 0.06072170980615323ITERATION : 37, loss : 0.06072173705638418ITERATION : 38, loss : 0.060721755874934995ITERATION : 39, loss : 0.06072176878730997ITERATION : 40, loss : 0.060721777722663225ITERATION : 41, loss : 0.060721783957322154ITERATION : 42, loss : 0.060721788084330625ITERATION : 43, loss : 0.06072179087993668ITERATION : 44, loss : 0.060721792919939684ITERATION : 45, loss : 0.060721794205262254ITERATION : 46, loss : 0.06072179503379473ITERATION : 47, loss : 0.06072179573425945ITERATION : 48, loss : 0.060721796041110304ITERATION : 49, loss : 0.0607217963938621ITERATION : 50, loss : 0.06072179662362576ITERATION : 51, loss : 0.06072179663982049ITERATION : 52, loss : 0.06072179680447388ITERATION : 53, loss : 0.06072179680405029ITERATION : 54, loss : 0.06072179680405029ITERATION : 55, loss : 0.06072179680405029ITERATION : 56, loss : 0.06072179680405029ITERATION : 57, loss : 0.06072179680405029ITERATION : 58, loss : 0.06072179680405029ITERATION : 59, loss : 0.06072179680405029ITERATION : 60, loss : 0.06072179680405029ITERATION : 61, loss : 0.06072179680405029ITERATION : 62, loss : 0.06072179680405029ITERATION : 63, loss : 0.06072179680405029ITERATION : 64, loss : 0.06072179680405029ITERATION : 65, loss : 0.06072179680405029ITERATION : 66, loss : 0.06072179680405029ITERATION : 67, loss : 0.06072179680405029ITERATION : 68, loss : 0.06072179680405029ITERATION : 69, loss : 0.06072179680405029ITERATION : 70, loss : 0.06072179680405029ITERATION : 71, loss : 0.06072179680405029ITERATION : 72, loss : 0.06072179680405029ITERATION : 73, loss : 0.06072179680405029ITERATION : 74, loss : 0.06072179680405029ITERATION : 75, loss : 0.06072179680405029ITERATION : 76, loss : 0.06072179680405029ITERATION : 77, loss : 0.06072179680405029ITERATION : 78, loss : 0.06072179680405029ITERATION : 79, loss : 0.06072179680405029ITERATION : 80, loss : 0.06072179680405029ITERATION : 81, loss : 0.06072179680405029ITERATION : 82, loss : 0.06072179680405029ITERATION : 83, loss : 0.06072179680405029ITERATION : 84, loss : 0.06072179680405029ITERATION : 85, loss : 0.06072179680405029ITERATION : 86, loss : 0.06072179680405029ITERATION : 87, loss : 0.06072179680405029ITERATION : 88, loss : 0.06072179680405029ITERATION : 89, loss : 0.06072179680405029ITERATION : 90, loss : 0.06072179680405029ITERATION : 91, loss : 0.06072179680405029ITERATION : 92, loss : 0.06072179680405029ITERATION : 93, loss : 0.06072179680405029ITERATION : 94, loss : 0.06072179680405029ITERATION : 95, loss : 0.06072179680405029ITERATION : 96, loss : 0.06072179680405029ITERATION : 97, loss : 0.06072179680405029ITERATION : 98, loss : 0.06072179680405029ITERATION : 99, loss : 0.06072179680405029ITERATION : 100, loss : 0.06072179680405029
ITERATION : 1, loss : 0.058138541581774165ITERATION : 2, loss : 0.0747377160469125ITERATION : 3, loss : 0.0872382545785983ITERATION : 4, loss : 0.09582523474244321ITERATION : 5, loss : 0.10173520624231087ITERATION : 6, loss : 0.1058458220854024ITERATION : 7, loss : 0.1087293735532734ITERATION : 8, loss : 0.11076431697653802ITERATION : 9, loss : 0.1122063309112614ITERATION : 10, loss : 0.11323108272693966ITERATION : 11, loss : 0.113960734584473ITERATION : 12, loss : 0.11448096515800507ITERATION : 13, loss : 0.11485222354080442ITERATION : 14, loss : 0.11511733625426665ITERATION : 15, loss : 0.11530673216452934ITERATION : 16, loss : 0.11544207531987685ITERATION : 17, loss : 0.11553881085444251ITERATION : 18, loss : 0.11560796074923008ITERATION : 19, loss : 0.11565739548494637ITERATION : 20, loss : 0.11569273787713893ITERATION : 21, loss : 0.11571800589855517ITERATION : 22, loss : 0.11573607174732545ITERATION : 23, loss : 0.11574898831441975ITERATION : 24, loss : 0.11575822327550246ITERATION : 25, loss : 0.11576482606961763ITERATION : 26, loss : 0.11576954685586077ITERATION : 27, loss : 0.11577292203184972ITERATION : 28, loss : 0.11577533511047096ITERATION : 29, loss : 0.11577706040219515ITERATION : 30, loss : 0.11577829385932864ITERATION : 31, loss : 0.11577917576056843ITERATION : 32, loss : 0.11577980623959912ITERATION : 33, loss : 0.11578025703992616ITERATION : 34, loss : 0.11578057926550758ITERATION : 35, loss : 0.11578080966179781ITERATION : 36, loss : 0.11578097437740847ITERATION : 37, loss : 0.11578109212994239ITERATION : 38, loss : 0.11578117632658305ITERATION : 39, loss : 0.1157812365576732ITERATION : 40, loss : 0.11578127959539783ITERATION : 41, loss : 0.11578131038532152ITERATION : 42, loss : 0.11578133238260606ITERATION : 43, loss : 0.11578134812837745ITERATION : 44, loss : 0.11578135938004165ITERATION : 45, loss : 0.11578136742138172ITERATION : 46, loss : 0.11578137317990914ITERATION : 47, loss : 0.11578137728204478ITERATION : 48, loss : 0.1157813802327535ITERATION : 49, loss : 0.11578138231400756ITERATION : 50, loss : 0.11578138381851023ITERATION : 51, loss : 0.11578138483199897ITERATION : 52, loss : 0.11578138564480082ITERATION : 53, loss : 0.11578138616216885ITERATION : 54, loss : 0.1157813865843918ITERATION : 55, loss : 0.1157813868336202ITERATION : 56, loss : 0.11578138705929017ITERATION : 57, loss : 0.11578138715125746ITERATION : 58, loss : 0.11578138726248247ITERATION : 59, loss : 0.11578138728562827ITERATION : 60, loss : 0.11578138728562827ITERATION : 61, loss : 0.11578138728562827ITERATION : 62, loss : 0.11578138728562827ITERATION : 63, loss : 0.11578138728562827ITERATION : 64, loss : 0.11578138728562827ITERATION : 65, loss : 0.11578138728562827ITERATION : 66, loss : 0.11578138728562827ITERATION : 67, loss : 0.11578138728562827ITERATION : 68, loss : 0.11578138728562827ITERATION : 69, loss : 0.11578138728562827ITERATION : 70, loss : 0.11578138728562827ITERATION : 71, loss : 0.11578138728562827ITERATION : 72, loss : 0.11578138728562827ITERATION : 73, loss : 0.11578138728562827ITERATION : 74, loss : 0.11578138728562827ITERATION : 75, loss : 0.11578138728562827ITERATION : 76, loss : 0.11578138728562827ITERATION : 77, loss : 0.11578138728562827ITERATION : 78, loss : 0.11578138728562827ITERATION : 79, loss : 0.11578138728562827ITERATION : 80, loss : 0.11578138728562827ITERATION : 81, loss : 0.11578138728562827ITERATION : 82, loss : 0.11578138728562827ITERATION : 83, loss : 0.11578138728562827ITERATION : 84, loss : 0.11578138728562827ITERATION : 85, loss : 0.11578138728562827ITERATION : 86, loss : 0.11578138728562827ITERATION : 87, loss : 0.11578138728562827ITERATION : 88, loss : 0.11578138728562827ITERATION : 89, loss : 0.11578138728562827ITERATION : 90, loss : 0.11578138728562827ITERATION : 91, loss : 0.11578138728562827ITERATION : 92, loss : 0.11578138728562827ITERATION : 93, loss : 0.11578138728562827ITERATION : 94, loss : 0.11578138728562827ITERATION : 95, loss : 0.11578138728562827ITERATION : 96, loss : 0.11578138728562827ITERATION : 97, loss : 0.11578138728562827ITERATION : 98, loss : 0.11578138728562827ITERATION : 99, loss : 0.11578138728562827ITERATION : 100, loss : 0.11578138728562827
ITERATION : 1, loss : 0.1622178965483697ITERATION : 2, loss : 0.2074551859293786ITERATION : 3, loss : 0.2363624710179739ITERATION : 4, loss : 0.2543507557884399ITERATION : 5, loss : 0.2657087939441089ITERATION : 6, loss : 0.27298628913603823ITERATION : 7, loss : 0.2777011881176097ITERATION : 8, loss : 0.28078018704050467ITERATION : 9, loss : 0.28280196685005127ITERATION : 10, loss : 0.284134395795919ITERATION : 11, loss : 0.2850145117668993ITERATION : 12, loss : 0.28559657777416975ITERATION : 13, loss : 0.28598170354991387ITERATION : 14, loss : 0.28623648671410384ITERATION : 15, loss : 0.2864049365327179ITERATION : 16, loss : 0.286516194668335ITERATION : 17, loss : 0.28658957962250553ITERATION : 18, loss : 0.2866379033861787ITERATION : 19, loss : 0.2866696619123312ITERATION : 20, loss : 0.2866904862051459ITERATION : 21, loss : 0.28670410516393346ITERATION : 22, loss : 0.28671298536448275ITERATION : 23, loss : 0.2867187560034027ITERATION : 24, loss : 0.2867224912793821ITERATION : 25, loss : 0.28672489831320985ITERATION : 26, loss : 0.28672644135040437ITERATION : 27, loss : 0.28672742448027866ITERATION : 28, loss : 0.28672804636847804ITERATION : 29, loss : 0.2867284363737893ITERATION : 30, loss : 0.28672867839877736ITERATION : 31, loss : 0.28672882660203425ITERATION : 32, loss : 0.286728915878023ITERATION : 33, loss : 0.28672896842130957ITERATION : 34, loss : 0.2867289984177311ITERATION : 35, loss : 0.28672901474186774ITERATION : 36, loss : 0.28672902306461356ITERATION : 37, loss : 0.2867290266452367ITERATION : 38, loss : 0.28672902770050257ITERATION : 39, loss : 0.28672902740110023ITERATION : 40, loss : 0.28672902644731607ITERATION : 41, loss : 0.28672902538107414ITERATION : 42, loss : 0.28672902424805ITERATION : 43, loss : 0.28672902327036ITERATION : 44, loss : 0.2867290224568229ITERATION : 45, loss : 0.2867290217492204ITERATION : 46, loss : 0.28672902119332927ITERATION : 47, loss : 0.28672902071842704ITERATION : 48, loss : 0.28672902035063375ITERATION : 49, loss : 0.2867290201033677ITERATION : 50, loss : 0.28672901986669336ITERATION : 51, loss : 0.2867290197039209ITERATION : 52, loss : 0.2867290195872946ITERATION : 53, loss : 0.28672901947991186ITERATION : 54, loss : 0.2867290194406472ITERATION : 55, loss : 0.286729019439517ITERATION : 56, loss : 0.286729019439517ITERATION : 57, loss : 0.286729019439517ITERATION : 58, loss : 0.286729019439517ITERATION : 59, loss : 0.286729019439517ITERATION : 60, loss : 0.286729019439517ITERATION : 61, loss : 0.286729019439517ITERATION : 62, loss : 0.286729019439517ITERATION : 63, loss : 0.286729019439517ITERATION : 64, loss : 0.286729019439517ITERATION : 65, loss : 0.286729019439517ITERATION : 66, loss : 0.286729019439517ITERATION : 67, loss : 0.286729019439517ITERATION : 68, loss : 0.286729019439517ITERATION : 69, loss : 0.286729019439517ITERATION : 70, loss : 0.286729019439517ITERATION : 71, loss : 0.286729019439517ITERATION : 72, loss : 0.286729019439517ITERATION : 73, loss : 0.286729019439517ITERATION : 74, loss : 0.286729019439517ITERATION : 75, loss : 0.286729019439517ITERATION : 76, loss : 0.286729019439517ITERATION : 77, loss : 0.286729019439517ITERATION : 78, loss : 0.286729019439517ITERATION : 79, loss : 0.286729019439517ITERATION : 80, loss : 0.286729019439517ITERATION : 81, loss : 0.286729019439517ITERATION : 82, loss : 0.286729019439517ITERATION : 83, loss : 0.286729019439517ITERATION : 84, loss : 0.286729019439517ITERATION : 85, loss : 0.286729019439517ITERATION : 86, loss : 0.286729019439517ITERATION : 87, loss : 0.286729019439517ITERATION : 88, loss : 0.286729019439517ITERATION : 89, loss : 0.286729019439517ITERATION : 90, loss : 0.286729019439517ITERATION : 91, loss : 0.286729019439517ITERATION : 92, loss : 0.286729019439517ITERATION : 93, loss : 0.286729019439517ITERATION : 94, loss : 0.286729019439517ITERATION : 95, loss : 0.286729019439517ITERATION : 96, loss : 0.286729019439517ITERATION : 97, loss : 0.286729019439517ITERATION : 98, loss : 0.286729019439517ITERATION : 99, loss : 0.286729019439517ITERATION : 100, loss : 0.286729019439517
ITERATION : 1, loss : 0.03879192510209712ITERATION : 2, loss : 0.048409522534016355ITERATION : 3, loss : 0.052289928762203036ITERATION : 4, loss : 0.05455494302942145ITERATION : 5, loss : 0.05598924053805827ITERATION : 6, loss : 0.0569500688201738ITERATION : 7, loss : 0.057616399334458016ITERATION : 8, loss : 0.05808799009823924ITERATION : 9, loss : 0.05842573763766371ITERATION : 10, loss : 0.05866933340685373ITERATION : 11, loss : 0.05884577630511423ITERATION : 12, loss : 0.05897392317249198ITERATION : 13, loss : 0.05906715731778248ITERATION : 14, loss : 0.059135071204239505ITERATION : 15, loss : 0.05918458236356924ITERATION : 16, loss : 0.05922069903740925ITERATION : 17, loss : 0.05924705661346696ITERATION : 18, loss : 0.05926629863231312ITERATION : 19, loss : 0.05928034961050179ITERATION : 20, loss : 0.05929061217223351ITERATION : 21, loss : 0.05929810895975567ITERATION : 22, loss : 0.059303586106676104ITERATION : 23, loss : 0.05930758815535739ITERATION : 24, loss : 0.05931051263592981ITERATION : 25, loss : 0.059312649907301986ITERATION : 26, loss : 0.05931421198600863ITERATION : 27, loss : 0.059315353764285955ITERATION : 28, loss : 0.05931618834935608ITERATION : 29, loss : 0.05931679840547212ITERATION : 30, loss : 0.059317244382003344ITERATION : 31, loss : 0.059317570447297664ITERATION : 32, loss : 0.059317808819462116ITERATION : 33, loss : 0.05931798311108465ITERATION : 34, loss : 0.05931811057832549ITERATION : 35, loss : 0.059318203757082515ITERATION : 36, loss : 0.05931827188091518ITERATION : 37, loss : 0.05931832166888443ITERATION : 38, loss : 0.05931835808528353ITERATION : 39, loss : 0.059318384692751726ITERATION : 40, loss : 0.059318404169338ITERATION : 41, loss : 0.05931841839687367ITERATION : 42, loss : 0.05931842882445049ITERATION : 43, loss : 0.05931843642820121ITERATION : 44, loss : 0.059318442012460865ITERATION : 45, loss : 0.059318446066810325ITERATION : 46, loss : 0.05931844905674693ITERATION : 47, loss : 0.059318451218317754ITERATION : 48, loss : 0.05931845282384552ITERATION : 49, loss : 0.05931845397285401ITERATION : 50, loss : 0.05931845483094117ITERATION : 51, loss : 0.05931845543222133ITERATION : 52, loss : 0.05931845590716458ITERATION : 53, loss : 0.059318456227803615ITERATION : 54, loss : 0.05931845647914804ITERATION : 55, loss : 0.05931845664212833ITERATION : 56, loss : 0.059318456791336446ITERATION : 57, loss : 0.05931845686044651ITERATION : 58, loss : 0.059318456940897456ITERATION : 59, loss : 0.059318456977975255ITERATION : 60, loss : 0.05931845702672869ITERATION : 61, loss : 0.05931845702575284ITERATION : 62, loss : 0.05931845703383158ITERATION : 63, loss : 0.05931845703767771ITERATION : 64, loss : 0.05931845703767771ITERATION : 65, loss : 0.05931845703767771ITERATION : 66, loss : 0.05931845703767771ITERATION : 67, loss : 0.05931845703767771ITERATION : 68, loss : 0.05931845703767771ITERATION : 69, loss : 0.05931845703767771ITERATION : 70, loss : 0.05931845703767771ITERATION : 71, loss : 0.05931845703767771ITERATION : 72, loss : 0.05931845703767771ITERATION : 73, loss : 0.05931845703767771ITERATION : 74, loss : 0.05931845703767771ITERATION : 75, loss : 0.05931845703767771ITERATION : 76, loss : 0.05931845703767771ITERATION : 77, loss : 0.05931845703767771ITERATION : 78, loss : 0.05931845703767771ITERATION : 79, loss : 0.05931845703767771ITERATION : 80, loss : 0.05931845703767771ITERATION : 81, loss : 0.05931845703767771ITERATION : 82, loss : 0.05931845703767771ITERATION : 83, loss : 0.05931845703767771ITERATION : 84, loss : 0.05931845703767771ITERATION : 85, loss : 0.05931845703767771ITERATION : 86, loss : 0.05931845703767771ITERATION : 87, loss : 0.05931845703767771ITERATION : 88, loss : 0.05931845703767771ITERATION : 89, loss : 0.05931845703767771ITERATION : 90, loss : 0.05931845703767771ITERATION : 91, loss : 0.05931845703767771ITERATION : 92, loss : 0.05931845703767771ITERATION : 93, loss : 0.05931845703767771ITERATION : 94, loss : 0.05931845703767771ITERATION : 95, loss : 0.05931845703767771ITERATION : 96, loss : 0.05931845703767771ITERATION : 97, loss : 0.05931845703767771ITERATION : 98, loss : 0.05931845703767771ITERATION : 99, loss : 0.05931845703767771ITERATION : 100, loss : 0.05931845703767771
ITERATION : 1, loss : 0.07207870751185817ITERATION : 2, loss : 0.09926883739733267ITERATION : 3, loss : 0.11390757321070015ITERATION : 4, loss : 0.12450619141471454ITERATION : 5, loss : 0.13144860270880876ITERATION : 6, loss : 0.1360750486112302ITERATION : 7, loss : 0.13919138395573977ITERATION : 8, loss : 0.1413046637620795ITERATION : 9, loss : 0.14274366416680098ITERATION : 10, loss : 0.14372592359213313ITERATION : 11, loss : 0.14439732873230646ITERATION : 12, loss : 0.1448565709958519ITERATION : 13, loss : 0.14517077943613185ITERATION : 14, loss : 0.14538576392591251ITERATION : 15, loss : 0.14553284325392854ITERATION : 16, loss : 0.145633449024767ITERATION : 17, loss : 0.14570225243549775ITERATION : 18, loss : 0.14574929732144018ITERATION : 19, loss : 0.14578145875014492ITERATION : 20, loss : 0.14580344167892417ITERATION : 21, loss : 0.1458184652099341ITERATION : 22, loss : 0.1458287312090567ITERATION : 23, loss : 0.1458357454942682ITERATION : 24, loss : 0.1458405375344272ITERATION : 25, loss : 0.14584381107802485ITERATION : 26, loss : 0.14584604716525096ITERATION : 27, loss : 0.14584757447219948ITERATION : 28, loss : 0.14584861760669027ITERATION : 29, loss : 0.1458493299692648ITERATION : 30, loss : 0.14584981647137693ITERATION : 31, loss : 0.14585014866441418ITERATION : 32, loss : 0.1458503754916589ITERATION : 33, loss : 0.14585053037663492ITERATION : 34, loss : 0.14585063613557311ITERATION : 35, loss : 0.145850708358918ITERATION : 36, loss : 0.1458507576680584ITERATION : 37, loss : 0.1458507913463513ITERATION : 38, loss : 0.1458508143405298ITERATION : 39, loss : 0.14585083003922125ITERATION : 40, loss : 0.14585084075227975ITERATION : 41, loss : 0.14585084806361884ITERATION : 42, loss : 0.1458508530347136ITERATION : 43, loss : 0.14585085644471077ITERATION : 44, loss : 0.14585085878989246ITERATION : 45, loss : 0.1458508603808022ITERATION : 46, loss : 0.14585086144384823ITERATION : 47, loss : 0.14585086216840135ITERATION : 48, loss : 0.1458508626372539ITERATION : 49, loss : 0.14585086296766253ITERATION : 50, loss : 0.145850863191829ITERATION : 51, loss : 0.1458508633524807ITERATION : 52, loss : 0.14585086344119758ITERATION : 53, loss : 0.14585086349735624ITERATION : 54, loss : 0.14585086355244806ITERATION : 55, loss : 0.14585086358443378ITERATION : 56, loss : 0.145850863607313ITERATION : 57, loss : 0.14585086361454258ITERATION : 58, loss : 0.14585086361454164ITERATION : 59, loss : 0.14585086361454164ITERATION : 60, loss : 0.14585086361454164ITERATION : 61, loss : 0.14585086361454164ITERATION : 62, loss : 0.14585086361454164ITERATION : 63, loss : 0.14585086361454164ITERATION : 64, loss : 0.14585086361454164ITERATION : 65, loss : 0.14585086361454164ITERATION : 66, loss : 0.14585086361454164ITERATION : 67, loss : 0.14585086361454164ITERATION : 68, loss : 0.14585086361454164ITERATION : 69, loss : 0.14585086361454164ITERATION : 70, loss : 0.14585086361454164ITERATION : 71, loss : 0.14585086361454164ITERATION : 72, loss : 0.14585086361454164ITERATION : 73, loss : 0.14585086361454164ITERATION : 74, loss : 0.14585086361454164ITERATION : 75, loss : 0.14585086361454164ITERATION : 76, loss : 0.14585086361454164ITERATION : 77, loss : 0.14585086361454164ITERATION : 78, loss : 0.14585086361454164ITERATION : 79, loss : 0.14585086361454164ITERATION : 80, loss : 0.14585086361454164ITERATION : 81, loss : 0.14585086361454164ITERATION : 82, loss : 0.14585086361454164ITERATION : 83, loss : 0.14585086361454164ITERATION : 84, loss : 0.14585086361454164ITERATION : 85, loss : 0.14585086361454164ITERATION : 86, loss : 0.14585086361454164ITERATION : 87, loss : 0.14585086361454164ITERATION : 88, loss : 0.14585086361454164ITERATION : 89, loss : 0.14585086361454164ITERATION : 90, loss : 0.14585086361454164ITERATION : 91, loss : 0.14585086361454164ITERATION : 92, loss : 0.14585086361454164ITERATION : 93, loss : 0.14585086361454164ITERATION : 94, loss : 0.14585086361454164ITERATION : 95, loss : 0.14585086361454164ITERATION : 96, loss : 0.14585086361454164ITERATION : 97, loss : 0.14585086361454164ITERATION : 98, loss : 0.14585086361454164ITERATION : 99, loss : 0.14585086361454164ITERATION : 100, loss : 0.14585086361454164
ITERATION : 1, loss : 0.029656244178597465ITERATION : 2, loss : 0.038041696044475014ITERATION : 3, loss : 0.043690557599138154ITERATION : 4, loss : 0.04761517400483025ITERATION : 5, loss : 0.05037368687072846ITERATION : 6, loss : 0.05232040925798216ITERATION : 7, loss : 0.05369583357478498ITERATION : 8, loss : 0.054667653937222324ITERATION : 9, loss : 0.05535403356318417ITERATION : 10, loss : 0.05583855204684583ITERATION : 11, loss : 0.05618038769573042ITERATION : 12, loss : 0.056421434772512706ITERATION : 13, loss : 0.056591333186344184ITERATION : 14, loss : 0.05671103676165596ITERATION : 15, loss : 0.05679534720650079ITERATION : 16, loss : 0.0568547128235758ITERATION : 17, loss : 0.05689650439497215ITERATION : 18, loss : 0.056925918977388065ITERATION : 19, loss : 0.05694661879492096ITERATION : 20, loss : 0.056961183987135684ITERATION : 21, loss : 0.056971431496814724ITERATION : 22, loss : 0.056978640659984446ITERATION : 23, loss : 0.05698371205916068ITERATION : 24, loss : 0.05698727925066311ITERATION : 25, loss : 0.05698978842159601ITERATION : 26, loss : 0.05699155313700904ITERATION : 27, loss : 0.056992794328454166ITERATION : 28, loss : 0.05699366724480388ITERATION : 29, loss : 0.05699428114629316ITERATION : 30, loss : 0.05699471286729252ITERATION : 31, loss : 0.056995016446145096ITERATION : 32, loss : 0.05699522994147192ITERATION : 33, loss : 0.056995380058456835ITERATION : 34, loss : 0.05699548562069296ITERATION : 35, loss : 0.05699555984853583ITERATION : 36, loss : 0.05699561203746236ITERATION : 37, loss : 0.0569956487389063ITERATION : 38, loss : 0.05699567456742535ITERATION : 39, loss : 0.05699569269810737ITERATION : 40, loss : 0.05699570540785004ITERATION : 41, loss : 0.05699571440520665ITERATION : 42, loss : 0.056995720697586ITERATION : 43, loss : 0.0569957251385568ITERATION : 44, loss : 0.05699572826784709ITERATION : 45, loss : 0.056995730456366446ITERATION : 46, loss : 0.05699573196894799ITERATION : 47, loss : 0.05699573301735436ITERATION : 48, loss : 0.05699573376442923ITERATION : 49, loss : 0.05699573429320461ITERATION : 50, loss : 0.056995734693826204ITERATION : 51, loss : 0.05699573494558711ITERATION : 52, loss : 0.0569957351476548ITERATION : 53, loss : 0.05699573529551044ITERATION : 54, loss : 0.05699573537019407ITERATION : 55, loss : 0.056995735427468594ITERATION : 56, loss : 0.05699573543828091ITERATION : 57, loss : 0.056995735440820164ITERATION : 58, loss : 0.056995735440820164ITERATION : 59, loss : 0.056995735440820164ITERATION : 60, loss : 0.056995735440820164ITERATION : 61, loss : 0.056995735440820164ITERATION : 62, loss : 0.056995735440820164ITERATION : 63, loss : 0.056995735440820164ITERATION : 64, loss : 0.056995735440820164ITERATION : 65, loss : 0.056995735440820164ITERATION : 66, loss : 0.056995735440820164ITERATION : 67, loss : 0.056995735440820164ITERATION : 68, loss : 0.056995735440820164ITERATION : 69, loss : 0.056995735440820164ITERATION : 70, loss : 0.056995735440820164ITERATION : 71, loss : 0.056995735440820164ITERATION : 72, loss : 0.056995735440820164ITERATION : 73, loss : 0.056995735440820164ITERATION : 74, loss : 0.056995735440820164ITERATION : 75, loss : 0.056995735440820164ITERATION : 76, loss : 0.056995735440820164ITERATION : 77, loss : 0.056995735440820164ITERATION : 78, loss : 0.056995735440820164ITERATION : 79, loss : 0.056995735440820164ITERATION : 80, loss : 0.056995735440820164ITERATION : 81, loss : 0.056995735440820164ITERATION : 82, loss : 0.056995735440820164ITERATION : 83, loss : 0.056995735440820164ITERATION : 84, loss : 0.056995735440820164ITERATION : 85, loss : 0.056995735440820164ITERATION : 86, loss : 0.056995735440820164ITERATION : 87, loss : 0.056995735440820164ITERATION : 88, loss : 0.056995735440820164ITERATION : 89, loss : 0.056995735440820164ITERATION : 90, loss : 0.056995735440820164ITERATION : 91, loss : 0.056995735440820164ITERATION : 92, loss : 0.056995735440820164ITERATION : 93, loss : 0.056995735440820164ITERATION : 94, loss : 0.056995735440820164ITERATION : 95, loss : 0.056995735440820164ITERATION : 96, loss : 0.056995735440820164ITERATION : 97, loss : 0.056995735440820164ITERATION : 98, loss : 0.056995735440820164ITERATION : 99, loss : 0.056995735440820164ITERATION : 100, loss : 0.056995735440820164
ITERATION : 1, loss : 0.08476219450973771ITERATION : 2, loss : 0.10193943674490084ITERATION : 3, loss : 0.113985525479356ITERATION : 4, loss : 0.12190411351900841ITERATION : 5, loss : 0.1270241275368341ITERATION : 6, loss : 0.13033313844635716ITERATION : 7, loss : 0.13247743635755013ITERATION : 8, loss : 0.13387039157708341ITERATION : 9, loss : 0.13477661241824426ITERATION : 10, loss : 0.1353664724656334ITERATION : 11, loss : 0.1357502926459956ITERATION : 12, loss : 0.1359998055345152ITERATION : 13, loss : 0.13616177283219727ITERATION : 14, loss : 0.13626671590619144ITERATION : 15, loss : 0.13633456083190734ITERATION : 16, loss : 0.13637831013386387ITERATION : 17, loss : 0.13640643977141123ITERATION : 18, loss : 0.13642446712495176ITERATION : 19, loss : 0.13643597732181578ITERATION : 20, loss : 0.1364432953371966ITERATION : 21, loss : 0.1364479254552578ITERATION : 22, loss : 0.13645083846005404ITERATION : 23, loss : 0.1364526590609447ITERATION : 24, loss : 0.13645378802104396ITERATION : 25, loss : 0.13645448144962857ITERATION : 26, loss : 0.1364549024062898ITERATION : 27, loss : 0.13645515419030543ITERATION : 28, loss : 0.1364553019164243ITERATION : 29, loss : 0.13645538633414447ITERATION : 30, loss : 0.13645543281149342ITERATION : 31, loss : 0.13645545696096542ITERATION : 32, loss : 0.1364554682086422ITERATION : 33, loss : 0.13645547246316742ITERATION : 34, loss : 0.1364554730368227ITERATION : 35, loss : 0.13645547176994996ITERATION : 36, loss : 0.13645546978893075ITERATION : 37, loss : 0.13645546767519245ITERATION : 38, loss : 0.1364554657415919ITERATION : 39, loss : 0.1364554640673158ITERATION : 40, loss : 0.13645546269882472ITERATION : 41, loss : 0.13645546157838892ITERATION : 42, loss : 0.13645546074387627ITERATION : 43, loss : 0.13645546007713583ITERATION : 44, loss : 0.1364554595403942ITERATION : 45, loss : 0.13645545913403756ITERATION : 46, loss : 0.13645545884061594ITERATION : 47, loss : 0.1364554586084612ITERATION : 48, loss : 0.13645545842028664ITERATION : 49, loss : 0.13645545830987668ITERATION : 50, loss : 0.13645545821129648ITERATION : 51, loss : 0.13645545814272905ITERATION : 52, loss : 0.13645545811386672ITERATION : 53, loss : 0.1364554580731807ITERATION : 54, loss : 0.1364554580723153ITERATION : 55, loss : 0.1364554580723153ITERATION : 56, loss : 0.1364554580723153ITERATION : 57, loss : 0.1364554580723153ITERATION : 58, loss : 0.1364554580723153ITERATION : 59, loss : 0.1364554580723153ITERATION : 60, loss : 0.1364554580723153ITERATION : 61, loss : 0.1364554580723153ITERATION : 62, loss : 0.1364554580723153ITERATION : 63, loss : 0.1364554580723153ITERATION : 64, loss : 0.1364554580723153ITERATION : 65, loss : 0.1364554580723153ITERATION : 66, loss : 0.1364554580723153ITERATION : 67, loss : 0.1364554580723153ITERATION : 68, loss : 0.1364554580723153ITERATION : 69, loss : 0.1364554580723153ITERATION : 70, loss : 0.1364554580723153ITERATION : 71, loss : 0.1364554580723153ITERATION : 72, loss : 0.1364554580723153ITERATION : 73, loss : 0.1364554580723153ITERATION : 74, loss : 0.1364554580723153ITERATION : 75, loss : 0.1364554580723153ITERATION : 76, loss : 0.1364554580723153ITERATION : 77, loss : 0.1364554580723153ITERATION : 78, loss : 0.1364554580723153ITERATION : 79, loss : 0.1364554580723153ITERATION : 80, loss : 0.1364554580723153ITERATION : 81, loss : 0.1364554580723153ITERATION : 82, loss : 0.1364554580723153ITERATION : 83, loss : 0.1364554580723153ITERATION : 84, loss : 0.1364554580723153ITERATION : 85, loss : 0.1364554580723153ITERATION : 86, loss : 0.1364554580723153ITERATION : 87, loss : 0.1364554580723153ITERATION : 88, loss : 0.1364554580723153ITERATION : 89, loss : 0.1364554580723153ITERATION : 90, loss : 0.1364554580723153ITERATION : 91, loss : 0.1364554580723153ITERATION : 92, loss : 0.1364554580723153ITERATION : 93, loss : 0.1364554580723153ITERATION : 94, loss : 0.1364554580723153ITERATION : 95, loss : 0.1364554580723153ITERATION : 96, loss : 0.1364554580723153ITERATION : 97, loss : 0.1364554580723153ITERATION : 98, loss : 0.1364554580723153ITERATION : 99, loss : 0.1364554580723153ITERATION : 100, loss : 0.1364554580723153
ITERATION : 1, loss : 0.03606409285897973ITERATION : 2, loss : 0.04358080195881216ITERATION : 3, loss : 0.050483841652605346ITERATION : 4, loss : 0.05578409579593547ITERATION : 5, loss : 0.059690578004229805ITERATION : 6, loss : 0.06288712241173103ITERATION : 7, loss : 0.06529380642862957ITERATION : 8, loss : 0.0669799361753534ITERATION : 9, loss : 0.06816395161771785ITERATION : 10, loss : 0.06899702882165974ITERATION : 11, loss : 0.06958408956350458ITERATION : 12, loss : 0.06999825671215072ITERATION : 13, loss : 0.07029068758931661ITERATION : 14, loss : 0.07049728299843502ITERATION : 15, loss : 0.07064329595680545ITERATION : 16, loss : 0.07074651998563575ITERATION : 17, loss : 0.07081950779934731ITERATION : 18, loss : 0.07087112244637946ITERATION : 19, loss : 0.07090762558161014ITERATION : 20, loss : 0.07093344271648651ITERATION : 21, loss : 0.07095170263358107ITERATION : 22, loss : 0.07096461766159691ITERATION : 23, loss : 0.07097375235254676ITERATION : 24, loss : 0.07098021324177486ITERATION : 25, loss : 0.07098478294204028ITERATION : 26, loss : 0.07098801498819436ITERATION : 27, loss : 0.07099030091657269ITERATION : 28, loss : 0.07099191766294097ITERATION : 29, loss : 0.07099306110652887ITERATION : 30, loss : 0.07099386980718161ITERATION : 31, loss : 0.07099444174416876ITERATION : 32, loss : 0.07099484624172735ITERATION : 33, loss : 0.07099513230299083ITERATION : 34, loss : 0.0709953346121065ITERATION : 35, loss : 0.07099547766458761ITERATION : 36, loss : 0.07099557884267613ITERATION : 37, loss : 0.07099565037747202ITERATION : 38, loss : 0.07099570097772896ITERATION : 39, loss : 0.07099573677349677ITERATION : 40, loss : 0.07099576207857639ITERATION : 41, loss : 0.07099577996684775ITERATION : 42, loss : 0.07099579260992303ITERATION : 43, loss : 0.07099580154452093ITERATION : 44, loss : 0.07099580786935862ITERATION : 45, loss : 0.070995812347326ITERATION : 46, loss : 0.07099581550634494ITERATION : 47, loss : 0.07099581774379184ITERATION : 48, loss : 0.07099581933035233ITERATION : 49, loss : 0.07099582044318749ITERATION : 50, loss : 0.07099582121136197ITERATION : 51, loss : 0.07099582176292549ITERATION : 52, loss : 0.07099582213720369ITERATION : 53, loss : 0.07099582240980655ITERATION : 54, loss : 0.07099582256141232ITERATION : 55, loss : 0.07099582263563238ITERATION : 56, loss : 0.07099582278210265ITERATION : 57, loss : 0.07099582279086596ITERATION : 58, loss : 0.07099582279086596ITERATION : 59, loss : 0.07099582279086596ITERATION : 60, loss : 0.07099582279086596ITERATION : 61, loss : 0.07099582279086596ITERATION : 62, loss : 0.07099582279086596ITERATION : 63, loss : 0.07099582279086596ITERATION : 64, loss : 0.07099582279086596ITERATION : 65, loss : 0.07099582279086596ITERATION : 66, loss : 0.07099582279086596ITERATION : 67, loss : 0.07099582279086596ITERATION : 68, loss : 0.07099582279086596ITERATION : 69, loss : 0.07099582279086596ITERATION : 70, loss : 0.07099582279086596ITERATION : 71, loss : 0.07099582279086596ITERATION : 72, loss : 0.07099582279086596ITERATION : 73, loss : 0.07099582279086596ITERATION : 74, loss : 0.07099582279086596ITERATION : 75, loss : 0.07099582279086596ITERATION : 76, loss : 0.07099582279086596ITERATION : 77, loss : 0.07099582279086596ITERATION : 78, loss : 0.07099582279086596ITERATION : 79, loss : 0.07099582279086596ITERATION : 80, loss : 0.07099582279086596ITERATION : 81, loss : 0.07099582279086596ITERATION : 82, loss : 0.07099582279086596ITERATION : 83, loss : 0.07099582279086596ITERATION : 84, loss : 0.07099582279086596ITERATION : 85, loss : 0.07099582279086596ITERATION : 86, loss : 0.07099582279086596ITERATION : 87, loss : 0.07099582279086596ITERATION : 88, loss : 0.07099582279086596ITERATION : 89, loss : 0.07099582279086596ITERATION : 90, loss : 0.07099582279086596ITERATION : 91, loss : 0.07099582279086596ITERATION : 92, loss : 0.07099582279086596ITERATION : 93, loss : 0.07099582279086596ITERATION : 94, loss : 0.07099582279086596ITERATION : 95, loss : 0.07099582279086596ITERATION : 96, loss : 0.07099582279086596ITERATION : 97, loss : 0.07099582279086596ITERATION : 98, loss : 0.07099582279086596ITERATION : 99, loss : 0.07099582279086596ITERATION : 100, loss : 0.07099582279086596
gradient norm in None layer : 0.023415152909445555
gradient norm in None layer : 0.0008407243763368438
gradient norm in None layer : 0.0005810298644665402
gradient norm in None layer : 0.012241654672905713
gradient norm in None layer : 0.0007397503177968667
gradient norm in None layer : 0.0006163492861505677
gradient norm in None layer : 0.004878904302023354
gradient norm in None layer : 0.00018079540947272922
gradient norm in None layer : 0.0001819434874842279
gradient norm in None layer : 0.004078276277123599
gradient norm in None layer : 0.00018198118885914282
gradient norm in None layer : 0.00016346892295573094
gradient norm in None layer : 0.0014603347000689225
gradient norm in None layer : 3.9386967838705086e-05
gradient norm in None layer : 3.808796196385286e-05
gradient norm in None layer : 0.0012381484426023698
gradient norm in None layer : 3.913768243421513e-05
gradient norm in None layer : 3.862163784977615e-05
gradient norm in None layer : 0.0015027926815522382
gradient norm in None layer : 2.5724935912284534e-05
gradient norm in None layer : 0.003603462350017738
gradient norm in None layer : 0.0001570548118909731
gradient norm in None layer : 0.0001364384257754954
gradient norm in None layer : 0.0037975959059748585
gradient norm in None layer : 0.00024034637821101283
gradient norm in None layer : 0.0003266087273160687
gradient norm in None layer : 0.006607419715382276
gradient norm in None layer : 9.560640603678786e-05
gradient norm in None layer : 0.011194989937102173
gradient norm in None layer : 0.0006882689356531467
gradient norm in None layer : 0.0006644342435694225
gradient norm in None layer : 0.012536637993668998
gradient norm in None layer : 0.0030033092839716826
gradient norm in None layer : 0.004197965179703305
gradient norm in None layer : 0.0018628155214893366
gradient norm in None layer : 0.0007004048575483344
Total gradient norm: 0.033643522406043104
invariance loss : 7.953363172767439, avg_den : 0.23783111572265625, density loss : 0.14033914551436882, mse loss : 0.11660606756067704, solver time : 110.34945678710938 sec , total loss : 0.12469976987895885, running loss : 0.12088886780989416
Epoch 0/10 , batch 3/12500 
ITERATION : 1, loss : 0.025971388941064117ITERATION : 2, loss : 0.030931475061315016ITERATION : 3, loss : 0.0347627969263925ITERATION : 4, loss : 0.03770596172787237ITERATION : 5, loss : 0.03989733727129567ITERATION : 6, loss : 0.041484543083581515ITERATION : 7, loss : 0.04261231468927703ITERATION : 8, loss : 0.04340369786169543ITERATION : 9, loss : 0.04395461912867097ITERATION : 10, loss : 0.044336195851724956ITERATION : 11, loss : 0.044599616794089414ITERATION : 12, loss : 0.044781079674703766ITERATION : 13, loss : 0.044905905481897386ITERATION : 14, loss : 0.044991687580954544ITERATION : 15, loss : 0.045050597725737156ITERATION : 16, loss : 0.045091033568531606ITERATION : 17, loss : 0.04511877815307446ITERATION : 18, loss : 0.045137809100808696ITERATION : 19, loss : 0.045150859934804395ITERATION : 20, loss : 0.04515980797281563ITERATION : 21, loss : 0.04516594188108364ITERATION : 22, loss : 0.04517014599152439ITERATION : 23, loss : 0.045173027078934114ITERATION : 24, loss : 0.04517500119959499ITERATION : 25, loss : 0.04517635368471316ITERATION : 26, loss : 0.045177280197902736ITERATION : 27, loss : 0.045177914806038756ITERATION : 28, loss : 0.04517834945218908ITERATION : 29, loss : 0.045178647122882075ITERATION : 30, loss : 0.04517885095366586ITERATION : 31, loss : 0.045178990519646654ITERATION : 32, loss : 0.04517908608848892ITERATION : 33, loss : 0.04517915150178027ITERATION : 34, loss : 0.045179196287590144ITERATION : 35, loss : 0.04517922691769295ITERATION : 36, loss : 0.045179247887075974ITERATION : 37, loss : 0.04517926224152189ITERATION : 38, loss : 0.04517927207168048ITERATION : 39, loss : 0.04517927881016584ITERATION : 40, loss : 0.045179283405982484ITERATION : 41, loss : 0.04517928656195805ITERATION : 42, loss : 0.04517928875248324ITERATION : 43, loss : 0.045179290224645646ITERATION : 44, loss : 0.04517929122750348ITERATION : 45, loss : 0.045179291919419176ITERATION : 46, loss : 0.0451792923961447ITERATION : 47, loss : 0.0451792927184137ITERATION : 48, loss : 0.04517929294524686ITERATION : 49, loss : 0.04517929308765301ITERATION : 50, loss : 0.04517929319656733ITERATION : 51, loss : 0.04517929325660156ITERATION : 52, loss : 0.045179293283226986ITERATION : 53, loss : 0.04517929328340056ITERATION : 54, loss : 0.04517929328340056ITERATION : 55, loss : 0.04517929328340056ITERATION : 56, loss : 0.04517929328340056ITERATION : 57, loss : 0.04517929328340056ITERATION : 58, loss : 0.04517929328340056ITERATION : 59, loss : 0.04517929328340056ITERATION : 60, loss : 0.04517929328340056ITERATION : 61, loss : 0.04517929328340056ITERATION : 62, loss : 0.04517929328340056ITERATION : 63, loss : 0.04517929328340056ITERATION : 64, loss : 0.04517929328340056ITERATION : 65, loss : 0.04517929328340056ITERATION : 66, loss : 0.04517929328340056ITERATION : 67, loss : 0.04517929328340056ITERATION : 68, loss : 0.04517929328340056ITERATION : 69, loss : 0.04517929328340056ITERATION : 70, loss : 0.04517929328340056ITERATION : 71, loss : 0.04517929328340056ITERATION : 72, loss : 0.04517929328340056ITERATION : 73, loss : 0.04517929328340056ITERATION : 74, loss : 0.04517929328340056ITERATION : 75, loss : 0.04517929328340056ITERATION : 76, loss : 0.04517929328340056ITERATION : 77, loss : 0.04517929328340056ITERATION : 78, loss : 0.04517929328340056ITERATION : 79, loss : 0.04517929328340056ITERATION : 80, loss : 0.04517929328340056ITERATION : 81, loss : 0.04517929328340056ITERATION : 82, loss : 0.04517929328340056ITERATION : 83, loss : 0.04517929328340056ITERATION : 84, loss : 0.04517929328340056ITERATION : 85, loss : 0.04517929328340056ITERATION : 86, loss : 0.04517929328340056ITERATION : 87, loss : 0.04517929328340056ITERATION : 88, loss : 0.04517929328340056ITERATION : 89, loss : 0.04517929328340056ITERATION : 90, loss : 0.04517929328340056ITERATION : 91, loss : 0.04517929328340056ITERATION : 92, loss : 0.04517929328340056ITERATION : 93, loss : 0.04517929328340056ITERATION : 94, loss : 0.04517929328340056ITERATION : 95, loss : 0.04517929328340056ITERATION : 96, loss : 0.04517929328340056ITERATION : 97, loss : 0.04517929328340056ITERATION : 98, loss : 0.04517929328340056ITERATION : 99, loss : 0.04517929328340056ITERATION : 100, loss : 0.04517929328340056
ITERATION : 1, loss : 0.04854564168663855ITERATION : 2, loss : 0.04578033871847137ITERATION : 3, loss : 0.05117500389024446ITERATION : 4, loss : 0.05552579132373576ITERATION : 5, loss : 0.05864882761006138ITERATION : 6, loss : 0.06080114409771086ITERATION : 7, loss : 0.06225230021537792ITERATION : 8, loss : 0.06321530184486432ITERATION : 9, loss : 0.06384550148943346ITERATION : 10, loss : 0.06425224070430635ITERATION : 11, loss : 0.06451087506736443ITERATION : 12, loss : 0.06467255128122926ITERATION : 13, loss : 0.06477154704756494ITERATION : 14, loss : 0.06483057024590819ITERATION : 15, loss : 0.06486449606662824ITERATION : 16, loss : 0.06488295621443407ITERATION : 17, loss : 0.06489210970049948ITERATION : 18, loss : 0.06489583898209979ITERATION : 19, loss : 0.06489655032134613ITERATION : 20, loss : 0.06489570358286935ITERATION : 21, loss : 0.06489415981843838ITERATION : 22, loss : 0.06489240707258552ITERATION : 23, loss : 0.06489070540424277ITERATION : 24, loss : 0.0648891790485824ITERATION : 25, loss : 0.06488787397720884ITERATION : 26, loss : 0.06488679342048052ITERATION : 27, loss : 0.06488591916682991ITERATION : 28, loss : 0.06488522396062171ITERATION : 29, loss : 0.06488467859208988ITERATION : 30, loss : 0.06488425534526579ITERATION : 31, loss : 0.06488392984224511ITERATION : 32, loss : 0.06488368139330722ITERATION : 33, loss : 0.06488349282099687ITERATION : 34, loss : 0.06488335052846676ITERATION : 35, loss : 0.06488324364361141ITERATION : 36, loss : 0.06488316370258147ITERATION : 37, loss : 0.06488310411481862ITERATION : 38, loss : 0.06488305984047486ITERATION : 39, loss : 0.06488302705872534ITERATION : 40, loss : 0.06488300285661665ITERATION : 41, loss : 0.06488298499905816ITERATION : 42, loss : 0.06488297185707718ITERATION : 43, loss : 0.06488296220043027ITERATION : 44, loss : 0.06488295513206226ITERATION : 45, loss : 0.0648829499548387ITERATION : 46, loss : 0.06488294618156837ITERATION : 47, loss : 0.06488294341047772ITERATION : 48, loss : 0.06488294134932183ITERATION : 49, loss : 0.06488293990406804ITERATION : 50, loss : 0.06488293884494248ITERATION : 51, loss : 0.06488293810618642ITERATION : 52, loss : 0.06488293753376922ITERATION : 53, loss : 0.06488293710656792ITERATION : 54, loss : 0.06488293683955507ITERATION : 55, loss : 0.06488293668122899ITERATION : 56, loss : 0.06488293650951522ITERATION : 57, loss : 0.06488293637332672ITERATION : 58, loss : 0.06488293631617527ITERATION : 59, loss : 0.06488293623646206ITERATION : 60, loss : 0.06488293623418821ITERATION : 61, loss : 0.06488293623418821ITERATION : 62, loss : 0.06488293623418821ITERATION : 63, loss : 0.06488293623418821ITERATION : 64, loss : 0.06488293623418821ITERATION : 65, loss : 0.06488293623418821ITERATION : 66, loss : 0.06488293623418821ITERATION : 67, loss : 0.06488293623418821ITERATION : 68, loss : 0.06488293623418821ITERATION : 69, loss : 0.06488293623418821ITERATION : 70, loss : 0.06488293623418821ITERATION : 71, loss : 0.06488293623418821ITERATION : 72, loss : 0.06488293623418821ITERATION : 73, loss : 0.06488293623418821ITERATION : 74, loss : 0.06488293623418821ITERATION : 75, loss : 0.06488293623418821ITERATION : 76, loss : 0.06488293623418821ITERATION : 77, loss : 0.06488293623418821ITERATION : 78, loss : 0.06488293623418821ITERATION : 79, loss : 0.06488293623418821ITERATION : 80, loss : 0.06488293623418821ITERATION : 81, loss : 0.06488293623418821ITERATION : 82, loss : 0.06488293623418821ITERATION : 83, loss : 0.06488293623418821ITERATION : 84, loss : 0.06488293623418821ITERATION : 85, loss : 0.06488293623418821ITERATION : 86, loss : 0.06488293623418821ITERATION : 87, loss : 0.06488293623418821ITERATION : 88, loss : 0.06488293623418821ITERATION : 89, loss : 0.06488293623418821ITERATION : 90, loss : 0.06488293623418821ITERATION : 91, loss : 0.06488293623418821ITERATION : 92, loss : 0.06488293623418821ITERATION : 93, loss : 0.06488293623418821ITERATION : 94, loss : 0.06488293623418821ITERATION : 95, loss : 0.06488293623418821ITERATION : 96, loss : 0.06488293623418821ITERATION : 97, loss : 0.06488293623418821ITERATION : 98, loss : 0.06488293623418821ITERATION : 99, loss : 0.06488293623418821ITERATION : 100, loss : 0.06488293623418821
ITERATION : 1, loss : 0.15292715742386734ITERATION : 2, loss : 0.1815206152197484ITERATION : 3, loss : 0.19775499788911985ITERATION : 4, loss : 0.20809569554127444ITERATION : 5, loss : 0.21509384925044867ITERATION : 6, loss : 0.21996125384207335ITERATION : 7, loss : 0.22339241311011315ITERATION : 8, loss : 0.22582845279760017ITERATION : 9, loss : 0.22756483922846946ITERATION : 10, loss : 0.22880524903961838ITERATION : 11, loss : 0.22969240543886535ITERATION : 12, loss : 0.230327284839521ITERATION : 13, loss : 0.2307817336721962ITERATION : 14, loss : 0.23110704025849363ITERATION : 15, loss : 0.23133988469228195ITERATION : 16, loss : 0.23150652471713518ITERATION : 17, loss : 0.23162576497619258ITERATION : 18, loss : 0.2317110739834731ITERATION : 19, loss : 0.23177209775564486ITERATION : 20, loss : 0.2318157433754126ITERATION : 21, loss : 0.23184695567802727ITERATION : 22, loss : 0.23186927393294812ITERATION : 23, loss : 0.23188523095259478ITERATION : 24, loss : 0.23189663890375753ITERATION : 25, loss : 0.23190479400222874ITERATION : 26, loss : 0.23191062337471008ITERATION : 27, loss : 0.23191479010382762ITERATION : 28, loss : 0.23191776818217502ITERATION : 29, loss : 0.2319198966478848ITERATION : 30, loss : 0.23192141779097672ITERATION : 31, loss : 0.2319225049013379ITERATION : 32, loss : 0.2319232817708842ITERATION : 33, loss : 0.23192383696166272ITERATION : 34, loss : 0.23192423371061127ITERATION : 35, loss : 0.23192451720904342ITERATION : 36, loss : 0.2319247198333162ITERATION : 37, loss : 0.23192486458290565ITERATION : 38, loss : 0.23192496803591808ITERATION : 39, loss : 0.2319250419378912ITERATION : 40, loss : 0.23192509474510412ITERATION : 41, loss : 0.23192513246390067ITERATION : 42, loss : 0.2319251594413062ITERATION : 43, loss : 0.23192517863183032ITERATION : 44, loss : 0.23192519241328086ITERATION : 45, loss : 0.23192520231702074ITERATION : 46, loss : 0.2319252093922811ITERATION : 47, loss : 0.23192521439089084ITERATION : 48, loss : 0.23192521792000947ITERATION : 49, loss : 0.23192522046255276ITERATION : 50, loss : 0.23192522222024622ITERATION : 51, loss : 0.2319252235232598ITERATION : 52, loss : 0.23192522442115454ITERATION : 53, loss : 0.2319252250895756ITERATION : 54, loss : 0.2319252255211336ITERATION : 55, loss : 0.2319252258118472ITERATION : 56, loss : 0.23192522606956362ITERATION : 57, loss : 0.23192522621444525ITERATION : 58, loss : 0.23192522634819787ITERATION : 59, loss : 0.23192522639382965ITERATION : 60, loss : 0.23192522646464644ITERATION : 61, loss : 0.23192522646574723ITERATION : 62, loss : 0.23192522646574723ITERATION : 63, loss : 0.23192522646574723ITERATION : 64, loss : 0.23192522646574723ITERATION : 65, loss : 0.23192522646574723ITERATION : 66, loss : 0.23192522646574723ITERATION : 67, loss : 0.23192522646574723ITERATION : 68, loss : 0.23192522646574723ITERATION : 69, loss : 0.23192522646574723ITERATION : 70, loss : 0.23192522646574723ITERATION : 71, loss : 0.23192522646574723ITERATION : 72, loss : 0.23192522646574723ITERATION : 73, loss : 0.23192522646574723ITERATION : 74, loss : 0.23192522646574723ITERATION : 75, loss : 0.23192522646574723ITERATION : 76, loss : 0.23192522646574723ITERATION : 77, loss : 0.23192522646574723ITERATION : 78, loss : 0.23192522646574723ITERATION : 79, loss : 0.23192522646574723ITERATION : 80, loss : 0.23192522646574723ITERATION : 81, loss : 0.23192522646574723ITERATION : 82, loss : 0.23192522646574723ITERATION : 83, loss : 0.23192522646574723ITERATION : 84, loss : 0.23192522646574723ITERATION : 85, loss : 0.23192522646574723ITERATION : 86, loss : 0.23192522646574723ITERATION : 87, loss : 0.23192522646574723ITERATION : 88, loss : 0.23192522646574723ITERATION : 89, loss : 0.23192522646574723ITERATION : 90, loss : 0.23192522646574723ITERATION : 91, loss : 0.23192522646574723ITERATION : 92, loss : 0.23192522646574723ITERATION : 93, loss : 0.23192522646574723ITERATION : 94, loss : 0.23192522646574723ITERATION : 95, loss : 0.23192522646574723ITERATION : 96, loss : 0.23192522646574723ITERATION : 97, loss : 0.23192522646574723ITERATION : 98, loss : 0.23192522646574723ITERATION : 99, loss : 0.23192522646574723ITERATION : 100, loss : 0.23192522646574723
ITERATION : 1, loss : 0.03515961819290184ITERATION : 2, loss : 0.029991350984189247ITERATION : 3, loss : 0.02897885085155022ITERATION : 4, loss : 0.028971537975165367ITERATION : 5, loss : 0.029238505521946172ITERATION : 6, loss : 0.029552898330864497ITERATION : 7, loss : 0.02983698768895308ITERATION : 8, loss : 0.03006881966364523ITERATION : 9, loss : 0.030248313310464306ITERATION : 10, loss : 0.030383051064867297ITERATION : 11, loss : 0.030482237419656895ITERATION : 12, loss : 0.030554321966288267ITERATION : 13, loss : 0.030606258360764745ITERATION : 14, loss : 0.030643456893245106ITERATION : 15, loss : 0.030669990454123625ITERATION : 16, loss : 0.030688862768260213ITERATION : 17, loss : 0.030702259338763638ITERATION : 18, loss : 0.03071175573730916ITERATION : 19, loss : 0.030718481108604714ITERATION : 20, loss : 0.030723240867527382ITERATION : 21, loss : 0.030726608071145432ITERATION : 22, loss : 0.030728989475233046ITERATION : 23, loss : 0.030730673409209826ITERATION : 24, loss : 0.03073186404684817ITERATION : 25, loss : 0.030732705850407598ITERATION : 26, loss : 0.030733301031401684ITERATION : 27, loss : 0.03073372183232301ITERATION : 28, loss : 0.03073401936927913ITERATION : 29, loss : 0.0307342298081091ITERATION : 30, loss : 0.030734378593342545ITERATION : 31, loss : 0.030734483820499914ITERATION : 32, loss : 0.030734558264028274ITERATION : 33, loss : 0.030734610923190808ITERATION : 34, loss : 0.03073464815261031ITERATION : 35, loss : 0.030734674550176453ITERATION : 36, loss : 0.03073469319163865ITERATION : 37, loss : 0.030734706382780443ITERATION : 38, loss : 0.03073471567074379ITERATION : 39, loss : 0.030734722274493254ITERATION : 40, loss : 0.030734726898919814ITERATION : 41, loss : 0.030734730210582865ITERATION : 42, loss : 0.030734732508153683ITERATION : 43, loss : 0.03073473417472237ITERATION : 44, loss : 0.030734735306440695ITERATION : 45, loss : 0.030734736157688635ITERATION : 46, loss : 0.030734736726131282ITERATION : 47, loss : 0.030734737146525724ITERATION : 48, loss : 0.03073473744117124ITERATION : 49, loss : 0.030734737667222206ITERATION : 50, loss : 0.030734737774371874ITERATION : 51, loss : 0.030734737880517327ITERATION : 52, loss : 0.030734737940095135ITERATION : 53, loss : 0.03073473799565992ITERATION : 54, loss : 0.0307347379985402ITERATION : 55, loss : 0.0307347379985402ITERATION : 56, loss : 0.0307347379985402ITERATION : 57, loss : 0.0307347379985402ITERATION : 58, loss : 0.0307347379985402ITERATION : 59, loss : 0.0307347379985402ITERATION : 60, loss : 0.0307347379985402ITERATION : 61, loss : 0.0307347379985402ITERATION : 62, loss : 0.0307347379985402ITERATION : 63, loss : 0.0307347379985402ITERATION : 64, loss : 0.0307347379985402ITERATION : 65, loss : 0.0307347379985402ITERATION : 66, loss : 0.0307347379985402ITERATION : 67, loss : 0.0307347379985402ITERATION : 68, loss : 0.0307347379985402ITERATION : 69, loss : 0.0307347379985402ITERATION : 70, loss : 0.0307347379985402ITERATION : 71, loss : 0.0307347379985402ITERATION : 72, loss : 0.0307347379985402ITERATION : 73, loss : 0.0307347379985402ITERATION : 74, loss : 0.0307347379985402ITERATION : 75, loss : 0.0307347379985402ITERATION : 76, loss : 0.0307347379985402ITERATION : 77, loss : 0.0307347379985402ITERATION : 78, loss : 0.0307347379985402ITERATION : 79, loss : 0.0307347379985402ITERATION : 80, loss : 0.0307347379985402ITERATION : 81, loss : 0.0307347379985402ITERATION : 82, loss : 0.0307347379985402ITERATION : 83, loss : 0.0307347379985402ITERATION : 84, loss : 0.0307347379985402ITERATION : 85, loss : 0.0307347379985402ITERATION : 86, loss : 0.0307347379985402ITERATION : 87, loss : 0.0307347379985402ITERATION : 88, loss : 0.0307347379985402ITERATION : 89, loss : 0.0307347379985402ITERATION : 90, loss : 0.0307347379985402ITERATION : 91, loss : 0.0307347379985402ITERATION : 92, loss : 0.0307347379985402ITERATION : 93, loss : 0.0307347379985402ITERATION : 94, loss : 0.0307347379985402ITERATION : 95, loss : 0.0307347379985402ITERATION : 96, loss : 0.0307347379985402ITERATION : 97, loss : 0.0307347379985402ITERATION : 98, loss : 0.0307347379985402ITERATION : 99, loss : 0.0307347379985402ITERATION : 100, loss : 0.0307347379985402
ITERATION : 1, loss : 0.029427301761057005ITERATION : 2, loss : 0.04039241302906648ITERATION : 3, loss : 0.04692686329945344ITERATION : 4, loss : 0.05123064104557112ITERATION : 5, loss : 0.05412581780593566ITERATION : 6, loss : 0.056088388854327ITERATION : 7, loss : 0.05733248491268902ITERATION : 8, loss : 0.05816060755400664ITERATION : 9, loss : 0.05872818501748334ITERATION : 10, loss : 0.05911745969610882ITERATION : 11, loss : 0.05938464701638563ITERATION : 12, loss : 0.059568155958221944ITERATION : 13, loss : 0.05969425530865916ITERATION : 14, loss : 0.059780934665214266ITERATION : 15, loss : 0.05984052954259058ITERATION : 16, loss : 0.05988150728615848ITERATION : 17, loss : 0.059909684440492154ITERATION : 18, loss : 0.059929058925323575ITERATION : 19, loss : 0.05994237964700377ITERATION : 20, loss : 0.05995153703952602ITERATION : 21, loss : 0.05995783140198785ITERATION : 22, loss : 0.05996215719363107ITERATION : 23, loss : 0.059965129515719945ITERATION : 24, loss : 0.059967171447501286ITERATION : 25, loss : 0.05996857393697347ITERATION : 26, loss : 0.059969537045453464ITERATION : 27, loss : 0.05997019830445647ITERATION : 28, loss : 0.05997065213576728ITERATION : 29, loss : 0.05997096362269931ITERATION : 30, loss : 0.05997117724880647ITERATION : 31, loss : 0.059971323763937795ITERATION : 32, loss : 0.05997142422024495ITERATION : 33, loss : 0.05997149314907382ITERATION : 34, loss : 0.05997154034109358ITERATION : 35, loss : 0.05997157268287304ITERATION : 36, loss : 0.05997159485843469ITERATION : 37, loss : 0.059971610051645134ITERATION : 38, loss : 0.059971620449316455ITERATION : 39, loss : 0.05997162760044748ITERATION : 40, loss : 0.059971632469394395ITERATION : 41, loss : 0.05997163577565228ITERATION : 42, loss : 0.05997163801581366ITERATION : 43, loss : 0.05997163953304802ITERATION : 44, loss : 0.05997164057714237ITERATION : 45, loss : 0.05997164126752058ITERATION : 46, loss : 0.059971641733420365ITERATION : 47, loss : 0.059971642058606285ITERATION : 48, loss : 0.05997164226642073ITERATION : 49, loss : 0.05997164241021676ITERATION : 50, loss : 0.05997164250210008ITERATION : 51, loss : 0.05997164258468383ITERATION : 52, loss : 0.05997164262554431ITERATION : 53, loss : 0.05997164264266859ITERATION : 54, loss : 0.05997164265784007ITERATION : 55, loss : 0.05997164265842528ITERATION : 56, loss : 0.059971642659450625ITERATION : 57, loss : 0.059971642659450625ITERATION : 58, loss : 0.059971642659450625ITERATION : 59, loss : 0.059971642659450625ITERATION : 60, loss : 0.059971642659450625ITERATION : 61, loss : 0.059971642659450625ITERATION : 62, loss : 0.059971642659450625ITERATION : 63, loss : 0.059971642659450625ITERATION : 64, loss : 0.059971642659450625ITERATION : 65, loss : 0.059971642659450625ITERATION : 66, loss : 0.059971642659450625ITERATION : 67, loss : 0.059971642659450625ITERATION : 68, loss : 0.059971642659450625ITERATION : 69, loss : 0.059971642659450625ITERATION : 70, loss : 0.059971642659450625ITERATION : 71, loss : 0.059971642659450625ITERATION : 72, loss : 0.059971642659450625ITERATION : 73, loss : 0.059971642659450625ITERATION : 74, loss : 0.059971642659450625ITERATION : 75, loss : 0.059971642659450625ITERATION : 76, loss : 0.059971642659450625ITERATION : 77, loss : 0.059971642659450625ITERATION : 78, loss : 0.059971642659450625ITERATION : 79, loss : 0.059971642659450625ITERATION : 80, loss : 0.059971642659450625ITERATION : 81, loss : 0.059971642659450625ITERATION : 82, loss : 0.059971642659450625ITERATION : 83, loss : 0.059971642659450625ITERATION : 84, loss : 0.059971642659450625ITERATION : 85, loss : 0.059971642659450625ITERATION : 86, loss : 0.059971642659450625ITERATION : 87, loss : 0.059971642659450625ITERATION : 88, loss : 0.059971642659450625ITERATION : 89, loss : 0.059971642659450625ITERATION : 90, loss : 0.059971642659450625ITERATION : 91, loss : 0.059971642659450625ITERATION : 92, loss : 0.059971642659450625ITERATION : 93, loss : 0.059971642659450625ITERATION : 94, loss : 0.059971642659450625ITERATION : 95, loss : 0.059971642659450625ITERATION : 96, loss : 0.059971642659450625ITERATION : 97, loss : 0.059971642659450625ITERATION : 98, loss : 0.059971642659450625ITERATION : 99, loss : 0.059971642659450625ITERATION : 100, loss : 0.059971642659450625
ITERATION : 1, loss : 0.08938794558293676ITERATION : 2, loss : 0.11907257263294717ITERATION : 3, loss : 0.13662065041913615ITERATION : 4, loss : 0.14759829019491036ITERATION : 5, loss : 0.15471694333205674ITERATION : 6, loss : 0.1593888202712455ITERATION : 7, loss : 0.16248539971077686ITERATION : 8, loss : 0.1645568888531923ITERATION : 9, loss : 0.16595195033930432ITERATION : 10, loss : 0.16689633211153654ITERATION : 11, loss : 0.16753812570698084ITERATION : 12, loss : 0.16797555400259054ITERATION : 13, loss : 0.16827433898659355ITERATION : 14, loss : 0.1684787548653591ITERATION : 15, loss : 0.16861877972718944ITERATION : 16, loss : 0.16871478844233614ITERATION : 17, loss : 0.16878066737389288ITERATION : 18, loss : 0.16882590035420356ITERATION : 19, loss : 0.16885697434787159ITERATION : 20, loss : 0.16887833150874737ITERATION : 21, loss : 0.16889301656989217ITERATION : 22, loss : 0.16890311793666266ITERATION : 23, loss : 0.16891006894330604ITERATION : 24, loss : 0.16891485387110303ITERATION : 25, loss : 0.16891814885806716ITERATION : 26, loss : 0.1689204186383265ITERATION : 27, loss : 0.16892198276733314ITERATION : 28, loss : 0.16892306093576812ITERATION : 29, loss : 0.1689238043713624ITERATION : 30, loss : 0.1689243171675115ITERATION : 31, loss : 0.16892467100733763ITERATION : 32, loss : 0.1689249152146727ITERATION : 33, loss : 0.16892508381203045ITERATION : 34, loss : 0.16892520027054184ITERATION : 35, loss : 0.16892528073114046ITERATION : 36, loss : 0.1689253363570612ITERATION : 37, loss : 0.16892537479060332ITERATION : 38, loss : 0.16892540141812817ITERATION : 39, loss : 0.16892541980434314ITERATION : 40, loss : 0.16892543251989842ITERATION : 41, loss : 0.1689254413111863ITERATION : 42, loss : 0.16892544740843246ITERATION : 43, loss : 0.1689254516103618ITERATION : 44, loss : 0.1689254545397315ITERATION : 45, loss : 0.16892545655124816ITERATION : 46, loss : 0.16892545792766542ITERATION : 47, loss : 0.16892545886558602ITERATION : 48, loss : 0.16892545955062277ITERATION : 49, loss : 0.16892545999469583ITERATION : 50, loss : 0.16892546035168984ITERATION : 51, loss : 0.16892546057814817ITERATION : 52, loss : 0.16892546075609016ITERATION : 53, loss : 0.1689254608292386ITERATION : 54, loss : 0.1689254608949776ITERATION : 55, loss : 0.16892546094551514ITERATION : 56, loss : 0.16892546097579755ITERATION : 57, loss : 0.16892546097614905ITERATION : 58, loss : 0.16892546097614905ITERATION : 59, loss : 0.16892546097614905ITERATION : 60, loss : 0.16892546097614905ITERATION : 61, loss : 0.16892546097614905ITERATION : 62, loss : 0.16892546097614905ITERATION : 63, loss : 0.16892546097614905ITERATION : 64, loss : 0.16892546097614905ITERATION : 65, loss : 0.16892546097614905ITERATION : 66, loss : 0.16892546097614905ITERATION : 67, loss : 0.16892546097614905ITERATION : 68, loss : 0.16892546097614905ITERATION : 69, loss : 0.16892546097614905ITERATION : 70, loss : 0.16892546097614905ITERATION : 71, loss : 0.16892546097614905ITERATION : 72, loss : 0.16892546097614905ITERATION : 73, loss : 0.16892546097614905ITERATION : 74, loss : 0.16892546097614905ITERATION : 75, loss : 0.16892546097614905ITERATION : 76, loss : 0.16892546097614905ITERATION : 77, loss : 0.16892546097614905ITERATION : 78, loss : 0.16892546097614905ITERATION : 79, loss : 0.16892546097614905ITERATION : 80, loss : 0.16892546097614905ITERATION : 81, loss : 0.16892546097614905ITERATION : 82, loss : 0.16892546097614905ITERATION : 83, loss : 0.16892546097614905ITERATION : 84, loss : 0.16892546097614905ITERATION : 85, loss : 0.16892546097614905ITERATION : 86, loss : 0.16892546097614905ITERATION : 87, loss : 0.16892546097614905ITERATION : 88, loss : 0.16892546097614905ITERATION : 89, loss : 0.16892546097614905ITERATION : 90, loss : 0.16892546097614905ITERATION : 91, loss : 0.16892546097614905ITERATION : 92, loss : 0.16892546097614905ITERATION : 93, loss : 0.16892546097614905ITERATION : 94, loss : 0.16892546097614905ITERATION : 95, loss : 0.16892546097614905ITERATION : 96, loss : 0.16892546097614905ITERATION : 97, loss : 0.16892546097614905ITERATION : 98, loss : 0.16892546097614905ITERATION : 99, loss : 0.16892546097614905ITERATION : 100, loss : 0.16892546097614905
ITERATION : 1, loss : 0.05630128157339602ITERATION : 2, loss : 0.06825418740622805ITERATION : 3, loss : 0.07647359339135566ITERATION : 4, loss : 0.08175187317014833ITERATION : 5, loss : 0.08506973197348344ITERATION : 6, loss : 0.08714237785711534ITERATION : 7, loss : 0.08843485581160494ITERATION : 8, loss : 0.08924002304468302ITERATION : 9, loss : 0.08974089214585831ITERATION : 10, loss : 0.09005177479489468ITERATION : 11, loss : 0.090244141060936ITERATION : 12, loss : 0.09036270162839545ITERATION : 13, loss : 0.09043541770562522ITERATION : 14, loss : 0.09047975347340058ITERATION : 15, loss : 0.09050659354966156ITERATION : 16, loss : 0.09052270231233409ITERATION : 17, loss : 0.0905322683339547ITERATION : 18, loss : 0.0905378739995464ITERATION : 19, loss : 0.09054110309437412ITERATION : 20, loss : 0.09054292125634335ITERATION : 21, loss : 0.0905439128739221ITERATION : 22, loss : 0.09054442857278379ITERATION : 23, loss : 0.09054467639842498ITERATION : 24, loss : 0.09054477826287757ITERATION : 25, loss : 0.09054480432208983ITERATION : 26, loss : 0.09054479401287512ITERATION : 27, loss : 0.09054476880762422ITERATION : 28, loss : 0.09054473986277191ITERATION : 29, loss : 0.0905447124803862ITERATION : 30, loss : 0.09054468890786017ITERATION : 31, loss : 0.09054466962639265ITERATION : 32, loss : 0.09054465440584934ITERATION : 33, loss : 0.09054464269446665ITERATION : 34, loss : 0.09054463384708812ITERATION : 35, loss : 0.09054462724487056ITERATION : 36, loss : 0.09054462238939096ITERATION : 37, loss : 0.09054461884016975ITERATION : 38, loss : 0.09054461626418349ITERATION : 39, loss : 0.09054461441535025ITERATION : 40, loss : 0.09054461306668857ITERATION : 41, loss : 0.09054461213372826ITERATION : 42, loss : 0.09054461147452618ITERATION : 43, loss : 0.09054461104025918ITERATION : 44, loss : 0.09054461070810567ITERATION : 45, loss : 0.09054461049025687ITERATION : 46, loss : 0.09054461029994376ITERATION : 47, loss : 0.09054461019973768ITERATION : 48, loss : 0.09054461010743516ITERATION : 49, loss : 0.09054461007030501ITERATION : 50, loss : 0.09054461001356058ITERATION : 51, loss : 0.09054461001379166ITERATION : 52, loss : 0.09054460999794808ITERATION : 53, loss : 0.09054460998963773ITERATION : 54, loss : 0.09054460998843702ITERATION : 55, loss : 0.09054460998843702ITERATION : 56, loss : 0.09054460998843702ITERATION : 57, loss : 0.09054460998843702ITERATION : 58, loss : 0.09054460998843702ITERATION : 59, loss : 0.09054460998843702ITERATION : 60, loss : 0.09054460998843702ITERATION : 61, loss : 0.09054460998843702ITERATION : 62, loss : 0.09054460998843702ITERATION : 63, loss : 0.09054460998843702ITERATION : 64, loss : 0.09054460998843702ITERATION : 65, loss : 0.09054460998843702ITERATION : 66, loss : 0.09054460998843702ITERATION : 67, loss : 0.09054460998843702ITERATION : 68, loss : 0.09054460998843702ITERATION : 69, loss : 0.09054460998843702ITERATION : 70, loss : 0.09054460998843702ITERATION : 71, loss : 0.09054460998843702ITERATION : 72, loss : 0.09054460998843702ITERATION : 73, loss : 0.09054460998843702ITERATION : 74, loss : 0.09054460998843702ITERATION : 75, loss : 0.09054460998843702ITERATION : 76, loss : 0.09054460998843702ITERATION : 77, loss : 0.09054460998843702ITERATION : 78, loss : 0.09054460998843702ITERATION : 79, loss : 0.09054460998843702ITERATION : 80, loss : 0.09054460998843702ITERATION : 81, loss : 0.09054460998843702ITERATION : 82, loss : 0.09054460998843702ITERATION : 83, loss : 0.09054460998843702ITERATION : 84, loss : 0.09054460998843702ITERATION : 85, loss : 0.09054460998843702ITERATION : 86, loss : 0.09054460998843702ITERATION : 87, loss : 0.09054460998843702ITERATION : 88, loss : 0.09054460998843702ITERATION : 89, loss : 0.09054460998843702ITERATION : 90, loss : 0.09054460998843702ITERATION : 91, loss : 0.09054460998843702ITERATION : 92, loss : 0.09054460998843702ITERATION : 93, loss : 0.09054460998843702ITERATION : 94, loss : 0.09054460998843702ITERATION : 95, loss : 0.09054460998843702ITERATION : 96, loss : 0.09054460998843702ITERATION : 97, loss : 0.09054460998843702ITERATION : 98, loss : 0.09054460998843702ITERATION : 99, loss : 0.09054460998843702ITERATION : 100, loss : 0.09054460998843702
ITERATION : 1, loss : 0.08002897261841507ITERATION : 2, loss : 0.09244986602169704ITERATION : 3, loss : 0.09687733587157743ITERATION : 4, loss : 0.09955911277988284ITERATION : 5, loss : 0.1011651848777545ITERATION : 6, loss : 0.10223587396178307ITERATION : 7, loss : 0.10295678108067645ITERATION : 8, loss : 0.10344654696190334ITERATION : 9, loss : 0.10378162504186782ITERATION : 10, loss : 0.10401206002470292ITERATION : 11, loss : 0.10417111689558656ITERATION : 12, loss : 0.10428119018630225ITERATION : 13, loss : 0.10435750337549965ITERATION : 14, loss : 0.10441047868401994ITERATION : 15, loss : 0.10444728716350524ITERATION : 16, loss : 0.10447288017013184ITERATION : 17, loss : 0.10449068460018834ITERATION : 18, loss : 0.10450307620132226ITERATION : 19, loss : 0.10451170387968567ITERATION : 20, loss : 0.10451771301191491ITERATION : 21, loss : 0.10452189973005943ITERATION : 22, loss : 0.10452481765673116ITERATION : 23, loss : 0.10452685192314776ITERATION : 24, loss : 0.10452827058717144ITERATION : 25, loss : 0.10452926025091994ITERATION : 26, loss : 0.10452995084708783ITERATION : 27, loss : 0.10453043289575076ITERATION : 28, loss : 0.10453076949269992ITERATION : 29, loss : 0.10453100456704953ITERATION : 30, loss : 0.10453116880950651ITERATION : 31, loss : 0.10453128358395024ITERATION : 32, loss : 0.10453136383474693ITERATION : 33, loss : 0.10453141994699343ITERATION : 34, loss : 0.1045314591951308ITERATION : 35, loss : 0.10453148665944313ITERATION : 36, loss : 0.10453150588960715ITERATION : 37, loss : 0.10453151933281923ITERATION : 38, loss : 0.1045315287475147ITERATION : 39, loss : 0.10453153534265837ITERATION : 40, loss : 0.10453153997265122ITERATION : 41, loss : 0.10453154320091798ITERATION : 42, loss : 0.10453154547687082ITERATION : 43, loss : 0.10453154707670943ITERATION : 44, loss : 0.10453154818633296ITERATION : 45, loss : 0.10453154898098825ITERATION : 46, loss : 0.10453154951364091ITERATION : 47, loss : 0.1045315498973181ITERATION : 48, loss : 0.10453155015357328ITERATION : 49, loss : 0.10453155035076232ITERATION : 50, loss : 0.10453155046738519ITERATION : 51, loss : 0.10453155057409688ITERATION : 52, loss : 0.10453155062725213ITERATION : 53, loss : 0.10453155067205042ITERATION : 54, loss : 0.10453155069477094ITERATION : 55, loss : 0.1045315507294184ITERATION : 56, loss : 0.1045315507324811ITERATION : 57, loss : 0.1045315507503486ITERATION : 58, loss : 0.10453155074604138ITERATION : 59, loss : 0.10453155074612115ITERATION : 60, loss : 0.10453155074612115ITERATION : 61, loss : 0.10453155074612115ITERATION : 62, loss : 0.10453155074612115ITERATION : 63, loss : 0.10453155074612115ITERATION : 64, loss : 0.10453155074612115ITERATION : 65, loss : 0.10453155074612115ITERATION : 66, loss : 0.10453155074612115ITERATION : 67, loss : 0.10453155074612115ITERATION : 68, loss : 0.10453155074612115ITERATION : 69, loss : 0.10453155074612115ITERATION : 70, loss : 0.10453155074612115ITERATION : 71, loss : 0.10453155074612115ITERATION : 72, loss : 0.10453155074612115ITERATION : 73, loss : 0.10453155074612115ITERATION : 74, loss : 0.10453155074612115ITERATION : 75, loss : 0.10453155074612115ITERATION : 76, loss : 0.10453155074612115ITERATION : 77, loss : 0.10453155074612115ITERATION : 78, loss : 0.10453155074612115ITERATION : 79, loss : 0.10453155074612115ITERATION : 80, loss : 0.10453155074612115ITERATION : 81, loss : 0.10453155074612115ITERATION : 82, loss : 0.10453155074612115ITERATION : 83, loss : 0.10453155074612115ITERATION : 84, loss : 0.10453155074612115ITERATION : 85, loss : 0.10453155074612115ITERATION : 86, loss : 0.10453155074612115ITERATION : 87, loss : 0.10453155074612115ITERATION : 88, loss : 0.10453155074612115ITERATION : 89, loss : 0.10453155074612115ITERATION : 90, loss : 0.10453155074612115ITERATION : 91, loss : 0.10453155074612115ITERATION : 92, loss : 0.10453155074612115ITERATION : 93, loss : 0.10453155074612115ITERATION : 94, loss : 0.10453155074612115ITERATION : 95, loss : 0.10453155074612115ITERATION : 96, loss : 0.10453155074612115ITERATION : 97, loss : 0.10453155074612115ITERATION : 98, loss : 0.10453155074612115ITERATION : 99, loss : 0.10453155074612115ITERATION : 100, loss : 0.10453155074612115
gradient norm in None layer : 0.020337861978026495
gradient norm in None layer : 0.000459577325133277
gradient norm in None layer : 0.00032420786255901333
gradient norm in None layer : 0.007148954981859284
gradient norm in None layer : 0.0004055064507111564
gradient norm in None layer : 0.0003449793750191334
gradient norm in None layer : 0.002345977389996958
gradient norm in None layer : 8.65867597754654e-05
gradient norm in None layer : 8.0475998297844e-05
gradient norm in None layer : 0.0019568063497518618
gradient norm in None layer : 9.05648866578761e-05
gradient norm in None layer : 8.144236479301658e-05
gradient norm in None layer : 0.0006785866724666755
gradient norm in None layer : 1.9462723204079143e-05
gradient norm in None layer : 1.6495731315611903e-05
gradient norm in None layer : 0.0006092550627068991
gradient norm in None layer : 2.1029837160269053e-05
gradient norm in None layer : 2.119205889769493e-05
gradient norm in None layer : 0.0007845398310644527
gradient norm in None layer : 9.743336521132195e-06
gradient norm in None layer : 0.0018181104495948149
gradient norm in None layer : 8.19044988313539e-05
gradient norm in None layer : 7.320823973648344e-05
gradient norm in None layer : 0.0018643042791510855
gradient norm in None layer : 0.000171101876414424
gradient norm in None layer : 0.00021768202637489597
gradient norm in None layer : 0.0038275418101806746
gradient norm in None layer : 2.4002400874943573e-05
gradient norm in None layer : 0.005430000404255592
gradient norm in None layer : 0.0003997241858425046
gradient norm in None layer : 0.0002713447315110966
gradient norm in None layer : 0.006576438745610797
gradient norm in None layer : 0.0015909812564583606
gradient norm in None layer : 0.0022400522303680135
gradient norm in None layer : 0.0010358046678758826
gradient norm in None layer : 0.0003745982915332166
Total gradient norm: 0.02407066561234417
invariance loss : 6.472886641890012, avg_den : 0.29674530029296875, density loss : 0.1981446883098855, mse loss : 0.09958693229400425, solver time : 109.55573892593384 sec , total loss : 0.10625796362420414, running loss : 0.11601189974799749
Epoch 0/10 , batch 4/12500 
ITERATION : 1, loss : 0.07004666520041136ITERATION : 2, loss : 0.09257859227598791ITERATION : 3, loss : 0.10221515855705937ITERATION : 4, loss : 0.10862429831127676ITERATION : 5, loss : 0.11281456750322055ITERATION : 6, loss : 0.11562775546617278ITERATION : 7, loss : 0.11754321214084788ITERATION : 8, loss : 0.11885761412347677ITERATION : 9, loss : 0.1197634948574374ITERATION : 10, loss : 0.12038932978798435ITERATION : 11, loss : 0.12082226384457058ITERATION : 12, loss : 0.12112795555997645ITERATION : 13, loss : 0.12134062465845193ITERATION : 14, loss : 0.12148786959062183ITERATION : 15, loss : 0.12158983506387137ITERATION : 16, loss : 0.12166045858536491ITERATION : 17, loss : 0.1217093859119636ITERATION : 18, loss : 0.12174329281391243ITERATION : 19, loss : 0.12176679958968593ITERATION : 20, loss : 0.12178310372021683ITERATION : 21, loss : 0.12179441825358403ITERATION : 22, loss : 0.12180227499204586ITERATION : 23, loss : 0.12180773440827528ITERATION : 24, loss : 0.12181153079825516ITERATION : 25, loss : 0.12181417300792537ITERATION : 26, loss : 0.12181601344336278ITERATION : 27, loss : 0.1218172966476517ITERATION : 28, loss : 0.121818192236081ITERATION : 29, loss : 0.121818817922618ITERATION : 30, loss : 0.12181925554054285ITERATION : 31, loss : 0.1218195619676579ITERATION : 32, loss : 0.12181977681964472ITERATION : 33, loss : 0.12181992763848823ITERATION : 34, loss : 0.12182003362961082ITERATION : 35, loss : 0.12182010820757598ITERATION : 36, loss : 0.1218201607607036ITERATION : 37, loss : 0.12182019784555778ITERATION : 38, loss : 0.12182022406019002ITERATION : 39, loss : 0.12182024261786259ITERATION : 40, loss : 0.12182025578320423ITERATION : 41, loss : 0.1218202651418884ITERATION : 42, loss : 0.12182027177761032ITERATION : 43, loss : 0.12182027649304371ITERATION : 44, loss : 0.12182027988480475ITERATION : 45, loss : 0.12182028229251818ITERATION : 46, loss : 0.12182028401551886ITERATION : 47, loss : 0.12182028526285972ITERATION : 48, loss : 0.12182028613187125ITERATION : 49, loss : 0.12182028677105966ITERATION : 50, loss : 0.12182028722989166ITERATION : 51, loss : 0.12182028756038159ITERATION : 52, loss : 0.12182028778779826ITERATION : 53, loss : 0.12182028793346993ITERATION : 54, loss : 0.12182028805304887ITERATION : 55, loss : 0.12182028814238684ITERATION : 56, loss : 0.12182028821815447ITERATION : 57, loss : 0.12182028823714437ITERATION : 58, loss : 0.12182028827893832ITERATION : 59, loss : 0.12182028829351405ITERATION : 60, loss : 0.12182028829472714ITERATION : 61, loss : 0.12182028829472714ITERATION : 62, loss : 0.12182028829472714ITERATION : 63, loss : 0.12182028829472714ITERATION : 64, loss : 0.12182028829472714ITERATION : 65, loss : 0.12182028829472714ITERATION : 66, loss : 0.12182028829472714ITERATION : 67, loss : 0.12182028829472714ITERATION : 68, loss : 0.12182028829472714ITERATION : 69, loss : 0.12182028829472714ITERATION : 70, loss : 0.12182028829472714ITERATION : 71, loss : 0.12182028829472714ITERATION : 72, loss : 0.12182028829472714ITERATION : 73, loss : 0.12182028829472714ITERATION : 74, loss : 0.12182028829472714ITERATION : 75, loss : 0.12182028829472714ITERATION : 76, loss : 0.12182028829472714ITERATION : 77, loss : 0.12182028829472714ITERATION : 78, loss : 0.12182028829472714ITERATION : 79, loss : 0.12182028829472714ITERATION : 80, loss : 0.12182028829472714ITERATION : 81, loss : 0.12182028829472714ITERATION : 82, loss : 0.12182028829472714ITERATION : 83, loss : 0.12182028829472714ITERATION : 84, loss : 0.12182028829472714ITERATION : 85, loss : 0.12182028829472714ITERATION : 86, loss : 0.12182028829472714ITERATION : 87, loss : 0.12182028829472714ITERATION : 88, loss : 0.12182028829472714ITERATION : 89, loss : 0.12182028829472714ITERATION : 90, loss : 0.12182028829472714ITERATION : 91, loss : 0.12182028829472714ITERATION : 92, loss : 0.12182028829472714ITERATION : 93, loss : 0.12182028829472714ITERATION : 94, loss : 0.12182028829472714ITERATION : 95, loss : 0.12182028829472714ITERATION : 96, loss : 0.12182028829472714ITERATION : 97, loss : 0.12182028829472714ITERATION : 98, loss : 0.12182028829472714ITERATION : 99, loss : 0.12182028829472714ITERATION : 100, loss : 0.12182028829472714
ITERATION : 1, loss : 0.015733264418117104ITERATION : 2, loss : 0.027512374676952625ITERATION : 3, loss : 0.035853112474404245ITERATION : 4, loss : 0.04114896207317788ITERATION : 5, loss : 0.044758923980067464ITERATION : 6, loss : 0.04721504745694234ITERATION : 7, loss : 0.048889953882689116ITERATION : 8, loss : 0.05003643978180077ITERATION : 9, loss : 0.050824242887814564ITERATION : 10, loss : 0.05136739551385967ITERATION : 11, loss : 0.051742887806399705ITERATION : 12, loss : 0.052003017688471986ITERATION : 13, loss : 0.05218351351600072ITERATION : 14, loss : 0.052308901900959576ITERATION : 15, loss : 0.05239608415759432ITERATION : 16, loss : 0.05245674111212078ITERATION : 17, loss : 0.052498963659457595ITERATION : 18, loss : 0.052528364927499155ITERATION : 19, loss : 0.05254884389715324ITERATION : 20, loss : 0.052563111331569ITERATION : 21, loss : 0.052573053009909713ITERATION : 22, loss : 0.052579981413365284ITERATION : 23, loss : 0.05258481043952162ITERATION : 24, loss : 0.05258817660813115ITERATION : 25, loss : 0.05259052320328362ITERATION : 26, loss : 0.05259215925700697ITERATION : 27, loss : 0.05259329995520059ITERATION : 28, loss : 0.05259409537693955ITERATION : 29, loss : 0.05259465007269359ITERATION : 30, loss : 0.052595036915198734ITERATION : 31, loss : 0.05259530672168386ITERATION : 32, loss : 0.052595494906348496ITERATION : 33, loss : 0.05259562618152993ITERATION : 34, loss : 0.05259571778423888ITERATION : 35, loss : 0.052595781652315515ITERATION : 36, loss : 0.05259582617998635ITERATION : 37, loss : 0.052595857272218875ITERATION : 38, loss : 0.05259587895451986ITERATION : 39, loss : 0.052595894078388945ITERATION : 40, loss : 0.05259590466878845ITERATION : 41, loss : 0.052595912023972666ITERATION : 42, loss : 0.05259591717132286ITERATION : 43, loss : 0.05259592073323009ITERATION : 44, loss : 0.05259592326615782ITERATION : 45, loss : 0.05259592498433019ITERATION : 46, loss : 0.052595926207781855ITERATION : 47, loss : 0.052595927080940096ITERATION : 48, loss : 0.05259592766278642ITERATION : 49, loss : 0.05259592802539072ITERATION : 50, loss : 0.05259592827602351ITERATION : 51, loss : 0.052595928501920394ITERATION : 52, loss : 0.05259592865866632ITERATION : 53, loss : 0.052595928753858504ITERATION : 54, loss : 0.05259592881187858ITERATION : 55, loss : 0.052595928815457334ITERATION : 56, loss : 0.052595928815457334ITERATION : 57, loss : 0.052595928815457334ITERATION : 58, loss : 0.052595928815457334ITERATION : 59, loss : 0.052595928815457334ITERATION : 60, loss : 0.052595928815457334ITERATION : 61, loss : 0.052595928815457334ITERATION : 62, loss : 0.052595928815457334ITERATION : 63, loss : 0.052595928815457334ITERATION : 64, loss : 0.052595928815457334ITERATION : 65, loss : 0.052595928815457334ITERATION : 66, loss : 0.052595928815457334ITERATION : 67, loss : 0.052595928815457334ITERATION : 68, loss : 0.052595928815457334ITERATION : 69, loss : 0.052595928815457334ITERATION : 70, loss : 0.052595928815457334ITERATION : 71, loss : 0.052595928815457334ITERATION : 72, loss : 0.052595928815457334ITERATION : 73, loss : 0.052595928815457334ITERATION : 74, loss : 0.052595928815457334ITERATION : 75, loss : 0.052595928815457334ITERATION : 76, loss : 0.052595928815457334ITERATION : 77, loss : 0.052595928815457334ITERATION : 78, loss : 0.052595928815457334ITERATION : 79, loss : 0.052595928815457334ITERATION : 80, loss : 0.052595928815457334ITERATION : 81, loss : 0.052595928815457334ITERATION : 82, loss : 0.052595928815457334ITERATION : 83, loss : 0.052595928815457334ITERATION : 84, loss : 0.052595928815457334ITERATION : 85, loss : 0.052595928815457334ITERATION : 86, loss : 0.052595928815457334ITERATION : 87, loss : 0.052595928815457334ITERATION : 88, loss : 0.052595928815457334ITERATION : 89, loss : 0.052595928815457334ITERATION : 90, loss : 0.052595928815457334ITERATION : 91, loss : 0.052595928815457334ITERATION : 92, loss : 0.052595928815457334ITERATION : 93, loss : 0.052595928815457334ITERATION : 94, loss : 0.052595928815457334ITERATION : 95, loss : 0.052595928815457334ITERATION : 96, loss : 0.052595928815457334ITERATION : 97, loss : 0.052595928815457334ITERATION : 98, loss : 0.052595928815457334ITERATION : 99, loss : 0.052595928815457334ITERATION : 100, loss : 0.052595928815457334
ITERATION : 1, loss : 0.0741670408579061ITERATION : 2, loss : 0.07571143801948346ITERATION : 3, loss : 0.08004930754344144ITERATION : 4, loss : 0.08346467219045418ITERATION : 5, loss : 0.08654364625224222ITERATION : 6, loss : 0.08898901803372819ITERATION : 7, loss : 0.09035826933327706ITERATION : 8, loss : 0.09121174099445148ITERATION : 9, loss : 0.09173534892658869ITERATION : 10, loss : 0.09205050021064144ITERATION : 11, loss : 0.09223562875636432ITERATION : 12, loss : 0.09234087212854834ITERATION : 13, loss : 0.09239791930098736ITERATION : 14, loss : 0.09242654889374165ITERATION : 15, loss : 0.09243892819706488ITERATION : 16, loss : 0.09244241344173913ITERATION : 17, loss : 0.09244135684723723ITERATION : 18, loss : 0.09243826028835629ITERATION : 19, loss : 0.09243450502687836ITERATION : 20, loss : 0.09243080739157725ITERATION : 21, loss : 0.09242749927465634ITERATION : 22, loss : 0.09242469886371538ITERATION : 23, loss : 0.09242241208830614ITERATION : 24, loss : 0.09242059121641089ITERATION : 25, loss : 0.09241916816433193ITERATION : 26, loss : 0.09241807177971792ITERATION : 27, loss : 0.09241723658923454ITERATION : 28, loss : 0.09241660608827001ITERATION : 29, loss : 0.09241613375111843ITERATION : 30, loss : 0.0924157820715759ITERATION : 31, loss : 0.09241552164606698ITERATION : 32, loss : 0.0924153296739685ITERATION : 33, loss : 0.09241518864774591ITERATION : 34, loss : 0.09241508545673068ITERATION : 35, loss : 0.09241501016248216ITERATION : 36, loss : 0.0924149554361574ITERATION : 37, loss : 0.09241491566045659ITERATION : 38, loss : 0.09241488686262019ITERATION : 39, loss : 0.09241486603735637ITERATION : 40, loss : 0.09241485099995361ITERATION : 41, loss : 0.09241484015247746ITERATION : 42, loss : 0.09241483237544788ITERATION : 43, loss : 0.09241482679537216ITERATION : 44, loss : 0.0924148227507982ITERATION : 45, loss : 0.09241481985181767ITERATION : 46, loss : 0.0924148177572281ITERATION : 47, loss : 0.09241481627368948ITERATION : 48, loss : 0.09241481518468474ITERATION : 49, loss : 0.09241481434017107ITERATION : 50, loss : 0.09241481382900116ITERATION : 51, loss : 0.0924148133624607ITERATION : 52, loss : 0.09241481311937932ITERATION : 53, loss : 0.09241481301770857ITERATION : 54, loss : 0.09241481289528461ITERATION : 55, loss : 0.09241481283561138ITERATION : 56, loss : 0.09241481276609056ITERATION : 57, loss : 0.09241481276456405ITERATION : 58, loss : 0.09241481276456405ITERATION : 59, loss : 0.09241481276456405ITERATION : 60, loss : 0.09241481276456405ITERATION : 61, loss : 0.09241481276456405ITERATION : 62, loss : 0.09241481276456405ITERATION : 63, loss : 0.09241481276456405ITERATION : 64, loss : 0.09241481276456405ITERATION : 65, loss : 0.09241481276456405ITERATION : 66, loss : 0.09241481276456405ITERATION : 67, loss : 0.09241481276456405ITERATION : 68, loss : 0.09241481276456405ITERATION : 69, loss : 0.09241481276456405ITERATION : 70, loss : 0.09241481276456405ITERATION : 71, loss : 0.09241481276456405ITERATION : 72, loss : 0.09241481276456405ITERATION : 73, loss : 0.09241481276456405ITERATION : 74, loss : 0.09241481276456405ITERATION : 75, loss : 0.09241481276456405ITERATION : 76, loss : 0.09241481276456405ITERATION : 77, loss : 0.09241481276456405ITERATION : 78, loss : 0.09241481276456405ITERATION : 79, loss : 0.09241481276456405ITERATION : 80, loss : 0.09241481276456405ITERATION : 81, loss : 0.09241481276456405ITERATION : 82, loss : 0.09241481276456405ITERATION : 83, loss : 0.09241481276456405ITERATION : 84, loss : 0.09241481276456405ITERATION : 85, loss : 0.09241481276456405ITERATION : 86, loss : 0.09241481276456405ITERATION : 87, loss : 0.09241481276456405ITERATION : 88, loss : 0.09241481276456405ITERATION : 89, loss : 0.09241481276456405ITERATION : 90, loss : 0.09241481276456405ITERATION : 91, loss : 0.09241481276456405ITERATION : 92, loss : 0.09241481276456405ITERATION : 93, loss : 0.09241481276456405ITERATION : 94, loss : 0.09241481276456405ITERATION : 95, loss : 0.09241481276456405ITERATION : 96, loss : 0.09241481276456405ITERATION : 97, loss : 0.09241481276456405ITERATION : 98, loss : 0.09241481276456405ITERATION : 99, loss : 0.09241481276456405ITERATION : 100, loss : 0.09241481276456405
ITERATION : 1, loss : 0.09497612446992351ITERATION : 2, loss : 0.1321254983047399ITERATION : 3, loss : 0.15279450200113406ITERATION : 4, loss : 0.1653395719069723ITERATION : 5, loss : 0.1733712843417587ITERATION : 6, loss : 0.17868767699091526ITERATION : 7, loss : 0.1822858252884823ITERATION : 8, loss : 0.18475901820857588ITERATION : 9, loss : 0.18647782644674077ITERATION : 10, loss : 0.18768193721458676ITERATION : 11, loss : 0.18853043027580293ITERATION : 12, loss : 0.1891309329956904ITERATION : 13, loss : 0.1895573131962859ITERATION : 14, loss : 0.18986081235611352ITERATION : 15, loss : 0.19007726059644925ITERATION : 16, loss : 0.1902318612765805ITERATION : 17, loss : 0.19034242236153426ITERATION : 18, loss : 0.19042156908618088ITERATION : 19, loss : 0.19047827585127067ITERATION : 20, loss : 0.19051893469105993ITERATION : 21, loss : 0.1905481060359229ITERATION : 22, loss : 0.1905690474224921ITERATION : 23, loss : 0.19058408858425255ITERATION : 24, loss : 0.19059489699448598ITERATION : 25, loss : 0.19060266713922738ITERATION : 26, loss : 0.19060825539047144ITERATION : 27, loss : 0.19061227593717311ITERATION : 28, loss : 0.19061516959505564ITERATION : 29, loss : 0.1906172529312783ITERATION : 30, loss : 0.19061875323003122ITERATION : 31, loss : 0.19061983406603009ITERATION : 32, loss : 0.19062061289658286ITERATION : 33, loss : 0.19062117427163924ITERATION : 34, loss : 0.19062157900577964ITERATION : 35, loss : 0.1906218708639639ITERATION : 36, loss : 0.1906220814220942ITERATION : 37, loss : 0.19062223330893566ITERATION : 38, loss : 0.19062234289361601ITERATION : 39, loss : 0.1906224220176323ITERATION : 40, loss : 0.19062247911452876ITERATION : 41, loss : 0.1906225203324412ITERATION : 42, loss : 0.190622550108586ITERATION : 43, loss : 0.19062257164014654ITERATION : 44, loss : 0.19062258717867422ITERATION : 45, loss : 0.19062259838591866ITERATION : 46, loss : 0.1906226064472641ITERATION : 47, loss : 0.19062261230730698ITERATION : 48, loss : 0.1906226165169019ITERATION : 49, loss : 0.19062261957916546ITERATION : 50, loss : 0.19062262174641814ITERATION : 51, loss : 0.1906226233590192ITERATION : 52, loss : 0.1906226245348907ITERATION : 53, loss : 0.1906226253508842ITERATION : 54, loss : 0.19062262598727683ITERATION : 55, loss : 0.19062262640724806ITERATION : 56, loss : 0.19062262672175737ITERATION : 57, loss : 0.19062262695175106ITERATION : 58, loss : 0.19062262710566774ITERATION : 59, loss : 0.19062262718481265ITERATION : 60, loss : 0.19062262728921398ITERATION : 61, loss : 0.19062262728963247ITERATION : 62, loss : 0.19062262728963247ITERATION : 63, loss : 0.19062262728963247ITERATION : 64, loss : 0.19062262728963247ITERATION : 65, loss : 0.19062262728963247ITERATION : 66, loss : 0.19062262728963247ITERATION : 67, loss : 0.19062262728963247ITERATION : 68, loss : 0.19062262728963247ITERATION : 69, loss : 0.19062262728963247ITERATION : 70, loss : 0.19062262728963247ITERATION : 71, loss : 0.19062262728963247ITERATION : 72, loss : 0.19062262728963247ITERATION : 73, loss : 0.19062262728963247ITERATION : 74, loss : 0.19062262728963247ITERATION : 75, loss : 0.19062262728963247ITERATION : 76, loss : 0.19062262728963247ITERATION : 77, loss : 0.19062262728963247ITERATION : 78, loss : 0.19062262728963247ITERATION : 79, loss : 0.19062262728963247ITERATION : 80, loss : 0.19062262728963247ITERATION : 81, loss : 0.19062262728963247ITERATION : 82, loss : 0.19062262728963247ITERATION : 83, loss : 0.19062262728963247ITERATION : 84, loss : 0.19062262728963247ITERATION : 85, loss : 0.19062262728963247ITERATION : 86, loss : 0.19062262728963247ITERATION : 87, loss : 0.19062262728963247ITERATION : 88, loss : 0.19062262728963247ITERATION : 89, loss : 0.19062262728963247ITERATION : 90, loss : 0.19062262728963247ITERATION : 91, loss : 0.19062262728963247ITERATION : 92, loss : 0.19062262728963247ITERATION : 93, loss : 0.19062262728963247ITERATION : 94, loss : 0.19062262728963247ITERATION : 95, loss : 0.19062262728963247ITERATION : 96, loss : 0.19062262728963247ITERATION : 97, loss : 0.19062262728963247ITERATION : 98, loss : 0.19062262728963247ITERATION : 99, loss : 0.19062262728963247ITERATION : 100, loss : 0.19062262728963247
ITERATION : 1, loss : 0.06303477938790743ITERATION : 2, loss : 0.06313359714749084ITERATION : 3, loss : 0.0652523363579098ITERATION : 4, loss : 0.06947604656074027ITERATION : 5, loss : 0.07238313605734509ITERATION : 6, loss : 0.07437585322343675ITERATION : 7, loss : 0.07574347577430852ITERATION : 8, loss : 0.07668220028887658ITERATION : 9, loss : 0.07732536993902074ITERATION : 10, loss : 0.07776449169711283ITERATION : 11, loss : 0.07806281587805622ITERATION : 12, loss : 0.07826421903769423ITERATION : 13, loss : 0.0783991588947538ITERATION : 14, loss : 0.07848874815016568ITERATION : 15, loss : 0.0785475791335532ITERATION : 16, loss : 0.0785856967543669ITERATION : 17, loss : 0.07860998145652787ITERATION : 18, loss : 0.07862511882216275ITERATION : 19, loss : 0.07863427854704272ITERATION : 20, loss : 0.07863958839068712ITERATION : 21, loss : 0.07864246394735616ITERATION : 22, loss : 0.0786438372151954ITERATION : 23, loss : 0.07864431402777208ITERATION : 24, loss : 0.07864428206386347ITERATION : 25, loss : 0.07864398432382048ITERATION : 26, loss : 0.0786435690852574ITERATION : 27, loss : 0.07864312347115973ITERATION : 28, loss : 0.07864269575699316ITERATION : 29, loss : 0.07864230992991875ITERATION : 30, loss : 0.07864197547239071ITERATION : 31, loss : 0.07864169349449955ITERATION : 32, loss : 0.07864146058888424ITERATION : 33, loss : 0.07864127121186983ITERATION : 34, loss : 0.07864111915128523ITERATION : 35, loss : 0.07864099829231457ITERATION : 36, loss : 0.07864090304972148ITERATION : 37, loss : 0.0786408285201531ITERATION : 38, loss : 0.07864077056490182ITERATION : 39, loss : 0.07864072573380873ITERATION : 40, loss : 0.07864069125992071ITERATION : 41, loss : 0.07864066482973274ITERATION : 42, loss : 0.07864064470834403ITERATION : 43, loss : 0.07864062933154681ITERATION : 44, loss : 0.07864061767641604ITERATION : 45, loss : 0.07864060885041874ITERATION : 46, loss : 0.0786406022074484ITERATION : 47, loss : 0.07864059721355579ITERATION : 48, loss : 0.07864059347477387ITERATION : 49, loss : 0.07864059071957422ITERATION : 50, loss : 0.07864058862158933ITERATION : 51, loss : 0.07864058703325895ITERATION : 52, loss : 0.07864058580856353ITERATION : 53, loss : 0.07864058495580856ITERATION : 54, loss : 0.07864058436467884ITERATION : 55, loss : 0.0786405838675508ITERATION : 56, loss : 0.07864058350214662ITERATION : 57, loss : 0.07864058319271691ITERATION : 58, loss : 0.07864058304307366ITERATION : 59, loss : 0.07864058294000285ITERATION : 60, loss : 0.07864058283369402ITERATION : 61, loss : 0.07864058271904892ITERATION : 62, loss : 0.07864058271538049ITERATION : 63, loss : 0.07864058271492125ITERATION : 64, loss : 0.07864058271492125ITERATION : 65, loss : 0.07864058271492125ITERATION : 66, loss : 0.07864058271492125ITERATION : 67, loss : 0.07864058271492125ITERATION : 68, loss : 0.07864058271492125ITERATION : 69, loss : 0.07864058271492125ITERATION : 70, loss : 0.07864058271492125ITERATION : 71, loss : 0.07864058271492125ITERATION : 72, loss : 0.07864058271492125ITERATION : 73, loss : 0.07864058271492125ITERATION : 74, loss : 0.07864058271492125ITERATION : 75, loss : 0.07864058271492125ITERATION : 76, loss : 0.07864058271492125ITERATION : 77, loss : 0.07864058271492125ITERATION : 78, loss : 0.07864058271492125ITERATION : 79, loss : 0.07864058271492125ITERATION : 80, loss : 0.07864058271492125ITERATION : 81, loss : 0.07864058271492125ITERATION : 82, loss : 0.07864058271492125ITERATION : 83, loss : 0.07864058271492125ITERATION : 84, loss : 0.07864058271492125ITERATION : 85, loss : 0.07864058271492125ITERATION : 86, loss : 0.07864058271492125ITERATION : 87, loss : 0.07864058271492125ITERATION : 88, loss : 0.07864058271492125ITERATION : 89, loss : 0.07864058271492125ITERATION : 90, loss : 0.07864058271492125ITERATION : 91, loss : 0.07864058271492125ITERATION : 92, loss : 0.07864058271492125ITERATION : 93, loss : 0.07864058271492125ITERATION : 94, loss : 0.07864058271492125ITERATION : 95, loss : 0.07864058271492125ITERATION : 96, loss : 0.07864058271492125ITERATION : 97, loss : 0.07864058271492125ITERATION : 98, loss : 0.07864058271492125ITERATION : 99, loss : 0.07864058271492125ITERATION : 100, loss : 0.07864058271492125
ITERATION : 1, loss : 0.05987121954132398ITERATION : 2, loss : 0.060824532744866405ITERATION : 3, loss : 0.06471633653684856ITERATION : 4, loss : 0.06792850283920121ITERATION : 5, loss : 0.07031456477793142ITERATION : 6, loss : 0.07203450805436437ITERATION : 7, loss : 0.07325770615272004ITERATION : 8, loss : 0.07412092948084074ITERATION : 9, loss : 0.07472703619351749ITERATION : 10, loss : 0.07515109979424027ITERATION : 11, loss : 0.07544702783177637ITERATION : 12, loss : 0.07565313582240193ITERATION : 13, loss : 0.07579646990387652ITERATION : 14, loss : 0.07589602903412584ITERATION : 15, loss : 0.0759651138355296ITERATION : 16, loss : 0.0760130115729828ITERATION : 17, loss : 0.07604619488832305ITERATION : 18, loss : 0.07606916814368207ITERATION : 19, loss : 0.07608506233527779ITERATION : 20, loss : 0.07609605179659348ITERATION : 21, loss : 0.07610364515555222ITERATION : 22, loss : 0.0761088885498667ITERATION : 23, loss : 0.07611250682485199ITERATION : 24, loss : 0.07611500197823562ITERATION : 25, loss : 0.07611672147716309ITERATION : 26, loss : 0.07611790558787619ITERATION : 27, loss : 0.07611872032573783ITERATION : 28, loss : 0.07611928046583982ITERATION : 29, loss : 0.07611966522471333ITERATION : 30, loss : 0.07611992929804921ITERATION : 31, loss : 0.07612011044812005ITERATION : 32, loss : 0.07612023446760378ITERATION : 33, loss : 0.07612031929324105ITERATION : 34, loss : 0.0761203773886245ITERATION : 35, loss : 0.07612041695711855ITERATION : 36, loss : 0.07612044392477606ITERATION : 37, loss : 0.07612046227504538ITERATION : 38, loss : 0.0761204748239455ITERATION : 39, loss : 0.07612048329907682ITERATION : 40, loss : 0.07612048916310415ITERATION : 41, loss : 0.07612049302912934ITERATION : 42, loss : 0.0761204956242691ITERATION : 43, loss : 0.07612049732314402ITERATION : 44, loss : 0.07612049846116871ITERATION : 45, loss : 0.07612049916108998ITERATION : 46, loss : 0.07612049966449544ITERATION : 47, loss : 0.07612049995683483ITERATION : 48, loss : 0.07612050017262792ITERATION : 49, loss : 0.07612050026625414ITERATION : 50, loss : 0.07612050036243734ITERATION : 51, loss : 0.07612050039845811ITERATION : 52, loss : 0.07612050044978358ITERATION : 53, loss : 0.07612050043511591ITERATION : 54, loss : 0.07612050047412001ITERATION : 55, loss : 0.07612050045693411ITERATION : 56, loss : 0.07612050046577291ITERATION : 57, loss : 0.07612050046678068ITERATION : 58, loss : 0.07612050046678068ITERATION : 59, loss : 0.07612050046678068ITERATION : 60, loss : 0.07612050046678068ITERATION : 61, loss : 0.07612050046678068ITERATION : 62, loss : 0.07612050046678068ITERATION : 63, loss : 0.07612050046678068ITERATION : 64, loss : 0.07612050046678068ITERATION : 65, loss : 0.07612050046678068ITERATION : 66, loss : 0.07612050046678068ITERATION : 67, loss : 0.07612050046678068ITERATION : 68, loss : 0.07612050046678068ITERATION : 69, loss : 0.07612050046678068ITERATION : 70, loss : 0.07612050046678068ITERATION : 71, loss : 0.07612050046678068ITERATION : 72, loss : 0.07612050046678068ITERATION : 73, loss : 0.07612050046678068ITERATION : 74, loss : 0.07612050046678068ITERATION : 75, loss : 0.07612050046678068ITERATION : 76, loss : 0.07612050046678068ITERATION : 77, loss : 0.07612050046678068ITERATION : 78, loss : 0.07612050046678068ITERATION : 79, loss : 0.07612050046678068ITERATION : 80, loss : 0.07612050046678068ITERATION : 81, loss : 0.07612050046678068ITERATION : 82, loss : 0.07612050046678068ITERATION : 83, loss : 0.07612050046678068ITERATION : 84, loss : 0.07612050046678068ITERATION : 85, loss : 0.07612050046678068ITERATION : 86, loss : 0.07612050046678068ITERATION : 87, loss : 0.07612050046678068ITERATION : 88, loss : 0.07612050046678068ITERATION : 89, loss : 0.07612050046678068ITERATION : 90, loss : 0.07612050046678068ITERATION : 91, loss : 0.07612050046678068ITERATION : 92, loss : 0.07612050046678068ITERATION : 93, loss : 0.07612050046678068ITERATION : 94, loss : 0.07612050046678068ITERATION : 95, loss : 0.07612050046678068ITERATION : 96, loss : 0.07612050046678068ITERATION : 97, loss : 0.07612050046678068ITERATION : 98, loss : 0.07612050046678068ITERATION : 99, loss : 0.07612050046678068ITERATION : 100, loss : 0.07612050046678068
ITERATION : 1, loss : 0.052522251558978265ITERATION : 2, loss : 0.05993746231619173ITERATION : 3, loss : 0.06674714881787011ITERATION : 4, loss : 0.0719075689987856ITERATION : 5, loss : 0.0756171358136102ITERATION : 6, loss : 0.07822583091505836ITERATION : 7, loss : 0.08004138231012826ITERATION : 8, loss : 0.08129829977398452ITERATION : 9, loss : 0.08216601070084552ITERATION : 10, loss : 0.08276405989190533ITERATION : 11, loss : 0.0831758378910369ITERATION : 12, loss : 0.0834591740485579ITERATION : 13, loss : 0.08365404224667447ITERATION : 14, loss : 0.08378802104681554ITERATION : 15, loss : 0.08388011411796319ITERATION : 16, loss : 0.0839434050579935ITERATION : 17, loss : 0.08398689650001553ITERATION : 18, loss : 0.08401677988870244ITERATION : 19, loss : 0.08403731206780786ITERATION : 20, loss : 0.08405141893507195ITERATION : 21, loss : 0.08406111116598121ITERATION : 22, loss : 0.08406777040523296ITERATION : 23, loss : 0.0840723459325585ITERATION : 24, loss : 0.08407548983288847ITERATION : 25, loss : 0.0840776501579902ITERATION : 26, loss : 0.0840791346926396ITERATION : 27, loss : 0.08408015491437124ITERATION : 28, loss : 0.08408085609581849ITERATION : 29, loss : 0.08408133802762266ITERATION : 30, loss : 0.08408166929629808ITERATION : 31, loss : 0.08408189700375299ITERATION : 32, loss : 0.08408205356049513ITERATION : 33, loss : 0.08408216119083434ITERATION : 34, loss : 0.08408223518188165ITERATION : 35, loss : 0.08408228605553868ITERATION : 36, loss : 0.08408232105673938ITERATION : 37, loss : 0.08408234508932344ITERATION : 38, loss : 0.08408236164255364ITERATION : 39, loss : 0.08408237303538206ITERATION : 40, loss : 0.08408238087541756ITERATION : 41, loss : 0.08408238628678733ITERATION : 42, loss : 0.08408238997985368ITERATION : 43, loss : 0.08408239249609505ITERATION : 44, loss : 0.08408239419917736ITERATION : 45, loss : 0.08408239538592932ITERATION : 46, loss : 0.08408239623523826ITERATION : 47, loss : 0.0840823968333748ITERATION : 48, loss : 0.08408239718785808ITERATION : 49, loss : 0.08408239739675394ITERATION : 50, loss : 0.08408239762123527ITERATION : 51, loss : 0.08408239768396819ITERATION : 52, loss : 0.08408239778785796ITERATION : 53, loss : 0.08408239786824212ITERATION : 54, loss : 0.08408239787580239ITERATION : 55, loss : 0.08408239787580239ITERATION : 56, loss : 0.08408239787580239ITERATION : 57, loss : 0.08408239787580239ITERATION : 58, loss : 0.08408239787580239ITERATION : 59, loss : 0.08408239787580239ITERATION : 60, loss : 0.08408239787580239ITERATION : 61, loss : 0.08408239787580239ITERATION : 62, loss : 0.08408239787580239ITERATION : 63, loss : 0.08408239787580239ITERATION : 64, loss : 0.08408239787580239ITERATION : 65, loss : 0.08408239787580239ITERATION : 66, loss : 0.08408239787580239ITERATION : 67, loss : 0.08408239787580239ITERATION : 68, loss : 0.08408239787580239ITERATION : 69, loss : 0.08408239787580239ITERATION : 70, loss : 0.08408239787580239ITERATION : 71, loss : 0.08408239787580239ITERATION : 72, loss : 0.08408239787580239ITERATION : 73, loss : 0.08408239787580239ITERATION : 74, loss : 0.08408239787580239ITERATION : 75, loss : 0.08408239787580239ITERATION : 76, loss : 0.08408239787580239ITERATION : 77, loss : 0.08408239787580239ITERATION : 78, loss : 0.08408239787580239ITERATION : 79, loss : 0.08408239787580239ITERATION : 80, loss : 0.08408239787580239ITERATION : 81, loss : 0.08408239787580239ITERATION : 82, loss : 0.08408239787580239ITERATION : 83, loss : 0.08408239787580239ITERATION : 84, loss : 0.08408239787580239ITERATION : 85, loss : 0.08408239787580239ITERATION : 86, loss : 0.08408239787580239ITERATION : 87, loss : 0.08408239787580239ITERATION : 88, loss : 0.08408239787580239ITERATION : 89, loss : 0.08408239787580239ITERATION : 90, loss : 0.08408239787580239ITERATION : 91, loss : 0.08408239787580239ITERATION : 92, loss : 0.08408239787580239ITERATION : 93, loss : 0.08408239787580239ITERATION : 94, loss : 0.08408239787580239ITERATION : 95, loss : 0.08408239787580239ITERATION : 96, loss : 0.08408239787580239ITERATION : 97, loss : 0.08408239787580239ITERATION : 98, loss : 0.08408239787580239ITERATION : 99, loss : 0.08408239787580239ITERATION : 100, loss : 0.08408239787580239
ITERATION : 1, loss : 0.026151266496928775ITERATION : 2, loss : 0.039821070975963116ITERATION : 3, loss : 0.048985803634970626ITERATION : 4, loss : 0.054868855257171216ITERATION : 5, loss : 0.05861757899685805ITERATION : 6, loss : 0.0609534086452103ITERATION : 7, loss : 0.06239785519820881ITERATION : 8, loss : 0.06328433301641105ITERATION : 9, loss : 0.06382373068218594ITERATION : 10, loss : 0.06414858256301492ITERATION : 11, loss : 0.06434174194344691ITERATION : 12, loss : 0.06445161030694735ITERATION : 13, loss : 0.06451267082604753ITERATION : 14, loss : 0.06454630124784705ITERATION : 15, loss : 0.06456390173130323ITERATION : 16, loss : 0.06457235020308061ITERATION : 17, loss : 0.064575740765609ITERATION : 18, loss : 0.06457646906528526ITERATION : 19, loss : 0.06457590591144194ITERATION : 20, loss : 0.06457481144467034ITERATION : 21, loss : 0.06457358784419245ITERATION : 22, loss : 0.06457243216524487ITERATION : 23, loss : 0.06457142735424794ITERATION : 24, loss : 0.06457059562733089ITERATION : 25, loss : 0.06456992910090258ITERATION : 26, loss : 0.0645694071374261ITERATION : 27, loss : 0.06456900529906899ITERATION : 28, loss : 0.06456869997409341ITERATION : 29, loss : 0.06456847035487945ITERATION : 30, loss : 0.06456829911284655ITERATION : 31, loss : 0.0645681722913606ITERATION : 32, loss : 0.06456807886789773ITERATION : 33, loss : 0.06456801038590504ITERATION : 34, loss : 0.06456796042470264ITERATION : 35, loss : 0.0645679240407332ITERATION : 36, loss : 0.0645678977022748ITERATION : 37, loss : 0.06456787857780565ITERATION : 38, loss : 0.06456786486633036ITERATION : 39, loss : 0.06456785495735387ITERATION : 40, loss : 0.0645678478665603ITERATION : 41, loss : 0.06456784273850129ITERATION : 42, loss : 0.06456783906449953ITERATION : 43, loss : 0.06456783641946263ITERATION : 44, loss : 0.0645678345462615ITERATION : 45, loss : 0.06456783319626487ITERATION : 46, loss : 0.06456783223670906ITERATION : 47, loss : 0.06456783150095062ITERATION : 48, loss : 0.06456783102788342ITERATION : 49, loss : 0.06456783072231936ITERATION : 50, loss : 0.06456783045836079ITERATION : 51, loss : 0.06456783028023681ITERATION : 52, loss : 0.06456783015887893ITERATION : 53, loss : 0.06456783004821119ITERATION : 54, loss : 0.06456783001362672ITERATION : 55, loss : 0.06456783001290138ITERATION : 56, loss : 0.06456783001290138ITERATION : 57, loss : 0.06456783001290138ITERATION : 58, loss : 0.06456783001290138ITERATION : 59, loss : 0.06456783001290138ITERATION : 60, loss : 0.06456783001290138ITERATION : 61, loss : 0.06456783001290138ITERATION : 62, loss : 0.06456783001290138ITERATION : 63, loss : 0.06456783001290138ITERATION : 64, loss : 0.06456783001290138ITERATION : 65, loss : 0.06456783001290138ITERATION : 66, loss : 0.06456783001290138ITERATION : 67, loss : 0.06456783001290138ITERATION : 68, loss : 0.06456783001290138ITERATION : 69, loss : 0.06456783001290138ITERATION : 70, loss : 0.06456783001290138ITERATION : 71, loss : 0.06456783001290138ITERATION : 72, loss : 0.06456783001290138ITERATION : 73, loss : 0.06456783001290138ITERATION : 74, loss : 0.06456783001290138ITERATION : 75, loss : 0.06456783001290138ITERATION : 76, loss : 0.06456783001290138ITERATION : 77, loss : 0.06456783001290138ITERATION : 78, loss : 0.06456783001290138ITERATION : 79, loss : 0.06456783001290138ITERATION : 80, loss : 0.06456783001290138ITERATION : 81, loss : 0.06456783001290138ITERATION : 82, loss : 0.06456783001290138ITERATION : 83, loss : 0.06456783001290138ITERATION : 84, loss : 0.06456783001290138ITERATION : 85, loss : 0.06456783001290138ITERATION : 86, loss : 0.06456783001290138ITERATION : 87, loss : 0.06456783001290138ITERATION : 88, loss : 0.06456783001290138ITERATION : 89, loss : 0.06456783001290138ITERATION : 90, loss : 0.06456783001290138ITERATION : 91, loss : 0.06456783001290138ITERATION : 92, loss : 0.06456783001290138ITERATION : 93, loss : 0.06456783001290138ITERATION : 94, loss : 0.06456783001290138ITERATION : 95, loss : 0.06456783001290138ITERATION : 96, loss : 0.06456783001290138ITERATION : 97, loss : 0.06456783001290138ITERATION : 98, loss : 0.06456783001290138ITERATION : 99, loss : 0.06456783001290138ITERATION : 100, loss : 0.06456783001290138
gradient norm in None layer : 0.011088067424777913
gradient norm in None layer : 0.00030493717706226836
gradient norm in None layer : 0.00042987562085344355
gradient norm in None layer : 0.005232008904841817
gradient norm in None layer : 0.0002859913961271198
gradient norm in None layer : 0.0003722182444557483
gradient norm in None layer : 0.0021840784503471076
gradient norm in None layer : 8.310932337380368e-05
gradient norm in None layer : 8.851020436553517e-05
gradient norm in None layer : 0.0018467382003551933
gradient norm in None layer : 9.43362651889547e-05
gradient norm in None layer : 8.16662818626655e-05
gradient norm in None layer : 0.0005009688108268707
gradient norm in None layer : 1.4543160501177308e-05
gradient norm in None layer : 1.3323045320644264e-05
gradient norm in None layer : 0.0004571809777194784
gradient norm in None layer : 1.7588988514591338e-05
gradient norm in None layer : 1.70464028526707e-05
gradient norm in None layer : 0.0006369842214799284
gradient norm in None layer : 8.8181378256155e-06
gradient norm in None layer : 0.0016235429061318407
gradient norm in None layer : 8.656843818557516e-05
gradient norm in None layer : 6.935391383931071e-05
gradient norm in None layer : 0.0017660795620036245
gradient norm in None layer : 0.00013985543285077693
gradient norm in None layer : 0.0001477980633006056
gradient norm in None layer : 0.003058349135909101
gradient norm in None layer : 1.622036628020188e-05
gradient norm in None layer : 0.0042654206314209474
gradient norm in None layer : 0.0002859783264868141
gradient norm in None layer : 0.0002793850824546898
gradient norm in None layer : 0.004437973448092112
gradient norm in None layer : 0.0009480932908060047
gradient norm in None layer : 0.00130576325700421
gradient norm in None layer : 0.000636768185339459
gradient norm in None layer : 0.00022004223407099934
Total gradient norm: 0.014702532945604155
invariance loss : 5.912614098127844, avg_den : 0.30446624755859375, density loss : 0.206023563477287, mse loss : 0.09510812102934835, solver time : 115.0750629901886 sec , total loss : 0.10122675869095349, running loss : 0.11231561448373649
Epoch 0/10 , batch 5/12500 
ITERATION : 1, loss : 0.06740962749181316ITERATION : 2, loss : 0.0732469576709512ITERATION : 3, loss : 0.081066129670776ITERATION : 4, loss : 0.08965192294671877ITERATION : 5, loss : 0.09570671758965962ITERATION : 6, loss : 0.10000514524841395ITERATION : 7, loss : 0.10306748901870998ITERATION : 8, loss : 0.10525348735802571ITERATION : 9, loss : 0.1068156929674244ITERATION : 10, loss : 0.10793282755968502ITERATION : 11, loss : 0.10873194991602013ITERATION : 12, loss : 0.10930365028151576ITERATION : 13, loss : 0.10971263546006473ITERATION : 14, loss : 0.11000517616268277ITERATION : 15, loss : 0.1102143819554555ITERATION : 16, loss : 0.11036395282872448ITERATION : 17, loss : 0.11047085603385254ITERATION : 18, loss : 0.11054723817702543ITERATION : 19, loss : 0.11060179355305368ITERATION : 20, loss : 0.11064074468971281ITERATION : 21, loss : 0.11066854376587887ITERATION : 22, loss : 0.11068837552433104ITERATION : 23, loss : 0.11070251719795403ITERATION : 24, loss : 0.11071259679250797ITERATION : 25, loss : 0.11071977775481752ITERATION : 26, loss : 0.11072489107508611ITERATION : 27, loss : 0.11072853032827032ITERATION : 28, loss : 0.11073111896684576ITERATION : 29, loss : 0.1107329592626531ITERATION : 30, loss : 0.11073426675936565ITERATION : 31, loss : 0.11073519520664739ITERATION : 32, loss : 0.11073585399586124ITERATION : 33, loss : 0.11073632119510318ITERATION : 34, loss : 0.11073665222636705ITERATION : 35, loss : 0.11073688662024314ITERATION : 36, loss : 0.1107370524475439ITERATION : 37, loss : 0.11073716964544639ITERATION : 38, loss : 0.11073725249180397ITERATION : 39, loss : 0.11073731094114063ITERATION : 40, loss : 0.11073735214217022ITERATION : 41, loss : 0.11073738117765917ITERATION : 42, loss : 0.11073740154910665ITERATION : 43, loss : 0.11073741583985477ITERATION : 44, loss : 0.11073742596369228ITERATION : 45, loss : 0.11073743305797082ITERATION : 46, loss : 0.11073743798022709ITERATION : 47, loss : 0.11073744140461625ITERATION : 48, loss : 0.11073744375598941ITERATION : 49, loss : 0.11073744539367723ITERATION : 50, loss : 0.11073744650726561ITERATION : 51, loss : 0.11073744724486055ITERATION : 52, loss : 0.11073744774003215ITERATION : 53, loss : 0.11073744810655257ITERATION : 54, loss : 0.11073744835260263ITERATION : 55, loss : 0.1107374485092674ITERATION : 56, loss : 0.11073744860836152ITERATION : 57, loss : 0.11073744866961029ITERATION : 58, loss : 0.11073744872096367ITERATION : 59, loss : 0.1107374487494972ITERATION : 60, loss : 0.11073744875430416ITERATION : 61, loss : 0.11073744875430416ITERATION : 62, loss : 0.11073744875430416ITERATION : 63, loss : 0.11073744875430416ITERATION : 64, loss : 0.11073744875430416ITERATION : 65, loss : 0.11073744875430416ITERATION : 66, loss : 0.11073744875430416ITERATION : 67, loss : 0.11073744875430416ITERATION : 68, loss : 0.11073744875430416ITERATION : 69, loss : 0.11073744875430416ITERATION : 70, loss : 0.11073744875430416ITERATION : 71, loss : 0.11073744875430416ITERATION : 72, loss : 0.11073744875430416ITERATION : 73, loss : 0.11073744875430416ITERATION : 74, loss : 0.11073744875430416ITERATION : 75, loss : 0.11073744875430416ITERATION : 76, loss : 0.11073744875430416ITERATION : 77, loss : 0.11073744875430416ITERATION : 78, loss : 0.11073744875430416ITERATION : 79, loss : 0.11073744875430416ITERATION : 80, loss : 0.11073744875430416ITERATION : 81, loss : 0.11073744875430416ITERATION : 82, loss : 0.11073744875430416ITERATION : 83, loss : 0.11073744875430416ITERATION : 84, loss : 0.11073744875430416ITERATION : 85, loss : 0.11073744875430416ITERATION : 86, loss : 0.11073744875430416ITERATION : 87, loss : 0.11073744875430416ITERATION : 88, loss : 0.11073744875430416ITERATION : 89, loss : 0.11073744875430416ITERATION : 90, loss : 0.11073744875430416ITERATION : 91, loss : 0.11073744875430416ITERATION : 92, loss : 0.11073744875430416ITERATION : 93, loss : 0.11073744875430416ITERATION : 94, loss : 0.11073744875430416ITERATION : 95, loss : 0.11073744875430416ITERATION : 96, loss : 0.11073744875430416ITERATION : 97, loss : 0.11073744875430416ITERATION : 98, loss : 0.11073744875430416ITERATION : 99, loss : 0.11073744875430416ITERATION : 100, loss : 0.11073744875430416
ITERATION : 1, loss : 0.10539954392704048ITERATION : 2, loss : 0.1362133993296257ITERATION : 3, loss : 0.14954091441498762ITERATION : 4, loss : 0.15764160300267116ITERATION : 5, loss : 0.162808263237313ITERATION : 6, loss : 0.1662053765890525ITERATION : 7, loss : 0.168485616857842ITERATION : 8, loss : 0.17003863690804732ITERATION : 9, loss : 0.17110745898666785ITERATION : 10, loss : 0.17184860015267425ITERATION : 11, loss : 0.17236532595876944ITERATION : 12, loss : 0.17272701592583894ITERATION : 13, loss : 0.17298091925746162ITERATION : 14, loss : 0.17315953829301137ITERATION : 15, loss : 0.17328539654939656ITERATION : 16, loss : 0.17337418698524149ITERATION : 17, loss : 0.1734368865370899ITERATION : 18, loss : 0.17348119574722914ITERATION : 19, loss : 0.17351252833605468ITERATION : 20, loss : 0.1735346964135621ITERATION : 21, loss : 0.17355038773276465ITERATION : 22, loss : 0.1735614991176248ITERATION : 23, loss : 0.17356937018483992ITERATION : 24, loss : 0.17357494775640867ITERATION : 25, loss : 0.17357890133858808ITERATION : 26, loss : 0.17358170461544306ITERATION : 27, loss : 0.1735836927687329ITERATION : 28, loss : 0.1735851032055279ITERATION : 29, loss : 0.17358610400318034ITERATION : 30, loss : 0.1735868143111612ITERATION : 31, loss : 0.1735873185462634ITERATION : 32, loss : 0.17358767658224586ITERATION : 33, loss : 0.17358793083865157ITERATION : 34, loss : 0.17358811145013908ITERATION : 35, loss : 0.17358823976734153ITERATION : 36, loss : 0.17358833094475218ITERATION : 37, loss : 0.17358839575003437ITERATION : 38, loss : 0.1735884417874161ITERATION : 39, loss : 0.17358847453563317ITERATION : 40, loss : 0.17358849781789465ITERATION : 41, loss : 0.17358851437420822ITERATION : 42, loss : 0.17358852613774706ITERATION : 43, loss : 0.17358853447903125ITERATION : 44, loss : 0.17358854042750968ITERATION : 45, loss : 0.17358854465190637ITERATION : 46, loss : 0.17358854765668286ITERATION : 47, loss : 0.17358854979637844ITERATION : 48, loss : 0.17358855131574963ITERATION : 49, loss : 0.17358855239712156ITERATION : 50, loss : 0.17358855316243665ITERATION : 51, loss : 0.17358855370682438ITERATION : 52, loss : 0.17358855408335205ITERATION : 53, loss : 0.17358855435940423ITERATION : 54, loss : 0.17358855454010796ITERATION : 55, loss : 0.17358855469021695ITERATION : 56, loss : 0.17358855482179805ITERATION : 57, loss : 0.17358855488473918ITERATION : 58, loss : 0.1735885549310737ITERATION : 59, loss : 0.17358855496618025ITERATION : 60, loss : 0.17358855498712653ITERATION : 61, loss : 0.17358855499707823ITERATION : 62, loss : 0.17358855499847736ITERATION : 63, loss : 0.17358855499847736ITERATION : 64, loss : 0.17358855499847736ITERATION : 65, loss : 0.17358855499847736ITERATION : 66, loss : 0.17358855499847736ITERATION : 67, loss : 0.17358855499847736ITERATION : 68, loss : 0.17358855499847736ITERATION : 69, loss : 0.17358855499847736ITERATION : 70, loss : 0.17358855499847736ITERATION : 71, loss : 0.17358855499847736ITERATION : 72, loss : 0.17358855499847736ITERATION : 73, loss : 0.17358855499847736ITERATION : 74, loss : 0.17358855499847736ITERATION : 75, loss : 0.17358855499847736ITERATION : 76, loss : 0.17358855499847736ITERATION : 77, loss : 0.17358855499847736ITERATION : 78, loss : 0.17358855499847736ITERATION : 79, loss : 0.17358855499847736ITERATION : 80, loss : 0.17358855499847736ITERATION : 81, loss : 0.17358855499847736ITERATION : 82, loss : 0.17358855499847736ITERATION : 83, loss : 0.17358855499847736ITERATION : 84, loss : 0.17358855499847736ITERATION : 85, loss : 0.17358855499847736ITERATION : 86, loss : 0.17358855499847736ITERATION : 87, loss : 0.17358855499847736ITERATION : 88, loss : 0.17358855499847736ITERATION : 89, loss : 0.17358855499847736ITERATION : 90, loss : 0.17358855499847736ITERATION : 91, loss : 0.17358855499847736ITERATION : 92, loss : 0.17358855499847736ITERATION : 93, loss : 0.17358855499847736ITERATION : 94, loss : 0.17358855499847736ITERATION : 95, loss : 0.17358855499847736ITERATION : 96, loss : 0.17358855499847736ITERATION : 97, loss : 0.17358855499847736ITERATION : 98, loss : 0.17358855499847736ITERATION : 99, loss : 0.17358855499847736ITERATION : 100, loss : 0.17358855499847736
ITERATION : 1, loss : 0.00983557475079373ITERATION : 2, loss : 0.010676571805275798ITERATION : 3, loss : 0.012710836353862996ITERATION : 4, loss : 0.013855288209673721ITERATION : 5, loss : 0.014644255159389578ITERATION : 6, loss : 0.015244465615676273ITERATION : 7, loss : 0.0156920785901272ITERATION : 8, loss : 0.01602221394254972ITERATION : 9, loss : 0.016264008880760076ITERATION : 10, loss : 0.01644028365285976ITERATION : 11, loss : 0.01656838949314908ITERATION : 12, loss : 0.01666128859245395ITERATION : 13, loss : 0.016728556292186083ITERATION : 14, loss : 0.016777213900600635ITERATION : 15, loss : 0.01681238459840853ITERATION : 16, loss : 0.016837793862059625ITERATION : 17, loss : 0.01685614435650051ITERATION : 18, loss : 0.01686939375515669ITERATION : 19, loss : 0.016878958421395245ITERATION : 20, loss : 0.016885862273791297ITERATION : 21, loss : 0.016890845097197297ITERATION : 22, loss : 0.01689444123740164ITERATION : 23, loss : 0.016897036498994787ITERATION : 24, loss : 0.016898909399185542ITERATION : 25, loss : 0.016900260978024852ITERATION : 26, loss : 0.016901236333594704ITERATION : 27, loss : 0.01690194020385776ITERATION : 28, loss : 0.01690244813692288ITERATION : 29, loss : 0.016902814675990113ITERATION : 30, loss : 0.01690307919203951ITERATION : 31, loss : 0.016903270080227222ITERATION : 32, loss : 0.016903407830450044ITERATION : 33, loss : 0.016903507248476547ITERATION : 34, loss : 0.016903578985997515ITERATION : 35, loss : 0.016903630771660256ITERATION : 36, loss : 0.016903668129772547ITERATION : 37, loss : 0.016903695105578667ITERATION : 38, loss : 0.01690371457971041ITERATION : 39, loss : 0.01690372862638427ITERATION : 40, loss : 0.01690373875783357ITERATION : 41, loss : 0.016903746063229107ITERATION : 42, loss : 0.016903751327368242ITERATION : 43, loss : 0.01690375512585125ITERATION : 44, loss : 0.016903757861311187ITERATION : 45, loss : 0.01690375983458517ITERATION : 46, loss : 0.016903761257094554ITERATION : 47, loss : 0.01690376230539746ITERATION : 48, loss : 0.016903763028231218ITERATION : 49, loss : 0.01690376354890154ITERATION : 50, loss : 0.016903763932339638ITERATION : 51, loss : 0.016903764204461137ITERATION : 52, loss : 0.016903764372044616ITERATION : 53, loss : 0.01690376451106255ITERATION : 54, loss : 0.01690376461896691ITERATION : 55, loss : 0.016903764691850818ITERATION : 56, loss : 0.01690376474545171ITERATION : 57, loss : 0.016903764767454078ITERATION : 58, loss : 0.016903764768688902ITERATION : 59, loss : 0.016903764768688902ITERATION : 60, loss : 0.016903764768688902ITERATION : 61, loss : 0.016903764768688902ITERATION : 62, loss : 0.016903764768688902ITERATION : 63, loss : 0.016903764768688902ITERATION : 64, loss : 0.016903764768688902ITERATION : 65, loss : 0.016903764768688902ITERATION : 66, loss : 0.016903764768688902ITERATION : 67, loss : 0.016903764768688902ITERATION : 68, loss : 0.016903764768688902ITERATION : 69, loss : 0.016903764768688902ITERATION : 70, loss : 0.016903764768688902ITERATION : 71, loss : 0.016903764768688902ITERATION : 72, loss : 0.016903764768688902ITERATION : 73, loss : 0.016903764768688902ITERATION : 74, loss : 0.016903764768688902ITERATION : 75, loss : 0.016903764768688902ITERATION : 76, loss : 0.016903764768688902ITERATION : 77, loss : 0.016903764768688902ITERATION : 78, loss : 0.016903764768688902ITERATION : 79, loss : 0.016903764768688902ITERATION : 80, loss : 0.016903764768688902ITERATION : 81, loss : 0.016903764768688902ITERATION : 82, loss : 0.016903764768688902ITERATION : 83, loss : 0.016903764768688902ITERATION : 84, loss : 0.016903764768688902ITERATION : 85, loss : 0.016903764768688902ITERATION : 86, loss : 0.016903764768688902ITERATION : 87, loss : 0.016903764768688902ITERATION : 88, loss : 0.016903764768688902ITERATION : 89, loss : 0.016903764768688902ITERATION : 90, loss : 0.016903764768688902ITERATION : 91, loss : 0.016903764768688902ITERATION : 92, loss : 0.016903764768688902ITERATION : 93, loss : 0.016903764768688902ITERATION : 94, loss : 0.016903764768688902ITERATION : 95, loss : 0.016903764768688902ITERATION : 96, loss : 0.016903764768688902ITERATION : 97, loss : 0.016903764768688902ITERATION : 98, loss : 0.016903764768688902ITERATION : 99, loss : 0.016903764768688902ITERATION : 100, loss : 0.016903764768688902
ITERATION : 1, loss : 0.07025181703236866ITERATION : 2, loss : 0.07174242025767523ITERATION : 3, loss : 0.0756537839492954ITERATION : 4, loss : 0.07766986060287259ITERATION : 5, loss : 0.07949814522733852ITERATION : 6, loss : 0.08097729623058052ITERATION : 7, loss : 0.08211889130967412ITERATION : 8, loss : 0.08297976689445084ITERATION : 9, loss : 0.08362081852201199ITERATION : 10, loss : 0.08409472337784032ITERATION : 11, loss : 0.08444355623994977ITERATION : 12, loss : 0.0846996664834185ITERATION : 13, loss : 0.08488741924917284ITERATION : 14, loss : 0.08502494807285954ITERATION : 15, loss : 0.08512565133346395ITERATION : 16, loss : 0.08519938510097431ITERATION : 17, loss : 0.0852533795126052ITERATION : 18, loss : 0.08529292970236226ITERATION : 19, loss : 0.08532191011721152ITERATION : 20, loss : 0.0853431541929506ITERATION : 21, loss : 0.08535873411176262ITERATION : 22, loss : 0.0853701652629453ITERATION : 23, loss : 0.08537855627913224ITERATION : 24, loss : 0.08538471844976153ITERATION : 25, loss : 0.08538924584715706ITERATION : 26, loss : 0.0853925735595467ITERATION : 27, loss : 0.08539502052496241ITERATION : 28, loss : 0.08539682054384892ITERATION : 29, loss : 0.08539814513300624ITERATION : 30, loss : 0.08539912021642239ITERATION : 31, loss : 0.08539983821922154ITERATION : 32, loss : 0.0854003671325889ITERATION : 33, loss : 0.08540075686186704ITERATION : 34, loss : 0.08540104412203536ITERATION : 35, loss : 0.08540125588474608ITERATION : 36, loss : 0.08540141205557007ITERATION : 37, loss : 0.0854015272376232ITERATION : 38, loss : 0.08540161220893716ITERATION : 39, loss : 0.08540167492206215ITERATION : 40, loss : 0.08540172119889909ITERATION : 41, loss : 0.08540175536951013ITERATION : 42, loss : 0.08540178059425958ITERATION : 43, loss : 0.08540179919593799ITERATION : 44, loss : 0.08540181293095458ITERATION : 45, loss : 0.08540182305901767ITERATION : 46, loss : 0.08540183054294716ITERATION : 47, loss : 0.08540183608852493ITERATION : 48, loss : 0.08540184016486993ITERATION : 49, loss : 0.08540184316942435ITERATION : 50, loss : 0.08540184539951073ITERATION : 51, loss : 0.08540184702916899ITERATION : 52, loss : 0.08540184824098905ITERATION : 53, loss : 0.08540184915540075ITERATION : 54, loss : 0.0854018498119131ITERATION : 55, loss : 0.0854018503160286ITERATION : 56, loss : 0.08540185062509281ITERATION : 57, loss : 0.0854018509072749ITERATION : 58, loss : 0.08540185109848929ITERATION : 59, loss : 0.08540185122588252ITERATION : 60, loss : 0.08540185133373354ITERATION : 61, loss : 0.08540185138828572ITERATION : 62, loss : 0.08540185141529857ITERATION : 63, loss : 0.08540185141593873ITERATION : 64, loss : 0.08540185141593873ITERATION : 65, loss : 0.08540185141593873ITERATION : 66, loss : 0.08540185141593873ITERATION : 67, loss : 0.08540185141593873ITERATION : 68, loss : 0.08540185141593873ITERATION : 69, loss : 0.08540185141593873ITERATION : 70, loss : 0.08540185141593873ITERATION : 71, loss : 0.08540185141593873ITERATION : 72, loss : 0.08540185141593873ITERATION : 73, loss : 0.08540185141593873ITERATION : 74, loss : 0.08540185141593873ITERATION : 75, loss : 0.08540185141593873ITERATION : 76, loss : 0.08540185141593873ITERATION : 77, loss : 0.08540185141593873ITERATION : 78, loss : 0.08540185141593873ITERATION : 79, loss : 0.08540185141593873ITERATION : 80, loss : 0.08540185141593873ITERATION : 81, loss : 0.08540185141593873ITERATION : 82, loss : 0.08540185141593873ITERATION : 83, loss : 0.08540185141593873ITERATION : 84, loss : 0.08540185141593873ITERATION : 85, loss : 0.08540185141593873ITERATION : 86, loss : 0.08540185141593873ITERATION : 87, loss : 0.08540185141593873ITERATION : 88, loss : 0.08540185141593873ITERATION : 89, loss : 0.08540185141593873ITERATION : 90, loss : 0.08540185141593873ITERATION : 91, loss : 0.08540185141593873ITERATION : 92, loss : 0.08540185141593873ITERATION : 93, loss : 0.08540185141593873ITERATION : 94, loss : 0.08540185141593873ITERATION : 95, loss : 0.08540185141593873ITERATION : 96, loss : 0.08540185141593873ITERATION : 97, loss : 0.08540185141593873ITERATION : 98, loss : 0.08540185141593873ITERATION : 99, loss : 0.08540185141593873ITERATION : 100, loss : 0.08540185141593873
ITERATION : 1, loss : 0.054568720026725887ITERATION : 2, loss : 0.07088829923569225ITERATION : 3, loss : 0.08041167165295998ITERATION : 4, loss : 0.08630894913168471ITERATION : 5, loss : 0.09008687300524974ITERATION : 6, loss : 0.09256004068887867ITERATION : 7, loss : 0.09420287908497554ITERATION : 8, loss : 0.09530515130993228ITERATION : 9, loss : 0.0960498452953141ITERATION : 10, loss : 0.09655534772346469ITERATION : 11, loss : 0.09689960068979409ITERATION : 12, loss : 0.09713456266585095ITERATION : 13, loss : 0.09729517590756752ITERATION : 14, loss : 0.09740508299899166ITERATION : 15, loss : 0.09748034836081282ITERATION : 16, loss : 0.0975319186556924ITERATION : 17, loss : 0.09756726792449542ITERATION : 18, loss : 0.09759150605775341ITERATION : 19, loss : 0.09760813002345725ITERATION : 20, loss : 0.09761953428153335ITERATION : 21, loss : 0.09762735947067115ITERATION : 22, loss : 0.09763272986889501ITERATION : 23, loss : 0.09763641626970965ITERATION : 24, loss : 0.09763894724320625ITERATION : 25, loss : 0.0976406852672954ITERATION : 26, loss : 0.09764187900092614ITERATION : 27, loss : 0.09764269906091848ITERATION : 28, loss : 0.09764326254717545ITERATION : 29, loss : 0.09764364983864869ITERATION : 30, loss : 0.09764391602051317ITERATION : 31, loss : 0.09764409900941666ITERATION : 32, loss : 0.0976442248926783ITERATION : 33, loss : 0.09764431147212722ITERATION : 34, loss : 0.09764437105911161ITERATION : 35, loss : 0.09764441205098916ITERATION : 36, loss : 0.09764444030889878ITERATION : 37, loss : 0.09764445972875165ITERATION : 38, loss : 0.09764447311246115ITERATION : 39, loss : 0.09764448233345341ITERATION : 40, loss : 0.09764448871205912ITERATION : 41, loss : 0.09764449310887223ITERATION : 42, loss : 0.09764449612626049ITERATION : 43, loss : 0.09764449821947396ITERATION : 44, loss : 0.09764449964541107ITERATION : 45, loss : 0.0976445006077249ITERATION : 46, loss : 0.09764450127676061ITERATION : 47, loss : 0.0976445017310599ITERATION : 48, loss : 0.0976445020531511ITERATION : 49, loss : 0.09764450227104779ITERATION : 50, loss : 0.09764450242233004ITERATION : 51, loss : 0.09764450251096038ITERATION : 52, loss : 0.09764450258222016ITERATION : 53, loss : 0.09764450263088324ITERATION : 54, loss : 0.09764450266648524ITERATION : 55, loss : 0.0976445027000261ITERATION : 56, loss : 0.0976445027000261ITERATION : 57, loss : 0.0976445027000261ITERATION : 58, loss : 0.0976445027000261ITERATION : 59, loss : 0.0976445027000261ITERATION : 60, loss : 0.0976445027000261ITERATION : 61, loss : 0.0976445027000261ITERATION : 62, loss : 0.0976445027000261ITERATION : 63, loss : 0.0976445027000261ITERATION : 64, loss : 0.0976445027000261ITERATION : 65, loss : 0.0976445027000261ITERATION : 66, loss : 0.0976445027000261ITERATION : 67, loss : 0.0976445027000261ITERATION : 68, loss : 0.0976445027000261ITERATION : 69, loss : 0.0976445027000261ITERATION : 70, loss : 0.0976445027000261ITERATION : 71, loss : 0.0976445027000261ITERATION : 72, loss : 0.0976445027000261ITERATION : 73, loss : 0.0976445027000261ITERATION : 74, loss : 0.0976445027000261ITERATION : 75, loss : 0.0976445027000261ITERATION : 76, loss : 0.0976445027000261ITERATION : 77, loss : 0.0976445027000261ITERATION : 78, loss : 0.0976445027000261ITERATION : 79, loss : 0.0976445027000261ITERATION : 80, loss : 0.0976445027000261ITERATION : 81, loss : 0.0976445027000261ITERATION : 82, loss : 0.0976445027000261ITERATION : 83, loss : 0.0976445027000261ITERATION : 84, loss : 0.0976445027000261ITERATION : 85, loss : 0.0976445027000261ITERATION : 86, loss : 0.0976445027000261ITERATION : 87, loss : 0.0976445027000261ITERATION : 88, loss : 0.0976445027000261ITERATION : 89, loss : 0.0976445027000261ITERATION : 90, loss : 0.0976445027000261ITERATION : 91, loss : 0.0976445027000261ITERATION : 92, loss : 0.0976445027000261ITERATION : 93, loss : 0.0976445027000261ITERATION : 94, loss : 0.0976445027000261ITERATION : 95, loss : 0.0976445027000261ITERATION : 96, loss : 0.0976445027000261ITERATION : 97, loss : 0.0976445027000261ITERATION : 98, loss : 0.0976445027000261ITERATION : 99, loss : 0.0976445027000261ITERATION : 100, loss : 0.0976445027000261
ITERATION : 1, loss : 0.04394603798051488ITERATION : 2, loss : 0.0556268662453546ITERATION : 3, loss : 0.06589188040054399ITERATION : 4, loss : 0.07327324939427124ITERATION : 5, loss : 0.07829535619108956ITERATION : 6, loss : 0.08164970853042643ITERATION : 7, loss : 0.0838764868477895ITERATION : 8, loss : 0.0853522884836272ITERATION : 9, loss : 0.08633029701440707ITERATION : 10, loss : 0.08697868047131037ITERATION : 11, loss : 0.08740874980074291ITERATION : 12, loss : 0.08769414925184327ITERATION : 13, loss : 0.08788362720008293ITERATION : 14, loss : 0.08800947426403494ITERATION : 15, loss : 0.08809309303734279ITERATION : 16, loss : 0.08814867644684317ITERATION : 17, loss : 0.08818564052171346ITERATION : 18, loss : 0.08821023429930372ITERATION : 19, loss : 0.0882266060689752ITERATION : 20, loss : 0.08823751097509527ITERATION : 21, loss : 0.08824477910233354ITERATION : 22, loss : 0.08824962666964052ITERATION : 23, loss : 0.08825286215284249ITERATION : 24, loss : 0.08825502344039378ITERATION : 25, loss : 0.08825646840423072ITERATION : 26, loss : 0.08825743547268064ITERATION : 27, loss : 0.0882580832261166ITERATION : 28, loss : 0.08825851763317089ITERATION : 29, loss : 0.08825880927050553ITERATION : 30, loss : 0.08825900531321743ITERATION : 31, loss : 0.08825913726025796ITERATION : 32, loss : 0.08825922618132995ITERATION : 33, loss : 0.08825928618009524ITERATION : 34, loss : 0.088259326735054ITERATION : 35, loss : 0.08825935416702499ITERATION : 36, loss : 0.08825937278114232ITERATION : 37, loss : 0.08825938546039679ITERATION : 38, loss : 0.08825939406465604ITERATION : 39, loss : 0.08825939996335992ITERATION : 40, loss : 0.08825940395982358ITERATION : 41, loss : 0.08825940674046912ITERATION : 42, loss : 0.08825940864203276ITERATION : 43, loss : 0.0882594099311192ITERATION : 44, loss : 0.08825941080465406ITERATION : 45, loss : 0.08825941138368612ITERATION : 46, loss : 0.08825941179252834ITERATION : 47, loss : 0.08825941206946486ITERATION : 48, loss : 0.08825941224996794ITERATION : 49, loss : 0.08825941236476509ITERATION : 50, loss : 0.08825941246778868ITERATION : 51, loss : 0.0882594125274091ITERATION : 52, loss : 0.08825941258362935ITERATION : 53, loss : 0.08825941261205861ITERATION : 54, loss : 0.08825941261978695ITERATION : 55, loss : 0.08825941261978695ITERATION : 56, loss : 0.08825941261978695ITERATION : 57, loss : 0.08825941261978695ITERATION : 58, loss : 0.08825941261978695ITERATION : 59, loss : 0.08825941261978695ITERATION : 60, loss : 0.08825941261978695ITERATION : 61, loss : 0.08825941261978695ITERATION : 62, loss : 0.08825941261978695ITERATION : 63, loss : 0.08825941261978695ITERATION : 64, loss : 0.08825941261978695ITERATION : 65, loss : 0.08825941261978695ITERATION : 66, loss : 0.08825941261978695ITERATION : 67, loss : 0.08825941261978695ITERATION : 68, loss : 0.08825941261978695ITERATION : 69, loss : 0.08825941261978695ITERATION : 70, loss : 0.08825941261978695ITERATION : 71, loss : 0.08825941261978695ITERATION : 72, loss : 0.08825941261978695ITERATION : 73, loss : 0.08825941261978695ITERATION : 74, loss : 0.08825941261978695ITERATION : 75, loss : 0.08825941261978695ITERATION : 76, loss : 0.08825941261978695ITERATION : 77, loss : 0.08825941261978695ITERATION : 78, loss : 0.08825941261978695ITERATION : 79, loss : 0.08825941261978695ITERATION : 80, loss : 0.08825941261978695ITERATION : 81, loss : 0.08825941261978695ITERATION : 82, loss : 0.08825941261978695ITERATION : 83, loss : 0.08825941261978695ITERATION : 84, loss : 0.08825941261978695ITERATION : 85, loss : 0.08825941261978695ITERATION : 86, loss : 0.08825941261978695ITERATION : 87, loss : 0.08825941261978695ITERATION : 88, loss : 0.08825941261978695ITERATION : 89, loss : 0.08825941261978695ITERATION : 90, loss : 0.08825941261978695ITERATION : 91, loss : 0.08825941261978695ITERATION : 92, loss : 0.08825941261978695ITERATION : 93, loss : 0.08825941261978695ITERATION : 94, loss : 0.08825941261978695ITERATION : 95, loss : 0.08825941261978695ITERATION : 96, loss : 0.08825941261978695ITERATION : 97, loss : 0.08825941261978695ITERATION : 98, loss : 0.08825941261978695ITERATION : 99, loss : 0.08825941261978695ITERATION : 100, loss : 0.08825941261978695
ITERATION : 1, loss : 0.15384978204165414ITERATION : 2, loss : 0.1607148564655588ITERATION : 3, loss : 0.16732784369714404ITERATION : 4, loss : 0.17717368865269492ITERATION : 5, loss : 0.1871993526881274ITERATION : 6, loss : 0.1945120190762875ITERATION : 7, loss : 0.19985030169448192ITERATION : 8, loss : 0.20375266497984793ITERATION : 9, loss : 0.2062106180653964ITERATION : 10, loss : 0.20793903278244869ITERATION : 11, loss : 0.20920837003592746ITERATION : 12, loss : 0.21014104546805493ITERATION : 13, loss : 0.21082660042315438ITERATION : 14, loss : 0.21133065252878389ITERATION : 15, loss : 0.2117013420406057ITERATION : 16, loss : 0.21197401170897312ITERATION : 17, loss : 0.21217462057105965ITERATION : 18, loss : 0.21232224179385692ITERATION : 19, loss : 0.21243089246646088ITERATION : 20, loss : 0.21251087604853192ITERATION : 21, loss : 0.21256976784435705ITERATION : 22, loss : 0.21261313840455864ITERATION : 23, loss : 0.21264508491604667ITERATION : 24, loss : 0.21266862104930703ITERATION : 25, loss : 0.21268596445851554ITERATION : 26, loss : 0.21269874721916213ITERATION : 27, loss : 0.2127081702131197ITERATION : 28, loss : 0.21271511794321785ITERATION : 29, loss : 0.21272024159277247ITERATION : 30, loss : 0.21272402074343738ITERATION : 31, loss : 0.21272680874719452ITERATION : 32, loss : 0.21272886591333987ITERATION : 33, loss : 0.21273038403880745ITERATION : 34, loss : 0.212731504565682ITERATION : 35, loss : 0.21273233180283804ITERATION : 36, loss : 0.21273294264523948ITERATION : 37, loss : 0.21273339377152145ITERATION : 38, loss : 0.21273372698416515ITERATION : 39, loss : 0.21273397315435932ITERATION : 40, loss : 0.21273415504210455ITERATION : 41, loss : 0.21273428944955589ITERATION : 42, loss : 0.21273438879527243ITERATION : 43, loss : 0.212734462203951ITERATION : 44, loss : 0.21273451651486214ITERATION : 45, loss : 0.21273455665011196ITERATION : 46, loss : 0.21273458634573164ITERATION : 47, loss : 0.21273460828584145ITERATION : 48, loss : 0.21273462453563083ITERATION : 49, loss : 0.21273463650246424ITERATION : 50, loss : 0.21273464541135506ITERATION : 51, loss : 0.21273465193205607ITERATION : 52, loss : 0.21273465682857318ITERATION : 53, loss : 0.21273466057056473ITERATION : 54, loss : 0.2127346631890469ITERATION : 55, loss : 0.2127346651467874ITERATION : 56, loss : 0.21273466657192716ITERATION : 57, loss : 0.21273466766100438ITERATION : 58, loss : 0.21273466839148467ITERATION : 59, loss : 0.21273466893268195ITERATION : 60, loss : 0.2127346693591094ITERATION : 61, loss : 0.21273466965255253ITERATION : 62, loss : 0.21273466988780762ITERATION : 63, loss : 0.2127346700981239ITERATION : 64, loss : 0.21273467018147554ITERATION : 65, loss : 0.2127346702688776ITERATION : 66, loss : 0.21273467030245102ITERATION : 67, loss : 0.21273467033257262ITERATION : 68, loss : 0.21273467034035692ITERATION : 69, loss : 0.21273467034035692ITERATION : 70, loss : 0.21273467034035692ITERATION : 71, loss : 0.21273467034035692ITERATION : 72, loss : 0.21273467034035692ITERATION : 73, loss : 0.21273467034035692ITERATION : 74, loss : 0.21273467034035692ITERATION : 75, loss : 0.21273467034035692ITERATION : 76, loss : 0.21273467034035692ITERATION : 77, loss : 0.21273467034035692ITERATION : 78, loss : 0.21273467034035692ITERATION : 79, loss : 0.21273467034035692ITERATION : 80, loss : 0.21273467034035692ITERATION : 81, loss : 0.21273467034035692ITERATION : 82, loss : 0.21273467034035692ITERATION : 83, loss : 0.21273467034035692ITERATION : 84, loss : 0.21273467034035692ITERATION : 85, loss : 0.21273467034035692ITERATION : 86, loss : 0.21273467034035692ITERATION : 87, loss : 0.21273467034035692ITERATION : 88, loss : 0.21273467034035692ITERATION : 89, loss : 0.21273467034035692ITERATION : 90, loss : 0.21273467034035692ITERATION : 91, loss : 0.21273467034035692ITERATION : 92, loss : 0.21273467034035692ITERATION : 93, loss : 0.21273467034035692ITERATION : 94, loss : 0.21273467034035692ITERATION : 95, loss : 0.21273467034035692ITERATION : 96, loss : 0.21273467034035692ITERATION : 97, loss : 0.21273467034035692ITERATION : 98, loss : 0.21273467034035692ITERATION : 99, loss : 0.21273467034035692ITERATION : 100, loss : 0.21273467034035692
ITERATION : 1, loss : 0.14594381623045527ITERATION : 2, loss : 0.18146621442915792ITERATION : 3, loss : 0.19064787188652232ITERATION : 4, loss : 0.1933409646971223ITERATION : 5, loss : 0.19505193443807592ITERATION : 6, loss : 0.19612050205060086ITERATION : 7, loss : 0.1967671038590016ITERATION : 8, loss : 0.19714087906264988ITERATION : 9, loss : 0.19734222448368263ITERATION : 10, loss : 0.1974377374125885ITERATION : 11, loss : 0.19747079689764252ITERATION : 12, loss : 0.1974690542987029ITERATION : 13, loss : 0.19744967307829042ITERATION : 14, loss : 0.19742294710253677ITERATION : 15, loss : 0.19739476119653762ITERATION : 16, loss : 0.19736823878027365ITERATION : 17, loss : 0.19734482545461793ITERATION : 18, loss : 0.19732498855332542ITERATION : 19, loss : 0.19730865825890218ITERATION : 20, loss : 0.19729549847106667ITERATION : 21, loss : 0.19728506707938068ITERATION : 22, loss : 0.1972769065609934ITERATION : 23, loss : 0.1972705909283353ITERATION : 24, loss : 0.19726574698896107ITERATION : 25, loss : 0.19726206012658315ITERATION : 26, loss : 0.19725927239452795ITERATION : 27, loss : 0.19725717665542122ITERATION : 28, loss : 0.1972556091430206ITERATION : 29, loss : 0.19725444199812506ITERATION : 30, loss : 0.1972535765097644ITERATION : 31, loss : 0.1972529371241242ITERATION : 32, loss : 0.19725246638586297ITERATION : 33, loss : 0.1972521208377005ITERATION : 34, loss : 0.197251867887593ITERATION : 35, loss : 0.19725168325016593ITERATION : 36, loss : 0.1972515488984955ITERATION : 37, loss : 0.19725145132491873ITERATION : 38, loss : 0.1972513805610684ITERATION : 39, loss : 0.19725132936742432ITERATION : 40, loss : 0.19725129242520317ITERATION : 41, loss : 0.1972512658357551ITERATION : 42, loss : 0.1972512467309765ITERATION : 43, loss : 0.1972512330039703ITERATION : 44, loss : 0.197251223187051ITERATION : 45, loss : 0.1972512161700636ITERATION : 46, loss : 0.19725121116852998ITERATION : 47, loss : 0.19725120759863765ITERATION : 48, loss : 0.1972512050751606ITERATION : 49, loss : 0.19725120325801346ITERATION : 50, loss : 0.19725120198406315ITERATION : 51, loss : 0.1972512010939471ITERATION : 52, loss : 0.19725120047251127ITERATION : 53, loss : 0.1972512000222147ITERATION : 54, loss : 0.1972511997045293ITERATION : 55, loss : 0.1972511994686529ITERATION : 56, loss : 0.19725119934059995ITERATION : 57, loss : 0.19725119922401751ITERATION : 58, loss : 0.19725119918425843ITERATION : 59, loss : 0.1972511991470367ITERATION : 60, loss : 0.19725119914407008ITERATION : 61, loss : 0.19725119914407008ITERATION : 62, loss : 0.19725119914407008ITERATION : 63, loss : 0.19725119914407008ITERATION : 64, loss : 0.19725119914407008ITERATION : 65, loss : 0.19725119914407008ITERATION : 66, loss : 0.19725119914407008ITERATION : 67, loss : 0.19725119914407008ITERATION : 68, loss : 0.19725119914407008ITERATION : 69, loss : 0.19725119914407008ITERATION : 70, loss : 0.19725119914407008ITERATION : 71, loss : 0.19725119914407008ITERATION : 72, loss : 0.19725119914407008ITERATION : 73, loss : 0.19725119914407008ITERATION : 74, loss : 0.19725119914407008ITERATION : 75, loss : 0.19725119914407008ITERATION : 76, loss : 0.19725119914407008ITERATION : 77, loss : 0.19725119914407008ITERATION : 78, loss : 0.19725119914407008ITERATION : 79, loss : 0.19725119914407008ITERATION : 80, loss : 0.19725119914407008ITERATION : 81, loss : 0.19725119914407008ITERATION : 82, loss : 0.19725119914407008ITERATION : 83, loss : 0.19725119914407008ITERATION : 84, loss : 0.19725119914407008ITERATION : 85, loss : 0.19725119914407008ITERATION : 86, loss : 0.19725119914407008ITERATION : 87, loss : 0.19725119914407008ITERATION : 88, loss : 0.19725119914407008ITERATION : 89, loss : 0.19725119914407008ITERATION : 90, loss : 0.19725119914407008ITERATION : 91, loss : 0.19725119914407008ITERATION : 92, loss : 0.19725119914407008ITERATION : 93, loss : 0.19725119914407008ITERATION : 94, loss : 0.19725119914407008ITERATION : 95, loss : 0.19725119914407008ITERATION : 96, loss : 0.19725119914407008ITERATION : 97, loss : 0.19725119914407008ITERATION : 98, loss : 0.19725119914407008ITERATION : 99, loss : 0.19725119914407008ITERATION : 100, loss : 0.19725119914407008
gradient norm in None layer : 0.011482200284896509
gradient norm in None layer : 0.0002152319237119977
gradient norm in None layer : 0.00020528975974228738
gradient norm in None layer : 0.0037002804366043354
gradient norm in None layer : 0.00021923256596606268
gradient norm in None layer : 0.00020777761768572424
gradient norm in None layer : 0.0012598217883030706
gradient norm in None layer : 4.699921786707481e-05
gradient norm in None layer : 4.0039048494637993e-05
gradient norm in None layer : 0.0010115963515621054
gradient norm in None layer : 5.093492972365377e-05
gradient norm in None layer : 4.17566356762256e-05
gradient norm in None layer : 0.00036111850220539116
gradient norm in None layer : 1.0517930188913416e-05
gradient norm in None layer : 8.263017531050424e-06
gradient norm in None layer : 0.00032258219125030294
gradient norm in None layer : 1.1588824019663254e-05
gradient norm in None layer : 1.0798983277085829e-05
gradient norm in None layer : 0.0004152660442994739
gradient norm in None layer : 7.0282838338184306e-06
gradient norm in None layer : 0.0009647903577651219
gradient norm in None layer : 4.122226399275968e-05
gradient norm in None layer : 3.787576344213972e-05
gradient norm in None layer : 0.0009729746228787059
gradient norm in None layer : 9.886864720997434e-05
gradient norm in None layer : 0.00011709585932136687
gradient norm in None layer : 0.001944353024676057
gradient norm in None layer : 1.750508571609187e-05
gradient norm in None layer : 0.0029897599887360933
gradient norm in None layer : 0.00021326312182440023
gradient norm in None layer : 0.00014826422769241153
gradient norm in None layer : 0.003331006563362197
gradient norm in None layer : 0.0007437442014055493
gradient norm in None layer : 0.001115804122835045
gradient norm in None layer : 0.0005119161493627337
gradient norm in None layer : 0.00018766414201667588
Total gradient norm: 0.013289877335288023
invariance loss : 5.640382363958773, avg_den : 0.3462066650390625, density loss : 0.24690901072234436, mse loss : 0.12281517559270615, solver time : 119.71259570121765 sec , total loss : 0.12870246696738727, running loss : 0.11559298498046665
Epoch 0/10 , batch 6/12500 
ITERATION : 1, loss : 0.03426675333531486ITERATION : 2, loss : 0.047971321861571804ITERATION : 3, loss : 0.05931631440625539ITERATION : 4, loss : 0.06737812193633033ITERATION : 5, loss : 0.07291437582303628ITERATION : 6, loss : 0.0766868845025026ITERATION : 7, loss : 0.07925725784581461ITERATION : 8, loss : 0.08101201580560709ITERATION : 9, loss : 0.08221264808066037ITERATION : 10, loss : 0.08303573998052756ITERATION : 11, loss : 0.08360087782318228ITERATION : 12, loss : 0.083989353662469ITERATION : 13, loss : 0.08425662000977169ITERATION : 14, loss : 0.08444060991709093ITERATION : 15, loss : 0.08456732814425563ITERATION : 16, loss : 0.08465463049243516ITERATION : 17, loss : 0.08471479168395846ITERATION : 18, loss : 0.08475625694048496ITERATION : 19, loss : 0.08478484011992506ITERATION : 20, loss : 0.08480454532030868ITERATION : 21, loss : 0.08481813125106101ITERATION : 22, loss : 0.08482749884644317ITERATION : 23, loss : 0.08483395822623394ITERATION : 24, loss : 0.08483841246979698ITERATION : 25, loss : 0.08484148416052618ITERATION : 26, loss : 0.08484360254279119ITERATION : 27, loss : 0.08484506355208599ITERATION : 28, loss : 0.08484607120334159ITERATION : 29, loss : 0.08484676621269654ITERATION : 30, loss : 0.08484724560534611ITERATION : 31, loss : 0.08484757626344375ITERATION : 32, loss : 0.08484780435973342ITERATION : 33, loss : 0.08484796170720678ITERATION : 34, loss : 0.08484807021739961ITERATION : 35, loss : 0.08484814510214822ITERATION : 36, loss : 0.08484819677289064ITERATION : 37, loss : 0.08484823242739577ITERATION : 38, loss : 0.08484825700698634ITERATION : 39, loss : 0.08484827397301895ITERATION : 40, loss : 0.08484828567087473ITERATION : 41, loss : 0.08484829374669037ITERATION : 42, loss : 0.08484829931497644ITERATION : 43, loss : 0.08484830315461081ITERATION : 44, loss : 0.08484830578124258ITERATION : 45, loss : 0.08484830758404617ITERATION : 46, loss : 0.08484830883878162ITERATION : 47, loss : 0.08484830977692488ITERATION : 48, loss : 0.08484831035354813ITERATION : 49, loss : 0.08484831077353439ITERATION : 50, loss : 0.08484831097591877ITERATION : 51, loss : 0.08484831112908962ITERATION : 52, loss : 0.08484831126921903ITERATION : 53, loss : 0.08484831136696508ITERATION : 54, loss : 0.0848483114120898ITERATION : 55, loss : 0.08484831143854465ITERATION : 56, loss : 0.08484831148862522ITERATION : 57, loss : 0.0848483114892394ITERATION : 58, loss : 0.0848483114892394ITERATION : 59, loss : 0.0848483114892394ITERATION : 60, loss : 0.0848483114892394ITERATION : 61, loss : 0.0848483114892394ITERATION : 62, loss : 0.0848483114892394ITERATION : 63, loss : 0.0848483114892394ITERATION : 64, loss : 0.0848483114892394ITERATION : 65, loss : 0.0848483114892394ITERATION : 66, loss : 0.0848483114892394ITERATION : 67, loss : 0.0848483114892394ITERATION : 68, loss : 0.0848483114892394ITERATION : 69, loss : 0.0848483114892394ITERATION : 70, loss : 0.0848483114892394ITERATION : 71, loss : 0.0848483114892394ITERATION : 72, loss : 0.0848483114892394ITERATION : 73, loss : 0.0848483114892394ITERATION : 74, loss : 0.0848483114892394ITERATION : 75, loss : 0.0848483114892394ITERATION : 76, loss : 0.0848483114892394ITERATION : 77, loss : 0.0848483114892394ITERATION : 78, loss : 0.0848483114892394ITERATION : 79, loss : 0.0848483114892394ITERATION : 80, loss : 0.0848483114892394ITERATION : 81, loss : 0.0848483114892394ITERATION : 82, loss : 0.0848483114892394ITERATION : 83, loss : 0.0848483114892394ITERATION : 84, loss : 0.0848483114892394ITERATION : 85, loss : 0.0848483114892394ITERATION : 86, loss : 0.0848483114892394ITERATION : 87, loss : 0.0848483114892394ITERATION : 88, loss : 0.0848483114892394ITERATION : 89, loss : 0.0848483114892394ITERATION : 90, loss : 0.0848483114892394ITERATION : 91, loss : 0.0848483114892394ITERATION : 92, loss : 0.0848483114892394ITERATION : 93, loss : 0.0848483114892394ITERATION : 94, loss : 0.0848483114892394ITERATION : 95, loss : 0.0848483114892394ITERATION : 96, loss : 0.0848483114892394ITERATION : 97, loss : 0.0848483114892394ITERATION : 98, loss : 0.0848483114892394ITERATION : 99, loss : 0.0848483114892394ITERATION : 100, loss : 0.0848483114892394
ITERATION : 1, loss : 0.04460607488661789ITERATION : 2, loss : 0.05408496592959074ITERATION : 3, loss : 0.05459311952640964ITERATION : 4, loss : 0.05587749048745912ITERATION : 5, loss : 0.05710118128235704ITERATION : 6, loss : 0.058077403703512887ITERATION : 7, loss : 0.05880511380356038ITERATION : 8, loss : 0.059329985777586094ITERATION : 9, loss : 0.05970170132965647ITERATION : 10, loss : 0.05996204772423129ITERATION : 11, loss : 0.06014308339119009ITERATION : 12, loss : 0.060268349839376886ITERATION : 13, loss : 0.06035472221433144ITERATION : 14, loss : 0.060414122256003774ITERATION : 15, loss : 0.0604548925580762ITERATION : 16, loss : 0.060482833206111884ITERATION : 17, loss : 0.06050195844065184ITERATION : 18, loss : 0.06051503686509888ITERATION : 19, loss : 0.060523973388126866ITERATION : 20, loss : 0.060530075692953864ITERATION : 21, loss : 0.0605342403083086ITERATION : 22, loss : 0.06053708109417763ITERATION : 23, loss : 0.060539018107437476ITERATION : 24, loss : 0.06054033837406953ITERATION : 25, loss : 0.060541237833598195ITERATION : 26, loss : 0.060541850465184226ITERATION : 27, loss : 0.06054226756635894ITERATION : 28, loss : 0.060542551445618625ITERATION : 29, loss : 0.06054274460389499ITERATION : 30, loss : 0.06054287605440083ITERATION : 31, loss : 0.06054296540817239ITERATION : 32, loss : 0.06054302602418228ITERATION : 33, loss : 0.06054306731767717ITERATION : 34, loss : 0.060543095363743325ITERATION : 35, loss : 0.06054311445852474ITERATION : 36, loss : 0.06054312740168747ITERATION : 37, loss : 0.060543136158711215ITERATION : 38, loss : 0.060543142149100816ITERATION : 39, loss : 0.06054314622837992ITERATION : 40, loss : 0.060543148968528096ITERATION : 41, loss : 0.060543150807328236ITERATION : 42, loss : 0.060543152069318006ITERATION : 43, loss : 0.06054315287928967ITERATION : 44, loss : 0.060543153409186916ITERATION : 45, loss : 0.06054315377977418ITERATION : 46, loss : 0.060543154005538447ITERATION : 47, loss : 0.06054315413831966ITERATION : 48, loss : 0.06054315425880877ITERATION : 49, loss : 0.060543154308449386ITERATION : 50, loss : 0.060543154372482616ITERATION : 51, loss : 0.06054315438644768ITERATION : 52, loss : 0.06054315440913861ITERATION : 53, loss : 0.06054315440957222ITERATION : 54, loss : 0.06054315440957222ITERATION : 55, loss : 0.06054315440957222ITERATION : 56, loss : 0.06054315440957222ITERATION : 57, loss : 0.06054315440957222ITERATION : 58, loss : 0.06054315440957222ITERATION : 59, loss : 0.06054315440957222ITERATION : 60, loss : 0.06054315440957222ITERATION : 61, loss : 0.06054315440957222ITERATION : 62, loss : 0.06054315440957222ITERATION : 63, loss : 0.06054315440957222ITERATION : 64, loss : 0.06054315440957222ITERATION : 65, loss : 0.06054315440957222ITERATION : 66, loss : 0.06054315440957222ITERATION : 67, loss : 0.06054315440957222ITERATION : 68, loss : 0.06054315440957222ITERATION : 69, loss : 0.06054315440957222ITERATION : 70, loss : 0.06054315440957222ITERATION : 71, loss : 0.06054315440957222ITERATION : 72, loss : 0.06054315440957222ITERATION : 73, loss : 0.06054315440957222ITERATION : 74, loss : 0.06054315440957222ITERATION : 75, loss : 0.06054315440957222ITERATION : 76, loss : 0.06054315440957222ITERATION : 77, loss : 0.06054315440957222ITERATION : 78, loss : 0.06054315440957222ITERATION : 79, loss : 0.06054315440957222ITERATION : 80, loss : 0.06054315440957222ITERATION : 81, loss : 0.06054315440957222ITERATION : 82, loss : 0.06054315440957222ITERATION : 83, loss : 0.06054315440957222ITERATION : 84, loss : 0.06054315440957222ITERATION : 85, loss : 0.06054315440957222ITERATION : 86, loss : 0.06054315440957222ITERATION : 87, loss : 0.06054315440957222ITERATION : 88, loss : 0.06054315440957222ITERATION : 89, loss : 0.06054315440957222ITERATION : 90, loss : 0.06054315440957222ITERATION : 91, loss : 0.06054315440957222ITERATION : 92, loss : 0.06054315440957222ITERATION : 93, loss : 0.06054315440957222ITERATION : 94, loss : 0.06054315440957222ITERATION : 95, loss : 0.06054315440957222ITERATION : 96, loss : 0.06054315440957222ITERATION : 97, loss : 0.06054315440957222ITERATION : 98, loss : 0.06054315440957222ITERATION : 99, loss : 0.06054315440957222ITERATION : 100, loss : 0.06054315440957222
ITERATION : 1, loss : 0.02364193574270177ITERATION : 2, loss : 0.024644206478315663ITERATION : 3, loss : 0.027098317346337948ITERATION : 4, loss : 0.029133676128618374ITERATION : 5, loss : 0.03064908009059422ITERATION : 6, loss : 0.0317418694190473ITERATION : 7, loss : 0.032517158804474484ITERATION : 8, loss : 0.0330612860801866ITERATION : 9, loss : 0.03344021548456598ITERATION : 10, loss : 0.033702585454016394ITERATION : 11, loss : 0.03388346308483106ITERATION : 12, loss : 0.03400774462339785ITERATION : 13, loss : 0.034092914907965746ITERATION : 14, loss : 0.03415115780108044ITERATION : 15, loss : 0.034190915556111694ITERATION : 16, loss : 0.03421801269194746ITERATION : 17, loss : 0.0342364549202172ITERATION : 18, loss : 0.034248990206498235ITERATION : 19, loss : 0.03425749967656193ITERATION : 20, loss : 0.03426326901904053ITERATION : 21, loss : 0.034267175658420544ITERATION : 22, loss : 0.03426981755028741ITERATION : 23, loss : 0.03427160174655278ITERATION : 24, loss : 0.03427280497297799ITERATION : 25, loss : 0.03427361519368593ITERATION : 26, loss : 0.03427415991518731ITERATION : 27, loss : 0.0342745254440216ITERATION : 28, loss : 0.03427477035164527ITERATION : 29, loss : 0.03427493405314232ITERATION : 30, loss : 0.03427504327225176ITERATION : 31, loss : 0.03427511591665201ITERATION : 32, loss : 0.03427516417892161ITERATION : 33, loss : 0.03427519610306909ITERATION : 34, loss : 0.034275217185907834ITERATION : 35, loss : 0.03427523101352852ITERATION : 36, loss : 0.034275240110408924ITERATION : 37, loss : 0.034275245985206974ITERATION : 38, loss : 0.03427524983882064ITERATION : 39, loss : 0.034275252292427845ITERATION : 40, loss : 0.034275253906448415ITERATION : 41, loss : 0.034275254921491353ITERATION : 42, loss : 0.03427525554195288ITERATION : 43, loss : 0.03427525590553494ITERATION : 44, loss : 0.034275256116080406ITERATION : 45, loss : 0.034275256230961115ITERATION : 46, loss : 0.03427525628772243ITERATION : 47, loss : 0.03427525631375328ITERATION : 48, loss : 0.03427525631755672ITERATION : 49, loss : 0.03427525631471124ITERATION : 50, loss : 0.034275256305883545ITERATION : 51, loss : 0.03427525630404212ITERATION : 52, loss : 0.03427525629252748ITERATION : 53, loss : 0.03427525629448382ITERATION : 54, loss : 0.034275256283512995ITERATION : 55, loss : 0.034275256283512995ITERATION : 56, loss : 0.034275256283512995ITERATION : 57, loss : 0.034275256283512995ITERATION : 58, loss : 0.034275256283512995ITERATION : 59, loss : 0.034275256283512995ITERATION : 60, loss : 0.034275256283512995ITERATION : 61, loss : 0.034275256283512995ITERATION : 62, loss : 0.034275256283512995ITERATION : 63, loss : 0.034275256283512995ITERATION : 64, loss : 0.034275256283512995ITERATION : 65, loss : 0.034275256283512995ITERATION : 66, loss : 0.034275256283512995ITERATION : 67, loss : 0.034275256283512995ITERATION : 68, loss : 0.034275256283512995ITERATION : 69, loss : 0.034275256283512995ITERATION : 70, loss : 0.034275256283512995ITERATION : 71, loss : 0.034275256283512995ITERATION : 72, loss : 0.034275256283512995ITERATION : 73, loss : 0.034275256283512995ITERATION : 74, loss : 0.034275256283512995ITERATION : 75, loss : 0.034275256283512995ITERATION : 76, loss : 0.034275256283512995ITERATION : 77, loss : 0.034275256283512995ITERATION : 78, loss : 0.034275256283512995ITERATION : 79, loss : 0.034275256283512995ITERATION : 80, loss : 0.034275256283512995ITERATION : 81, loss : 0.034275256283512995ITERATION : 82, loss : 0.034275256283512995ITERATION : 83, loss : 0.034275256283512995ITERATION : 84, loss : 0.034275256283512995ITERATION : 85, loss : 0.034275256283512995ITERATION : 86, loss : 0.034275256283512995ITERATION : 87, loss : 0.034275256283512995ITERATION : 88, loss : 0.034275256283512995ITERATION : 89, loss : 0.034275256283512995ITERATION : 90, loss : 0.034275256283512995ITERATION : 91, loss : 0.034275256283512995ITERATION : 92, loss : 0.034275256283512995ITERATION : 93, loss : 0.034275256283512995ITERATION : 94, loss : 0.034275256283512995ITERATION : 95, loss : 0.034275256283512995ITERATION : 96, loss : 0.034275256283512995ITERATION : 97, loss : 0.034275256283512995ITERATION : 98, loss : 0.034275256283512995ITERATION : 99, loss : 0.034275256283512995ITERATION : 100, loss : 0.034275256283512995
ITERATION : 1, loss : 0.02599881057028464ITERATION : 2, loss : 0.033352376505836785ITERATION : 3, loss : 0.040953214360540995ITERATION : 4, loss : 0.04665389274261275ITERATION : 5, loss : 0.05056696978994605ITERATION : 6, loss : 0.05314589799426995ITERATION : 7, loss : 0.054806394988829166ITERATION : 8, loss : 0.05585850308504188ITERATION : 9, loss : 0.05651649320728586ITERATION : 10, loss : 0.056923050857695674ITERATION : 11, loss : 0.057171148434584346ITERATION : 12, loss : 0.057320470961476466ITERATION : 13, loss : 0.05740889078345122ITERATION : 14, loss : 0.057460195033675554ITERATION : 15, loss : 0.05748917850463015ITERATION : 16, loss : 0.05750494972419811ITERATION : 17, loss : 0.057513054495014856ITERATION : 18, loss : 0.057516826148396434ITERATION : 19, loss : 0.05751823742276064ITERATION : 20, loss : 0.05751843316323934ITERATION : 21, loss : 0.057518060844111596ITERATION : 22, loss : 0.05751747347421629ITERATION : 23, loss : 0.0575168532824982ITERATION : 24, loss : 0.05751628572140307ITERATION : 25, loss : 0.05751580361291238ITERATION : 26, loss : 0.05751541229205182ITERATION : 27, loss : 0.05751510437529109ITERATION : 28, loss : 0.05751486741742362ITERATION : 29, loss : 0.05751468812236289ITERATION : 30, loss : 0.05751455424341119ITERATION : 31, loss : 0.057514455342050415ITERATION : 32, loss : 0.0575143829022502ITERATION : 33, loss : 0.05751433023743288ITERATION : 34, loss : 0.057514292182597654ITERATION : 35, loss : 0.057514264836360275ITERATION : 36, loss : 0.05751424530995214ITERATION : 37, loss : 0.057514231312636624ITERATION : 38, loss : 0.05751422145726659ITERATION : 39, loss : 0.057514214529268574ITERATION : 40, loss : 0.05751420961244335ITERATION : 41, loss : 0.057514206169804266ITERATION : 42, loss : 0.05751420373296883ITERATION : 43, loss : 0.05751420201843196ITERATION : 44, loss : 0.057514200828606206ITERATION : 45, loss : 0.05751420004047594ITERATION : 46, loss : 0.05751419941028161ITERATION : 47, loss : 0.05751419902569155ITERATION : 48, loss : 0.057514198735000986ITERATION : 49, loss : 0.057514198531634766ITERATION : 50, loss : 0.057514198417958996ITERATION : 51, loss : 0.057514198335093025ITERATION : 52, loss : 0.05751419829203609ITERATION : 53, loss : 0.057514198276288196ITERATION : 54, loss : 0.05751419825996272ITERATION : 55, loss : 0.05751419825996272ITERATION : 56, loss : 0.05751419825996272ITERATION : 57, loss : 0.05751419825996272ITERATION : 58, loss : 0.05751419825996272ITERATION : 59, loss : 0.05751419825996272ITERATION : 60, loss : 0.05751419825996272ITERATION : 61, loss : 0.05751419825996272ITERATION : 62, loss : 0.05751419825996272ITERATION : 63, loss : 0.05751419825996272ITERATION : 64, loss : 0.05751419825996272ITERATION : 65, loss : 0.05751419825996272ITERATION : 66, loss : 0.05751419825996272ITERATION : 67, loss : 0.05751419825996272ITERATION : 68, loss : 0.05751419825996272ITERATION : 69, loss : 0.05751419825996272ITERATION : 70, loss : 0.05751419825996272ITERATION : 71, loss : 0.05751419825996272ITERATION : 72, loss : 0.05751419825996272ITERATION : 73, loss : 0.05751419825996272ITERATION : 74, loss : 0.05751419825996272ITERATION : 75, loss : 0.05751419825996272ITERATION : 76, loss : 0.05751419825996272ITERATION : 77, loss : 0.05751419825996272ITERATION : 78, loss : 0.05751419825996272ITERATION : 79, loss : 0.05751419825996272ITERATION : 80, loss : 0.05751419825996272ITERATION : 81, loss : 0.05751419825996272ITERATION : 82, loss : 0.05751419825996272ITERATION : 83, loss : 0.05751419825996272ITERATION : 84, loss : 0.05751419825996272ITERATION : 85, loss : 0.05751419825996272ITERATION : 86, loss : 0.05751419825996272ITERATION : 87, loss : 0.05751419825996272ITERATION : 88, loss : 0.05751419825996272ITERATION : 89, loss : 0.05751419825996272ITERATION : 90, loss : 0.05751419825996272ITERATION : 91, loss : 0.05751419825996272ITERATION : 92, loss : 0.05751419825996272ITERATION : 93, loss : 0.05751419825996272ITERATION : 94, loss : 0.05751419825996272ITERATION : 95, loss : 0.05751419825996272ITERATION : 96, loss : 0.05751419825996272ITERATION : 97, loss : 0.05751419825996272ITERATION : 98, loss : 0.05751419825996272ITERATION : 99, loss : 0.05751419825996272ITERATION : 100, loss : 0.05751419825996272
ITERATION : 1, loss : 0.05153529015230283ITERATION : 2, loss : 0.05907192742637719ITERATION : 3, loss : 0.0674139105971131ITERATION : 4, loss : 0.07437798535453292ITERATION : 5, loss : 0.07964376892972723ITERATION : 6, loss : 0.08342248611365655ITERATION : 7, loss : 0.08605622929266263ITERATION : 8, loss : 0.087862077982178ITERATION : 9, loss : 0.0890888343701654ITERATION : 10, loss : 0.08991779950191697ITERATION : 11, loss : 0.09047625176967987ITERATION : 12, loss : 0.09085178731429298ITERATION : 13, loss : 0.09110403853452993ITERATION : 14, loss : 0.09127335479911851ITERATION : 15, loss : 0.09138694381345679ITERATION : 16, loss : 0.09146311509768501ITERATION : 17, loss : 0.09151417569430867ITERATION : 18, loss : 0.0915483914093409ITERATION : 19, loss : 0.09157131135149157ITERATION : 20, loss : 0.091586659052286ITERATION : 21, loss : 0.09159693244739475ITERATION : 22, loss : 0.09160380659606573ITERATION : 23, loss : 0.09160840439109509ITERATION : 24, loss : 0.09161147828466296ITERATION : 25, loss : 0.09161353250135933ITERATION : 26, loss : 0.09161490469556574ITERATION : 27, loss : 0.09161582089187179ITERATION : 28, loss : 0.09161643230271324ITERATION : 29, loss : 0.09161684018240994ITERATION : 30, loss : 0.09161711202241081ITERATION : 31, loss : 0.09161729327305111ITERATION : 32, loss : 0.09161741396365061ITERATION : 33, loss : 0.0916174942968897ITERATION : 34, loss : 0.09161754769655571ITERATION : 35, loss : 0.09161758317010435ITERATION : 36, loss : 0.09161760681881528ITERATION : 37, loss : 0.09161762252582317ITERATION : 38, loss : 0.09161763285349359ITERATION : 39, loss : 0.09161763975740261ITERATION : 40, loss : 0.09161764440097814ITERATION : 41, loss : 0.09161764744253009ITERATION : 42, loss : 0.0916176494049804ITERATION : 43, loss : 0.09161765072282749ITERATION : 44, loss : 0.09161765156161987ITERATION : 45, loss : 0.09161765209302357ITERATION : 46, loss : 0.09161765247379457ITERATION : 47, loss : 0.09161765264116413ITERATION : 48, loss : 0.0916176528394798ITERATION : 49, loss : 0.09161765288047603ITERATION : 50, loss : 0.09161765298365099ITERATION : 51, loss : 0.09161765297320719ITERATION : 52, loss : 0.09161765302508783ITERATION : 53, loss : 0.09161765301373061ITERATION : 54, loss : 0.09161765301559016ITERATION : 55, loss : 0.09161765301559016ITERATION : 56, loss : 0.09161765301559016ITERATION : 57, loss : 0.09161765301559016ITERATION : 58, loss : 0.09161765301559016ITERATION : 59, loss : 0.09161765301559016ITERATION : 60, loss : 0.09161765301559016ITERATION : 61, loss : 0.09161765301559016ITERATION : 62, loss : 0.09161765301559016ITERATION : 63, loss : 0.09161765301559016ITERATION : 64, loss : 0.09161765301559016ITERATION : 65, loss : 0.09161765301559016ITERATION : 66, loss : 0.09161765301559016ITERATION : 67, loss : 0.09161765301559016ITERATION : 68, loss : 0.09161765301559016ITERATION : 69, loss : 0.09161765301559016ITERATION : 70, loss : 0.09161765301559016ITERATION : 71, loss : 0.09161765301559016ITERATION : 72, loss : 0.09161765301559016ITERATION : 73, loss : 0.09161765301559016ITERATION : 74, loss : 0.09161765301559016ITERATION : 75, loss : 0.09161765301559016ITERATION : 76, loss : 0.09161765301559016ITERATION : 77, loss : 0.09161765301559016ITERATION : 78, loss : 0.09161765301559016ITERATION : 79, loss : 0.09161765301559016ITERATION : 80, loss : 0.09161765301559016ITERATION : 81, loss : 0.09161765301559016ITERATION : 82, loss : 0.09161765301559016ITERATION : 83, loss : 0.09161765301559016ITERATION : 84, loss : 0.09161765301559016ITERATION : 85, loss : 0.09161765301559016ITERATION : 86, loss : 0.09161765301559016ITERATION : 87, loss : 0.09161765301559016ITERATION : 88, loss : 0.09161765301559016ITERATION : 89, loss : 0.09161765301559016ITERATION : 90, loss : 0.09161765301559016ITERATION : 91, loss : 0.09161765301559016ITERATION : 92, loss : 0.09161765301559016ITERATION : 93, loss : 0.09161765301559016ITERATION : 94, loss : 0.09161765301559016ITERATION : 95, loss : 0.09161765301559016ITERATION : 96, loss : 0.09161765301559016ITERATION : 97, loss : 0.09161765301559016ITERATION : 98, loss : 0.09161765301559016ITERATION : 99, loss : 0.09161765301559016ITERATION : 100, loss : 0.09161765301559016
ITERATION : 1, loss : 0.1785695583003123ITERATION : 2, loss : 0.203416389330641ITERATION : 3, loss : 0.2138599192045881ITERATION : 4, loss : 0.2202335278009347ITERATION : 5, loss : 0.22449256442131563ITERATION : 6, loss : 0.22744121371784717ITERATION : 7, loss : 0.22951355143342947ITERATION : 8, loss : 0.2309806906044347ITERATION : 9, loss : 0.23202360282686166ITERATION : 10, loss : 0.2327668124518797ITERATION : 11, loss : 0.23329731506637136ITERATION : 12, loss : 0.23367640892990713ITERATION : 13, loss : 0.23394751429836488ITERATION : 14, loss : 0.2341414949196329ITERATION : 15, loss : 0.2342803418449801ITERATION : 16, loss : 0.23437974969205833ITERATION : 17, loss : 0.234450932734855ITERATION : 18, loss : 0.23450191021303887ITERATION : 19, loss : 0.2345384199980892ITERATION : 20, loss : 0.23456456904763878ITERATION : 21, loss : 0.23458329784614343ITERATION : 22, loss : 0.2345967120899074ITERATION : 23, loss : 0.23460631979857877ITERATION : 24, loss : 0.23461320101209684ITERATION : 25, loss : 0.23461812942568666ITERATION : 26, loss : 0.23462165910743887ITERATION : 27, loss : 0.23462418696164541ITERATION : 28, loss : 0.2346259973169798ITERATION : 29, loss : 0.23462729376755684ITERATION : 30, loss : 0.23462822217141266ITERATION : 31, loss : 0.23462888701388485ITERATION : 32, loss : 0.23462936307013055ITERATION : 33, loss : 0.23462970396497962ITERATION : 34, loss : 0.23462994804990395ITERATION : 35, loss : 0.23463012283272433ITERATION : 36, loss : 0.2346302479699175ITERATION : 37, loss : 0.23463033755927898ITERATION : 38, loss : 0.2346304017025752ITERATION : 39, loss : 0.23463044765821894ITERATION : 40, loss : 0.23463048053586655ITERATION : 41, loss : 0.23463050409858446ITERATION : 42, loss : 0.23463052093938008ITERATION : 43, loss : 0.23463053303833867ITERATION : 44, loss : 0.23463054167580535ITERATION : 45, loss : 0.23463054786124593ITERATION : 46, loss : 0.23463055227414023ITERATION : 47, loss : 0.23463055541764333ITERATION : 48, loss : 0.2346305576736085ITERATION : 49, loss : 0.23463055926565335ITERATION : 50, loss : 0.2346305604179428ITERATION : 51, loss : 0.23463056123725945ITERATION : 52, loss : 0.23463056181238054ITERATION : 53, loss : 0.23463056220283482ITERATION : 54, loss : 0.23463056246941358ITERATION : 55, loss : 0.2346305626741361ITERATION : 56, loss : 0.23463056284363737ITERATION : 57, loss : 0.23463056295607498ITERATION : 58, loss : 0.23463056303894708ITERATION : 59, loss : 0.23463056307664498ITERATION : 60, loss : 0.23463056312527275ITERATION : 61, loss : 0.23463056312713937ITERATION : 62, loss : 0.23463056312713937ITERATION : 63, loss : 0.23463056312713937ITERATION : 64, loss : 0.23463056312713937ITERATION : 65, loss : 0.23463056312713937ITERATION : 66, loss : 0.23463056312713937ITERATION : 67, loss : 0.23463056312713937ITERATION : 68, loss : 0.23463056312713937ITERATION : 69, loss : 0.23463056312713937ITERATION : 70, loss : 0.23463056312713937ITERATION : 71, loss : 0.23463056312713937ITERATION : 72, loss : 0.23463056312713937ITERATION : 73, loss : 0.23463056312713937ITERATION : 74, loss : 0.23463056312713937ITERATION : 75, loss : 0.23463056312713937ITERATION : 76, loss : 0.23463056312713937ITERATION : 77, loss : 0.23463056312713937ITERATION : 78, loss : 0.23463056312713937ITERATION : 79, loss : 0.23463056312713937ITERATION : 80, loss : 0.23463056312713937ITERATION : 81, loss : 0.23463056312713937ITERATION : 82, loss : 0.23463056312713937ITERATION : 83, loss : 0.23463056312713937ITERATION : 84, loss : 0.23463056312713937ITERATION : 85, loss : 0.23463056312713937ITERATION : 86, loss : 0.23463056312713937ITERATION : 87, loss : 0.23463056312713937ITERATION : 88, loss : 0.23463056312713937ITERATION : 89, loss : 0.23463056312713937ITERATION : 90, loss : 0.23463056312713937ITERATION : 91, loss : 0.23463056312713937ITERATION : 92, loss : 0.23463056312713937ITERATION : 93, loss : 0.23463056312713937ITERATION : 94, loss : 0.23463056312713937ITERATION : 95, loss : 0.23463056312713937ITERATION : 96, loss : 0.23463056312713937ITERATION : 97, loss : 0.23463056312713937ITERATION : 98, loss : 0.23463056312713937ITERATION : 99, loss : 0.23463056312713937ITERATION : 100, loss : 0.23463056312713937
ITERATION : 1, loss : 0.04731869949545888ITERATION : 2, loss : 0.07119140813962146ITERATION : 3, loss : 0.08751896676051805ITERATION : 4, loss : 0.09800552013375104ITERATION : 5, loss : 0.10556983901950823ITERATION : 6, loss : 0.1108385363313577ITERATION : 7, loss : 0.11290740976221222ITERATION : 8, loss : 0.11441002876604406ITERATION : 9, loss : 0.11550834053690277ITERATION : 10, loss : 0.11631451384757822ITERATION : 11, loss : 0.11690795751303029ITERATION : 12, loss : 0.11734569716625651ITERATION : 13, loss : 0.1176690692333595ITERATION : 14, loss : 0.11790822643777865ITERATION : 15, loss : 0.118085258416377ITERATION : 16, loss : 0.1182163976355746ITERATION : 17, loss : 0.11831359895597968ITERATION : 18, loss : 0.11838568161288474ITERATION : 19, loss : 0.1184391600358256ITERATION : 20, loss : 0.11847885108499596ITERATION : 21, loss : 0.1185083193217181ITERATION : 22, loss : 0.11853020434188623ITERATION : 23, loss : 0.11854646214633766ITERATION : 24, loss : 0.11855854260139831ITERATION : 25, loss : 0.11856752117118587ITERATION : 26, loss : 0.11857419569209125ITERATION : 27, loss : 0.11857915838553515ITERATION : 28, loss : 0.11858284892657368ITERATION : 29, loss : 0.11858559388911272ITERATION : 30, loss : 0.11858763585924782ITERATION : 31, loss : 0.11858915506151586ITERATION : 32, loss : 0.11859028549018347ITERATION : 33, loss : 0.11859112673990034ITERATION : 34, loss : 0.118591752847975ITERATION : 35, loss : 0.11859221885522135ITERATION : 36, loss : 0.11859256577922957ITERATION : 37, loss : 0.11859282405729882ITERATION : 38, loss : 0.11859301636445685ITERATION : 39, loss : 0.1185931595324834ITERATION : 40, loss : 0.11859326614373979ITERATION : 41, loss : 0.11859334553841253ITERATION : 42, loss : 0.11859340467405662ITERATION : 43, loss : 0.11859344870763025ITERATION : 44, loss : 0.1185934815146064ITERATION : 45, loss : 0.11859350593073525ITERATION : 46, loss : 0.11859352410633536ITERATION : 47, loss : 0.11859353765248988ITERATION : 48, loss : 0.11859354771872722ITERATION : 49, loss : 0.11859355523480988ITERATION : 50, loss : 0.11859356082298333ITERATION : 51, loss : 0.11859356500611ITERATION : 52, loss : 0.11859356811333188ITERATION : 53, loss : 0.1185935704286971ITERATION : 54, loss : 0.11859357215237046ITERATION : 55, loss : 0.1185935734391033ITERATION : 56, loss : 0.11859357439012067ITERATION : 57, loss : 0.11859357509435399ITERATION : 58, loss : 0.11859357562962286ITERATION : 59, loss : 0.11859357603245654ITERATION : 60, loss : 0.118593576295974ITERATION : 61, loss : 0.11859357648362874ITERATION : 62, loss : 0.1185935766298579ITERATION : 63, loss : 0.1185935767432965ITERATION : 64, loss : 0.11859357685680065ITERATION : 65, loss : 0.1185935769304692ITERATION : 66, loss : 0.11859357699083654ITERATION : 67, loss : 0.11859357700903896ITERATION : 68, loss : 0.11859357703502806ITERATION : 69, loss : 0.11859357704041848ITERATION : 70, loss : 0.11859357704041848ITERATION : 71, loss : 0.11859357704041848ITERATION : 72, loss : 0.11859357704041848ITERATION : 73, loss : 0.11859357704041848ITERATION : 74, loss : 0.11859357704041848ITERATION : 75, loss : 0.11859357704041848ITERATION : 76, loss : 0.11859357704041848ITERATION : 77, loss : 0.11859357704041848ITERATION : 78, loss : 0.11859357704041848ITERATION : 79, loss : 0.11859357704041848ITERATION : 80, loss : 0.11859357704041848ITERATION : 81, loss : 0.11859357704041848ITERATION : 82, loss : 0.11859357704041848ITERATION : 83, loss : 0.11859357704041848ITERATION : 84, loss : 0.11859357704041848ITERATION : 85, loss : 0.11859357704041848ITERATION : 86, loss : 0.11859357704041848ITERATION : 87, loss : 0.11859357704041848ITERATION : 88, loss : 0.11859357704041848ITERATION : 89, loss : 0.11859357704041848ITERATION : 90, loss : 0.11859357704041848ITERATION : 91, loss : 0.11859357704041848ITERATION : 92, loss : 0.11859357704041848ITERATION : 93, loss : 0.11859357704041848ITERATION : 94, loss : 0.11859357704041848ITERATION : 95, loss : 0.11859357704041848ITERATION : 96, loss : 0.11859357704041848ITERATION : 97, loss : 0.11859357704041848ITERATION : 98, loss : 0.11859357704041848ITERATION : 99, loss : 0.11859357704041848ITERATION : 100, loss : 0.11859357704041848
ITERATION : 1, loss : 0.028737491181959403ITERATION : 2, loss : 0.03712202811344543ITERATION : 3, loss : 0.04537077016926574ITERATION : 4, loss : 0.05200368194473703ITERATION : 5, loss : 0.05695368764206198ITERATION : 6, loss : 0.06051846821816177ITERATION : 7, loss : 0.06303960513917624ITERATION : 8, loss : 0.06480513229724717ITERATION : 9, loss : 0.06603427253940768ITERATION : 10, loss : 0.06688668569659581ITERATION : 11, loss : 0.06747617811169689ITERATION : 12, loss : 0.0678829363085535ITERATION : 13, loss : 0.06816307107649232ITERATION : 14, loss : 0.06835566970650155ITERATION : 15, loss : 0.06848787306676614ITERATION : 16, loss : 0.06857847963990618ITERATION : 17, loss : 0.06864048273012978ITERATION : 18, loss : 0.06868284703235454ITERATION : 19, loss : 0.06871174748456559ITERATION : 20, loss : 0.0687314310864567ITERATION : 21, loss : 0.06874481446545311ITERATION : 22, loss : 0.06875389784826877ITERATION : 23, loss : 0.06876005100174923ITERATION : 24, loss : 0.06876421068283504ITERATION : 25, loss : 0.06876701646013443ITERATION : 26, loss : 0.06876890448681099ITERATION : 27, loss : 0.06877017156113696ITERATION : 28, loss : 0.0687710194557438ITERATION : 29, loss : 0.06877158497402816ITERATION : 30, loss : 0.0687719608233861ITERATION : 31, loss : 0.06877220958935823ITERATION : 32, loss : 0.06877237350709994ITERATION : 33, loss : 0.06877248091149132ITERATION : 34, loss : 0.06877255087896074ITERATION : 35, loss : 0.06877259610702897ITERATION : 36, loss : 0.06877262510057154ITERATION : 37, loss : 0.06877264351727232ITERATION : 38, loss : 0.0687726550380994ITERATION : 39, loss : 0.06877266213203917ITERATION : 40, loss : 0.0687726664168887ITERATION : 41, loss : 0.06877266897020894ITERATION : 42, loss : 0.06877267035492915ITERATION : 43, loss : 0.06877267109337852ITERATION : 44, loss : 0.0687726714637166ITERATION : 45, loss : 0.06877267162562009ITERATION : 46, loss : 0.06877267159419549ITERATION : 47, loss : 0.06877267150663866ITERATION : 48, loss : 0.0687726713978772ITERATION : 49, loss : 0.0687726712590407ITERATION : 50, loss : 0.06877267114733276ITERATION : 51, loss : 0.06877267103442583ITERATION : 52, loss : 0.06877267095988059ITERATION : 53, loss : 0.06877267087356057ITERATION : 54, loss : 0.06877267082795445ITERATION : 55, loss : 0.06877267077596709ITERATION : 56, loss : 0.06877267075182163ITERATION : 57, loss : 0.06877267071838127ITERATION : 58, loss : 0.06877267071090455ITERATION : 59, loss : 0.06877267069165273ITERATION : 60, loss : 0.06877267069025378ITERATION : 61, loss : 0.06877267069027952ITERATION : 62, loss : 0.06877267068936019ITERATION : 63, loss : 0.06877267068936019ITERATION : 64, loss : 0.06877267068936019ITERATION : 65, loss : 0.06877267068936019ITERATION : 66, loss : 0.06877267068936019ITERATION : 67, loss : 0.06877267068936019ITERATION : 68, loss : 0.06877267068936019ITERATION : 69, loss : 0.06877267068936019ITERATION : 70, loss : 0.06877267068936019ITERATION : 71, loss : 0.06877267068936019ITERATION : 72, loss : 0.06877267068936019ITERATION : 73, loss : 0.06877267068936019ITERATION : 74, loss : 0.06877267068936019ITERATION : 75, loss : 0.06877267068936019ITERATION : 76, loss : 0.06877267068936019ITERATION : 77, loss : 0.06877267068936019ITERATION : 78, loss : 0.06877267068936019ITERATION : 79, loss : 0.06877267068936019ITERATION : 80, loss : 0.06877267068936019ITERATION : 81, loss : 0.06877267068936019ITERATION : 82, loss : 0.06877267068936019ITERATION : 83, loss : 0.06877267068936019ITERATION : 84, loss : 0.06877267068936019ITERATION : 85, loss : 0.06877267068936019ITERATION : 86, loss : 0.06877267068936019ITERATION : 87, loss : 0.06877267068936019ITERATION : 88, loss : 0.06877267068936019ITERATION : 89, loss : 0.06877267068936019ITERATION : 90, loss : 0.06877267068936019ITERATION : 91, loss : 0.06877267068936019ITERATION : 92, loss : 0.06877267068936019ITERATION : 93, loss : 0.06877267068936019ITERATION : 94, loss : 0.06877267068936019ITERATION : 95, loss : 0.06877267068936019ITERATION : 96, loss : 0.06877267068936019ITERATION : 97, loss : 0.06877267068936019ITERATION : 98, loss : 0.06877267068936019ITERATION : 99, loss : 0.06877267068936019ITERATION : 100, loss : 0.06877267068936019
gradient norm in None layer : 0.006321480925390441
gradient norm in None layer : 0.00013021336806887733
gradient norm in None layer : 0.00011945422135651272
gradient norm in None layer : 0.0022174895425700536
gradient norm in None layer : 0.000151323895547093
gradient norm in None layer : 0.0001405025607683696
gradient norm in None layer : 0.000861924540248786
gradient norm in None layer : 3.424249943187158e-05
gradient norm in None layer : 3.030473071955375e-05
gradient norm in None layer : 0.0007361676615784861
gradient norm in None layer : 3.4591031098099025e-05
gradient norm in None layer : 3.106836666815162e-05
gradient norm in None layer : 0.000266101815971701
gradient norm in None layer : 7.799024891472651e-06
gradient norm in None layer : 5.728198594123193e-06
gradient norm in None layer : 0.0002466797565615324
gradient norm in None layer : 8.290959208112873e-06
gradient norm in None layer : 6.985425885876898e-06
gradient norm in None layer : 0.0003123854904159434
gradient norm in None layer : 3.2619112588245586e-06
gradient norm in None layer : 0.0006616406262172252
gradient norm in None layer : 2.9595198725006974e-05
gradient norm in None layer : 2.4473781138306925e-05
gradient norm in None layer : 0.000655282105373578
gradient norm in None layer : 6.552107767021951e-05
gradient norm in None layer : 7.615096164818602e-05
gradient norm in None layer : 0.001273908803970527
gradient norm in None layer : 1.2207788211429897e-05
gradient norm in None layer : 0.0018541340837839812
gradient norm in None layer : 0.00013203008975594348
gradient norm in None layer : 0.00010287681208902731
gradient norm in None layer : 0.0021658295923807037
gradient norm in None layer : 0.0004519532386083135
gradient norm in None layer : 0.0006865236688423674
gradient norm in None layer : 0.00031610835921326135
gradient norm in None layer : 0.0001137828847346266
Total gradient norm: 0.0076103639395338615
invariance loss : 5.329840211308763, avg_den : 0.39080810546875, density loss : 0.2908051540062616, mse loss : 0.09384942303934944, solver time : 111.98344016075134 sec , total loss : 0.09947006840466446, running loss : 0.11290583221783293
Epoch 0/10 , batch 7/12500 
ITERATION : 1, loss : 0.1735122268972129ITERATION : 2, loss : 0.21835728036933064ITERATION : 3, loss : 0.23982386464270558ITERATION : 4, loss : 0.252646809632603ITERATION : 5, loss : 0.2605535950038142ITERATION : 6, loss : 0.26554816858774294ITERATION : 7, loss : 0.26880615788151163ITERATION : 8, loss : 0.2709754685823712ITERATION : 9, loss : 0.2724392070858638ITERATION : 10, loss : 0.27343539252964105ITERATION : 11, loss : 0.27411712956675294ITERATION : 12, loss : 0.2745853075399876ITERATION : 13, loss : 0.2749075166123575ITERATION : 14, loss : 0.2751295459767455ITERATION : 15, loss : 0.2752826455149504ITERATION : 16, loss : 0.2753882448478264ITERATION : 17, loss : 0.2754610837232608ITERATION : 18, loss : 0.27551131919747746ITERATION : 19, loss : 0.2755459575649086ITERATION : 20, loss : 0.2755698343377231ITERATION : 21, loss : 0.2755862874425031ITERATION : 22, loss : 0.27559762085955813ITERATION : 23, loss : 0.2756054246497243ITERATION : 24, loss : 0.2756107959526852ITERATION : 25, loss : 0.2756144914550447ITERATION : 26, loss : 0.27561703292280776ITERATION : 27, loss : 0.27561878000263ITERATION : 28, loss : 0.27561998044520414ITERATION : 29, loss : 0.27562080491285584ITERATION : 30, loss : 0.27562137090429567ITERATION : 31, loss : 0.2756217592551047ITERATION : 32, loss : 0.27562202557873744ITERATION : 33, loss : 0.2756222081161068ITERATION : 34, loss : 0.2756223331536823ITERATION : 35, loss : 0.27562241877146365ITERATION : 36, loss : 0.2756224773458706ITERATION : 37, loss : 0.27562251740064436ITERATION : 38, loss : 0.27562254476167825ITERATION : 39, loss : 0.2756225634331126ITERATION : 40, loss : 0.27562257618700864ITERATION : 41, loss : 0.2756225848743862ITERATION : 42, loss : 0.27562259079837237ITERATION : 43, loss : 0.27562259483084806ITERATION : 44, loss : 0.2756225975714678ITERATION : 45, loss : 0.2756225994628809ITERATION : 46, loss : 0.2756226007167911ITERATION : 47, loss : 0.27562260158474655ITERATION : 48, loss : 0.27562260216798323ITERATION : 49, loss : 0.2756226025547067ITERATION : 50, loss : 0.27562260280941ITERATION : 51, loss : 0.275622602972659ITERATION : 52, loss : 0.27562260307757785ITERATION : 53, loss : 0.27562260314441267ITERATION : 54, loss : 0.2756226032015468ITERATION : 55, loss : 0.2756226032307831ITERATION : 56, loss : 0.275622603246678ITERATION : 57, loss : 0.2756226032669365ITERATION : 58, loss : 0.2756226032775855ITERATION : 59, loss : 0.2756226032857496ITERATION : 60, loss : 0.27562260328778154ITERATION : 61, loss : 0.27562260328778154ITERATION : 62, loss : 0.27562260328778154ITERATION : 63, loss : 0.27562260328778154ITERATION : 64, loss : 0.27562260328778154ITERATION : 65, loss : 0.27562260328778154ITERATION : 66, loss : 0.27562260328778154ITERATION : 67, loss : 0.27562260328778154ITERATION : 68, loss : 0.27562260328778154ITERATION : 69, loss : 0.27562260328778154ITERATION : 70, loss : 0.27562260328778154ITERATION : 71, loss : 0.27562260328778154ITERATION : 72, loss : 0.27562260328778154ITERATION : 73, loss : 0.27562260328778154ITERATION : 74, loss : 0.27562260328778154ITERATION : 75, loss : 0.27562260328778154ITERATION : 76, loss : 0.27562260328778154ITERATION : 77, loss : 0.27562260328778154ITERATION : 78, loss : 0.27562260328778154ITERATION : 79, loss : 0.27562260328778154ITERATION : 80, loss : 0.27562260328778154ITERATION : 81, loss : 0.27562260328778154ITERATION : 82, loss : 0.27562260328778154ITERATION : 83, loss : 0.27562260328778154ITERATION : 84, loss : 0.27562260328778154ITERATION : 85, loss : 0.27562260328778154ITERATION : 86, loss : 0.27562260328778154ITERATION : 87, loss : 0.27562260328778154ITERATION : 88, loss : 0.27562260328778154ITERATION : 89, loss : 0.27562260328778154ITERATION : 90, loss : 0.27562260328778154ITERATION : 91, loss : 0.27562260328778154ITERATION : 92, loss : 0.27562260328778154ITERATION : 93, loss : 0.27562260328778154ITERATION : 94, loss : 0.27562260328778154ITERATION : 95, loss : 0.27562260328778154ITERATION : 96, loss : 0.27562260328778154ITERATION : 97, loss : 0.27562260328778154ITERATION : 98, loss : 0.27562260328778154ITERATION : 99, loss : 0.27562260328778154ITERATION : 100, loss : 0.27562260328778154
ITERATION : 1, loss : 0.051982553731685546ITERATION : 2, loss : 0.052276080990236ITERATION : 3, loss : 0.05404424162686515ITERATION : 4, loss : 0.055947313558967975ITERATION : 5, loss : 0.05750612721784474ITERATION : 6, loss : 0.058692559157350284ITERATION : 7, loss : 0.05956639537393972ITERATION : 8, loss : 0.060236554950361614ITERATION : 9, loss : 0.06072939046494077ITERATION : 10, loss : 0.06108261843087828ITERATION : 11, loss : 0.061335011119180514ITERATION : 12, loss : 0.06151501846170908ITERATION : 13, loss : 0.06164325048011616ITERATION : 14, loss : 0.06173453034541858ITERATION : 15, loss : 0.06179947410763076ITERATION : 16, loss : 0.06184566478472859ITERATION : 17, loss : 0.06187850998472935ITERATION : 18, loss : 0.0619018618639466ITERATION : 19, loss : 0.061918462495046865ITERATION : 20, loss : 0.06193026279151039ITERATION : 21, loss : 0.061938650312381174ITERATION : 22, loss : 0.061944611908729255ITERATION : 23, loss : 0.06194884907292837ITERATION : 24, loss : 0.061951860548207345ITERATION : 25, loss : 0.061954000851496306ITERATION : 26, loss : 0.061955521996240326ITERATION : 27, loss : 0.06195660306852346ITERATION : 28, loss : 0.061957371374671534ITERATION : 29, loss : 0.06195791740148466ITERATION : 30, loss : 0.061958305459410175ITERATION : 31, loss : 0.0619585812571488ITERATION : 32, loss : 0.0619587772776848ITERATION : 33, loss : 0.06195891655411173ITERATION : 34, loss : 0.06195901559000384ITERATION : 35, loss : 0.061959085903861975ITERATION : 36, loss : 0.06195913590896076ITERATION : 37, loss : 0.06195917142931107ITERATION : 38, loss : 0.061959196638059454ITERATION : 39, loss : 0.061959214556060994ITERATION : 40, loss : 0.06195922726307721ITERATION : 41, loss : 0.06195923630400289ITERATION : 42, loss : 0.061959242719284074ITERATION : 43, loss : 0.06195924728817316ITERATION : 44, loss : 0.061959250511647225ITERATION : 45, loss : 0.06195925280599324ITERATION : 46, loss : 0.06195925442067233ITERATION : 47, loss : 0.061959255579430814ITERATION : 48, loss : 0.0619592564137492ITERATION : 49, loss : 0.06195925701262185ITERATION : 50, loss : 0.06195925743326186ITERATION : 51, loss : 0.061959257668649126ITERATION : 52, loss : 0.06195925790521047ITERATION : 53, loss : 0.06195925806785214ITERATION : 54, loss : 0.06195925818115318ITERATION : 55, loss : 0.061959258212334306ITERATION : 56, loss : 0.0619592582127823ITERATION : 57, loss : 0.0619592582127823ITERATION : 58, loss : 0.0619592582127823ITERATION : 59, loss : 0.0619592582127823ITERATION : 60, loss : 0.0619592582127823ITERATION : 61, loss : 0.0619592582127823ITERATION : 62, loss : 0.0619592582127823ITERATION : 63, loss : 0.0619592582127823ITERATION : 64, loss : 0.0619592582127823ITERATION : 65, loss : 0.0619592582127823ITERATION : 66, loss : 0.0619592582127823ITERATION : 67, loss : 0.0619592582127823ITERATION : 68, loss : 0.0619592582127823ITERATION : 69, loss : 0.0619592582127823ITERATION : 70, loss : 0.0619592582127823ITERATION : 71, loss : 0.0619592582127823ITERATION : 72, loss : 0.0619592582127823ITERATION : 73, loss : 0.0619592582127823ITERATION : 74, loss : 0.0619592582127823ITERATION : 75, loss : 0.0619592582127823ITERATION : 76, loss : 0.0619592582127823ITERATION : 77, loss : 0.0619592582127823ITERATION : 78, loss : 0.0619592582127823ITERATION : 79, loss : 0.0619592582127823ITERATION : 80, loss : 0.0619592582127823ITERATION : 81, loss : 0.0619592582127823ITERATION : 82, loss : 0.0619592582127823ITERATION : 83, loss : 0.0619592582127823ITERATION : 84, loss : 0.0619592582127823ITERATION : 85, loss : 0.0619592582127823ITERATION : 86, loss : 0.0619592582127823ITERATION : 87, loss : 0.0619592582127823ITERATION : 88, loss : 0.0619592582127823ITERATION : 89, loss : 0.0619592582127823ITERATION : 90, loss : 0.0619592582127823ITERATION : 91, loss : 0.0619592582127823ITERATION : 92, loss : 0.0619592582127823ITERATION : 93, loss : 0.0619592582127823ITERATION : 94, loss : 0.0619592582127823ITERATION : 95, loss : 0.0619592582127823ITERATION : 96, loss : 0.0619592582127823ITERATION : 97, loss : 0.0619592582127823ITERATION : 98, loss : 0.0619592582127823ITERATION : 99, loss : 0.0619592582127823ITERATION : 100, loss : 0.0619592582127823
ITERATION : 1, loss : 0.024951520705460913ITERATION : 2, loss : 0.03174364972898318ITERATION : 3, loss : 0.03708310523352124ITERATION : 4, loss : 0.04027000592772801ITERATION : 5, loss : 0.0422748596160464ITERATION : 6, loss : 0.0436772480485521ITERATION : 7, loss : 0.04465258961483526ITERATION : 8, loss : 0.04532944501879837ITERATION : 9, loss : 0.0457988046455915ITERATION : 10, loss : 0.04612421463957811ITERATION : 11, loss : 0.04634982484527493ITERATION : 12, loss : 0.04650625131675837ITERATION : 13, loss : 0.04661471415090591ITERATION : 14, loss : 0.046689920930268435ITERATION : 15, loss : 0.04674206724501353ITERATION : 16, loss : 0.046778222145682155ITERATION : 17, loss : 0.04680328755189962ITERATION : 18, loss : 0.04682066300623783ITERATION : 19, loss : 0.046832706367329106ITERATION : 20, loss : 0.04684105272585909ITERATION : 21, loss : 0.0468468360836151ITERATION : 22, loss : 0.04685084284158875ITERATION : 23, loss : 0.04685361829329768ITERATION : 24, loss : 0.04685554045137589ITERATION : 25, loss : 0.04685687137978071ITERATION : 26, loss : 0.046857792778124616ITERATION : 27, loss : 0.046858430532907204ITERATION : 28, loss : 0.04685887188231839ITERATION : 29, loss : 0.04685917720914266ITERATION : 30, loss : 0.04685938839013139ITERATION : 31, loss : 0.0468595344132083ITERATION : 32, loss : 0.04685963536435055ITERATION : 33, loss : 0.0468597051454042ITERATION : 34, loss : 0.04685975335222681ITERATION : 35, loss : 0.04685978665496673ITERATION : 36, loss : 0.046859809641133454ITERATION : 37, loss : 0.04685982552264985ITERATION : 38, loss : 0.046859836480038525ITERATION : 39, loss : 0.046859844045884745ITERATION : 40, loss : 0.04685984927136096ITERATION : 41, loss : 0.04685985286612555ITERATION : 42, loss : 0.04685985534088716ITERATION : 43, loss : 0.04685985704237672ITERATION : 44, loss : 0.046859858209987344ITERATION : 45, loss : 0.046859858996779184ITERATION : 46, loss : 0.04685985953314853ITERATION : 47, loss : 0.046859859892807366ITERATION : 48, loss : 0.046859860133680355ITERATION : 49, loss : 0.04685986033727656ITERATION : 50, loss : 0.04685986051475072ITERATION : 51, loss : 0.046859860559354406ITERATION : 52, loss : 0.046859860618263256ITERATION : 53, loss : 0.04685986063787879ITERATION : 54, loss : 0.04685986063809995ITERATION : 55, loss : 0.04685986063809995ITERATION : 56, loss : 0.04685986063809995ITERATION : 57, loss : 0.04685986063809995ITERATION : 58, loss : 0.04685986063809995ITERATION : 59, loss : 0.04685986063809995ITERATION : 60, loss : 0.04685986063809995ITERATION : 61, loss : 0.04685986063809995ITERATION : 62, loss : 0.04685986063809995ITERATION : 63, loss : 0.04685986063809995ITERATION : 64, loss : 0.04685986063809995ITERATION : 65, loss : 0.04685986063809995ITERATION : 66, loss : 0.04685986063809995ITERATION : 67, loss : 0.04685986063809995ITERATION : 68, loss : 0.04685986063809995ITERATION : 69, loss : 0.04685986063809995ITERATION : 70, loss : 0.04685986063809995ITERATION : 71, loss : 0.04685986063809995ITERATION : 72, loss : 0.04685986063809995ITERATION : 73, loss : 0.04685986063809995ITERATION : 74, loss : 0.04685986063809995ITERATION : 75, loss : 0.04685986063809995ITERATION : 76, loss : 0.04685986063809995ITERATION : 77, loss : 0.04685986063809995ITERATION : 78, loss : 0.04685986063809995ITERATION : 79, loss : 0.04685986063809995ITERATION : 80, loss : 0.04685986063809995ITERATION : 81, loss : 0.04685986063809995ITERATION : 82, loss : 0.04685986063809995ITERATION : 83, loss : 0.04685986063809995ITERATION : 84, loss : 0.04685986063809995ITERATION : 85, loss : 0.04685986063809995ITERATION : 86, loss : 0.04685986063809995ITERATION : 87, loss : 0.04685986063809995ITERATION : 88, loss : 0.04685986063809995ITERATION : 89, loss : 0.04685986063809995ITERATION : 90, loss : 0.04685986063809995ITERATION : 91, loss : 0.04685986063809995ITERATION : 92, loss : 0.04685986063809995ITERATION : 93, loss : 0.04685986063809995ITERATION : 94, loss : 0.04685986063809995ITERATION : 95, loss : 0.04685986063809995ITERATION : 96, loss : 0.04685986063809995ITERATION : 97, loss : 0.04685986063809995ITERATION : 98, loss : 0.04685986063809995ITERATION : 99, loss : 0.04685986063809995ITERATION : 100, loss : 0.04685986063809995
ITERATION : 1, loss : 0.04197404363402334ITERATION : 2, loss : 0.04584444083432658ITERATION : 3, loss : 0.050983894167719694ITERATION : 4, loss : 0.05458347959042749ITERATION : 5, loss : 0.05739576074731794ITERATION : 6, loss : 0.05948909204640361ITERATION : 7, loss : 0.061014422198491694ITERATION : 8, loss : 0.062114802225891705ITERATION : 9, loss : 0.06290498697875736ITERATION : 10, loss : 0.06347142388691727ITERATION : 11, loss : 0.06387739204130681ITERATION : 12, loss : 0.06416855894904848ITERATION : 13, loss : 0.06437764898715664ITERATION : 14, loss : 0.06452803323388719ITERATION : 15, loss : 0.06463638245272303ITERATION : 16, loss : 0.06471458886965417ITERATION : 17, loss : 0.06477114285349374ITERATION : 18, loss : 0.06481211424166824ITERATION : 19, loss : 0.06484184968011757ITERATION : 20, loss : 0.06486346762338865ITERATION : 21, loss : 0.06487920976417536ITERATION : 22, loss : 0.06489069084465969ITERATION : 23, loss : 0.0648990763413203ITERATION : 24, loss : 0.0649052091017932ITERATION : 25, loss : 0.06490969984055842ITERATION : 26, loss : 0.06491299202618209ITERATION : 27, loss : 0.0649154080677453ITERATION : 28, loss : 0.06491718282637725ITERATION : 29, loss : 0.06491848763987687ITERATION : 30, loss : 0.06491944782579487ITERATION : 31, loss : 0.06492015470126693ITERATION : 32, loss : 0.06492067557768476ITERATION : 33, loss : 0.06492105960386955ITERATION : 34, loss : 0.06492134283789797ITERATION : 35, loss : 0.0649215518640045ITERATION : 36, loss : 0.06492170617040247ITERATION : 37, loss : 0.06492182014939167ITERATION : 38, loss : 0.0649219043635696ITERATION : 39, loss : 0.06492196667188249ITERATION : 40, loss : 0.06492201270429779ITERATION : 41, loss : 0.0649220467069112ITERATION : 42, loss : 0.06492207184524633ITERATION : 43, loss : 0.06492209042124851ITERATION : 44, loss : 0.06492210415147283ITERATION : 45, loss : 0.0649221143128775ITERATION : 46, loss : 0.0649221218460575ITERATION : 47, loss : 0.06492212741440714ITERATION : 48, loss : 0.06492213154830916ITERATION : 49, loss : 0.06492213460236654ITERATION : 50, loss : 0.0649221368696037ITERATION : 51, loss : 0.06492213854990032ITERATION : 52, loss : 0.06492213978826633ITERATION : 53, loss : 0.06492214067183182ITERATION : 54, loss : 0.0649221413389974ITERATION : 55, loss : 0.06492214185507401ITERATION : 56, loss : 0.06492214223360189ITERATION : 57, loss : 0.0649221425000173ITERATION : 58, loss : 0.06492214268593972ITERATION : 59, loss : 0.06492214284919312ITERATION : 60, loss : 0.06492214299432582ITERATION : 61, loss : 0.06492214306076095ITERATION : 62, loss : 0.06492214311034158ITERATION : 63, loss : 0.06492214313176276ITERATION : 64, loss : 0.06492214315495173ITERATION : 65, loss : 0.06492214315722315ITERATION : 66, loss : 0.06492214315722315ITERATION : 67, loss : 0.06492214315722315ITERATION : 68, loss : 0.06492214315722315ITERATION : 69, loss : 0.06492214315722315ITERATION : 70, loss : 0.06492214315722315ITERATION : 71, loss : 0.06492214315722315ITERATION : 72, loss : 0.06492214315722315ITERATION : 73, loss : 0.06492214315722315ITERATION : 74, loss : 0.06492214315722315ITERATION : 75, loss : 0.06492214315722315ITERATION : 76, loss : 0.06492214315722315ITERATION : 77, loss : 0.06492214315722315ITERATION : 78, loss : 0.06492214315722315ITERATION : 79, loss : 0.06492214315722315ITERATION : 80, loss : 0.06492214315722315ITERATION : 81, loss : 0.06492214315722315ITERATION : 82, loss : 0.06492214315722315ITERATION : 83, loss : 0.06492214315722315ITERATION : 84, loss : 0.06492214315722315ITERATION : 85, loss : 0.06492214315722315ITERATION : 86, loss : 0.06492214315722315ITERATION : 87, loss : 0.06492214315722315ITERATION : 88, loss : 0.06492214315722315ITERATION : 89, loss : 0.06492214315722315ITERATION : 90, loss : 0.06492214315722315ITERATION : 91, loss : 0.06492214315722315ITERATION : 92, loss : 0.06492214315722315ITERATION : 93, loss : 0.06492214315722315ITERATION : 94, loss : 0.06492214315722315ITERATION : 95, loss : 0.06492214315722315ITERATION : 96, loss : 0.06492214315722315ITERATION : 97, loss : 0.06492214315722315ITERATION : 98, loss : 0.06492214315722315ITERATION : 99, loss : 0.06492214315722315ITERATION : 100, loss : 0.06492214315722315
ITERATION : 1, loss : 0.024082439835285ITERATION : 2, loss : 0.03365939256391514ITERATION : 3, loss : 0.04246352140509919ITERATION : 4, loss : 0.04906873926820182ITERATION : 5, loss : 0.05378644822455942ITERATION : 6, loss : 0.05706217624758472ITERATION : 7, loss : 0.059333719366529585ITERATION : 8, loss : 0.060912292928514085ITERATION : 9, loss : 0.062011862098832936ITERATION : 10, loss : 0.06277911434135719ITERATION : 11, loss : 0.06331505886259683ITERATION : 12, loss : 0.06368961881749959ITERATION : 13, loss : 0.06395140984636427ITERATION : 14, loss : 0.06413433947308426ITERATION : 15, loss : 0.0642621047709959ITERATION : 16, loss : 0.06435128635520171ITERATION : 17, loss : 0.06441349139141354ITERATION : 18, loss : 0.06445684581769325ITERATION : 19, loss : 0.06448703682456694ITERATION : 20, loss : 0.06450804294405514ITERATION : 21, loss : 0.06452264548158208ITERATION : 22, loss : 0.06453278727811176ITERATION : 23, loss : 0.06453982448786749ITERATION : 24, loss : 0.06454470298297546ITERATION : 25, loss : 0.06454808172088769ITERATION : 26, loss : 0.06455041954705698ITERATION : 27, loss : 0.06455203555697342ITERATION : 28, loss : 0.0645531515355885ITERATION : 29, loss : 0.06455392141889693ITERATION : 30, loss : 0.06455445200775535ITERATION : 31, loss : 0.06455481732166164ITERATION : 32, loss : 0.06455506855535977ITERATION : 33, loss : 0.06455524116571351ITERATION : 34, loss : 0.06455535955844231ITERATION : 35, loss : 0.06455544074277832ITERATION : 36, loss : 0.06455549634179203ITERATION : 37, loss : 0.0645555343464324ITERATION : 38, loss : 0.06455556029552523ITERATION : 39, loss : 0.06455557798232778ITERATION : 40, loss : 0.06455559003867034ITERATION : 41, loss : 0.0645555982244089ITERATION : 42, loss : 0.0645556037980943ITERATION : 43, loss : 0.06455560757784899ITERATION : 44, loss : 0.06455561012246093ITERATION : 45, loss : 0.06455561184522467ITERATION : 46, loss : 0.06455561301545053ITERATION : 47, loss : 0.06455561377137586ITERATION : 48, loss : 0.06455561429009585ITERATION : 49, loss : 0.06455561461398937ITERATION : 50, loss : 0.0645556148231411ITERATION : 51, loss : 0.06455561497138941ITERATION : 52, loss : 0.06455561505932336ITERATION : 53, loss : 0.06455561512016617ITERATION : 54, loss : 0.06455561515905385ITERATION : 55, loss : 0.06455561520082533ITERATION : 56, loss : 0.06455561522464734ITERATION : 57, loss : 0.06455561522514708ITERATION : 58, loss : 0.06455561522514708ITERATION : 59, loss : 0.06455561522514708ITERATION : 60, loss : 0.06455561522514708ITERATION : 61, loss : 0.06455561522514708ITERATION : 62, loss : 0.06455561522514708ITERATION : 63, loss : 0.06455561522514708ITERATION : 64, loss : 0.06455561522514708ITERATION : 65, loss : 0.06455561522514708ITERATION : 66, loss : 0.06455561522514708ITERATION : 67, loss : 0.06455561522514708ITERATION : 68, loss : 0.06455561522514708ITERATION : 69, loss : 0.06455561522514708ITERATION : 70, loss : 0.06455561522514708ITERATION : 71, loss : 0.06455561522514708ITERATION : 72, loss : 0.06455561522514708ITERATION : 73, loss : 0.06455561522514708ITERATION : 74, loss : 0.06455561522514708ITERATION : 75, loss : 0.06455561522514708ITERATION : 76, loss : 0.06455561522514708ITERATION : 77, loss : 0.06455561522514708ITERATION : 78, loss : 0.06455561522514708ITERATION : 79, loss : 0.06455561522514708ITERATION : 80, loss : 0.06455561522514708ITERATION : 81, loss : 0.06455561522514708ITERATION : 82, loss : 0.06455561522514708ITERATION : 83, loss : 0.06455561522514708ITERATION : 84, loss : 0.06455561522514708ITERATION : 85, loss : 0.06455561522514708ITERATION : 86, loss : 0.06455561522514708ITERATION : 87, loss : 0.06455561522514708ITERATION : 88, loss : 0.06455561522514708ITERATION : 89, loss : 0.06455561522514708ITERATION : 90, loss : 0.06455561522514708ITERATION : 91, loss : 0.06455561522514708ITERATION : 92, loss : 0.06455561522514708ITERATION : 93, loss : 0.06455561522514708ITERATION : 94, loss : 0.06455561522514708ITERATION : 95, loss : 0.06455561522514708ITERATION : 96, loss : 0.06455561522514708ITERATION : 97, loss : 0.06455561522514708ITERATION : 98, loss : 0.06455561522514708ITERATION : 99, loss : 0.06455561522514708ITERATION : 100, loss : 0.06455561522514708
ITERATION : 1, loss : 0.06832445709603731ITERATION : 2, loss : 0.06823559902310426ITERATION : 3, loss : 0.07408920777344899ITERATION : 4, loss : 0.08088448408694313ITERATION : 5, loss : 0.08636436810401554ITERATION : 6, loss : 0.09052815692281126ITERATION : 7, loss : 0.09362123147425505ITERATION : 8, loss : 0.09589402254904686ITERATION : 9, loss : 0.09755477600843838ITERATION : 10, loss : 0.09876465739202585ITERATION : 11, loss : 0.09964456847788646ITERATION : 12, loss : 0.1002838535172729ITERATION : 13, loss : 0.10074802380625282ITERATION : 14, loss : 0.10108491089416227ITERATION : 15, loss : 0.10132935205808097ITERATION : 16, loss : 0.1015066831292956ITERATION : 17, loss : 0.10163531228185109ITERATION : 18, loss : 0.10172860635985964ITERATION : 19, loss : 0.10179626778909845ITERATION : 20, loss : 0.10184533675628744ITERATION : 21, loss : 0.1018809209874715ITERATION : 22, loss : 0.10190672556605421ITERATION : 23, loss : 0.10192543789953486ITERATION : 24, loss : 0.10193900703686447ITERATION : 25, loss : 0.10194884650914257ITERATION : 26, loss : 0.10195598134461803ITERATION : 27, loss : 0.10196115497308496ITERATION : 28, loss : 0.10196490645231314ITERATION : 29, loss : 0.10196762671208325ITERATION : 30, loss : 0.1019695992236289ITERATION : 31, loss : 0.10197102950675485ITERATION : 32, loss : 0.10197206660282264ITERATION : 33, loss : 0.10197281862045272ITERATION : 34, loss : 0.10197336389318168ITERATION : 35, loss : 0.10197375926795768ITERATION : 36, loss : 0.10197404593914547ITERATION : 37, loss : 0.10197425381132533ITERATION : 38, loss : 0.10197440449155461ITERATION : 39, loss : 0.10197451378601956ITERATION : 40, loss : 0.10197459299403062ITERATION : 41, loss : 0.10197465044725157ITERATION : 42, loss : 0.10197469208214292ITERATION : 43, loss : 0.10197472230958815ITERATION : 44, loss : 0.10197474418272998ITERATION : 45, loss : 0.10197476005575419ITERATION : 46, loss : 0.10197477156168913ITERATION : 47, loss : 0.10197477989684449ITERATION : 48, loss : 0.10197478605785427ITERATION : 49, loss : 0.10197479038818028ITERATION : 50, loss : 0.10197479352072465ITERATION : 51, loss : 0.10197479577429434ITERATION : 52, loss : 0.101974797428929ITERATION : 53, loss : 0.10197479864720874ITERATION : 54, loss : 0.101974799491869ITERATION : 55, loss : 0.10197480015777871ITERATION : 56, loss : 0.10197480052810783ITERATION : 57, loss : 0.1019748008646708ITERATION : 58, loss : 0.10197480110134365ITERATION : 59, loss : 0.10197480123314538ITERATION : 60, loss : 0.10197480137180193ITERATION : 61, loss : 0.10197480137472867ITERATION : 62, loss : 0.10197480137472867ITERATION : 63, loss : 0.10197480137472867ITERATION : 64, loss : 0.10197480137472867ITERATION : 65, loss : 0.10197480137472867ITERATION : 66, loss : 0.10197480137472867ITERATION : 67, loss : 0.10197480137472867ITERATION : 68, loss : 0.10197480137472867ITERATION : 69, loss : 0.10197480137472867ITERATION : 70, loss : 0.10197480137472867ITERATION : 71, loss : 0.10197480137472867ITERATION : 72, loss : 0.10197480137472867ITERATION : 73, loss : 0.10197480137472867ITERATION : 74, loss : 0.10197480137472867ITERATION : 75, loss : 0.10197480137472867ITERATION : 76, loss : 0.10197480137472867ITERATION : 77, loss : 0.10197480137472867ITERATION : 78, loss : 0.10197480137472867ITERATION : 79, loss : 0.10197480137472867ITERATION : 80, loss : 0.10197480137472867ITERATION : 81, loss : 0.10197480137472867ITERATION : 82, loss : 0.10197480137472867ITERATION : 83, loss : 0.10197480137472867ITERATION : 84, loss : 0.10197480137472867ITERATION : 85, loss : 0.10197480137472867ITERATION : 86, loss : 0.10197480137472867ITERATION : 87, loss : 0.10197480137472867ITERATION : 88, loss : 0.10197480137472867ITERATION : 89, loss : 0.10197480137472867ITERATION : 90, loss : 0.10197480137472867ITERATION : 91, loss : 0.10197480137472867ITERATION : 92, loss : 0.10197480137472867ITERATION : 93, loss : 0.10197480137472867ITERATION : 94, loss : 0.10197480137472867ITERATION : 95, loss : 0.10197480137472867ITERATION : 96, loss : 0.10197480137472867ITERATION : 97, loss : 0.10197480137472867ITERATION : 98, loss : 0.10197480137472867ITERATION : 99, loss : 0.10197480137472867ITERATION : 100, loss : 0.10197480137472867
ITERATION : 1, loss : 0.04092478042581065ITERATION : 2, loss : 0.042422476518431465ITERATION : 3, loss : 0.04497566459896368ITERATION : 4, loss : 0.048113360900585614ITERATION : 5, loss : 0.050672796186529735ITERATION : 6, loss : 0.05256868631780089ITERATION : 7, loss : 0.05392019239375647ITERATION : 8, loss : 0.05486683443956781ITERATION : 9, loss : 0.05552415889287339ITERATION : 10, loss : 0.05597854119039544ITERATION : 11, loss : 0.056291886074547014ITERATION : 12, loss : 0.056507689714369376ITERATION : 13, loss : 0.05665620983579422ITERATION : 14, loss : 0.05675838424435934ITERATION : 15, loss : 0.05682866035375722ITERATION : 16, loss : 0.056876991444379264ITERATION : 17, loss : 0.056910228638694105ITERATION : 18, loss : 0.05693308539216866ITERATION : 19, loss : 0.056948803759842404ITERATION : 20, loss : 0.056959613287856994ITERATION : 21, loss : 0.056967047172847375ITERATION : 22, loss : 0.056972159723937134ITERATION : 23, loss : 0.05697567593099114ITERATION : 24, loss : 0.05697809429488108ITERATION : 25, loss : 0.05697975762018565ITERATION : 26, loss : 0.05698090169515909ITERATION : 27, loss : 0.05698168864222416ITERATION : 28, loss : 0.05698222997018767ITERATION : 29, loss : 0.056982602328540896ITERATION : 30, loss : 0.05698285848790113ITERATION : 31, loss : 0.05698303467116256ITERATION : 32, loss : 0.05698315588249468ITERATION : 33, loss : 0.05698323930779861ITERATION : 34, loss : 0.05698329672371952ITERATION : 35, loss : 0.0569833361799452ITERATION : 36, loss : 0.05698336337595209ITERATION : 37, loss : 0.05698338204056084ITERATION : 38, loss : 0.0569833949053192ITERATION : 39, loss : 0.0569834037333346ITERATION : 40, loss : 0.056983409823391444ITERATION : 41, loss : 0.0569834139874172ITERATION : 42, loss : 0.056983416882513ITERATION : 43, loss : 0.056983418822573545ITERATION : 44, loss : 0.05698342015074324ITERATION : 45, loss : 0.05698342109510963ITERATION : 46, loss : 0.056983421773107966ITERATION : 47, loss : 0.056983422166692846ITERATION : 48, loss : 0.05698342248231982ITERATION : 49, loss : 0.056983422692016256ITERATION : 50, loss : 0.056983422767025754ITERATION : 51, loss : 0.056983422905950556ITERATION : 52, loss : 0.05698342295307873ITERATION : 53, loss : 0.05698342296297856ITERATION : 54, loss : 0.05698342296297856ITERATION : 55, loss : 0.05698342296297856ITERATION : 56, loss : 0.05698342296297856ITERATION : 57, loss : 0.05698342296297856ITERATION : 58, loss : 0.05698342296297856ITERATION : 59, loss : 0.05698342296297856ITERATION : 60, loss : 0.05698342296297856ITERATION : 61, loss : 0.05698342296297856ITERATION : 62, loss : 0.05698342296297856ITERATION : 63, loss : 0.05698342296297856ITERATION : 64, loss : 0.05698342296297856ITERATION : 65, loss : 0.05698342296297856ITERATION : 66, loss : 0.05698342296297856ITERATION : 67, loss : 0.05698342296297856ITERATION : 68, loss : 0.05698342296297856ITERATION : 69, loss : 0.05698342296297856ITERATION : 70, loss : 0.05698342296297856ITERATION : 71, loss : 0.05698342296297856ITERATION : 72, loss : 0.05698342296297856ITERATION : 73, loss : 0.05698342296297856ITERATION : 74, loss : 0.05698342296297856ITERATION : 75, loss : 0.05698342296297856ITERATION : 76, loss : 0.05698342296297856ITERATION : 77, loss : 0.05698342296297856ITERATION : 78, loss : 0.05698342296297856ITERATION : 79, loss : 0.05698342296297856ITERATION : 80, loss : 0.05698342296297856ITERATION : 81, loss : 0.05698342296297856ITERATION : 82, loss : 0.05698342296297856ITERATION : 83, loss : 0.05698342296297856ITERATION : 84, loss : 0.05698342296297856ITERATION : 85, loss : 0.05698342296297856ITERATION : 86, loss : 0.05698342296297856ITERATION : 87, loss : 0.05698342296297856ITERATION : 88, loss : 0.05698342296297856ITERATION : 89, loss : 0.05698342296297856ITERATION : 90, loss : 0.05698342296297856ITERATION : 91, loss : 0.05698342296297856ITERATION : 92, loss : 0.05698342296297856ITERATION : 93, loss : 0.05698342296297856ITERATION : 94, loss : 0.05698342296297856ITERATION : 95, loss : 0.05698342296297856ITERATION : 96, loss : 0.05698342296297856ITERATION : 97, loss : 0.05698342296297856ITERATION : 98, loss : 0.05698342296297856ITERATION : 99, loss : 0.05698342296297856ITERATION : 100, loss : 0.05698342296297856
ITERATION : 1, loss : 0.07878599042835017ITERATION : 2, loss : 0.11740792304045748ITERATION : 3, loss : 0.14278560604500307ITERATION : 4, loss : 0.15855663952062157ITERATION : 5, loss : 0.1666305042551475ITERATION : 6, loss : 0.1713309114943341ITERATION : 7, loss : 0.17450021604149354ITERATION : 8, loss : 0.17666160089905628ITERATION : 9, loss : 0.17814568155551755ITERATION : 10, loss : 0.1791689166147982ITERATION : 11, loss : 0.17987619637545943ITERATION : 12, loss : 0.1803658525987701ITERATION : 13, loss : 0.18070519128567875ITERATION : 14, loss : 0.18094052324007093ITERATION : 15, loss : 0.18110381429388525ITERATION : 16, loss : 0.1812171705614412ITERATION : 17, loss : 0.18129589708375735ITERATION : 18, loss : 0.1813505977418932ITERATION : 19, loss : 0.18138862282519944ITERATION : 20, loss : 0.18141506928942344ITERATION : 21, loss : 0.18143347251973505ITERATION : 22, loss : 0.18144628591897227ITERATION : 23, loss : 0.18145521249971444ITERATION : 24, loss : 0.1814614350811518ITERATION : 25, loss : 0.18146577536067ITERATION : 26, loss : 0.1814688045650197ITERATION : 27, loss : 0.18147092006121898ITERATION : 28, loss : 0.18147239836822313ITERATION : 29, loss : 0.18147343205254726ITERATION : 30, loss : 0.18147415529895877ITERATION : 31, loss : 0.18147466162857742ITERATION : 32, loss : 0.18147501631538515ITERATION : 33, loss : 0.18147526489890317ITERATION : 34, loss : 0.1814754392320379ITERATION : 35, loss : 0.1814755615582894ITERATION : 36, loss : 0.181475647441417ITERATION : 37, loss : 0.18147570777598984ITERATION : 38, loss : 0.18147575017616807ITERATION : 39, loss : 0.18147577998359998ITERATION : 40, loss : 0.18147580096365193ITERATION : 41, loss : 0.1814758157370092ITERATION : 42, loss : 0.181475826117515ITERATION : 43, loss : 0.18147583344287266ITERATION : 44, loss : 0.1814758386141883ITERATION : 45, loss : 0.18147584226050353ITERATION : 46, loss : 0.1814758448232638ITERATION : 47, loss : 0.18147584662134705ITERATION : 48, loss : 0.1814758478849524ITERATION : 49, loss : 0.1814758487766127ITERATION : 50, loss : 0.1814758493855142ITERATION : 51, loss : 0.18147584983507886ITERATION : 52, loss : 0.18147585014621825ITERATION : 53, loss : 0.18147585038418151ITERATION : 54, loss : 0.1814758505133513ITERATION : 55, loss : 0.18147585061486593ITERATION : 56, loss : 0.18147585069805233ITERATION : 57, loss : 0.1814758507577975ITERATION : 58, loss : 0.18147585079821735ITERATION : 59, loss : 0.18147585081767273ITERATION : 60, loss : 0.18147585083799037ITERATION : 61, loss : 0.18147585083827045ITERATION : 62, loss : 0.18147585084505033ITERATION : 63, loss : 0.18147585084532417ITERATION : 64, loss : 0.18147585084532417ITERATION : 65, loss : 0.18147585084532417ITERATION : 66, loss : 0.18147585084532417ITERATION : 67, loss : 0.18147585084532417ITERATION : 68, loss : 0.18147585084532417ITERATION : 69, loss : 0.18147585084532417ITERATION : 70, loss : 0.18147585084532417ITERATION : 71, loss : 0.18147585084532417ITERATION : 72, loss : 0.18147585084532417ITERATION : 73, loss : 0.18147585084532417ITERATION : 74, loss : 0.18147585084532417ITERATION : 75, loss : 0.18147585084532417ITERATION : 76, loss : 0.18147585084532417ITERATION : 77, loss : 0.18147585084532417ITERATION : 78, loss : 0.18147585084532417ITERATION : 79, loss : 0.18147585084532417ITERATION : 80, loss : 0.18147585084532417ITERATION : 81, loss : 0.18147585084532417ITERATION : 82, loss : 0.18147585084532417ITERATION : 83, loss : 0.18147585084532417ITERATION : 84, loss : 0.18147585084532417ITERATION : 85, loss : 0.18147585084532417ITERATION : 86, loss : 0.18147585084532417ITERATION : 87, loss : 0.18147585084532417ITERATION : 88, loss : 0.18147585084532417ITERATION : 89, loss : 0.18147585084532417ITERATION : 90, loss : 0.18147585084532417ITERATION : 91, loss : 0.18147585084532417ITERATION : 92, loss : 0.18147585084532417ITERATION : 93, loss : 0.18147585084532417ITERATION : 94, loss : 0.18147585084532417ITERATION : 95, loss : 0.18147585084532417ITERATION : 96, loss : 0.18147585084532417ITERATION : 97, loss : 0.18147585084532417ITERATION : 98, loss : 0.18147585084532417ITERATION : 99, loss : 0.18147585084532417ITERATION : 100, loss : 0.18147585084532417
gradient norm in None layer : 0.025274739462850183
gradient norm in None layer : 0.0004612875932753953
gradient norm in None layer : 0.0006807904857042052
gradient norm in None layer : 0.008233454055723422
gradient norm in None layer : 0.00042735865297941297
gradient norm in None layer : 0.0005140791987467893
gradient norm in None layer : 0.004264392376531919
gradient norm in None layer : 0.00015375186947332516
gradient norm in None layer : 0.00019801757575967473
gradient norm in None layer : 0.0034790781118815553
gradient norm in None layer : 0.00016885646998206802
gradient norm in None layer : 0.0001762938825792027
gradient norm in None layer : 0.0009050114309897215
gradient norm in None layer : 2.6932945698266522e-05
gradient norm in None layer : 2.384311973579432e-05
gradient norm in None layer : 0.0008594056329646907
gradient norm in None layer : 3.708288258572237e-05
gradient norm in None layer : 3.2589909183731925e-05
gradient norm in None layer : 0.001307413941931108
gradient norm in None layer : 7.864984683304123e-06
gradient norm in None layer : 0.0031598169877306245
gradient norm in None layer : 0.00019807907749240462
gradient norm in None layer : 0.00014186137407049915
gradient norm in None layer : 0.0034770073929328506
gradient norm in None layer : 0.00027106044074428807
gradient norm in None layer : 0.0002387708238238076
gradient norm in None layer : 0.005780767794347819
gradient norm in None layer : 1.4272966541031977e-05
gradient norm in None layer : 0.00636559143867122
gradient norm in None layer : 0.00042158351983656
gradient norm in None layer : 0.0004756962129092437
gradient norm in None layer : 0.006349411378986154
gradient norm in None layer : 0.0006325633710364639
gradient norm in None layer : 0.0009873719143741544
gradient norm in None layer : 0.00048356622930647544
gradient norm in None layer : 0.00015772488179891382
Total gradient norm: 0.029664162111357154
invariance loss : 5.282912837965403, avg_den : 0.376922607421875, density loss : 0.2764994485318257, mse loss : 0.10679419446300817, solver time : 112.45156717300415 sec , total loss : 0.1123536067495054, running loss : 0.11282694286521472
Epoch 0/10 , batch 8/12500 
ITERATION : 1, loss : 0.023699173493835157ITERATION : 2, loss : 0.03507664702675418ITERATION : 3, loss : 0.04146086563010331ITERATION : 4, loss : 0.04580211502731562ITERATION : 5, loss : 0.04872946452534171ITERATION : 6, loss : 0.05068653291962379ITERATION : 7, loss : 0.051983760250023436ITERATION : 8, loss : 0.05283717430290456ITERATION : 9, loss : 0.05339509446230385ITERATION : 10, loss : 0.053748486613972454ITERATION : 11, loss : 0.05397654418886143ITERATION : 12, loss : 0.05412337059500095ITERATION : 13, loss : 0.05421759975057936ITERATION : 14, loss : 0.05427789335974719ITERATION : 15, loss : 0.054316362048113645ITERATION : 16, loss : 0.05434083581688902ITERATION : 17, loss : 0.05435636070594839ITERATION : 18, loss : 0.05436617891366376ITERATION : 19, loss : 0.05437236803196002ITERATION : 20, loss : 0.05437625573188883ITERATION : 21, loss : 0.054378688278147645ITERATION : 22, loss : 0.0543802037372946ITERATION : 23, loss : 0.05438114316334517ITERATION : 24, loss : 0.05438172221996362ITERATION : 25, loss : 0.05438207674019878ITERATION : 26, loss : 0.054382292103812895ITERATION : 27, loss : 0.05438242163888079ITERATION : 28, loss : 0.054382498645919125ITERATION : 29, loss : 0.05438254375312648ITERATION : 30, loss : 0.054382569664405044ITERATION : 31, loss : 0.05438258416330865ITERATION : 32, loss : 0.054382591969875695ITERATION : 33, loss : 0.05438259593328665ITERATION : 34, loss : 0.05438259775127068ITERATION : 35, loss : 0.05438259842542739ITERATION : 36, loss : 0.05438259843025799ITERATION : 37, loss : 0.05438259823608476ITERATION : 38, loss : 0.054382597933732305ITERATION : 39, loss : 0.05438259764748048ITERATION : 40, loss : 0.05438259736000268ITERATION : 41, loss : 0.054382597104194956ITERATION : 42, loss : 0.05438259691147289ITERATION : 43, loss : 0.05438259676670015ITERATION : 44, loss : 0.05438259664569189ITERATION : 45, loss : 0.05438259654746687ITERATION : 46, loss : 0.05438259647098372ITERATION : 47, loss : 0.05438259642202667ITERATION : 48, loss : 0.0543825963852911ITERATION : 49, loss : 0.05438259636252481ITERATION : 50, loss : 0.054382596340134946ITERATION : 51, loss : 0.054382596340213064ITERATION : 52, loss : 0.054382596340213064ITERATION : 53, loss : 0.054382596340213064ITERATION : 54, loss : 0.054382596340213064ITERATION : 55, loss : 0.054382596340213064ITERATION : 56, loss : 0.054382596340213064ITERATION : 57, loss : 0.054382596340213064ITERATION : 58, loss : 0.054382596340213064ITERATION : 59, loss : 0.054382596340213064ITERATION : 60, loss : 0.054382596340213064ITERATION : 61, loss : 0.054382596340213064ITERATION : 62, loss : 0.054382596340213064ITERATION : 63, loss : 0.054382596340213064ITERATION : 64, loss : 0.054382596340213064ITERATION : 65, loss : 0.054382596340213064ITERATION : 66, loss : 0.054382596340213064ITERATION : 67, loss : 0.054382596340213064ITERATION : 68, loss : 0.054382596340213064ITERATION : 69, loss : 0.054382596340213064ITERATION : 70, loss : 0.054382596340213064ITERATION : 71, loss : 0.054382596340213064ITERATION : 72, loss : 0.054382596340213064ITERATION : 73, loss : 0.054382596340213064ITERATION : 74, loss : 0.054382596340213064ITERATION : 75, loss : 0.054382596340213064ITERATION : 76, loss : 0.054382596340213064ITERATION : 77, loss : 0.054382596340213064ITERATION : 78, loss : 0.054382596340213064ITERATION : 79, loss : 0.054382596340213064ITERATION : 80, loss : 0.054382596340213064ITERATION : 81, loss : 0.054382596340213064ITERATION : 82, loss : 0.054382596340213064ITERATION : 83, loss : 0.054382596340213064ITERATION : 84, loss : 0.054382596340213064ITERATION : 85, loss : 0.054382596340213064ITERATION : 86, loss : 0.054382596340213064ITERATION : 87, loss : 0.054382596340213064ITERATION : 88, loss : 0.054382596340213064ITERATION : 89, loss : 0.054382596340213064ITERATION : 90, loss : 0.054382596340213064ITERATION : 91, loss : 0.054382596340213064ITERATION : 92, loss : 0.054382596340213064ITERATION : 93, loss : 0.054382596340213064ITERATION : 94, loss : 0.054382596340213064ITERATION : 95, loss : 0.054382596340213064ITERATION : 96, loss : 0.054382596340213064ITERATION : 97, loss : 0.054382596340213064ITERATION : 98, loss : 0.054382596340213064ITERATION : 99, loss : 0.054382596340213064ITERATION : 100, loss : 0.054382596340213064
ITERATION : 1, loss : 0.20569429722561192ITERATION : 2, loss : 0.24910216906607682ITERATION : 3, loss : 0.2680544314223098ITERATION : 4, loss : 0.2785968270353981ITERATION : 5, loss : 0.28520908927965877ITERATION : 6, loss : 0.2896116769394591ITERATION : 7, loss : 0.2926492520507776ITERATION : 8, loss : 0.29479295144241435ITERATION : 9, loss : 0.2963288122353086ITERATION : 10, loss : 0.2974407554009392ITERATION : 11, loss : 0.29825182764075187ITERATION : 12, loss : 0.29884668412107396ITERATION : 13, loss : 0.29928474852945186ITERATION : 14, loss : 0.29960834943091635ITERATION : 15, loss : 0.2998479672241849ITERATION : 16, loss : 0.30002572967229935ITERATION : 17, loss : 0.3001577999671378ITERATION : 18, loss : 0.3002560400917663ITERATION : 19, loss : 0.3003291870851067ITERATION : 20, loss : 0.30038369446034013ITERATION : 21, loss : 0.30042433973201454ITERATION : 22, loss : 0.30045466580993313ITERATION : 23, loss : 0.3004773039277177ITERATION : 24, loss : 0.30049421045557007ITERATION : 25, loss : 0.3005068414270257ITERATION : 26, loss : 0.300516281363458ITERATION : 27, loss : 0.300523338627681ITERATION : 28, loss : 0.30052861611472853ITERATION : 29, loss : 0.3005325636526743ITERATION : 30, loss : 0.3005355171164098ITERATION : 31, loss : 0.30053772728432404ITERATION : 32, loss : 0.3005393815236919ITERATION : 33, loss : 0.300540619966923ITERATION : 34, loss : 0.30054154724200866ITERATION : 35, loss : 0.3005422416055655ITERATION : 36, loss : 0.30054276169602584ITERATION : 37, loss : 0.30054315130537956ITERATION : 38, loss : 0.30054344319047244ITERATION : 39, loss : 0.30054366187245113ITERATION : 40, loss : 0.3005438257459473ITERATION : 41, loss : 0.3005439485401188ITERATION : 42, loss : 0.3005440405915248ITERATION : 43, loss : 0.3005441095959494ITERATION : 44, loss : 0.300544161333872ITERATION : 45, loss : 0.3005442001454984ITERATION : 46, loss : 0.30054422921672036ITERATION : 47, loss : 0.3005442509938897ITERATION : 48, loss : 0.3005442673248684ITERATION : 49, loss : 0.3005442795788114ITERATION : 50, loss : 0.30054428875450606ITERATION : 51, loss : 0.3005442956354969ITERATION : 52, loss : 0.30054430078982974ITERATION : 53, loss : 0.30054430465365234ITERATION : 54, loss : 0.30054430755680617ITERATION : 55, loss : 0.30054430972105445ITERATION : 56, loss : 0.3005443113369565ITERATION : 57, loss : 0.30054431254044317ITERATION : 58, loss : 0.30054431345755467ITERATION : 59, loss : 0.3005443141473549ITERATION : 60, loss : 0.3005443146539171ITERATION : 61, loss : 0.3005443150486527ITERATION : 62, loss : 0.3005443153532449ITERATION : 63, loss : 0.3005443155696249ITERATION : 64, loss : 0.3005443157405082ITERATION : 65, loss : 0.30054431586304997ITERATION : 66, loss : 0.3005443159590257ITERATION : 67, loss : 0.30054431600610504ITERATION : 68, loss : 0.3005443160407489ITERATION : 69, loss : 0.3005443160688201ITERATION : 70, loss : 0.3005443161023308ITERATION : 71, loss : 0.3005443161278417ITERATION : 72, loss : 0.30054431612791793ITERATION : 73, loss : 0.30054431612791793ITERATION : 74, loss : 0.30054431612791793ITERATION : 75, loss : 0.30054431612791793ITERATION : 76, loss : 0.30054431612791793ITERATION : 77, loss : 0.30054431612791793ITERATION : 78, loss : 0.30054431612791793ITERATION : 79, loss : 0.30054431612791793ITERATION : 80, loss : 0.30054431612791793ITERATION : 81, loss : 0.30054431612791793ITERATION : 82, loss : 0.30054431612791793ITERATION : 83, loss : 0.30054431612791793ITERATION : 84, loss : 0.30054431612791793ITERATION : 85, loss : 0.30054431612791793ITERATION : 86, loss : 0.30054431612791793ITERATION : 87, loss : 0.30054431612791793ITERATION : 88, loss : 0.30054431612791793ITERATION : 89, loss : 0.30054431612791793ITERATION : 90, loss : 0.30054431612791793ITERATION : 91, loss : 0.30054431612791793ITERATION : 92, loss : 0.30054431612791793ITERATION : 93, loss : 0.30054431612791793ITERATION : 94, loss : 0.30054431612791793ITERATION : 95, loss : 0.30054431612791793ITERATION : 96, loss : 0.30054431612791793ITERATION : 97, loss : 0.30054431612791793ITERATION : 98, loss : 0.30054431612791793ITERATION : 99, loss : 0.30054431612791793ITERATION : 100, loss : 0.30054431612791793
ITERATION : 1, loss : 0.05524014720501748ITERATION : 2, loss : 0.06533847946060697ITERATION : 3, loss : 0.07689530341551484ITERATION : 4, loss : 0.08531637278988885ITERATION : 5, loss : 0.09137065553569579ITERATION : 6, loss : 0.09555540919515462ITERATION : 7, loss : 0.09821905372723638ITERATION : 8, loss : 0.09999900633153935ITERATION : 9, loss : 0.10118435854692025ITERATION : 10, loss : 0.1019723352927046ITERATION : 11, loss : 0.10249571090804595ITERATION : 12, loss : 0.10284322626845495ITERATION : 13, loss : 0.10307396386354906ITERATION : 14, loss : 0.10322718302528258ITERATION : 15, loss : 0.1033289469585906ITERATION : 16, loss : 0.10339655222363338ITERATION : 17, loss : 0.10344147641678424ITERATION : 18, loss : 0.10347133735877855ITERATION : 19, loss : 0.10349119139259642ITERATION : 20, loss : 0.10350439585954731ITERATION : 21, loss : 0.10351318048184621ITERATION : 22, loss : 0.10351902637625407ITERATION : 23, loss : 0.1035229179332869ITERATION : 24, loss : 0.10352550922760159ITERATION : 25, loss : 0.1035272352460624ITERATION : 26, loss : 0.10352838525785335ITERATION : 27, loss : 0.10352915177311843ITERATION : 28, loss : 0.10352966274540049ITERATION : 29, loss : 0.10353000356435811ITERATION : 30, loss : 0.10353023096151698ITERATION : 31, loss : 0.10353038273790588ITERATION : 32, loss : 0.1035304840946008ITERATION : 33, loss : 0.10353055179036838ITERATION : 34, loss : 0.10353059697120227ITERATION : 35, loss : 0.10353062720045235ITERATION : 36, loss : 0.10353064742665546ITERATION : 37, loss : 0.10353066094524374ITERATION : 38, loss : 0.10353066999196384ITERATION : 39, loss : 0.10353067600090184ITERATION : 40, loss : 0.1035306800269906ITERATION : 41, loss : 0.10353068271236732ITERATION : 42, loss : 0.1035306844778131ITERATION : 43, loss : 0.10353068563983106ITERATION : 44, loss : 0.10353068644249978ITERATION : 45, loss : 0.10353068699444239ITERATION : 46, loss : 0.10353068734652159ITERATION : 47, loss : 0.10353068760103949ITERATION : 48, loss : 0.10353068771762484ITERATION : 49, loss : 0.10353068781951867ITERATION : 50, loss : 0.10353068783629257ITERATION : 51, loss : 0.10353068783629257ITERATION : 52, loss : 0.10353068783629257ITERATION : 53, loss : 0.10353068783629257ITERATION : 54, loss : 0.10353068783629257ITERATION : 55, loss : 0.10353068783629257ITERATION : 56, loss : 0.10353068783629257ITERATION : 57, loss : 0.10353068783629257ITERATION : 58, loss : 0.10353068783629257ITERATION : 59, loss : 0.10353068783629257ITERATION : 60, loss : 0.10353068783629257ITERATION : 61, loss : 0.10353068783629257ITERATION : 62, loss : 0.10353068783629257ITERATION : 63, loss : 0.10353068783629257ITERATION : 64, loss : 0.10353068783629257ITERATION : 65, loss : 0.10353068783629257ITERATION : 66, loss : 0.10353068783629257ITERATION : 67, loss : 0.10353068783629257ITERATION : 68, loss : 0.10353068783629257ITERATION : 69, loss : 0.10353068783629257ITERATION : 70, loss : 0.10353068783629257ITERATION : 71, loss : 0.10353068783629257ITERATION : 72, loss : 0.10353068783629257ITERATION : 73, loss : 0.10353068783629257ITERATION : 74, loss : 0.10353068783629257ITERATION : 75, loss : 0.10353068783629257ITERATION : 76, loss : 0.10353068783629257ITERATION : 77, loss : 0.10353068783629257ITERATION : 78, loss : 0.10353068783629257ITERATION : 79, loss : 0.10353068783629257ITERATION : 80, loss : 0.10353068783629257ITERATION : 81, loss : 0.10353068783629257ITERATION : 82, loss : 0.10353068783629257ITERATION : 83, loss : 0.10353068783629257ITERATION : 84, loss : 0.10353068783629257ITERATION : 85, loss : 0.10353068783629257ITERATION : 86, loss : 0.10353068783629257ITERATION : 87, loss : 0.10353068783629257ITERATION : 88, loss : 0.10353068783629257ITERATION : 89, loss : 0.10353068783629257ITERATION : 90, loss : 0.10353068783629257ITERATION : 91, loss : 0.10353068783629257ITERATION : 92, loss : 0.10353068783629257ITERATION : 93, loss : 0.10353068783629257ITERATION : 94, loss : 0.10353068783629257ITERATION : 95, loss : 0.10353068783629257ITERATION : 96, loss : 0.10353068783629257ITERATION : 97, loss : 0.10353068783629257ITERATION : 98, loss : 0.10353068783629257ITERATION : 99, loss : 0.10353068783629257ITERATION : 100, loss : 0.10353068783629257
ITERATION : 1, loss : 0.03560811214580172ITERATION : 2, loss : 0.049584608057120415ITERATION : 3, loss : 0.05869834666444121ITERATION : 4, loss : 0.06473682309176156ITERATION : 5, loss : 0.06877643262839202ITERATION : 6, loss : 0.07149530483421175ITERATION : 7, loss : 0.07333025725238576ITERATION : 8, loss : 0.07456919000845917ITERATION : 9, loss : 0.075404876964342ITERATION : 10, loss : 0.07596753459130809ITERATION : 11, loss : 0.07634547209814001ITERATION : 12, loss : 0.07659864255488033ITERATION : 13, loss : 0.07676772447981532ITERATION : 14, loss : 0.07688027669491246ITERATION : 15, loss : 0.07695493152965471ITERATION : 16, loss : 0.07700425612984063ITERATION : 17, loss : 0.0770367049085113ITERATION : 18, loss : 0.07705794961344055ITERATION : 19, loss : 0.07707178406770239ITERATION : 20, loss : 0.07708073771097021ITERATION : 21, loss : 0.07708649145836433ITERATION : 22, loss : 0.07709015825068648ITERATION : 23, loss : 0.07709247180806654ITERATION : 24, loss : 0.07709391396328312ITERATION : 25, loss : 0.07709479931788346ITERATION : 26, loss : 0.07709533235682699ITERATION : 27, loss : 0.07709564485653447ITERATION : 28, loss : 0.07709582135616494ITERATION : 29, loss : 0.0770959154631765ITERATION : 30, loss : 0.07709596084800008ITERATION : 31, loss : 0.07709597833370868ITERATION : 32, loss : 0.07709598066899455ITERATION : 33, loss : 0.07709597544939467ITERATION : 34, loss : 0.07709596699653745ITERATION : 35, loss : 0.07709595778616954ITERATION : 36, loss : 0.07709594903845207ITERATION : 37, loss : 0.07709594134155705ITERATION : 38, loss : 0.07709593486930119ITERATION : 39, loss : 0.07709592958254355ITERATION : 40, loss : 0.07709592538831087ITERATION : 41, loss : 0.07709592205126287ITERATION : 42, loss : 0.07709591952984374ITERATION : 43, loss : 0.07709591756025985ITERATION : 44, loss : 0.07709591606712733ITERATION : 45, loss : 0.07709591496590362ITERATION : 46, loss : 0.07709591414443223ITERATION : 47, loss : 0.0770959134868325ITERATION : 48, loss : 0.0770959129999821ITERATION : 49, loss : 0.077095912650687ITERATION : 50, loss : 0.07709591238489215ITERATION : 51, loss : 0.07709591218641473ITERATION : 52, loss : 0.07709591207954117ITERATION : 53, loss : 0.0770959119681941ITERATION : 54, loss : 0.07709591190228676ITERATION : 55, loss : 0.07709591185491073ITERATION : 56, loss : 0.07709591183691733ITERATION : 57, loss : 0.07709591183709945ITERATION : 58, loss : 0.07709591183709945ITERATION : 59, loss : 0.07709591183709945ITERATION : 60, loss : 0.07709591183709945ITERATION : 61, loss : 0.07709591183709945ITERATION : 62, loss : 0.07709591183709945ITERATION : 63, loss : 0.07709591183709945ITERATION : 64, loss : 0.07709591183709945ITERATION : 65, loss : 0.07709591183709945ITERATION : 66, loss : 0.07709591183709945ITERATION : 67, loss : 0.07709591183709945ITERATION : 68, loss : 0.07709591183709945ITERATION : 69, loss : 0.07709591183709945ITERATION : 70, loss : 0.07709591183709945ITERATION : 71, loss : 0.07709591183709945ITERATION : 72, loss : 0.07709591183709945ITERATION : 73, loss : 0.07709591183709945ITERATION : 74, loss : 0.07709591183709945ITERATION : 75, loss : 0.07709591183709945ITERATION : 76, loss : 0.07709591183709945ITERATION : 77, loss : 0.07709591183709945ITERATION : 78, loss : 0.07709591183709945ITERATION : 79, loss : 0.07709591183709945ITERATION : 80, loss : 0.07709591183709945ITERATION : 81, loss : 0.07709591183709945ITERATION : 82, loss : 0.07709591183709945ITERATION : 83, loss : 0.07709591183709945ITERATION : 84, loss : 0.07709591183709945ITERATION : 85, loss : 0.07709591183709945ITERATION : 86, loss : 0.07709591183709945ITERATION : 87, loss : 0.07709591183709945ITERATION : 88, loss : 0.07709591183709945ITERATION : 89, loss : 0.07709591183709945ITERATION : 90, loss : 0.07709591183709945ITERATION : 91, loss : 0.07709591183709945ITERATION : 92, loss : 0.07709591183709945ITERATION : 93, loss : 0.07709591183709945ITERATION : 94, loss : 0.07709591183709945ITERATION : 95, loss : 0.07709591183709945ITERATION : 96, loss : 0.07709591183709945ITERATION : 97, loss : 0.07709591183709945ITERATION : 98, loss : 0.07709591183709945ITERATION : 99, loss : 0.07709591183709945ITERATION : 100, loss : 0.07709591183709945
ITERATION : 1, loss : 0.014768054340377522ITERATION : 2, loss : 0.012354384182095302ITERATION : 3, loss : 0.012410367575544186ITERATION : 4, loss : 0.013121210284214266ITERATION : 5, loss : 0.01391431295417639ITERATION : 6, loss : 0.014596525990723225ITERATION : 7, loss : 0.015125307969713517ITERATION : 8, loss : 0.015514287690487412ITERATION : 9, loss : 0.015556873300265632ITERATION : 10, loss : 0.015390850545106114ITERATION : 11, loss : 0.015279184056573377ITERATION : 12, loss : 0.015202698584210163ITERATION : 13, loss : 0.015149623215245975ITERATION : 14, loss : 0.015112455165019582ITERATION : 15, loss : 0.015086263499901888ITERATION : 16, loss : 0.015067729255388245ITERATION : 17, loss : 0.0150545778475298ITERATION : 18, loss : 0.015045229870359804ITERATION : 19, loss : 0.015038578649620089ITERATION : 20, loss : 0.015033843645407809ITERATION : 21, loss : 0.015030472080120278ITERATION : 22, loss : 0.015028071364775958ITERATION : 23, loss : 0.015026362171278146ITERATION : 24, loss : 0.015025145579541302ITERATION : 25, loss : 0.015024279867300197ITERATION : 26, loss : 0.015023664040068527ITERATION : 27, loss : 0.015023226078018677ITERATION : 28, loss : 0.01502291472901755ITERATION : 29, loss : 0.015022693460301734ITERATION : 30, loss : 0.015022536223487027ITERATION : 31, loss : 0.015022424558940003ITERATION : 32, loss : 0.015022345240665088ITERATION : 33, loss : 0.015022288929591357ITERATION : 34, loss : 0.015022248989461431ITERATION : 35, loss : 0.01502222064651481ITERATION : 36, loss : 0.015022200552500087ITERATION : 37, loss : 0.015022186279148232ITERATION : 38, loss : 0.015022176159998134ITERATION : 39, loss : 0.015022168957256596ITERATION : 40, loss : 0.015022163865158594ITERATION : 41, loss : 0.015022160234240458ITERATION : 42, loss : 0.015022157679830858ITERATION : 43, loss : 0.01502215584794898ITERATION : 44, loss : 0.015022154559773796ITERATION : 45, loss : 0.015022153646532664ITERATION : 46, loss : 0.01502215301175076ITERATION : 47, loss : 0.015022152546715318ITERATION : 48, loss : 0.01502215223006986ITERATION : 49, loss : 0.015022152001056272ITERATION : 50, loss : 0.015022151845488779ITERATION : 51, loss : 0.01502215171470562ITERATION : 52, loss : 0.015022151658767536ITERATION : 53, loss : 0.015022151593794884ITERATION : 54, loss : 0.015022151565097444ITERATION : 55, loss : 0.015022151553150178ITERATION : 56, loss : 0.015022151545948353ITERATION : 57, loss : 0.015022151545948353ITERATION : 58, loss : 0.015022151545948353ITERATION : 59, loss : 0.015022151545948353ITERATION : 60, loss : 0.015022151545948353ITERATION : 61, loss : 0.015022151545948353ITERATION : 62, loss : 0.015022151545948353ITERATION : 63, loss : 0.015022151545948353ITERATION : 64, loss : 0.015022151545948353ITERATION : 65, loss : 0.015022151545948353ITERATION : 66, loss : 0.015022151545948353ITERATION : 67, loss : 0.015022151545948353ITERATION : 68, loss : 0.015022151545948353ITERATION : 69, loss : 0.015022151545948353ITERATION : 70, loss : 0.015022151545948353ITERATION : 71, loss : 0.015022151545948353ITERATION : 72, loss : 0.015022151545948353ITERATION : 73, loss : 0.015022151545948353ITERATION : 74, loss : 0.015022151545948353ITERATION : 75, loss : 0.015022151545948353ITERATION : 76, loss : 0.015022151545948353ITERATION : 77, loss : 0.015022151545948353ITERATION : 78, loss : 0.015022151545948353ITERATION : 79, loss : 0.015022151545948353ITERATION : 80, loss : 0.015022151545948353ITERATION : 81, loss : 0.015022151545948353ITERATION : 82, loss : 0.015022151545948353ITERATION : 83, loss : 0.015022151545948353ITERATION : 84, loss : 0.015022151545948353ITERATION : 85, loss : 0.015022151545948353ITERATION : 86, loss : 0.015022151545948353ITERATION : 87, loss : 0.015022151545948353ITERATION : 88, loss : 0.015022151545948353ITERATION : 89, loss : 0.015022151545948353ITERATION : 90, loss : 0.015022151545948353ITERATION : 91, loss : 0.015022151545948353ITERATION : 92, loss : 0.015022151545948353ITERATION : 93, loss : 0.015022151545948353ITERATION : 94, loss : 0.015022151545948353ITERATION : 95, loss : 0.015022151545948353ITERATION : 96, loss : 0.015022151545948353ITERATION : 97, loss : 0.015022151545948353ITERATION : 98, loss : 0.015022151545948353ITERATION : 99, loss : 0.015022151545948353ITERATION : 100, loss : 0.015022151545948353
ITERATION : 1, loss : 0.015753219804063332ITERATION : 2, loss : 0.009929109052805394ITERATION : 3, loss : 0.009723581203069763ITERATION : 4, loss : 0.010626926080129247ITERATION : 5, loss : 0.011460478767893719ITERATION : 6, loss : 0.012397964788941404ITERATION : 7, loss : 0.013134179591054415ITERATION : 8, loss : 0.013687115772348185ITERATION : 9, loss : 0.014094229161673247ITERATION : 10, loss : 0.014390456345367002ITERATION : 11, loss : 0.014604374767501236ITERATION : 12, loss : 0.014758069415279525ITERATION : 13, loss : 0.014868104004933155ITERATION : 14, loss : 0.014946682665176313ITERATION : 15, loss : 0.015002695794965274ITERATION : 16, loss : 0.015042570719847291ITERATION : 17, loss : 0.015070929614059798ITERATION : 18, loss : 0.01509108410690896ITERATION : 19, loss : 0.01510540037982657ITERATION : 20, loss : 0.01511556580109234ITERATION : 21, loss : 0.015122781905508045ITERATION : 22, loss : 0.015127903455281089ITERATION : 23, loss : 0.015131537918738428ITERATION : 24, loss : 0.01513411689144664ITERATION : 25, loss : 0.015135946792781203ITERATION : 26, loss : 0.015137245214505135ITERATION : 27, loss : 0.015138166445819994ITERATION : 28, loss : 0.015138820087845917ITERATION : 29, loss : 0.015139283893829658ITERATION : 30, loss : 0.015139612975708863ITERATION : 31, loss : 0.015139846488003974ITERATION : 32, loss : 0.015140012201327631ITERATION : 33, loss : 0.015140129802989647ITERATION : 34, loss : 0.015140213268611343ITERATION : 35, loss : 0.015140272490710465ITERATION : 36, loss : 0.015140314546333009ITERATION : 37, loss : 0.015140344367061482ITERATION : 38, loss : 0.015140365579396403ITERATION : 39, loss : 0.015140380600076074ITERATION : 40, loss : 0.01514039130584404ITERATION : 41, loss : 0.01514039889374992ITERATION : 42, loss : 0.015140404259481351ITERATION : 43, loss : 0.015140408107720865ITERATION : 44, loss : 0.015140410813791317ITERATION : 45, loss : 0.015140412737338205ITERATION : 46, loss : 0.015140414111888299ITERATION : 47, loss : 0.015140415079362495ITERATION : 48, loss : 0.015140415734827474ITERATION : 49, loss : 0.015140416230668387ITERATION : 50, loss : 0.015140416535130138ITERATION : 51, loss : 0.015140416797205642ITERATION : 52, loss : 0.01514041693155084ITERATION : 53, loss : 0.015140417065394518ITERATION : 54, loss : 0.015140417148225773ITERATION : 55, loss : 0.015140417213777208ITERATION : 56, loss : 0.015140417245440919ITERATION : 57, loss : 0.015140417281579062ITERATION : 58, loss : 0.01514041729130226ITERATION : 59, loss : 0.01514041729130226ITERATION : 60, loss : 0.01514041729130226ITERATION : 61, loss : 0.01514041729130226ITERATION : 62, loss : 0.01514041729130226ITERATION : 63, loss : 0.01514041729130226ITERATION : 64, loss : 0.01514041729130226ITERATION : 65, loss : 0.01514041729130226ITERATION : 66, loss : 0.01514041729130226ITERATION : 67, loss : 0.01514041729130226ITERATION : 68, loss : 0.01514041729130226ITERATION : 69, loss : 0.01514041729130226ITERATION : 70, loss : 0.01514041729130226ITERATION : 71, loss : 0.01514041729130226ITERATION : 72, loss : 0.01514041729130226ITERATION : 73, loss : 0.01514041729130226ITERATION : 74, loss : 0.01514041729130226ITERATION : 75, loss : 0.01514041729130226ITERATION : 76, loss : 0.01514041729130226ITERATION : 77, loss : 0.01514041729130226ITERATION : 78, loss : 0.01514041729130226ITERATION : 79, loss : 0.01514041729130226ITERATION : 80, loss : 0.01514041729130226ITERATION : 81, loss : 0.01514041729130226ITERATION : 82, loss : 0.01514041729130226ITERATION : 83, loss : 0.01514041729130226ITERATION : 84, loss : 0.01514041729130226ITERATION : 85, loss : 0.01514041729130226ITERATION : 86, loss : 0.01514041729130226ITERATION : 87, loss : 0.01514041729130226ITERATION : 88, loss : 0.01514041729130226ITERATION : 89, loss : 0.01514041729130226ITERATION : 90, loss : 0.01514041729130226ITERATION : 91, loss : 0.01514041729130226ITERATION : 92, loss : 0.01514041729130226ITERATION : 93, loss : 0.01514041729130226ITERATION : 94, loss : 0.01514041729130226ITERATION : 95, loss : 0.01514041729130226ITERATION : 96, loss : 0.01514041729130226ITERATION : 97, loss : 0.01514041729130226ITERATION : 98, loss : 0.01514041729130226ITERATION : 99, loss : 0.01514041729130226ITERATION : 100, loss : 0.01514041729130226
ITERATION : 1, loss : 0.06315923686732987ITERATION : 2, loss : 0.0791588739276459ITERATION : 3, loss : 0.08944077750716997ITERATION : 4, loss : 0.09607195751503399ITERATION : 5, loss : 0.1003249834877259ITERATION : 6, loss : 0.10302650763670443ITERATION : 7, loss : 0.10472395284942837ITERATION : 8, loss : 0.10577759438121612ITERATION : 9, loss : 0.10642221559798187ITERATION : 10, loss : 0.1068094997986642ITERATION : 11, loss : 0.1070366837821623ITERATION : 12, loss : 0.1071656096917467ITERATION : 13, loss : 0.10723526000122031ITERATION : 14, loss : 0.10726994838694623ITERATION : 15, loss : 0.10728464346082948ITERATION : 16, loss : 0.10728841042311946ITERATION : 17, loss : 0.10728662353029647ITERATION : 18, loss : 0.10728237757277918ITERATION : 19, loss : 0.10727738227244038ITERATION : 20, loss : 0.10727252225906188ITERATION : 21, loss : 0.10726820359798085ITERATION : 22, loss : 0.1072645657758774ITERATION : 23, loss : 0.10726160808066801ITERATION : 24, loss : 0.10725926321369808ITERATION : 25, loss : 0.10725743918403458ITERATION : 26, loss : 0.10725604126649266ITERATION : 27, loss : 0.10725498256192292ITERATION : 28, loss : 0.10725418868149396ITERATION : 29, loss : 0.10725359827638317ITERATION : 30, loss : 0.1072531622663087ITERATION : 31, loss : 0.10725284230949467ITERATION : 32, loss : 0.10725260878155282ITERATION : 33, loss : 0.1072524391108164ITERATION : 34, loss : 0.10725231639995605ITERATION : 35, loss : 0.10725222802673139ITERATION : 36, loss : 0.10725216448993397ITERATION : 37, loss : 0.1072521190903456ITERATION : 38, loss : 0.10725208673657019ITERATION : 39, loss : 0.10725206368920503ITERATION : 40, loss : 0.10725204736486744ITERATION : 41, loss : 0.10725203582836142ITERATION : 42, loss : 0.10725202764569432ITERATION : 43, loss : 0.1072520219645152ITERATION : 44, loss : 0.10725201793311422ITERATION : 45, loss : 0.10725201510600488ITERATION : 46, loss : 0.10725201310394009ITERATION : 47, loss : 0.10725201173106115ITERATION : 48, loss : 0.10725201079307707ITERATION : 49, loss : 0.10725201010607581ITERATION : 50, loss : 0.10725200958920159ITERATION : 51, loss : 0.10725200927954454ITERATION : 52, loss : 0.10725200910786997ITERATION : 53, loss : 0.10725200897283818ITERATION : 54, loss : 0.10725200883201645ITERATION : 55, loss : 0.10725200879089658ITERATION : 56, loss : 0.10725200872258274ITERATION : 57, loss : 0.10725200872030566ITERATION : 58, loss : 0.10725200871775152ITERATION : 59, loss : 0.10725200871775152ITERATION : 60, loss : 0.10725200871775152ITERATION : 61, loss : 0.10725200871775152ITERATION : 62, loss : 0.10725200871775152ITERATION : 63, loss : 0.10725200871775152ITERATION : 64, loss : 0.10725200871775152ITERATION : 65, loss : 0.10725200871775152ITERATION : 66, loss : 0.10725200871775152ITERATION : 67, loss : 0.10725200871775152ITERATION : 68, loss : 0.10725200871775152ITERATION : 69, loss : 0.10725200871775152ITERATION : 70, loss : 0.10725200871775152ITERATION : 71, loss : 0.10725200871775152ITERATION : 72, loss : 0.10725200871775152ITERATION : 73, loss : 0.10725200871775152ITERATION : 74, loss : 0.10725200871775152ITERATION : 75, loss : 0.10725200871775152ITERATION : 76, loss : 0.10725200871775152ITERATION : 77, loss : 0.10725200871775152ITERATION : 78, loss : 0.10725200871775152ITERATION : 79, loss : 0.10725200871775152ITERATION : 80, loss : 0.10725200871775152ITERATION : 81, loss : 0.10725200871775152ITERATION : 82, loss : 0.10725200871775152ITERATION : 83, loss : 0.10725200871775152ITERATION : 84, loss : 0.10725200871775152ITERATION : 85, loss : 0.10725200871775152ITERATION : 86, loss : 0.10725200871775152ITERATION : 87, loss : 0.10725200871775152ITERATION : 88, loss : 0.10725200871775152ITERATION : 89, loss : 0.10725200871775152ITERATION : 90, loss : 0.10725200871775152ITERATION : 91, loss : 0.10725200871775152ITERATION : 92, loss : 0.10725200871775152ITERATION : 93, loss : 0.10725200871775152ITERATION : 94, loss : 0.10725200871775152ITERATION : 95, loss : 0.10725200871775152ITERATION : 96, loss : 0.10725200871775152ITERATION : 97, loss : 0.10725200871775152ITERATION : 98, loss : 0.10725200871775152ITERATION : 99, loss : 0.10725200871775152ITERATION : 100, loss : 0.10725200871775152
ITERATION : 1, loss : 0.05288354607560327ITERATION : 2, loss : 0.053272594882332526ITERATION : 3, loss : 0.06160122864594872ITERATION : 4, loss : 0.06793868169245583ITERATION : 5, loss : 0.07261386318304175ITERATION : 6, loss : 0.07599153234317788ITERATION : 7, loss : 0.07840080680341753ITERATION : 8, loss : 0.08010528408656885ITERATION : 9, loss : 0.08130455544591765ITERATION : 10, loss : 0.0821452441967057ITERATION : 11, loss : 0.08273310363992636ITERATION : 12, loss : 0.08314350303521488ITERATION : 13, loss : 0.08342972558172844ITERATION : 14, loss : 0.08362923284982979ITERATION : 15, loss : 0.08376826406056007ITERATION : 16, loss : 0.08386515109280608ITERATION : 17, loss : 0.08393268060340137ITERATION : 18, loss : 0.08397976179886321ITERATION : 19, loss : 0.08401259910459143ITERATION : 20, loss : 0.08403551195622865ITERATION : 21, loss : 0.08405150763129225ITERATION : 22, loss : 0.0840626802190592ITERATION : 23, loss : 0.08407048823903837ITERATION : 24, loss : 0.08407594785762865ITERATION : 25, loss : 0.08407976762338258ITERATION : 26, loss : 0.0840824415321852ITERATION : 27, loss : 0.0840843144528417ITERATION : 28, loss : 0.08408562703631636ITERATION : 29, loss : 0.08408654741333461ITERATION : 30, loss : 0.0840871931848921ITERATION : 31, loss : 0.08408764653163256ITERATION : 32, loss : 0.08408796490698872ITERATION : 33, loss : 0.08408818861060356ITERATION : 34, loss : 0.08408834590965714ITERATION : 35, loss : 0.08408845658316097ITERATION : 36, loss : 0.08408853447736135ITERATION : 37, loss : 0.08408858934816701ITERATION : 38, loss : 0.08408862801571608ITERATION : 39, loss : 0.08408865525614238ITERATION : 40, loss : 0.08408867448511945ITERATION : 41, loss : 0.08408868803414557ITERATION : 42, loss : 0.08408869759431212ITERATION : 43, loss : 0.08408870435588353ITERATION : 44, loss : 0.0840887091111203ITERATION : 45, loss : 0.08408871250523914ITERATION : 46, loss : 0.08408871488455853ITERATION : 47, loss : 0.08408871659343144ITERATION : 48, loss : 0.08408871777307696ITERATION : 49, loss : 0.08408871863409416ITERATION : 50, loss : 0.0840887192167848ITERATION : 51, loss : 0.0840887196442406ITERATION : 52, loss : 0.08408871994668238ITERATION : 53, loss : 0.0840887201693856ITERATION : 54, loss : 0.08408872029645133ITERATION : 55, loss : 0.0840887203916448ITERATION : 56, loss : 0.08408872042029147ITERATION : 57, loss : 0.0840887204691783ITERATION : 58, loss : 0.0840887204728605ITERATION : 59, loss : 0.0840887204728605ITERATION : 60, loss : 0.0840887204728605ITERATION : 61, loss : 0.0840887204728605ITERATION : 62, loss : 0.0840887204728605ITERATION : 63, loss : 0.0840887204728605ITERATION : 64, loss : 0.0840887204728605ITERATION : 65, loss : 0.0840887204728605ITERATION : 66, loss : 0.0840887204728605ITERATION : 67, loss : 0.0840887204728605ITERATION : 68, loss : 0.0840887204728605ITERATION : 69, loss : 0.0840887204728605ITERATION : 70, loss : 0.0840887204728605ITERATION : 71, loss : 0.0840887204728605ITERATION : 72, loss : 0.0840887204728605ITERATION : 73, loss : 0.0840887204728605ITERATION : 74, loss : 0.0840887204728605ITERATION : 75, loss : 0.0840887204728605ITERATION : 76, loss : 0.0840887204728605ITERATION : 77, loss : 0.0840887204728605ITERATION : 78, loss : 0.0840887204728605ITERATION : 79, loss : 0.0840887204728605ITERATION : 80, loss : 0.0840887204728605ITERATION : 81, loss : 0.0840887204728605ITERATION : 82, loss : 0.0840887204728605ITERATION : 83, loss : 0.0840887204728605ITERATION : 84, loss : 0.0840887204728605ITERATION : 85, loss : 0.0840887204728605ITERATION : 86, loss : 0.0840887204728605ITERATION : 87, loss : 0.0840887204728605ITERATION : 88, loss : 0.0840887204728605ITERATION : 89, loss : 0.0840887204728605ITERATION : 90, loss : 0.0840887204728605ITERATION : 91, loss : 0.0840887204728605ITERATION : 92, loss : 0.0840887204728605ITERATION : 93, loss : 0.0840887204728605ITERATION : 94, loss : 0.0840887204728605ITERATION : 95, loss : 0.0840887204728605ITERATION : 96, loss : 0.0840887204728605ITERATION : 97, loss : 0.0840887204728605ITERATION : 98, loss : 0.0840887204728605ITERATION : 99, loss : 0.0840887204728605ITERATION : 100, loss : 0.0840887204728605
gradient norm in None layer : 0.005198735430359078
gradient norm in None layer : 0.00020852062507207956
gradient norm in None layer : 0.0001378788878540798
gradient norm in None layer : 0.003207140619166286
gradient norm in None layer : 0.0002414592056987984
gradient norm in None layer : 0.00025256318166006067
gradient norm in None layer : 0.0013809977306618532
gradient norm in None layer : 5.3758242938099924e-05
gradient norm in None layer : 4.4997402879784854e-05
gradient norm in None layer : 0.001143807554352093
gradient norm in None layer : 5.660687866908408e-05
gradient norm in None layer : 4.424916729154441e-05
gradient norm in None layer : 0.0004291402292866761
gradient norm in None layer : 1.184723758439215e-05
gradient norm in None layer : 1.1547505700215039e-05
gradient norm in None layer : 0.00041715201988344066
gradient norm in None layer : 1.5013442052023214e-05
gradient norm in None layer : 1.3640328742084627e-05
gradient norm in None layer : 0.0004936586809495448
gradient norm in None layer : 1.6078527975543027e-05
gradient norm in None layer : 0.001092196810661367
gradient norm in None layer : 5.80525944119952e-05
gradient norm in None layer : 4.380074076682381e-05
gradient norm in None layer : 0.0010374422796771531
gradient norm in None layer : 0.00011412515778996982
gradient norm in None layer : 0.0001425056138291549
gradient norm in None layer : 0.0019417577697397137
gradient norm in None layer : 4.1662953195863307e-05
gradient norm in None layer : 0.0028462423249629226
gradient norm in None layer : 0.00016317669669093534
gradient norm in None layer : 0.00015775674707257904
gradient norm in None layer : 0.002948617819492588
gradient norm in None layer : 0.0004883050547002169
gradient norm in None layer : 0.0007519582523246058
gradient norm in None layer : 0.00037873866287796854
gradient norm in None layer : 0.00012874835667714234
Total gradient norm: 0.008075664009366197
invariance loss : 5.687774543666469, avg_den : 0.4079437255859375, density loss : 0.3086730121318083, mse loss : 0.0946321012711732, solver time : 117.67655730247498 sec , total loss : 0.10062854882697148, running loss : 0.11130214361043432
Epoch 0/10 , batch 9/12500 
ITERATION : 1, loss : 0.0499452708618002ITERATION : 2, loss : 0.03651807267548836ITERATION : 3, loss : 0.0329335788049353ITERATION : 4, loss : 0.031975643513079986ITERATION : 5, loss : 0.031850427899487006ITERATION : 6, loss : 0.03197778496211311ITERATION : 7, loss : 0.03215748595211195ITERATION : 8, loss : 0.032323078648289136ITERATION : 9, loss : 0.03245699150552851ITERATION : 10, loss : 0.032559016504005625ITERATION : 11, loss : 0.032634285950660175ITERATION : 12, loss : 0.03268876936151521ITERATION : 13, loss : 0.032727739534967444ITERATION : 14, loss : 0.03275539831569868ITERATION : 15, loss : 0.032774927719413205ITERATION : 16, loss : 0.03278866876519497ITERATION : 17, loss : 0.03279831367222868ITERATION : 18, loss : 0.032805072178508934ITERATION : 19, loss : 0.03280980248005027ITERATION : 20, loss : 0.032813110458266424ITERATION : 21, loss : 0.03281542240338421ITERATION : 22, loss : 0.03281703752141973ITERATION : 23, loss : 0.03281816548513623ITERATION : 24, loss : 0.032818953039480105ITERATION : 25, loss : 0.03281950281504389ITERATION : 26, loss : 0.03281988654762542ITERATION : 27, loss : 0.032820154362368105ITERATION : 28, loss : 0.0328203412487095ITERATION : 29, loss : 0.0328204716655673ITERATION : 30, loss : 0.03282056265055891ITERATION : 31, loss : 0.03282062612326043ITERATION : 32, loss : 0.03282067041544947ITERATION : 33, loss : 0.03282070130020369ITERATION : 34, loss : 0.03282072284995703ITERATION : 35, loss : 0.03282073788992892ITERATION : 36, loss : 0.03282074838397711ITERATION : 37, loss : 0.03282075570457776ITERATION : 38, loss : 0.03282076080148818ITERATION : 39, loss : 0.03282076435141258ITERATION : 40, loss : 0.03282076681485835ITERATION : 41, loss : 0.03282076853482409ITERATION : 42, loss : 0.03282076973410797ITERATION : 43, loss : 0.0328207705688145ITERATION : 44, loss : 0.03282077115067862ITERATION : 45, loss : 0.032820771549300964ITERATION : 46, loss : 0.03282077182753589ITERATION : 47, loss : 0.03282077201411231ITERATION : 48, loss : 0.0328207721609348ITERATION : 49, loss : 0.03282077224560083ITERATION : 50, loss : 0.0328207723215767ITERATION : 51, loss : 0.032820772357936845ITERATION : 52, loss : 0.032820772388896684ITERATION : 53, loss : 0.03282077241060005ITERATION : 54, loss : 0.03282077242223782ITERATION : 55, loss : 0.03282077242429023ITERATION : 56, loss : 0.03282077242429023ITERATION : 57, loss : 0.03282077242429023ITERATION : 58, loss : 0.03282077242429023ITERATION : 59, loss : 0.03282077242429023ITERATION : 60, loss : 0.03282077242429023ITERATION : 61, loss : 0.03282077242429023ITERATION : 62, loss : 0.03282077242429023ITERATION : 63, loss : 0.03282077242429023ITERATION : 64, loss : 0.03282077242429023ITERATION : 65, loss : 0.03282077242429023ITERATION : 66, loss : 0.03282077242429023ITERATION : 67, loss : 0.03282077242429023ITERATION : 68, loss : 0.03282077242429023ITERATION : 69, loss : 0.03282077242429023ITERATION : 70, loss : 0.03282077242429023ITERATION : 71, loss : 0.03282077242429023ITERATION : 72, loss : 0.03282077242429023ITERATION : 73, loss : 0.03282077242429023ITERATION : 74, loss : 0.03282077242429023ITERATION : 75, loss : 0.03282077242429023ITERATION : 76, loss : 0.03282077242429023ITERATION : 77, loss : 0.03282077242429023ITERATION : 78, loss : 0.03282077242429023ITERATION : 79, loss : 0.03282077242429023ITERATION : 80, loss : 0.03282077242429023ITERATION : 81, loss : 0.03282077242429023ITERATION : 82, loss : 0.03282077242429023ITERATION : 83, loss : 0.03282077242429023ITERATION : 84, loss : 0.03282077242429023ITERATION : 85, loss : 0.03282077242429023ITERATION : 86, loss : 0.03282077242429023ITERATION : 87, loss : 0.03282077242429023ITERATION : 88, loss : 0.03282077242429023ITERATION : 89, loss : 0.03282077242429023ITERATION : 90, loss : 0.03282077242429023ITERATION : 91, loss : 0.03282077242429023ITERATION : 92, loss : 0.03282077242429023ITERATION : 93, loss : 0.03282077242429023ITERATION : 94, loss : 0.03282077242429023ITERATION : 95, loss : 0.03282077242429023ITERATION : 96, loss : 0.03282077242429023ITERATION : 97, loss : 0.03282077242429023ITERATION : 98, loss : 0.03282077242429023ITERATION : 99, loss : 0.03282077242429023ITERATION : 100, loss : 0.03282077242429023
ITERATION : 1, loss : 0.0776940515090219ITERATION : 2, loss : 0.08719476725166637ITERATION : 3, loss : 0.09994418444516857ITERATION : 4, loss : 0.10927179523807512ITERATION : 5, loss : 0.11429239719260394ITERATION : 6, loss : 0.11692494035568957ITERATION : 7, loss : 0.11825969832159015ITERATION : 8, loss : 0.1189026217733186ITERATION : 9, loss : 0.11918590664391801ITERATION : 10, loss : 0.11928893417073703ITERATION : 11, loss : 0.11930668712055952ITERATION : 12, loss : 0.11928811166094021ITERATION : 13, loss : 0.11925743403275849ITERATION : 14, loss : 0.11922588189293769ITERATION : 15, loss : 0.11919804631790899ITERATION : 16, loss : 0.11917527863597596ITERATION : 17, loss : 0.11915746450951274ITERATION : 18, loss : 0.11914391970865512ITERATION : 19, loss : 0.11913382117970782ITERATION : 20, loss : 0.11912639698073153ITERATION : 21, loss : 0.1191209948504063ITERATION : 22, loss : 0.11911709435519119ITERATION : 23, loss : 0.119114294540621ITERATION : 24, loss : 0.11911229402149812ITERATION : 25, loss : 0.11911086962818516ITERATION : 26, loss : 0.11910985822555385ITERATION : 27, loss : 0.11910914167354601ITERATION : 28, loss : 0.1191086349595033ITERATION : 29, loss : 0.11910827703860048ITERATION : 30, loss : 0.1191080244692963ITERATION : 31, loss : 0.11910784646137049ITERATION : 32, loss : 0.11910772111692061ITERATION : 33, loss : 0.11910763288032461ITERATION : 34, loss : 0.1191075707858173ITERATION : 35, loss : 0.1191075271277012ITERATION : 36, loss : 0.11910749635662504ITERATION : 37, loss : 0.1191074747853086ITERATION : 38, loss : 0.119107459622887ITERATION : 39, loss : 0.11910744894975149ITERATION : 40, loss : 0.11910744147762195ITERATION : 41, loss : 0.11910743622083537ITERATION : 42, loss : 0.11910743253162216ITERATION : 43, loss : 0.11910742990148353ITERATION : 44, loss : 0.11910742811727168ITERATION : 45, loss : 0.11910742688436526ITERATION : 46, loss : 0.11910742599666854ITERATION : 47, loss : 0.11910742534945869ITERATION : 48, loss : 0.1191074248838728ITERATION : 49, loss : 0.11910742457337807ITERATION : 50, loss : 0.11910742435120276ITERATION : 51, loss : 0.11910742426102472ITERATION : 52, loss : 0.11910742414955038ITERATION : 53, loss : 0.11910742411531035ITERATION : 54, loss : 0.11910742411510865ITERATION : 55, loss : 0.11910742411510865ITERATION : 56, loss : 0.11910742411510865ITERATION : 57, loss : 0.11910742411510865ITERATION : 58, loss : 0.11910742411510865ITERATION : 59, loss : 0.11910742411510865ITERATION : 60, loss : 0.11910742411510865ITERATION : 61, loss : 0.11910742411510865ITERATION : 62, loss : 0.11910742411510865ITERATION : 63, loss : 0.11910742411510865ITERATION : 64, loss : 0.11910742411510865ITERATION : 65, loss : 0.11910742411510865ITERATION : 66, loss : 0.11910742411510865ITERATION : 67, loss : 0.11910742411510865ITERATION : 68, loss : 0.11910742411510865ITERATION : 69, loss : 0.11910742411510865ITERATION : 70, loss : 0.11910742411510865ITERATION : 71, loss : 0.11910742411510865ITERATION : 72, loss : 0.11910742411510865ITERATION : 73, loss : 0.11910742411510865ITERATION : 74, loss : 0.11910742411510865ITERATION : 75, loss : 0.11910742411510865ITERATION : 76, loss : 0.11910742411510865ITERATION : 77, loss : 0.11910742411510865ITERATION : 78, loss : 0.11910742411510865ITERATION : 79, loss : 0.11910742411510865ITERATION : 80, loss : 0.11910742411510865ITERATION : 81, loss : 0.11910742411510865ITERATION : 82, loss : 0.11910742411510865ITERATION : 83, loss : 0.11910742411510865ITERATION : 84, loss : 0.11910742411510865ITERATION : 85, loss : 0.11910742411510865ITERATION : 86, loss : 0.11910742411510865ITERATION : 87, loss : 0.11910742411510865ITERATION : 88, loss : 0.11910742411510865ITERATION : 89, loss : 0.11910742411510865ITERATION : 90, loss : 0.11910742411510865ITERATION : 91, loss : 0.11910742411510865ITERATION : 92, loss : 0.11910742411510865ITERATION : 93, loss : 0.11910742411510865ITERATION : 94, loss : 0.11910742411510865ITERATION : 95, loss : 0.11910742411510865ITERATION : 96, loss : 0.11910742411510865ITERATION : 97, loss : 0.11910742411510865ITERATION : 98, loss : 0.11910742411510865ITERATION : 99, loss : 0.11910742411510865ITERATION : 100, loss : 0.11910742411510865
ITERATION : 1, loss : 0.07028786151183758ITERATION : 2, loss : 0.10209368434482742ITERATION : 3, loss : 0.12519203511114033ITERATION : 4, loss : 0.1400471442387688ITERATION : 5, loss : 0.1498534927406532ITERATION : 6, loss : 0.15644953479475218ITERATION : 7, loss : 0.1609465879539897ITERATION : 8, loss : 0.16404270613217264ITERATION : 9, loss : 0.16618937846337828ITERATION : 10, loss : 0.16768529124017573ITERATION : 11, loss : 0.1687314821637949ITERATION : 12, loss : 0.16946502978843375ITERATION : 13, loss : 0.169980300547279ITERATION : 14, loss : 0.17034271288111205ITERATION : 15, loss : 0.1705978467451806ITERATION : 16, loss : 0.17077757475340777ITERATION : 17, loss : 0.17090424213712976ITERATION : 18, loss : 0.17099354347884ITERATION : 19, loss : 0.17105651654805615ITERATION : 20, loss : 0.1711009313029456ITERATION : 21, loss : 0.17113226099288434ITERATION : 22, loss : 0.17115436272683132ITERATION : 23, loss : 0.1711699556812585ITERATION : 24, loss : 0.17118095727300353ITERATION : 25, loss : 0.1711887197985268ITERATION : 26, loss : 0.17119419714066275ITERATION : 27, loss : 0.1711980621554775ITERATION : 28, loss : 0.17120078953260112ITERATION : 29, loss : 0.1712027141501159ITERATION : 30, loss : 0.17120407233321083ITERATION : 31, loss : 0.17120503082396193ITERATION : 32, loss : 0.1712057072655224ITERATION : 33, loss : 0.17120618465267495ITERATION : 34, loss : 0.1712065215789259ITERATION : 35, loss : 0.1712067593731961ITERATION : 36, loss : 0.1712069272033763ITERATION : 37, loss : 0.1712070456544936ITERATION : 38, loss : 0.17120712926345688ITERATION : 39, loss : 0.1712071882906215ITERATION : 40, loss : 0.1712072299637861ITERATION : 41, loss : 0.1712072593466796ITERATION : 42, loss : 0.17120728011550426ITERATION : 43, loss : 0.1712072947621338ITERATION : 44, loss : 0.17120730511337623ITERATION : 45, loss : 0.17120731243626894ITERATION : 46, loss : 0.1712073176003681ITERATION : 47, loss : 0.17120732119001522ITERATION : 48, loss : 0.17120732381552992ITERATION : 49, loss : 0.17120732555803603ITERATION : 50, loss : 0.17120732682341705ITERATION : 51, loss : 0.1712073276663483ITERATION : 52, loss : 0.17120732833686703ITERATION : 53, loss : 0.17120732876870393ITERATION : 54, loss : 0.17120732913265058ITERATION : 55, loss : 0.17120732932703947ITERATION : 56, loss : 0.17120732944076228ITERATION : 57, loss : 0.1712073295663194ITERATION : 58, loss : 0.17120732959190432ITERATION : 59, loss : 0.17120732959212331ITERATION : 60, loss : 0.17120732959212331ITERATION : 61, loss : 0.17120732959212331ITERATION : 62, loss : 0.17120732959212331ITERATION : 63, loss : 0.17120732959212331ITERATION : 64, loss : 0.17120732959212331ITERATION : 65, loss : 0.17120732959212331ITERATION : 66, loss : 0.17120732959212331ITERATION : 67, loss : 0.17120732959212331ITERATION : 68, loss : 0.17120732959212331ITERATION : 69, loss : 0.17120732959212331ITERATION : 70, loss : 0.17120732959212331ITERATION : 71, loss : 0.17120732959212331ITERATION : 72, loss : 0.17120732959212331ITERATION : 73, loss : 0.17120732959212331ITERATION : 74, loss : 0.17120732959212331ITERATION : 75, loss : 0.17120732959212331ITERATION : 76, loss : 0.17120732959212331ITERATION : 77, loss : 0.17120732959212331ITERATION : 78, loss : 0.17120732959212331ITERATION : 79, loss : 0.17120732959212331ITERATION : 80, loss : 0.17120732959212331ITERATION : 81, loss : 0.17120732959212331ITERATION : 82, loss : 0.17120732959212331ITERATION : 83, loss : 0.17120732959212331ITERATION : 84, loss : 0.17120732959212331ITERATION : 85, loss : 0.17120732959212331ITERATION : 86, loss : 0.17120732959212331ITERATION : 87, loss : 0.17120732959212331ITERATION : 88, loss : 0.17120732959212331ITERATION : 89, loss : 0.17120732959212331ITERATION : 90, loss : 0.17120732959212331ITERATION : 91, loss : 0.17120732959212331ITERATION : 92, loss : 0.17120732959212331ITERATION : 93, loss : 0.17120732959212331ITERATION : 94, loss : 0.17120732959212331ITERATION : 95, loss : 0.17120732959212331ITERATION : 96, loss : 0.17120732959212331ITERATION : 97, loss : 0.17120732959212331ITERATION : 98, loss : 0.17120732959212331ITERATION : 99, loss : 0.17120732959212331ITERATION : 100, loss : 0.17120732959212331
ITERATION : 1, loss : 0.11897150549820436ITERATION : 2, loss : 0.13775440522572308ITERATION : 3, loss : 0.14850286283515085ITERATION : 4, loss : 0.15469106518049783ITERATION : 5, loss : 0.1583073209369736ITERATION : 6, loss : 0.16045039105165568ITERATION : 7, loss : 0.16171745339807247ITERATION : 8, loss : 0.16244869132761663ITERATION : 9, loss : 0.16284865907448415ITERATION : 10, loss : 0.1630450779061243ITERATION : 11, loss : 0.16311915129742285ITERATION : 12, loss : 0.16312262606725944ITERATION : 13, loss : 0.1630881596681073ITERATION : 14, loss : 0.16303594862627627ITERATION : 15, loss : 0.162978084181961ITERATION : 16, loss : 0.16292144485311266ITERATION : 17, loss : 0.1628696185160474ITERATION : 18, loss : 0.16282416954719167ITERATION : 19, loss : 0.16278546523171689ITERATION : 20, loss : 0.16275320605102994ITERATION : 21, loss : 0.1627267582938233ITERATION : 22, loss : 0.1627053562846659ITERATION : 23, loss : 0.16268822026120333ITERATION : 24, loss : 0.1626746202821258ITERATION : 25, loss : 0.16266390674705614ITERATION : 26, loss : 0.1626555207712727ITERATION : 27, loss : 0.1626489931375172ITERATION : 28, loss : 0.16264393653711573ITERATION : 29, loss : 0.1626400365951281ITERATION : 30, loss : 0.1626370402098706ITERATION : 31, loss : 0.16263474611278864ITERATION : 32, loss : 0.16263299529209546ITERATION : 33, loss : 0.16263166291605952ITERATION : 34, loss : 0.16263065170525737ITERATION : 35, loss : 0.16262988607830559ITERATION : 36, loss : 0.16262930772648274ITERATION : 37, loss : 0.16262887178048283ITERATION : 38, loss : 0.16262854377439415ITERATION : 39, loss : 0.16262829744823692ITERATION : 40, loss : 0.16262811275714117ITERATION : 41, loss : 0.16262797456783404ITERATION : 42, loss : 0.16262787134288267ITERATION : 43, loss : 0.16262779438952157ITERATION : 44, loss : 0.16262773707380307ITERATION : 45, loss : 0.16262769447876976ITERATION : 46, loss : 0.16262766284573182ITERATION : 47, loss : 0.1626276393986217ITERATION : 48, loss : 0.16262762200801012ITERATION : 49, loss : 0.16262760910538682ITERATION : 50, loss : 0.16262759959062628ITERATION : 51, loss : 0.16262759258725715ITERATION : 52, loss : 0.16262758738739794ITERATION : 53, loss : 0.162627583671455ITERATION : 54, loss : 0.1626275808965371ITERATION : 55, loss : 0.1626275788652742ITERATION : 56, loss : 0.1626275773419965ITERATION : 57, loss : 0.16262757625959873ITERATION : 58, loss : 0.16262757547858894ITERATION : 59, loss : 0.16262757490972202ITERATION : 60, loss : 0.16262757453808543ITERATION : 61, loss : 0.1626275741968758ITERATION : 62, loss : 0.1626275739407372ITERATION : 63, loss : 0.16262757383451362ITERATION : 64, loss : 0.16262757371550793ITERATION : 65, loss : 0.16262757371382228ITERATION : 66, loss : 0.16262757371382228ITERATION : 67, loss : 0.16262757371382228ITERATION : 68, loss : 0.16262757371382228ITERATION : 69, loss : 0.16262757371382228ITERATION : 70, loss : 0.16262757371382228ITERATION : 71, loss : 0.16262757371382228ITERATION : 72, loss : 0.16262757371382228ITERATION : 73, loss : 0.16262757371382228ITERATION : 74, loss : 0.16262757371382228ITERATION : 75, loss : 0.16262757371382228ITERATION : 76, loss : 0.16262757371382228ITERATION : 77, loss : 0.16262757371382228ITERATION : 78, loss : 0.16262757371382228ITERATION : 79, loss : 0.16262757371382228ITERATION : 80, loss : 0.16262757371382228ITERATION : 81, loss : 0.16262757371382228ITERATION : 82, loss : 0.16262757371382228ITERATION : 83, loss : 0.16262757371382228ITERATION : 84, loss : 0.16262757371382228ITERATION : 85, loss : 0.16262757371382228ITERATION : 86, loss : 0.16262757371382228ITERATION : 87, loss : 0.16262757371382228ITERATION : 88, loss : 0.16262757371382228ITERATION : 89, loss : 0.16262757371382228ITERATION : 90, loss : 0.16262757371382228ITERATION : 91, loss : 0.16262757371382228ITERATION : 92, loss : 0.16262757371382228ITERATION : 93, loss : 0.16262757371382228ITERATION : 94, loss : 0.16262757371382228ITERATION : 95, loss : 0.16262757371382228ITERATION : 96, loss : 0.16262757371382228ITERATION : 97, loss : 0.16262757371382228ITERATION : 98, loss : 0.16262757371382228ITERATION : 99, loss : 0.16262757371382228ITERATION : 100, loss : 0.16262757371382228
ITERATION : 1, loss : 0.007084438733079589ITERATION : 2, loss : 0.005281060037717728ITERATION : 3, loss : 0.0090062154664564ITERATION : 4, loss : 0.010786904841087876ITERATION : 5, loss : 0.012447591109786101ITERATION : 6, loss : 0.013824594819525754ITERATION : 7, loss : 0.014880473315102664ITERATION : 8, loss : 0.015658787754646644ITERATION : 9, loss : 0.016219619176811854ITERATION : 10, loss : 0.016618080489437744ITERATION : 11, loss : 0.01689859910112437ITERATION : 12, loss : 0.01709488049304525ITERATION : 13, loss : 0.017231650257145386ITERATION : 14, loss : 0.01732668014454783ITERATION : 15, loss : 0.0173925785625618ITERATION : 16, loss : 0.01743821365808041ITERATION : 17, loss : 0.017469786724768ITERATION : 18, loss : 0.017491616948776458ITERATION : 19, loss : 0.017506704390829103ITERATION : 20, loss : 0.017517128846699623ITERATION : 21, loss : 0.017524330321580867ITERATION : 22, loss : 0.017529304865420182ITERATION : 23, loss : 0.017532740967210823ITERATION : 24, loss : 0.01753511446722445ITERATION : 25, loss : 0.01753675405230698ITERATION : 26, loss : 0.01753788669265216ITERATION : 27, loss : 0.017538669244731964ITERATION : 28, loss : 0.017539209902458316ITERATION : 29, loss : 0.01753958351363083ITERATION : 30, loss : 0.01753984174092861ITERATION : 31, loss : 0.017540020246354485ITERATION : 32, loss : 0.017540143649512924ITERATION : 33, loss : 0.017540228956692633ITERATION : 34, loss : 0.01754028787951257ITERATION : 35, loss : 0.017540328707873733ITERATION : 36, loss : 0.017540356961249996ITERATION : 37, loss : 0.01754037650695931ITERATION : 38, loss : 0.017540390067829804ITERATION : 39, loss : 0.017540399444977466ITERATION : 40, loss : 0.017540405925875257ITERATION : 41, loss : 0.01754041040101949ITERATION : 42, loss : 0.017540413473345715ITERATION : 43, loss : 0.017540415605625952ITERATION : 44, loss : 0.017540417040983004ITERATION : 45, loss : 0.017540418074630464ITERATION : 46, loss : 0.017540418766647435ITERATION : 47, loss : 0.017540419248185363ITERATION : 48, loss : 0.01754041956827638ITERATION : 49, loss : 0.01754041981737273ITERATION : 50, loss : 0.017540419959705417ITERATION : 51, loss : 0.017540420080022913ITERATION : 52, loss : 0.017540420146508102ITERATION : 53, loss : 0.017540420198444158ITERATION : 54, loss : 0.017540420222632695ITERATION : 55, loss : 0.017540420235951048ITERATION : 56, loss : 0.017540420235951048ITERATION : 57, loss : 0.017540420235951048ITERATION : 58, loss : 0.017540420235951048ITERATION : 59, loss : 0.017540420235951048ITERATION : 60, loss : 0.017540420235951048ITERATION : 61, loss : 0.017540420235951048ITERATION : 62, loss : 0.017540420235951048ITERATION : 63, loss : 0.017540420235951048ITERATION : 64, loss : 0.017540420235951048ITERATION : 65, loss : 0.017540420235951048ITERATION : 66, loss : 0.017540420235951048ITERATION : 67, loss : 0.017540420235951048ITERATION : 68, loss : 0.017540420235951048ITERATION : 69, loss : 0.017540420235951048ITERATION : 70, loss : 0.017540420235951048ITERATION : 71, loss : 0.017540420235951048ITERATION : 72, loss : 0.017540420235951048ITERATION : 73, loss : 0.017540420235951048ITERATION : 74, loss : 0.017540420235951048ITERATION : 75, loss : 0.017540420235951048ITERATION : 76, loss : 0.017540420235951048ITERATION : 77, loss : 0.017540420235951048ITERATION : 78, loss : 0.017540420235951048ITERATION : 79, loss : 0.017540420235951048ITERATION : 80, loss : 0.017540420235951048ITERATION : 81, loss : 0.017540420235951048ITERATION : 82, loss : 0.017540420235951048ITERATION : 83, loss : 0.017540420235951048ITERATION : 84, loss : 0.017540420235951048ITERATION : 85, loss : 0.017540420235951048ITERATION : 86, loss : 0.017540420235951048ITERATION : 87, loss : 0.017540420235951048ITERATION : 88, loss : 0.017540420235951048ITERATION : 89, loss : 0.017540420235951048ITERATION : 90, loss : 0.017540420235951048ITERATION : 91, loss : 0.017540420235951048ITERATION : 92, loss : 0.017540420235951048ITERATION : 93, loss : 0.017540420235951048ITERATION : 94, loss : 0.017540420235951048ITERATION : 95, loss : 0.017540420235951048ITERATION : 96, loss : 0.017540420235951048ITERATION : 97, loss : 0.017540420235951048ITERATION : 98, loss : 0.017540420235951048ITERATION : 99, loss : 0.017540420235951048ITERATION : 100, loss : 0.017540420235951048
ITERATION : 1, loss : 0.07118241920764533ITERATION : 2, loss : 0.09923110180774646ITERATION : 3, loss : 0.11612706175712267ITERATION : 4, loss : 0.12694243754592782ITERATION : 5, loss : 0.13406393359185403ITERATION : 6, loss : 0.13868894518244135ITERATION : 7, loss : 0.14166399463424945ITERATION : 8, loss : 0.143559297885827ITERATION : 9, loss : 0.14475408635953296ITERATION : 10, loss : 0.1454982134287235ITERATION : 11, loss : 0.1459550086261889ITERATION : 12, loss : 0.1462304438976086ITERATION : 13, loss : 0.1463927394443189ITERATION : 14, loss : 0.14648543497896818ITERATION : 15, loss : 0.1465360476945667ITERATION : 16, loss : 0.1465617716201455ITERATION : 17, loss : 0.1465732054022606ITERATION : 18, loss : 0.1465767742723207ITERATION : 19, loss : 0.14657629161015925ITERATION : 20, loss : 0.146573957842443ITERATION : 21, loss : 0.14657099283382438ITERATION : 22, loss : 0.14656803294701168ITERATION : 23, loss : 0.14656537595991337ITERATION : 24, loss : 0.14656313070903956ITERATION : 25, loss : 0.14656130645504292ITERATION : 26, loss : 0.14655986476847715ITERATION : 27, loss : 0.1465587488069251ITERATION : 28, loss : 0.14655789887257892ITERATION : 29, loss : 0.14655725989897184ITERATION : 30, loss : 0.14655678467206473ITERATION : 31, loss : 0.14655643447627814ITERATION : 32, loss : 0.14655617845986058ITERATION : 33, loss : 0.14655599255083912ITERATION : 34, loss : 0.14655585834663057ITERATION : 35, loss : 0.14655576207567245ITERATION : 36, loss : 0.14655569332749746ITERATION : 37, loss : 0.1465556444417421ITERATION : 38, loss : 0.146555609847403ITERATION : 39, loss : 0.14655558546647598ITERATION : 40, loss : 0.14655556833844305ITERATION : 41, loss : 0.14655555635315856ITERATION : 42, loss : 0.1465555479889783ITERATION : 43, loss : 0.14655554218567912ITERATION : 44, loss : 0.14655553818518566ITERATION : 45, loss : 0.14655553542884145ITERATION : 46, loss : 0.14655553349749487ITERATION : 47, loss : 0.146555532185107ITERATION : 48, loss : 0.1465555311991266ITERATION : 49, loss : 0.14655553061619156ITERATION : 50, loss : 0.14655553020968815ITERATION : 51, loss : 0.14655552994122412ITERATION : 52, loss : 0.14655552979711767ITERATION : 53, loss : 0.1465555297139506ITERATION : 54, loss : 0.14655552962056576ITERATION : 55, loss : 0.14655552956469606ITERATION : 56, loss : 0.14655552954305545ITERATION : 57, loss : 0.14655552954305545ITERATION : 58, loss : 0.14655552954305545ITERATION : 59, loss : 0.14655552954305545ITERATION : 60, loss : 0.14655552954305545ITERATION : 61, loss : 0.14655552954305545ITERATION : 62, loss : 0.14655552954305545ITERATION : 63, loss : 0.14655552954305545ITERATION : 64, loss : 0.14655552954305545ITERATION : 65, loss : 0.14655552954305545ITERATION : 66, loss : 0.14655552954305545ITERATION : 67, loss : 0.14655552954305545ITERATION : 68, loss : 0.14655552954305545ITERATION : 69, loss : 0.14655552954305545ITERATION : 70, loss : 0.14655552954305545ITERATION : 71, loss : 0.14655552954305545ITERATION : 72, loss : 0.14655552954305545ITERATION : 73, loss : 0.14655552954305545ITERATION : 74, loss : 0.14655552954305545ITERATION : 75, loss : 0.14655552954305545ITERATION : 76, loss : 0.14655552954305545ITERATION : 77, loss : 0.14655552954305545ITERATION : 78, loss : 0.14655552954305545ITERATION : 79, loss : 0.14655552954305545ITERATION : 80, loss : 0.14655552954305545ITERATION : 81, loss : 0.14655552954305545ITERATION : 82, loss : 0.14655552954305545ITERATION : 83, loss : 0.14655552954305545ITERATION : 84, loss : 0.14655552954305545ITERATION : 85, loss : 0.14655552954305545ITERATION : 86, loss : 0.14655552954305545ITERATION : 87, loss : 0.14655552954305545ITERATION : 88, loss : 0.14655552954305545ITERATION : 89, loss : 0.14655552954305545ITERATION : 90, loss : 0.14655552954305545ITERATION : 91, loss : 0.14655552954305545ITERATION : 92, loss : 0.14655552954305545ITERATION : 93, loss : 0.14655552954305545ITERATION : 94, loss : 0.14655552954305545ITERATION : 95, loss : 0.14655552954305545ITERATION : 96, loss : 0.14655552954305545ITERATION : 97, loss : 0.14655552954305545ITERATION : 98, loss : 0.14655552954305545ITERATION : 99, loss : 0.14655552954305545ITERATION : 100, loss : 0.14655552954305545
ITERATION : 1, loss : 0.010718047170352673ITERATION : 2, loss : 0.013798401375843258ITERATION : 3, loss : 0.016843865389898256ITERATION : 4, loss : 0.019742139239934405ITERATION : 5, loss : 0.022127092517439568ITERATION : 6, loss : 0.02395352553039233ITERATION : 7, loss : 0.025304405614948955ITERATION : 8, loss : 0.026276024603624177ITERATION : 9, loss : 0.026965648939167166ITERATION : 10, loss : 0.02745168668754202ITERATION : 11, loss : 0.027792900375621852ITERATION : 12, loss : 0.02803190170020179ITERATION : 13, loss : 0.028199082916816047ITERATION : 14, loss : 0.02831592904345569ITERATION : 15, loss : 0.028397552379273782ITERATION : 16, loss : 0.028454551572359835ITERATION : 17, loss : 0.028494346493488008ITERATION : 18, loss : 0.02852212601034037ITERATION : 19, loss : 0.02854151611820869ITERATION : 20, loss : 0.028555049577392626ITERATION : 21, loss : 0.028564494906947284ITERATION : 22, loss : 0.028571086905040243ITERATION : 23, loss : 0.028575687504441716ITERATION : 24, loss : 0.02857889821278306ITERATION : 25, loss : 0.02858113891992986ITERATION : 26, loss : 0.028582702677606778ITERATION : 27, loss : 0.028583793998535473ITERATION : 28, loss : 0.028584555623639447ITERATION : 29, loss : 0.02858508713975809ITERATION : 30, loss : 0.02858545809457011ITERATION : 31, loss : 0.02858571698810747ITERATION : 32, loss : 0.028585897657733988ITERATION : 33, loss : 0.028586023743030806ITERATION : 34, loss : 0.028586111769888903ITERATION : 35, loss : 0.028586173180016952ITERATION : 36, loss : 0.02858621605822211ITERATION : 37, loss : 0.028586245983111467ITERATION : 38, loss : 0.028586266865917524ITERATION : 39, loss : 0.028586281454496085ITERATION : 40, loss : 0.028586291641927424ITERATION : 41, loss : 0.028586298728388377ITERATION : 42, loss : 0.028586303670206495ITERATION : 43, loss : 0.028586307123289346ITERATION : 44, loss : 0.02858630951588515ITERATION : 45, loss : 0.028586311174534706ITERATION : 46, loss : 0.02858631234450885ITERATION : 47, loss : 0.028586313156393234ITERATION : 48, loss : 0.02858631367793892ITERATION : 49, loss : 0.028586314040912438ITERATION : 50, loss : 0.028586314319949187ITERATION : 51, loss : 0.02858631452449957ITERATION : 52, loss : 0.02858631466071017ITERATION : 53, loss : 0.0285863147473479ITERATION : 54, loss : 0.028586314781479768ITERATION : 55, loss : 0.02858631481374787ITERATION : 56, loss : 0.02858631483269451ITERATION : 57, loss : 0.028586314833249707ITERATION : 58, loss : 0.028586314833249707ITERATION : 59, loss : 0.028586314833249707ITERATION : 60, loss : 0.028586314833249707ITERATION : 61, loss : 0.028586314833249707ITERATION : 62, loss : 0.028586314833249707ITERATION : 63, loss : 0.028586314833249707ITERATION : 64, loss : 0.028586314833249707ITERATION : 65, loss : 0.028586314833249707ITERATION : 66, loss : 0.028586314833249707ITERATION : 67, loss : 0.028586314833249707ITERATION : 68, loss : 0.028586314833249707ITERATION : 69, loss : 0.028586314833249707ITERATION : 70, loss : 0.028586314833249707ITERATION : 71, loss : 0.028586314833249707ITERATION : 72, loss : 0.028586314833249707ITERATION : 73, loss : 0.028586314833249707ITERATION : 74, loss : 0.028586314833249707ITERATION : 75, loss : 0.028586314833249707ITERATION : 76, loss : 0.028586314833249707ITERATION : 77, loss : 0.028586314833249707ITERATION : 78, loss : 0.028586314833249707ITERATION : 79, loss : 0.028586314833249707ITERATION : 80, loss : 0.028586314833249707ITERATION : 81, loss : 0.028586314833249707ITERATION : 82, loss : 0.028586314833249707ITERATION : 83, loss : 0.028586314833249707ITERATION : 84, loss : 0.028586314833249707ITERATION : 85, loss : 0.028586314833249707ITERATION : 86, loss : 0.028586314833249707ITERATION : 87, loss : 0.028586314833249707ITERATION : 88, loss : 0.028586314833249707ITERATION : 89, loss : 0.028586314833249707ITERATION : 90, loss : 0.028586314833249707ITERATION : 91, loss : 0.028586314833249707ITERATION : 92, loss : 0.028586314833249707ITERATION : 93, loss : 0.028586314833249707ITERATION : 94, loss : 0.028586314833249707ITERATION : 95, loss : 0.028586314833249707ITERATION : 96, loss : 0.028586314833249707ITERATION : 97, loss : 0.028586314833249707ITERATION : 98, loss : 0.028586314833249707ITERATION : 99, loss : 0.028586314833249707ITERATION : 100, loss : 0.028586314833249707
ITERATION : 1, loss : 0.06514813267266836ITERATION : 2, loss : 0.08472628512019455ITERATION : 3, loss : 0.09647376539997105ITERATION : 4, loss : 0.10364572104782177ITERATION : 5, loss : 0.10827318743345152ITERATION : 6, loss : 0.111270894447972ITERATION : 7, loss : 0.11321961905408003ITERATION : 8, loss : 0.11448998536314237ITERATION : 9, loss : 0.115319886342681ITERATION : 10, loss : 0.1158628842696719ITERATION : 11, loss : 0.11621857042200245ITERATION : 12, loss : 0.1164517622461821ITERATION : 13, loss : 0.11660475205115654ITERATION : 14, loss : 0.1167051839895381ITERATION : 15, loss : 0.11677115001044201ITERATION : 16, loss : 0.11681450123747741ITERATION : 17, loss : 0.11684300609241739ITERATION : 18, loss : 0.11686175976474018ITERATION : 19, loss : 0.1168741053939317ITERATION : 20, loss : 0.11688223784465776ITERATION : 21, loss : 0.1168875987146746ITERATION : 22, loss : 0.11689113519883486ITERATION : 23, loss : 0.11689347006417396ITERATION : 24, loss : 0.1168950130372761ITERATION : 25, loss : 0.11689603349245306ITERATION : 26, loss : 0.1168967091043722ITERATION : 27, loss : 0.11689715683057139ITERATION : 28, loss : 0.11689745394341917ITERATION : 29, loss : 0.11689765137985024ITERATION : 30, loss : 0.11689778271332894ITERATION : 31, loss : 0.11689787020330296ITERATION : 32, loss : 0.11689792852742212ITERATION : 33, loss : 0.11689796749587059ITERATION : 34, loss : 0.11689799363608115ITERATION : 35, loss : 0.1168980111503768ITERATION : 36, loss : 0.11689802289748143ITERATION : 37, loss : 0.11689803081489183ITERATION : 38, loss : 0.1168980361531907ITERATION : 39, loss : 0.11689803977497157ITERATION : 40, loss : 0.11689804224405784ITERATION : 41, loss : 0.11689804392263327ITERATION : 42, loss : 0.11689804505479807ITERATION : 43, loss : 0.1168980457945916ITERATION : 44, loss : 0.11689804628064313ITERATION : 45, loss : 0.11689804660589116ITERATION : 46, loss : 0.11689804683088828ITERATION : 47, loss : 0.11689804698584455ITERATION : 48, loss : 0.11689804709778788ITERATION : 49, loss : 0.11689804715092225ITERATION : 50, loss : 0.116898047213684ITERATION : 51, loss : 0.11689804726516367ITERATION : 52, loss : 0.11689804728265564ITERATION : 53, loss : 0.11689804730302228ITERATION : 54, loss : 0.11689804730322004ITERATION : 55, loss : 0.11689804730322004ITERATION : 56, loss : 0.11689804730322004ITERATION : 57, loss : 0.11689804730322004ITERATION : 58, loss : 0.11689804730322004ITERATION : 59, loss : 0.11689804730322004ITERATION : 60, loss : 0.11689804730322004ITERATION : 61, loss : 0.11689804730322004ITERATION : 62, loss : 0.11689804730322004ITERATION : 63, loss : 0.11689804730322004ITERATION : 64, loss : 0.11689804730322004ITERATION : 65, loss : 0.11689804730322004ITERATION : 66, loss : 0.11689804730322004ITERATION : 67, loss : 0.11689804730322004ITERATION : 68, loss : 0.11689804730322004ITERATION : 69, loss : 0.11689804730322004ITERATION : 70, loss : 0.11689804730322004ITERATION : 71, loss : 0.11689804730322004ITERATION : 72, loss : 0.11689804730322004ITERATION : 73, loss : 0.11689804730322004ITERATION : 74, loss : 0.11689804730322004ITERATION : 75, loss : 0.11689804730322004ITERATION : 76, loss : 0.11689804730322004ITERATION : 77, loss : 0.11689804730322004ITERATION : 78, loss : 0.11689804730322004ITERATION : 79, loss : 0.11689804730322004ITERATION : 80, loss : 0.11689804730322004ITERATION : 81, loss : 0.11689804730322004ITERATION : 82, loss : 0.11689804730322004ITERATION : 83, loss : 0.11689804730322004ITERATION : 84, loss : 0.11689804730322004ITERATION : 85, loss : 0.11689804730322004ITERATION : 86, loss : 0.11689804730322004ITERATION : 87, loss : 0.11689804730322004ITERATION : 88, loss : 0.11689804730322004ITERATION : 89, loss : 0.11689804730322004ITERATION : 90, loss : 0.11689804730322004ITERATION : 91, loss : 0.11689804730322004ITERATION : 92, loss : 0.11689804730322004ITERATION : 93, loss : 0.11689804730322004ITERATION : 94, loss : 0.11689804730322004ITERATION : 95, loss : 0.11689804730322004ITERATION : 96, loss : 0.11689804730322004ITERATION : 97, loss : 0.11689804730322004ITERATION : 98, loss : 0.11689804730322004ITERATION : 99, loss : 0.11689804730322004ITERATION : 100, loss : 0.11689804730322004
gradient norm in None layer : 0.003258618821305628
gradient norm in None layer : 8.096279390876542e-05
gradient norm in None layer : 7.585105487015775e-05
gradient norm in None layer : 0.0012743397518259869
gradient norm in None layer : 9.977745710365133e-05
gradient norm in None layer : 0.00010660779672098203
gradient norm in None layer : 0.00046450185507234374
gradient norm in None layer : 1.8444745374139433e-05
gradient norm in None layer : 1.6360675771974253e-05
gradient norm in None layer : 0.00041317257445695006
gradient norm in None layer : 1.8348142379369495e-05
gradient norm in None layer : 1.62650401224126e-05
gradient norm in None layer : 0.0001461362428988011
gradient norm in None layer : 4.4749151945581295e-06
gradient norm in None layer : 3.816160457242092e-06
gradient norm in None layer : 0.00013840903894578394
gradient norm in None layer : 5.7756674566147224e-06
gradient norm in None layer : 5.1160411539699335e-06
gradient norm in None layer : 0.00019275617131879022
gradient norm in None layer : 2.4849740317043963e-06
gradient norm in None layer : 0.00042871177592253317
gradient norm in None layer : 2.4118204180062013e-05
gradient norm in None layer : 2.0567480980045754e-05
gradient norm in None layer : 0.000490888889992673
gradient norm in None layer : 5.284443278281065e-05
gradient norm in None layer : 7.270013136760571e-05
gradient norm in None layer : 0.0009643647729247888
gradient norm in None layer : 5.095749365652635e-06
gradient norm in None layer : 0.0012301052608178268
gradient norm in None layer : 8.097355665877299e-05
gradient norm in None layer : 6.211149930102995e-05
gradient norm in None layer : 0.0013734205310785108
gradient norm in None layer : 0.000239640654716038
gradient norm in None layer : 0.00037235457765026744
gradient norm in None layer : 0.00017250439113315497
gradient norm in None layer : 5.999724814555084e-05
Total gradient norm: 0.004212455623231404
invariance loss : 4.9066065527660045, avg_den : 0.39896392822265625, density loss : 0.29918147172641235, mse loss : 0.09941792647010259, solver time : 118.48480653762817 sec , total loss : 0.10462371449459501, running loss : 0.11056009593089662
Epoch 0/10 , batch 10/12500 
ITERATION : 1, loss : 0.041962805850627546ITERATION : 2, loss : 0.06115264502779349ITERATION : 3, loss : 0.07655371338194027ITERATION : 4, loss : 0.08742280003615442ITERATION : 5, loss : 0.0948272267570701ITERATION : 6, loss : 0.09979671112352927ITERATION : 7, loss : 0.10311245935763956ITERATION : 8, loss : 0.10531982894674873ITERATION : 9, loss : 0.10678796748334089ITERATION : 10, loss : 0.10776392826260402ITERATION : 11, loss : 0.10841240698960189ITERATION : 12, loss : 0.10884306532070515ITERATION : 13, loss : 0.10912889595251911ITERATION : 14, loss : 0.10931847517175478ITERATION : 15, loss : 0.10944412168931701ITERATION : 16, loss : 0.10952732927074106ITERATION : 17, loss : 0.10958238529567138ITERATION : 18, loss : 0.1096187811489272ITERATION : 19, loss : 0.10964281801198345ITERATION : 20, loss : 0.10965867613724824ITERATION : 21, loss : 0.10966912668364336ITERATION : 22, loss : 0.10967600528310859ITERATION : 23, loss : 0.10968052680824142ITERATION : 24, loss : 0.10968349468988674ITERATION : 25, loss : 0.10968543957407498ITERATION : 26, loss : 0.10968671187426261ITERATION : 27, loss : 0.10968754261369426ITERATION : 28, loss : 0.1096880836810644ITERATION : 29, loss : 0.10968843534629193ITERATION : 30, loss : 0.10968866321926589ITERATION : 31, loss : 0.1096888104174938ITERATION : 32, loss : 0.10968890516752436ITERATION : 33, loss : 0.10968896592507389ITERATION : 34, loss : 0.1096890046367461ITERATION : 35, loss : 0.1096890291925043ITERATION : 36, loss : 0.10968904465014749ITERATION : 37, loss : 0.10968905429043532ITERATION : 38, loss : 0.10968906027389384ITERATION : 39, loss : 0.10968906392808035ITERATION : 40, loss : 0.109689066109196ITERATION : 41, loss : 0.10968906742142984ITERATION : 42, loss : 0.10968906819320899ITERATION : 43, loss : 0.10968906863492274ITERATION : 44, loss : 0.10968906884591807ITERATION : 45, loss : 0.10968906890546669ITERATION : 46, loss : 0.10968906890339537ITERATION : 47, loss : 0.10968906886499778ITERATION : 48, loss : 0.1096890688155551ITERATION : 49, loss : 0.10968906877643751ITERATION : 50, loss : 0.10968906871757345ITERATION : 51, loss : 0.10968906870043288ITERATION : 52, loss : 0.10968906865168537ITERATION : 53, loss : 0.10968906863954873ITERATION : 54, loss : 0.10968906859956434ITERATION : 55, loss : 0.10968906861114386ITERATION : 56, loss : 0.10968906858826984ITERATION : 57, loss : 0.10968906859884421ITERATION : 58, loss : 0.10968906857546044ITERATION : 59, loss : 0.10968906858912991ITERATION : 60, loss : 0.10968906857488013ITERATION : 61, loss : 0.10968906857488013ITERATION : 62, loss : 0.10968906857488013ITERATION : 63, loss : 0.10968906857488013ITERATION : 64, loss : 0.10968906857488013ITERATION : 65, loss : 0.10968906857488013ITERATION : 66, loss : 0.10968906857488013ITERATION : 67, loss : 0.10968906857488013ITERATION : 68, loss : 0.10968906857488013ITERATION : 69, loss : 0.10968906857488013ITERATION : 70, loss : 0.10968906857488013ITERATION : 71, loss : 0.10968906857488013ITERATION : 72, loss : 0.10968906857488013ITERATION : 73, loss : 0.10968906857488013ITERATION : 74, loss : 0.10968906857488013ITERATION : 75, loss : 0.10968906857488013ITERATION : 76, loss : 0.10968906857488013ITERATION : 77, loss : 0.10968906857488013ITERATION : 78, loss : 0.10968906857488013ITERATION : 79, loss : 0.10968906857488013ITERATION : 80, loss : 0.10968906857488013ITERATION : 81, loss : 0.10968906857488013ITERATION : 82, loss : 0.10968906857488013ITERATION : 83, loss : 0.10968906857488013ITERATION : 84, loss : 0.10968906857488013ITERATION : 85, loss : 0.10968906857488013ITERATION : 86, loss : 0.10968906857488013ITERATION : 87, loss : 0.10968906857488013ITERATION : 88, loss : 0.10968906857488013ITERATION : 89, loss : 0.10968906857488013ITERATION : 90, loss : 0.10968906857488013ITERATION : 91, loss : 0.10968906857488013ITERATION : 92, loss : 0.10968906857488013ITERATION : 93, loss : 0.10968906857488013ITERATION : 94, loss : 0.10968906857488013ITERATION : 95, loss : 0.10968906857488013ITERATION : 96, loss : 0.10968906857488013ITERATION : 97, loss : 0.10968906857488013ITERATION : 98, loss : 0.10968906857488013ITERATION : 99, loss : 0.10968906857488013ITERATION : 100, loss : 0.10968906857488013
ITERATION : 1, loss : 0.032722467320022104ITERATION : 2, loss : 0.0370794380162365ITERATION : 3, loss : 0.04147867308217737ITERATION : 4, loss : 0.045102156466917856ITERATION : 5, loss : 0.04790177537567504ITERATION : 6, loss : 0.04999810901869774ITERATION : 7, loss : 0.05150495063533738ITERATION : 8, loss : 0.052575658797535965ITERATION : 9, loss : 0.053348145820992426ITERATION : 10, loss : 0.05390296990992277ITERATION : 11, loss : 0.054300253474817615ITERATION : 12, loss : 0.0545841376993239ITERATION : 13, loss : 0.05478669754593403ITERATION : 14, loss : 0.0549310836412755ITERATION : 15, loss : 0.05503392935120477ITERATION : 16, loss : 0.05510714884795717ITERATION : 17, loss : 0.055159257512252355ITERATION : 18, loss : 0.05519633248072125ITERATION : 19, loss : 0.055222706047630334ITERATION : 20, loss : 0.055241464716694874ITERATION : 21, loss : 0.05525480574564358ITERATION : 22, loss : 0.05526429317279977ITERATION : 23, loss : 0.0552710397434808ITERATION : 24, loss : 0.05527583701918338ITERATION : 25, loss : 0.055279248160995186ITERATION : 26, loss : 0.05528167359902655ITERATION : 27, loss : 0.0552833981704367ITERATION : 28, loss : 0.05528462431924437ITERATION : 29, loss : 0.055285496174652955ITERATION : 30, loss : 0.055286115991090755ITERATION : 31, loss : 0.05528655674005869ITERATION : 32, loss : 0.05528687007473334ITERATION : 33, loss : 0.055287092865258194ITERATION : 34, loss : 0.055287251265663466ITERATION : 35, loss : 0.05528736382120957ITERATION : 36, loss : 0.05528744387292429ITERATION : 37, loss : 0.05528750080360151ITERATION : 38, loss : 0.05528754127276797ITERATION : 39, loss : 0.05528757006725569ITERATION : 40, loss : 0.05528759053708093ITERATION : 41, loss : 0.05528760507838632ITERATION : 42, loss : 0.05528761551062196ITERATION : 43, loss : 0.05528762282974226ITERATION : 44, loss : 0.05528762808434506ITERATION : 45, loss : 0.05528763175895313ITERATION : 46, loss : 0.055287634369373936ITERATION : 47, loss : 0.05528763620446144ITERATION : 48, loss : 0.05528763758153031ITERATION : 49, loss : 0.0552876385345054ITERATION : 50, loss : 0.05528763924585021ITERATION : 51, loss : 0.05528763970903489ITERATION : 52, loss : 0.05528764006890365ITERATION : 53, loss : 0.05528764024050886ITERATION : 54, loss : 0.055287640424781195ITERATION : 55, loss : 0.05528764045628128ITERATION : 56, loss : 0.055287640517699896ITERATION : 57, loss : 0.055287640519250704ITERATION : 58, loss : 0.055287640519250704ITERATION : 59, loss : 0.055287640519250704ITERATION : 60, loss : 0.055287640519250704ITERATION : 61, loss : 0.055287640519250704ITERATION : 62, loss : 0.055287640519250704ITERATION : 63, loss : 0.055287640519250704ITERATION : 64, loss : 0.055287640519250704ITERATION : 65, loss : 0.055287640519250704ITERATION : 66, loss : 0.055287640519250704ITERATION : 67, loss : 0.055287640519250704ITERATION : 68, loss : 0.055287640519250704ITERATION : 69, loss : 0.055287640519250704ITERATION : 70, loss : 0.055287640519250704ITERATION : 71, loss : 0.055287640519250704ITERATION : 72, loss : 0.055287640519250704ITERATION : 73, loss : 0.055287640519250704ITERATION : 74, loss : 0.055287640519250704ITERATION : 75, loss : 0.055287640519250704ITERATION : 76, loss : 0.055287640519250704ITERATION : 77, loss : 0.055287640519250704ITERATION : 78, loss : 0.055287640519250704ITERATION : 79, loss : 0.055287640519250704ITERATION : 80, loss : 0.055287640519250704ITERATION : 81, loss : 0.055287640519250704ITERATION : 82, loss : 0.055287640519250704ITERATION : 83, loss : 0.055287640519250704ITERATION : 84, loss : 0.055287640519250704ITERATION : 85, loss : 0.055287640519250704ITERATION : 86, loss : 0.055287640519250704ITERATION : 87, loss : 0.055287640519250704ITERATION : 88, loss : 0.055287640519250704ITERATION : 89, loss : 0.055287640519250704ITERATION : 90, loss : 0.055287640519250704ITERATION : 91, loss : 0.055287640519250704ITERATION : 92, loss : 0.055287640519250704ITERATION : 93, loss : 0.055287640519250704ITERATION : 94, loss : 0.055287640519250704ITERATION : 95, loss : 0.055287640519250704ITERATION : 96, loss : 0.055287640519250704ITERATION : 97, loss : 0.055287640519250704ITERATION : 98, loss : 0.055287640519250704ITERATION : 99, loss : 0.055287640519250704ITERATION : 100, loss : 0.055287640519250704
ITERATION : 1, loss : 0.023993321537296185ITERATION : 2, loss : 0.02720448518532254ITERATION : 3, loss : 0.03223950397723609ITERATION : 4, loss : 0.03619959753653134ITERATION : 5, loss : 0.03915086334925126ITERATION : 6, loss : 0.04132426585888482ITERATION : 7, loss : 0.042919968454651125ITERATION : 8, loss : 0.04409120726590969ITERATION : 9, loss : 0.044951470693845016ITERATION : 10, loss : 0.04558390825711761ITERATION : 11, loss : 0.04604927032811243ITERATION : 12, loss : 0.0463919557963382ITERATION : 13, loss : 0.04664445922180718ITERATION : 14, loss : 0.04683060187703822ITERATION : 15, loss : 0.04696787362919298ITERATION : 16, loss : 0.047069132355039914ITERATION : 17, loss : 0.04714384045106097ITERATION : 18, loss : 0.04719896729359792ITERATION : 19, loss : 0.04723964906307176ITERATION : 20, loss : 0.047269672667850385ITERATION : 21, loss : 0.04729183138431721ITERATION : 22, loss : 0.047308185810678874ITERATION : 23, loss : 0.0473202563971807ITERATION : 24, loss : 0.047329165224721956ITERATION : 25, loss : 0.04733574042707233ITERATION : 26, loss : 0.04734059325422042ITERATION : 27, loss : 0.04734417476213412ITERATION : 28, loss : 0.04734681793507816ITERATION : 29, loss : 0.04734876856231045ITERATION : 30, loss : 0.04735020806284332ITERATION : 31, loss : 0.047351270328744144ITERATION : 32, loss : 0.04735205420975227ITERATION : 33, loss : 0.04735263265406535ITERATION : 34, loss : 0.047353059485812345ITERATION : 35, loss : 0.047353374407510224ITERATION : 36, loss : 0.0473536067902521ITERATION : 37, loss : 0.047353778235835316ITERATION : 38, loss : 0.04735390473861913ITERATION : 39, loss : 0.04735399806247844ITERATION : 40, loss : 0.04735406690648434ITERATION : 41, loss : 0.04735411768660804ITERATION : 42, loss : 0.04735415516027348ITERATION : 43, loss : 0.04735418281327676ITERATION : 44, loss : 0.047354203191082404ITERATION : 45, loss : 0.047354218231353454ITERATION : 46, loss : 0.0473542293184158ITERATION : 47, loss : 0.04735423748957282ITERATION : 48, loss : 0.047354243543000746ITERATION : 49, loss : 0.047354247977538975ITERATION : 50, loss : 0.04735425126231263ITERATION : 51, loss : 0.04735425367402259ITERATION : 52, loss : 0.04735425544565513ITERATION : 53, loss : 0.047354256707416484ITERATION : 54, loss : 0.04735425768449372ITERATION : 55, loss : 0.0473542583637591ITERATION : 56, loss : 0.047354258919506176ITERATION : 57, loss : 0.047354259312003516ITERATION : 58, loss : 0.04735425963976739ITERATION : 59, loss : 0.04735425977280183ITERATION : 60, loss : 0.04735425996448578ITERATION : 61, loss : 0.04735426008133349ITERATION : 62, loss : 0.04735426013054941ITERATION : 63, loss : 0.04735426017545711ITERATION : 64, loss : 0.04735426017679798ITERATION : 65, loss : 0.04735426017679798ITERATION : 66, loss : 0.04735426017679798ITERATION : 67, loss : 0.04735426017679798ITERATION : 68, loss : 0.04735426017679798ITERATION : 69, loss : 0.04735426017679798ITERATION : 70, loss : 0.04735426017679798ITERATION : 71, loss : 0.04735426017679798ITERATION : 72, loss : 0.04735426017679798ITERATION : 73, loss : 0.04735426017679798ITERATION : 74, loss : 0.04735426017679798ITERATION : 75, loss : 0.04735426017679798ITERATION : 76, loss : 0.04735426017679798ITERATION : 77, loss : 0.04735426017679798ITERATION : 78, loss : 0.04735426017679798ITERATION : 79, loss : 0.04735426017679798ITERATION : 80, loss : 0.04735426017679798ITERATION : 81, loss : 0.04735426017679798ITERATION : 82, loss : 0.04735426017679798ITERATION : 83, loss : 0.04735426017679798ITERATION : 84, loss : 0.04735426017679798ITERATION : 85, loss : 0.04735426017679798ITERATION : 86, loss : 0.04735426017679798ITERATION : 87, loss : 0.04735426017679798ITERATION : 88, loss : 0.04735426017679798ITERATION : 89, loss : 0.04735426017679798ITERATION : 90, loss : 0.04735426017679798ITERATION : 91, loss : 0.04735426017679798ITERATION : 92, loss : 0.04735426017679798ITERATION : 93, loss : 0.04735426017679798ITERATION : 94, loss : 0.04735426017679798ITERATION : 95, loss : 0.04735426017679798ITERATION : 96, loss : 0.04735426017679798ITERATION : 97, loss : 0.04735426017679798ITERATION : 98, loss : 0.04735426017679798ITERATION : 99, loss : 0.04735426017679798ITERATION : 100, loss : 0.04735426017679798
ITERATION : 1, loss : 0.10345268998165674ITERATION : 2, loss : 0.12061724561256477ITERATION : 3, loss : 0.12903996381746005ITERATION : 4, loss : 0.13418640891041425ITERATION : 5, loss : 0.13756988321721872ITERATION : 6, loss : 0.13986863372666442ITERATION : 7, loss : 0.14145951729617665ITERATION : 8, loss : 0.1425735125290262ITERATION : 9, loss : 0.14335975571855442ITERATION : 10, loss : 0.14391769498282153ITERATION : 11, loss : 0.14431511252642756ITERATION : 12, loss : 0.14459892689598947ITERATION : 13, loss : 0.14480197476615195ITERATION : 14, loss : 0.14494741843178394ITERATION : 15, loss : 0.14505168698496007ITERATION : 16, loss : 0.14512647916605378ITERATION : 17, loss : 0.14518014809671637ITERATION : 18, loss : 0.14521866909095676ITERATION : 19, loss : 0.14524632209986732ITERATION : 20, loss : 0.1452661753401237ITERATION : 21, loss : 0.14528042979350622ITERATION : 22, loss : 0.14529066476270733ITERATION : 23, loss : 0.14529801380941706ITERATION : 24, loss : 0.14530329075592407ITERATION : 25, loss : 0.14530707985385358ITERATION : 26, loss : 0.1453098005929087ITERATION : 27, loss : 0.14531175423792864ITERATION : 28, loss : 0.145313157069799ITERATION : 29, loss : 0.14531416435827088ITERATION : 30, loss : 0.1453148876242581ITERATION : 31, loss : 0.1453154069751192ITERATION : 32, loss : 0.14531577990118508ITERATION : 33, loss : 0.14531604768367637ITERATION : 34, loss : 0.145316239962105ITERATION : 35, loss : 0.14531637802550873ITERATION : 36, loss : 0.14531647717131776ITERATION : 37, loss : 0.14531654834332813ITERATION : 38, loss : 0.14531659946117395ITERATION : 39, loss : 0.14531663616981522ITERATION : 40, loss : 0.14531666256109455ITERATION : 41, loss : 0.14531668146941967ITERATION : 42, loss : 0.1453166950377532ITERATION : 43, loss : 0.1453167047818175ITERATION : 44, loss : 0.14531671180333347ITERATION : 45, loss : 0.1453167168322331ITERATION : 46, loss : 0.14531672043906949ITERATION : 47, loss : 0.1453167230264645ITERATION : 48, loss : 0.14531672487498465ITERATION : 49, loss : 0.14531672623963887ITERATION : 50, loss : 0.14531672717525737ITERATION : 51, loss : 0.14531672785643893ITERATION : 52, loss : 0.1453167283397334ITERATION : 53, loss : 0.14531672866806392ITERATION : 54, loss : 0.1453167289251298ITERATION : 55, loss : 0.1453167291444369ITERATION : 56, loss : 0.1453167292725637ITERATION : 57, loss : 0.14531672936487727ITERATION : 58, loss : 0.14531672943206614ITERATION : 59, loss : 0.14531672947618957ITERATION : 60, loss : 0.14531672949783778ITERATION : 61, loss : 0.14531672951703872ITERATION : 62, loss : 0.14531672953289998ITERATION : 63, loss : 0.145316729536544ITERATION : 64, loss : 0.14531672953680408ITERATION : 65, loss : 0.14531672953680408ITERATION : 66, loss : 0.14531672953680408ITERATION : 67, loss : 0.14531672953680408ITERATION : 68, loss : 0.14531672953680408ITERATION : 69, loss : 0.14531672953680408ITERATION : 70, loss : 0.14531672953680408ITERATION : 71, loss : 0.14531672953680408ITERATION : 72, loss : 0.14531672953680408ITERATION : 73, loss : 0.14531672953680408ITERATION : 74, loss : 0.14531672953680408ITERATION : 75, loss : 0.14531672953680408ITERATION : 76, loss : 0.14531672953680408ITERATION : 77, loss : 0.14531672953680408ITERATION : 78, loss : 0.14531672953680408ITERATION : 79, loss : 0.14531672953680408ITERATION : 80, loss : 0.14531672953680408ITERATION : 81, loss : 0.14531672953680408ITERATION : 82, loss : 0.14531672953680408ITERATION : 83, loss : 0.14531672953680408ITERATION : 84, loss : 0.14531672953680408ITERATION : 85, loss : 0.14531672953680408ITERATION : 86, loss : 0.14531672953680408ITERATION : 87, loss : 0.14531672953680408ITERATION : 88, loss : 0.14531672953680408ITERATION : 89, loss : 0.14531672953680408ITERATION : 90, loss : 0.14531672953680408ITERATION : 91, loss : 0.14531672953680408ITERATION : 92, loss : 0.14531672953680408ITERATION : 93, loss : 0.14531672953680408ITERATION : 94, loss : 0.14531672953680408ITERATION : 95, loss : 0.14531672953680408ITERATION : 96, loss : 0.14531672953680408ITERATION : 97, loss : 0.14531672953680408ITERATION : 98, loss : 0.14531672953680408ITERATION : 99, loss : 0.14531672953680408ITERATION : 100, loss : 0.14531672953680408
ITERATION : 1, loss : 0.012530083086548202ITERATION : 2, loss : 0.015891410906082502ITERATION : 3, loss : 0.018200706269573805ITERATION : 4, loss : 0.019918084934618913ITERATION : 5, loss : 0.02118447494948958ITERATION : 6, loss : 0.02210522404987589ITERATION : 7, loss : 0.02276843941912673ITERATION : 8, loss : 0.023243213096501982ITERATION : 9, loss : 0.02358166870020365ITERATION : 10, loss : 0.02382224926015368ITERATION : 11, loss : 0.023992912910922558ITERATION : 12, loss : 0.024113806541771653ITERATION : 13, loss : 0.024199358616877827ITERATION : 14, loss : 0.024259858133096727ITERATION : 15, loss : 0.024302620270874927ITERATION : 16, loss : 0.024332835055793176ITERATION : 17, loss : 0.024354179281904448ITERATION : 18, loss : 0.024369254833644206ITERATION : 19, loss : 0.024379901919849584ITERATION : 20, loss : 0.02438742100442901ITERATION : 21, loss : 0.024392730922295724ITERATION : 22, loss : 0.024396480772206512ITERATION : 23, loss : 0.02439912894694438ITERATION : 24, loss : 0.024400999161584787ITERATION : 25, loss : 0.02440232001622562ITERATION : 26, loss : 0.024403252937983066ITERATION : 27, loss : 0.02440391191330599ITERATION : 28, loss : 0.02440437739113735ITERATION : 29, loss : 0.024404706219893425ITERATION : 30, loss : 0.024404938518903622ITERATION : 31, loss : 0.024405102658661762ITERATION : 32, loss : 0.024405218619950833ITERATION : 33, loss : 0.024405300557344276ITERATION : 34, loss : 0.024405358473551107ITERATION : 35, loss : 0.024405399397624653ITERATION : 36, loss : 0.024405428323954023ITERATION : 37, loss : 0.024405448783455186ITERATION : 38, loss : 0.02440546323407742ITERATION : 39, loss : 0.024405473462969098ITERATION : 40, loss : 0.02440548068165302ITERATION : 41, loss : 0.024405485778307725ITERATION : 42, loss : 0.024405489354332347ITERATION : 43, loss : 0.024405491898736ITERATION : 44, loss : 0.024405493694700565ITERATION : 45, loss : 0.02440549495334114ITERATION : 46, loss : 0.02440549585149399ITERATION : 47, loss : 0.02440549649633655ITERATION : 48, loss : 0.024405496917357022ITERATION : 49, loss : 0.02440549714718535ITERATION : 50, loss : 0.024405497399137572ITERATION : 51, loss : 0.024405497570583846ITERATION : 52, loss : 0.024405497656866067ITERATION : 53, loss : 0.02440549775483772ITERATION : 54, loss : 0.0244054977596327ITERATION : 55, loss : 0.02440549782259778ITERATION : 56, loss : 0.024405497823246438ITERATION : 57, loss : 0.024405497823246438ITERATION : 58, loss : 0.024405497823246438ITERATION : 59, loss : 0.024405497823246438ITERATION : 60, loss : 0.024405497823246438ITERATION : 61, loss : 0.024405497823246438ITERATION : 62, loss : 0.024405497823246438ITERATION : 63, loss : 0.024405497823246438ITERATION : 64, loss : 0.024405497823246438ITERATION : 65, loss : 0.024405497823246438ITERATION : 66, loss : 0.024405497823246438ITERATION : 67, loss : 0.024405497823246438ITERATION : 68, loss : 0.024405497823246438ITERATION : 69, loss : 0.024405497823246438ITERATION : 70, loss : 0.024405497823246438ITERATION : 71, loss : 0.024405497823246438ITERATION : 72, loss : 0.024405497823246438ITERATION : 73, loss : 0.024405497823246438ITERATION : 74, loss : 0.024405497823246438ITERATION : 75, loss : 0.024405497823246438ITERATION : 76, loss : 0.024405497823246438ITERATION : 77, loss : 0.024405497823246438ITERATION : 78, loss : 0.024405497823246438ITERATION : 79, loss : 0.024405497823246438ITERATION : 80, loss : 0.024405497823246438ITERATION : 81, loss : 0.024405497823246438ITERATION : 82, loss : 0.024405497823246438ITERATION : 83, loss : 0.024405497823246438ITERATION : 84, loss : 0.024405497823246438ITERATION : 85, loss : 0.024405497823246438ITERATION : 86, loss : 0.024405497823246438ITERATION : 87, loss : 0.024405497823246438ITERATION : 88, loss : 0.024405497823246438ITERATION : 89, loss : 0.024405497823246438ITERATION : 90, loss : 0.024405497823246438ITERATION : 91, loss : 0.024405497823246438ITERATION : 92, loss : 0.024405497823246438ITERATION : 93, loss : 0.024405497823246438ITERATION : 94, loss : 0.024405497823246438ITERATION : 95, loss : 0.024405497823246438ITERATION : 96, loss : 0.024405497823246438ITERATION : 97, loss : 0.024405497823246438ITERATION : 98, loss : 0.024405497823246438ITERATION : 99, loss : 0.024405497823246438ITERATION : 100, loss : 0.024405497823246438
ITERATION : 1, loss : 0.06860869371908873ITERATION : 2, loss : 0.08491552675507405ITERATION : 3, loss : 0.09632545382064432ITERATION : 4, loss : 0.10266692507733786ITERATION : 5, loss : 0.10553193421237088ITERATION : 6, loss : 0.10759716656952213ITERATION : 7, loss : 0.10908174177106554ITERATION : 8, loss : 0.1101455087199625ITERATION : 9, loss : 0.11090777999614027ITERATION : 10, loss : 0.11145351836871911ITERATION : 11, loss : 0.11184375848804312ITERATION : 12, loss : 0.11212244545998855ITERATION : 13, loss : 0.11232121175283005ITERATION : 14, loss : 0.11246279964144094ITERATION : 15, loss : 0.11256353558952444ITERATION : 16, loss : 0.11263512221316864ITERATION : 17, loss : 0.11268593563176374ITERATION : 18, loss : 0.1127219626195255ITERATION : 19, loss : 0.1127474766423866ITERATION : 20, loss : 0.11276552447779666ITERATION : 21, loss : 0.11277827585024398ITERATION : 22, loss : 0.11278727403904308ITERATION : 23, loss : 0.11279361566616967ITERATION : 24, loss : 0.11279807905044177ITERATION : 25, loss : 0.11280121606688917ITERATION : 26, loss : 0.11280341759845586ITERATION : 27, loss : 0.11280496011918965ITERATION : 28, loss : 0.11280603912173992ITERATION : 29, loss : 0.11280679246632869ITERATION : 30, loss : 0.11280731742120564ITERATION : 31, loss : 0.11280768244339885ITERATION : 32, loss : 0.11280793563319444ITERATION : 33, loss : 0.1128081108321375ITERATION : 34, loss : 0.11280823169779089ITERATION : 35, loss : 0.11280831487687581ITERATION : 36, loss : 0.1128083718634899ITERATION : 37, loss : 0.11280841081878419ITERATION : 38, loss : 0.11280843723735035ITERATION : 39, loss : 0.11280845515936715ITERATION : 40, loss : 0.1128084671386282ITERATION : 41, loss : 0.11280847513748747ITERATION : 42, loss : 0.11280848046539359ITERATION : 43, loss : 0.11280848395578605ITERATION : 44, loss : 0.11280848625159373ITERATION : 45, loss : 0.11280848763900342ITERATION : 46, loss : 0.11280848861222267ITERATION : 47, loss : 0.11280848918256645ITERATION : 48, loss : 0.11280848949295491ITERATION : 49, loss : 0.11280848964202447ITERATION : 50, loss : 0.11280848970020751ITERATION : 51, loss : 0.11280848969615391ITERATION : 52, loss : 0.1128084896716561ITERATION : 53, loss : 0.11280848962833889ITERATION : 54, loss : 0.11280848958903984ITERATION : 55, loss : 0.11280848954047985ITERATION : 56, loss : 0.11280848950488748ITERATION : 57, loss : 0.11280848946486345ITERATION : 58, loss : 0.11280848944029077ITERATION : 59, loss : 0.11280848941692556ITERATION : 60, loss : 0.1128084893965662ITERATION : 61, loss : 0.11280848938151386ITERATION : 62, loss : 0.11280848937780014ITERATION : 63, loss : 0.11280848937755679ITERATION : 64, loss : 0.11280848937755679ITERATION : 65, loss : 0.11280848937755679ITERATION : 66, loss : 0.11280848937755679ITERATION : 67, loss : 0.11280848937755679ITERATION : 68, loss : 0.11280848937755679ITERATION : 69, loss : 0.11280848937755679ITERATION : 70, loss : 0.11280848937755679ITERATION : 71, loss : 0.11280848937755679ITERATION : 72, loss : 0.11280848937755679ITERATION : 73, loss : 0.11280848937755679ITERATION : 74, loss : 0.11280848937755679ITERATION : 75, loss : 0.11280848937755679ITERATION : 76, loss : 0.11280848937755679ITERATION : 77, loss : 0.11280848937755679ITERATION : 78, loss : 0.11280848937755679ITERATION : 79, loss : 0.11280848937755679ITERATION : 80, loss : 0.11280848937755679ITERATION : 81, loss : 0.11280848937755679ITERATION : 82, loss : 0.11280848937755679ITERATION : 83, loss : 0.11280848937755679ITERATION : 84, loss : 0.11280848937755679ITERATION : 85, loss : 0.11280848937755679ITERATION : 86, loss : 0.11280848937755679ITERATION : 87, loss : 0.11280848937755679ITERATION : 88, loss : 0.11280848937755679ITERATION : 89, loss : 0.11280848937755679ITERATION : 90, loss : 0.11280848937755679ITERATION : 91, loss : 0.11280848937755679ITERATION : 92, loss : 0.11280848937755679ITERATION : 93, loss : 0.11280848937755679ITERATION : 94, loss : 0.11280848937755679ITERATION : 95, loss : 0.11280848937755679ITERATION : 96, loss : 0.11280848937755679ITERATION : 97, loss : 0.11280848937755679ITERATION : 98, loss : 0.11280848937755679ITERATION : 99, loss : 0.11280848937755679ITERATION : 100, loss : 0.11280848937755679
ITERATION : 1, loss : 0.06383834216381346ITERATION : 2, loss : 0.08540942098419411ITERATION : 3, loss : 0.0969548700468433ITERATION : 4, loss : 0.1038018004879085ITERATION : 5, loss : 0.10808412561537295ITERATION : 6, loss : 0.11084997399757712ITERATION : 7, loss : 0.11267356897702785ITERATION : 8, loss : 0.11389221775239725ITERATION : 9, loss : 0.11471383415725343ITERATION : 10, loss : 0.11527099119585778ITERATION : 11, loss : 0.11565024812812275ITERATION : 12, loss : 0.11590905039837203ITERATION : 13, loss : 0.1160859453470665ITERATION : 14, loss : 0.11620698990817976ITERATION : 15, loss : 0.11628988272360345ITERATION : 16, loss : 0.11634668285584975ITERATION : 17, loss : 0.11638562314285385ITERATION : 18, loss : 0.11641233151988356ITERATION : 19, loss : 0.11643065844548886ITERATION : 20, loss : 0.11644323985935609ITERATION : 21, loss : 0.11645188112526213ITERATION : 22, loss : 0.11645781913592934ITERATION : 23, loss : 0.11646190176166292ITERATION : 24, loss : 0.11646471024904145ITERATION : 25, loss : 0.1164666434150107ITERATION : 26, loss : 0.11646797481716874ITERATION : 27, loss : 0.11646889234449781ITERATION : 28, loss : 0.11646952509922896ITERATION : 29, loss : 0.11646996174264236ITERATION : 30, loss : 0.11647026323390773ITERATION : 31, loss : 0.11647047158088139ITERATION : 32, loss : 0.11647061563449665ITERATION : 33, loss : 0.11647071530968262ITERATION : 34, loss : 0.11647078431370697ITERATION : 35, loss : 0.11647083213088866ITERATION : 36, loss : 0.11647086529673185ITERATION : 37, loss : 0.1164708883161786ITERATION : 38, loss : 0.11647090431641682ITERATION : 39, loss : 0.11647091542072202ITERATION : 40, loss : 0.11647092315464283ITERATION : 41, loss : 0.1164709285315536ITERATION : 42, loss : 0.11647093227171458ITERATION : 43, loss : 0.11647093488916571ITERATION : 44, loss : 0.11647093674386479ITERATION : 45, loss : 0.11647093800625243ITERATION : 46, loss : 0.11647093890134635ITERATION : 47, loss : 0.11647093949930608ITERATION : 48, loss : 0.116470939932083ITERATION : 49, loss : 0.11647094022942614ITERATION : 50, loss : 0.11647094042964345ITERATION : 51, loss : 0.11647094057755385ITERATION : 52, loss : 0.11647094065545752ITERATION : 53, loss : 0.11647094072915007ITERATION : 54, loss : 0.11647094075815553ITERATION : 55, loss : 0.11647094077703088ITERATION : 56, loss : 0.11647094080621397ITERATION : 57, loss : 0.11647094083099341ITERATION : 58, loss : 0.11647094083332195ITERATION : 59, loss : 0.11647094083332195ITERATION : 60, loss : 0.11647094083332195ITERATION : 61, loss : 0.11647094083332195ITERATION : 62, loss : 0.11647094083332195ITERATION : 63, loss : 0.11647094083332195ITERATION : 64, loss : 0.11647094083332195ITERATION : 65, loss : 0.11647094083332195ITERATION : 66, loss : 0.11647094083332195ITERATION : 67, loss : 0.11647094083332195ITERATION : 68, loss : 0.11647094083332195ITERATION : 69, loss : 0.11647094083332195ITERATION : 70, loss : 0.11647094083332195ITERATION : 71, loss : 0.11647094083332195ITERATION : 72, loss : 0.11647094083332195ITERATION : 73, loss : 0.11647094083332195ITERATION : 74, loss : 0.11647094083332195ITERATION : 75, loss : 0.11647094083332195ITERATION : 76, loss : 0.11647094083332195ITERATION : 77, loss : 0.11647094083332195ITERATION : 78, loss : 0.11647094083332195ITERATION : 79, loss : 0.11647094083332195ITERATION : 80, loss : 0.11647094083332195ITERATION : 81, loss : 0.11647094083332195ITERATION : 82, loss : 0.11647094083332195ITERATION : 83, loss : 0.11647094083332195ITERATION : 84, loss : 0.11647094083332195ITERATION : 85, loss : 0.11647094083332195ITERATION : 86, loss : 0.11647094083332195ITERATION : 87, loss : 0.11647094083332195ITERATION : 88, loss : 0.11647094083332195ITERATION : 89, loss : 0.11647094083332195ITERATION : 90, loss : 0.11647094083332195ITERATION : 91, loss : 0.11647094083332195ITERATION : 92, loss : 0.11647094083332195ITERATION : 93, loss : 0.11647094083332195ITERATION : 94, loss : 0.11647094083332195ITERATION : 95, loss : 0.11647094083332195ITERATION : 96, loss : 0.11647094083332195ITERATION : 97, loss : 0.11647094083332195ITERATION : 98, loss : 0.11647094083332195ITERATION : 99, loss : 0.11647094083332195ITERATION : 100, loss : 0.11647094083332195
ITERATION : 1, loss : 0.045533174109576705ITERATION : 2, loss : 0.057395871021175354ITERATION : 3, loss : 0.06523008813273709ITERATION : 4, loss : 0.0703253804107168ITERATION : 5, loss : 0.07376174989051648ITERATION : 6, loss : 0.07614833660903525ITERATION : 7, loss : 0.07783637452028856ITERATION : 8, loss : 0.07904301271821298ITERATION : 9, loss : 0.07991074105251482ITERATION : 10, loss : 0.08053688766997648ITERATION : 11, loss : 0.08098958831230403ITERATION : 12, loss : 0.08131724245495682ITERATION : 13, loss : 0.08155452568001863ITERATION : 14, loss : 0.08172640743462008ITERATION : 15, loss : 0.08185092193435624ITERATION : 16, loss : 0.0819411172046812ITERATION : 17, loss : 0.08200644349506746ITERATION : 18, loss : 0.08205374884088726ITERATION : 19, loss : 0.08208799698143687ITERATION : 20, loss : 0.08211278598770275ITERATION : 21, loss : 0.08213072384178198ITERATION : 22, loss : 0.0821437006309766ITERATION : 23, loss : 0.08215308588251531ITERATION : 24, loss : 0.08215987178158683ITERATION : 25, loss : 0.082164776872159ITERATION : 26, loss : 0.08216832146426228ITERATION : 27, loss : 0.082170882243967ITERATION : 28, loss : 0.0821727316388769ITERATION : 29, loss : 0.08217406692758976ITERATION : 30, loss : 0.08217503074119213ITERATION : 31, loss : 0.08217572622457313ITERATION : 32, loss : 0.08217622789200024ITERATION : 33, loss : 0.08217658976393633ITERATION : 34, loss : 0.08217685057316494ITERATION : 35, loss : 0.08217703858439361ITERATION : 36, loss : 0.08217717405240071ITERATION : 37, loss : 0.08217727160607648ITERATION : 38, loss : 0.08217734188003481ITERATION : 39, loss : 0.08217739241544714ITERATION : 40, loss : 0.08217742875126406ITERATION : 41, loss : 0.08217745491497537ITERATION : 42, loss : 0.08217747374781108ITERATION : 43, loss : 0.08217748726803105ITERATION : 44, loss : 0.08217749700442135ITERATION : 45, loss : 0.08217750398262318ITERATION : 46, loss : 0.08217750902315026ITERATION : 47, loss : 0.08217751261236478ITERATION : 48, loss : 0.08217751518948847ITERATION : 49, loss : 0.08217751702506941ITERATION : 50, loss : 0.0821775183247416ITERATION : 51, loss : 0.08217751925462122ITERATION : 52, loss : 0.08217751991565143ITERATION : 53, loss : 0.08217752039508304ITERATION : 54, loss : 0.08217752072554198ITERATION : 55, loss : 0.08217752095971627ITERATION : 56, loss : 0.08217752113608226ITERATION : 57, loss : 0.08217752122177961ITERATION : 58, loss : 0.08217752130139654ITERATION : 59, loss : 0.0821775213313309ITERATION : 60, loss : 0.08217752139577608ITERATION : 61, loss : 0.08217752140904765ITERATION : 62, loss : 0.08217752144660545ITERATION : 63, loss : 0.08217752144577005ITERATION : 64, loss : 0.08217752144577005ITERATION : 65, loss : 0.08217752144577005ITERATION : 66, loss : 0.08217752144577005ITERATION : 67, loss : 0.08217752144577005ITERATION : 68, loss : 0.08217752144577005ITERATION : 69, loss : 0.08217752144577005ITERATION : 70, loss : 0.08217752144577005ITERATION : 71, loss : 0.08217752144577005ITERATION : 72, loss : 0.08217752144577005ITERATION : 73, loss : 0.08217752144577005ITERATION : 74, loss : 0.08217752144577005ITERATION : 75, loss : 0.08217752144577005ITERATION : 76, loss : 0.08217752144577005ITERATION : 77, loss : 0.08217752144577005ITERATION : 78, loss : 0.08217752144577005ITERATION : 79, loss : 0.08217752144577005ITERATION : 80, loss : 0.08217752144577005ITERATION : 81, loss : 0.08217752144577005ITERATION : 82, loss : 0.08217752144577005ITERATION : 83, loss : 0.08217752144577005ITERATION : 84, loss : 0.08217752144577005ITERATION : 85, loss : 0.08217752144577005ITERATION : 86, loss : 0.08217752144577005ITERATION : 87, loss : 0.08217752144577005ITERATION : 88, loss : 0.08217752144577005ITERATION : 89, loss : 0.08217752144577005ITERATION : 90, loss : 0.08217752144577005ITERATION : 91, loss : 0.08217752144577005ITERATION : 92, loss : 0.08217752144577005ITERATION : 93, loss : 0.08217752144577005ITERATION : 94, loss : 0.08217752144577005ITERATION : 95, loss : 0.08217752144577005ITERATION : 96, loss : 0.08217752144577005ITERATION : 97, loss : 0.08217752144577005ITERATION : 98, loss : 0.08217752144577005ITERATION : 99, loss : 0.08217752144577005ITERATION : 100, loss : 0.08217752144577005
gradient norm in None layer : 0.0013124619120280677
gradient norm in None layer : 5.542506395776022e-05
gradient norm in None layer : 5.40109670839088e-05
gradient norm in None layer : 0.0008340138549879478
gradient norm in None layer : 5.558770835234713e-05
gradient norm in None layer : 5.282450481381807e-05
gradient norm in None layer : 0.0005305740223042215
gradient norm in None layer : 2.259202044580583e-05
gradient norm in None layer : 2.2254327081541428e-05
gradient norm in None layer : 0.0004929647173484168
gradient norm in None layer : 2.4170188343973607e-05
gradient norm in None layer : 2.552704282266828e-05
gradient norm in None layer : 0.00018255313245380008
gradient norm in None layer : 5.04039410251064e-06
gradient norm in None layer : 4.525692064751088e-06
gradient norm in None layer : 0.00016655581803843076
gradient norm in None layer : 6.394875065134186e-06
gradient norm in None layer : 5.895754295628159e-06
gradient norm in None layer : 0.0002366018758329545
gradient norm in None layer : 2.6355915216248726e-06
gradient norm in None layer : 0.0005158218922853539
gradient norm in None layer : 2.994116737586494e-05
gradient norm in None layer : 2.4712828012243402e-05
gradient norm in None layer : 0.0005510520779565396
gradient norm in None layer : 4.1446434673440073e-05
gradient norm in None layer : 4.772814886749139e-05
gradient norm in None layer : 0.0010080388689957503
gradient norm in None layer : 1.0603833929891013e-05
gradient norm in None layer : 0.0009381394642255206
gradient norm in None layer : 8.233589129025847e-05
gradient norm in None layer : 6.970447221265115e-05
gradient norm in None layer : 0.0011024107451606086
gradient norm in None layer : 0.00011514531105118027
gradient norm in None layer : 0.000173192170134378
gradient norm in None layer : 7.51087976704126e-05
gradient norm in None layer : 2.0857817985749912e-05
Total gradient norm: 0.0026118607551016376
invariance loss : 4.695382900764242, avg_den : 0.437164306640625, density loss : 0.3373243005317693, mse loss : 0.0866887685359535, solver time : 115.74819278717041 sec , total loss : 0.09172147573724951, running loss : 0.10867623391153192
saving checkpoint
Epoch 0/10 , batch 11/12500 
ITERATION : 1, loss : 0.033001255273610405ITERATION : 2, loss : 0.03247345868777334ITERATION : 3, loss : 0.03575313885851622ITERATION : 4, loss : 0.03872403007715116ITERATION : 5, loss : 0.040779343308778825ITERATION : 6, loss : 0.04227546523964341ITERATION : 7, loss : 0.043322577232761145ITERATION : 8, loss : 0.044042124486964956ITERATION : 9, loss : 0.04453208606221672ITERATION : 10, loss : 0.04486415187896902ITERATION : 11, loss : 0.04508865288536922ITERATION : 12, loss : 0.04524023630768371ITERATION : 13, loss : 0.0453425170373206ITERATION : 14, loss : 0.04541150760157697ITERATION : 15, loss : 0.04545803583354088ITERATION : 16, loss : 0.04548941320952351ITERATION : 17, loss : 0.0455105730574454ITERATION : 18, loss : 0.04552484282481882ITERATION : 19, loss : 0.04553446628624498ITERATION : 20, loss : 0.04554095653613873ITERATION : 21, loss : 0.0455453338671335ITERATION : 22, loss : 0.045548286223542414ITERATION : 23, loss : 0.04555027752523229ITERATION : 24, loss : 0.045551620711079856ITERATION : 25, loss : 0.045552526749744006ITERATION : 26, loss : 0.04555313791030313ITERATION : 27, loss : 0.04555355020577858ITERATION : 28, loss : 0.04555382833593622ITERATION : 29, loss : 0.04555401594480862ITERATION : 30, loss : 0.04555414253000999ITERATION : 31, loss : 0.04555422791537474ITERATION : 32, loss : 0.04555428554980027ITERATION : 33, loss : 0.045554324415738576ITERATION : 34, loss : 0.04555435066696631ITERATION : 35, loss : 0.04555436837023308ITERATION : 36, loss : 0.04555438029496573ITERATION : 37, loss : 0.04555438834597824ITERATION : 38, loss : 0.04555439375571263ITERATION : 39, loss : 0.04555439741360538ITERATION : 40, loss : 0.045554399856269495ITERATION : 41, loss : 0.04555440152650054ITERATION : 42, loss : 0.0455544026153901ITERATION : 43, loss : 0.04555440336767458ITERATION : 44, loss : 0.04555440386829368ITERATION : 45, loss : 0.045554404210454036ITERATION : 46, loss : 0.04555440444444602ITERATION : 47, loss : 0.045554404611760226ITERATION : 48, loss : 0.045554404705021ITERATION : 49, loss : 0.04555440476960529ITERATION : 50, loss : 0.04555440482952685ITERATION : 51, loss : 0.04555440482983825ITERATION : 52, loss : 0.04555440482983825ITERATION : 53, loss : 0.04555440482983825ITERATION : 54, loss : 0.04555440482983825ITERATION : 55, loss : 0.04555440482983825ITERATION : 56, loss : 0.04555440482983825ITERATION : 57, loss : 0.04555440482983825ITERATION : 58, loss : 0.04555440482983825ITERATION : 59, loss : 0.04555440482983825ITERATION : 60, loss : 0.04555440482983825ITERATION : 61, loss : 0.04555440482983825ITERATION : 62, loss : 0.04555440482983825ITERATION : 63, loss : 0.04555440482983825ITERATION : 64, loss : 0.04555440482983825ITERATION : 65, loss : 0.04555440482983825ITERATION : 66, loss : 0.04555440482983825ITERATION : 67, loss : 0.04555440482983825ITERATION : 68, loss : 0.04555440482983825ITERATION : 69, loss : 0.04555440482983825ITERATION : 70, loss : 0.04555440482983825ITERATION : 71, loss : 0.04555440482983825ITERATION : 72, loss : 0.04555440482983825ITERATION : 73, loss : 0.04555440482983825ITERATION : 74, loss : 0.04555440482983825ITERATION : 75, loss : 0.04555440482983825ITERATION : 76, loss : 0.04555440482983825ITERATION : 77, loss : 0.04555440482983825ITERATION : 78, loss : 0.04555440482983825ITERATION : 79, loss : 0.04555440482983825ITERATION : 80, loss : 0.04555440482983825ITERATION : 81, loss : 0.04555440482983825ITERATION : 82, loss : 0.04555440482983825ITERATION : 83, loss : 0.04555440482983825ITERATION : 84, loss : 0.04555440482983825ITERATION : 85, loss : 0.04555440482983825ITERATION : 86, loss : 0.04555440482983825ITERATION : 87, loss : 0.04555440482983825ITERATION : 88, loss : 0.04555440482983825ITERATION : 89, loss : 0.04555440482983825ITERATION : 90, loss : 0.04555440482983825ITERATION : 91, loss : 0.04555440482983825ITERATION : 92, loss : 0.04555440482983825ITERATION : 93, loss : 0.04555440482983825ITERATION : 94, loss : 0.04555440482983825ITERATION : 95, loss : 0.04555440482983825ITERATION : 96, loss : 0.04555440482983825ITERATION : 97, loss : 0.04555440482983825ITERATION : 98, loss : 0.04555440482983825ITERATION : 99, loss : 0.04555440482983825ITERATION : 100, loss : 0.04555440482983825
ITERATION : 1, loss : 0.030553835039365363ITERATION : 2, loss : 0.027004335866366774ITERATION : 3, loss : 0.029872678677637456ITERATION : 4, loss : 0.034297259429532885ITERATION : 5, loss : 0.037726793315509904ITERATION : 6, loss : 0.04024572898698215ITERATION : 7, loss : 0.04207232185058032ITERATION : 8, loss : 0.0433874439289558ITERATION : 9, loss : 0.044330240938812555ITERATION : 10, loss : 0.04500420082512947ITERATION : 11, loss : 0.04561866556132898ITERATION : 12, loss : 0.04608964402130751ITERATION : 13, loss : 0.04642803604448728ITERATION : 14, loss : 0.04667088316146827ITERATION : 15, loss : 0.046844990541375ITERATION : 16, loss : 0.04696970820059955ITERATION : 17, loss : 0.04705897770449968ITERATION : 18, loss : 0.04712282884201115ITERATION : 19, loss : 0.047168468767806164ITERATION : 20, loss : 0.04720107056508899ITERATION : 21, loss : 0.04722434423525397ITERATION : 22, loss : 0.047240948485809446ITERATION : 23, loss : 0.04725278717832953ITERATION : 24, loss : 0.04726122270674024ITERATION : 25, loss : 0.04726722947155208ITERATION : 26, loss : 0.04727150412500458ITERATION : 27, loss : 0.047274543940853786ITERATION : 28, loss : 0.04727670414750641ITERATION : 29, loss : 0.04727823824380879ITERATION : 30, loss : 0.04727932679360215ITERATION : 31, loss : 0.047280098602850464ITERATION : 32, loss : 0.04728064543530552ITERATION : 33, loss : 0.0472810324819436ITERATION : 34, loss : 0.04728130614833702ITERATION : 35, loss : 0.04728149948236051ITERATION : 36, loss : 0.04728163598080185ITERATION : 37, loss : 0.047281732202356465ITERATION : 38, loss : 0.04728179991221761ITERATION : 39, loss : 0.04728184753980316ITERATION : 40, loss : 0.047281881002092324ITERATION : 41, loss : 0.04728190452801303ITERATION : 42, loss : 0.04728192096210996ITERATION : 43, loss : 0.047281932499719403ITERATION : 44, loss : 0.047281940499498444ITERATION : 45, loss : 0.047281946090662996ITERATION : 46, loss : 0.047281949978499486ITERATION : 47, loss : 0.047281952741583ITERATION : 48, loss : 0.04728195466087752ITERATION : 49, loss : 0.04728195592066704ITERATION : 50, loss : 0.0472819567916775ITERATION : 51, loss : 0.04728195735540534ITERATION : 52, loss : 0.047281957764897915ITERATION : 53, loss : 0.04728195800230224ITERATION : 54, loss : 0.04728195819232116ITERATION : 55, loss : 0.04728195829497074ITERATION : 56, loss : 0.047281958388989326ITERATION : 57, loss : 0.04728195841582606ITERATION : 58, loss : 0.047281958454823206ITERATION : 59, loss : 0.047281958454579595ITERATION : 60, loss : 0.047281958479908154ITERATION : 61, loss : 0.04728195847990055ITERATION : 62, loss : 0.04728195847990055ITERATION : 63, loss : 0.04728195847990055ITERATION : 64, loss : 0.04728195847990055ITERATION : 65, loss : 0.04728195847990055ITERATION : 66, loss : 0.04728195847990055ITERATION : 67, loss : 0.04728195847990055ITERATION : 68, loss : 0.04728195847990055ITERATION : 69, loss : 0.04728195847990055ITERATION : 70, loss : 0.04728195847990055ITERATION : 71, loss : 0.04728195847990055ITERATION : 72, loss : 0.04728195847990055ITERATION : 73, loss : 0.04728195847990055ITERATION : 74, loss : 0.04728195847990055ITERATION : 75, loss : 0.04728195847990055ITERATION : 76, loss : 0.04728195847990055ITERATION : 77, loss : 0.04728195847990055ITERATION : 78, loss : 0.04728195847990055ITERATION : 79, loss : 0.04728195847990055ITERATION : 80, loss : 0.04728195847990055ITERATION : 81, loss : 0.04728195847990055ITERATION : 82, loss : 0.04728195847990055ITERATION : 83, loss : 0.04728195847990055ITERATION : 84, loss : 0.04728195847990055ITERATION : 85, loss : 0.04728195847990055ITERATION : 86, loss : 0.04728195847990055ITERATION : 87, loss : 0.04728195847990055ITERATION : 88, loss : 0.04728195847990055ITERATION : 89, loss : 0.04728195847990055ITERATION : 90, loss : 0.04728195847990055ITERATION : 91, loss : 0.04728195847990055ITERATION : 92, loss : 0.04728195847990055ITERATION : 93, loss : 0.04728195847990055ITERATION : 94, loss : 0.04728195847990055ITERATION : 95, loss : 0.04728195847990055ITERATION : 96, loss : 0.04728195847990055ITERATION : 97, loss : 0.04728195847990055ITERATION : 98, loss : 0.04728195847990055ITERATION : 99, loss : 0.04728195847990055ITERATION : 100, loss : 0.04728195847990055
ITERATION : 1, loss : 0.018204384903320783ITERATION : 2, loss : 0.02260676508916333ITERATION : 3, loss : 0.02869918147397827ITERATION : 4, loss : 0.03367672280699222ITERATION : 5, loss : 0.03733002557891928ITERATION : 6, loss : 0.039918431756662556ITERATION : 7, loss : 0.041726786609528825ITERATION : 8, loss : 0.04299255698728888ITERATION : 9, loss : 0.04387481037461951ITERATION : 10, loss : 0.04448460846488564ITERATION : 11, loss : 0.04490577203436864ITERATION : 12, loss : 0.045196526788948276ITERATION : 13, loss : 0.04539719657067534ITERATION : 14, loss : 0.045535664149776583ITERATION : 15, loss : 0.045631194481646715ITERATION : 16, loss : 0.04569709206526314ITERATION : 17, loss : 0.04574254235919405ITERATION : 18, loss : 0.04577388571336176ITERATION : 19, loss : 0.04579549784180258ITERATION : 20, loss : 0.04581039806369614ITERATION : 21, loss : 0.04582066954478411ITERATION : 22, loss : 0.04582774933283047ITERATION : 23, loss : 0.04583262860245813ITERATION : 24, loss : 0.04583599084388916ITERATION : 25, loss : 0.04583830740255475ITERATION : 26, loss : 0.04583990342305676ITERATION : 27, loss : 0.04584100284101618ITERATION : 28, loss : 0.045841760098680066ITERATION : 29, loss : 0.045842281571511836ITERATION : 30, loss : 0.04584264067096534ITERATION : 31, loss : 0.045842887860670534ITERATION : 32, loss : 0.045843058061545094ITERATION : 33, loss : 0.04584317518749507ITERATION : 34, loss : 0.045843255806364426ITERATION : 35, loss : 0.04584331133371715ITERATION : 36, loss : 0.04584334949592417ITERATION : 37, loss : 0.04584337583486567ITERATION : 38, loss : 0.045843393936550465ITERATION : 39, loss : 0.045843406350382886ITERATION : 40, loss : 0.04584341488243071ITERATION : 41, loss : 0.045843420705451284ITERATION : 42, loss : 0.0458434247165761ITERATION : 43, loss : 0.04584342743879413ITERATION : 44, loss : 0.04584342932291459ITERATION : 45, loss : 0.04584343060011908ITERATION : 46, loss : 0.045843431491684725ITERATION : 47, loss : 0.045843432090581795ITERATION : 48, loss : 0.04584343249859305ITERATION : 49, loss : 0.045843432771447895ITERATION : 50, loss : 0.04584343296248117ITERATION : 51, loss : 0.045843433092037394ITERATION : 52, loss : 0.045843433181763024ITERATION : 53, loss : 0.045843433213940486ITERATION : 54, loss : 0.04584343321988417ITERATION : 55, loss : 0.04584343325875091ITERATION : 56, loss : 0.045843433258703464ITERATION : 57, loss : 0.045843433258703464ITERATION : 58, loss : 0.045843433258703464ITERATION : 59, loss : 0.045843433258703464ITERATION : 60, loss : 0.045843433258703464ITERATION : 61, loss : 0.045843433258703464ITERATION : 62, loss : 0.045843433258703464ITERATION : 63, loss : 0.045843433258703464ITERATION : 64, loss : 0.045843433258703464ITERATION : 65, loss : 0.045843433258703464ITERATION : 66, loss : 0.045843433258703464ITERATION : 67, loss : 0.045843433258703464ITERATION : 68, loss : 0.045843433258703464ITERATION : 69, loss : 0.045843433258703464ITERATION : 70, loss : 0.045843433258703464ITERATION : 71, loss : 0.045843433258703464ITERATION : 72, loss : 0.045843433258703464ITERATION : 73, loss : 0.045843433258703464ITERATION : 74, loss : 0.045843433258703464ITERATION : 75, loss : 0.045843433258703464ITERATION : 76, loss : 0.045843433258703464ITERATION : 77, loss : 0.045843433258703464ITERATION : 78, loss : 0.045843433258703464ITERATION : 79, loss : 0.045843433258703464ITERATION : 80, loss : 0.045843433258703464ITERATION : 81, loss : 0.045843433258703464ITERATION : 82, loss : 0.045843433258703464ITERATION : 83, loss : 0.045843433258703464ITERATION : 84, loss : 0.045843433258703464ITERATION : 85, loss : 0.045843433258703464ITERATION : 86, loss : 0.045843433258703464ITERATION : 87, loss : 0.045843433258703464ITERATION : 88, loss : 0.045843433258703464ITERATION : 89, loss : 0.045843433258703464ITERATION : 90, loss : 0.045843433258703464ITERATION : 91, loss : 0.045843433258703464ITERATION : 92, loss : 0.045843433258703464ITERATION : 93, loss : 0.045843433258703464ITERATION : 94, loss : 0.045843433258703464ITERATION : 95, loss : 0.045843433258703464ITERATION : 96, loss : 0.045843433258703464ITERATION : 97, loss : 0.045843433258703464ITERATION : 98, loss : 0.045843433258703464ITERATION : 99, loss : 0.045843433258703464ITERATION : 100, loss : 0.045843433258703464
ITERATION : 1, loss : 0.012677089367099978ITERATION : 2, loss : 0.0099038297854627ITERATION : 3, loss : 0.013142489458038845ITERATION : 4, loss : 0.015956302528397335ITERATION : 5, loss : 0.01818107387089611ITERATION : 6, loss : 0.019872183835442907ITERATION : 7, loss : 0.021131632767508297ITERATION : 8, loss : 0.02205847032902531ITERATION : 9, loss : 0.0227354833221018ITERATION : 10, loss : 0.023227650335726473ITERATION : 11, loss : 0.023584326756862475ITERATION : 12, loss : 0.02384228988704307ITERATION : 13, loss : 0.024028619505259877ITERATION : 14, loss : 0.024163102044120154ITERATION : 15, loss : 0.024260121896542853ITERATION : 16, loss : 0.024330101527915066ITERATION : 17, loss : 0.024380575959698045ITERATION : 18, loss : 0.02441698502441907ITERATION : 19, loss : 0.024443252695533605ITERATION : 20, loss : 0.024462208028861655ITERATION : 21, loss : 0.024475890127368085ITERATION : 22, loss : 0.02448576860400012ITERATION : 23, loss : 0.024492903041674972ITERATION : 24, loss : 0.024498057168661193ITERATION : 25, loss : 0.02450178177685647ITERATION : 26, loss : 0.024504474118757794ITERATION : 27, loss : 0.02450642086138471ITERATION : 28, loss : 0.024507828838552105ITERATION : 29, loss : 0.0245088474714618ITERATION : 30, loss : 0.024509584585901533ITERATION : 31, loss : 0.02451011814807832ITERATION : 32, loss : 0.024510504487290072ITERATION : 33, loss : 0.024510784274862068ITERATION : 34, loss : 0.02451098693682582ITERATION : 35, loss : 0.024511133774691128ITERATION : 36, loss : 0.02451124019014142ITERATION : 37, loss : 0.024511317301866602ITERATION : 38, loss : 0.02451137321094166ITERATION : 39, loss : 0.02451141376676217ITERATION : 40, loss : 0.02451144316440353ITERATION : 41, loss : 0.024511464507186477ITERATION : 42, loss : 0.02451147997911203ITERATION : 43, loss : 0.02451149119346094ITERATION : 44, loss : 0.024511499329605505ITERATION : 45, loss : 0.02451150528376215ITERATION : 46, loss : 0.024511509545590667ITERATION : 47, loss : 0.024511512617034045ITERATION : 48, loss : 0.0245115148541558ITERATION : 49, loss : 0.0245115164609998ITERATION : 50, loss : 0.024511517649197585ITERATION : 51, loss : 0.024511518519618206ITERATION : 52, loss : 0.024511519146826095ITERATION : 53, loss : 0.02451151960559592ITERATION : 54, loss : 0.024511519913108615ITERATION : 55, loss : 0.02451152014646603ITERATION : 56, loss : 0.024511520324899772ITERATION : 57, loss : 0.02451152041845902ITERATION : 58, loss : 0.0245115205166399ITERATION : 59, loss : 0.024511520574594114ITERATION : 60, loss : 0.024511520576138878ITERATION : 61, loss : 0.024511520576138878ITERATION : 62, loss : 0.024511520576138878ITERATION : 63, loss : 0.024511520576138878ITERATION : 64, loss : 0.024511520576138878ITERATION : 65, loss : 0.024511520576138878ITERATION : 66, loss : 0.024511520576138878ITERATION : 67, loss : 0.024511520576138878ITERATION : 68, loss : 0.024511520576138878ITERATION : 69, loss : 0.024511520576138878ITERATION : 70, loss : 0.024511520576138878ITERATION : 71, loss : 0.024511520576138878ITERATION : 72, loss : 0.024511520576138878ITERATION : 73, loss : 0.024511520576138878ITERATION : 74, loss : 0.024511520576138878ITERATION : 75, loss : 0.024511520576138878ITERATION : 76, loss : 0.024511520576138878ITERATION : 77, loss : 0.024511520576138878ITERATION : 78, loss : 0.024511520576138878ITERATION : 79, loss : 0.024511520576138878ITERATION : 80, loss : 0.024511520576138878ITERATION : 81, loss : 0.024511520576138878ITERATION : 82, loss : 0.024511520576138878ITERATION : 83, loss : 0.024511520576138878ITERATION : 84, loss : 0.024511520576138878ITERATION : 85, loss : 0.024511520576138878ITERATION : 86, loss : 0.024511520576138878ITERATION : 87, loss : 0.024511520576138878ITERATION : 88, loss : 0.024511520576138878ITERATION : 89, loss : 0.024511520576138878ITERATION : 90, loss : 0.024511520576138878ITERATION : 91, loss : 0.024511520576138878ITERATION : 92, loss : 0.024511520576138878ITERATION : 93, loss : 0.024511520576138878ITERATION : 94, loss : 0.024511520576138878ITERATION : 95, loss : 0.024511520576138878ITERATION : 96, loss : 0.024511520576138878ITERATION : 97, loss : 0.024511520576138878ITERATION : 98, loss : 0.024511520576138878ITERATION : 99, loss : 0.024511520576138878ITERATION : 100, loss : 0.024511520576138878
ITERATION : 1, loss : 0.056319297934902916ITERATION : 2, loss : 0.07106682617374808ITERATION : 3, loss : 0.08509705793091372ITERATION : 4, loss : 0.09776842522539957ITERATION : 5, loss : 0.10722037859288654ITERATION : 6, loss : 0.1142049093082145ITERATION : 7, loss : 0.11934116269538515ITERATION : 8, loss : 0.12310910378365417ITERATION : 9, loss : 0.12587006782307164ITERATION : 10, loss : 0.12789214601667998ITERATION : 11, loss : 0.12937281503372366ITERATION : 12, loss : 0.13045702367094403ITERATION : 13, loss : 0.13125098359562024ITERATION : 14, loss : 0.13183246138046578ITERATION : 15, loss : 0.13225837857280567ITERATION : 16, loss : 0.13257039584836638ITERATION : 17, loss : 0.13279900548347043ITERATION : 18, loss : 0.13296652783129087ITERATION : 19, loss : 0.1330893034087203ITERATION : 20, loss : 0.133179296808775ITERATION : 21, loss : 0.13324526982238552ITERATION : 22, loss : 0.13329364009832448ITERATION : 23, loss : 0.1333291083479114ITERATION : 24, loss : 0.13335511924008336ITERATION : 25, loss : 0.13337419659198388ITERATION : 26, loss : 0.13338819004515068ITERATION : 27, loss : 0.1333984553252777ITERATION : 28, loss : 0.13340598653690156ITERATION : 29, loss : 0.13341151240240898ITERATION : 30, loss : 0.13341556719521566ITERATION : 31, loss : 0.133418542792929ITERATION : 32, loss : 0.1334207265964447ITERATION : 33, loss : 0.13342232937938608ITERATION : 34, loss : 0.13342350581505188ITERATION : 35, loss : 0.13342436938028984ITERATION : 36, loss : 0.13342500330330953ITERATION : 37, loss : 0.1334254686988452ITERATION : 38, loss : 0.1334258103665874ITERATION : 39, loss : 0.13342606119871395ITERATION : 40, loss : 0.13342624538696266ITERATION : 41, loss : 0.13342638066131124ITERATION : 42, loss : 0.1334264800085476ITERATION : 43, loss : 0.13342655294903383ITERATION : 44, loss : 0.13342660647474217ITERATION : 45, loss : 0.1334266457995762ITERATION : 46, loss : 0.13342667465674496ITERATION : 47, loss : 0.1334266958591696ITERATION : 48, loss : 0.1334267114215679ITERATION : 49, loss : 0.13342672284474183ITERATION : 50, loss : 0.13342673120796733ITERATION : 51, loss : 0.133426737373118ITERATION : 52, loss : 0.13342674190010934ITERATION : 53, loss : 0.13342674515271627ITERATION : 54, loss : 0.13342674753296563ITERATION : 55, loss : 0.1334267493171733ITERATION : 56, loss : 0.13342675063670908ITERATION : 57, loss : 0.13342675165518317ITERATION : 58, loss : 0.13342675238024612ITERATION : 59, loss : 0.13342675291733158ITERATION : 60, loss : 0.1334267532861409ITERATION : 61, loss : 0.1334267535846863ITERATION : 62, loss : 0.1334267537511972ITERATION : 63, loss : 0.13342675375552426ITERATION : 64, loss : 0.1334267538498868ITERATION : 65, loss : 0.13342675388555436ITERATION : 66, loss : 0.1334267538965391ITERATION : 67, loss : 0.1334267538965391ITERATION : 68, loss : 0.1334267538965391ITERATION : 69, loss : 0.1334267538965391ITERATION : 70, loss : 0.1334267538965391ITERATION : 71, loss : 0.1334267538965391ITERATION : 72, loss : 0.1334267538965391ITERATION : 73, loss : 0.1334267538965391ITERATION : 74, loss : 0.1334267538965391ITERATION : 75, loss : 0.1334267538965391ITERATION : 76, loss : 0.1334267538965391ITERATION : 77, loss : 0.1334267538965391ITERATION : 78, loss : 0.1334267538965391ITERATION : 79, loss : 0.1334267538965391ITERATION : 80, loss : 0.1334267538965391ITERATION : 81, loss : 0.1334267538965391ITERATION : 82, loss : 0.1334267538965391ITERATION : 83, loss : 0.1334267538965391ITERATION : 84, loss : 0.1334267538965391ITERATION : 85, loss : 0.1334267538965391ITERATION : 86, loss : 0.1334267538965391ITERATION : 87, loss : 0.1334267538965391ITERATION : 88, loss : 0.1334267538965391ITERATION : 89, loss : 0.1334267538965391ITERATION : 90, loss : 0.1334267538965391ITERATION : 91, loss : 0.1334267538965391ITERATION : 92, loss : 0.1334267538965391ITERATION : 93, loss : 0.1334267538965391ITERATION : 94, loss : 0.1334267538965391ITERATION : 95, loss : 0.1334267538965391ITERATION : 96, loss : 0.1334267538965391ITERATION : 97, loss : 0.1334267538965391ITERATION : 98, loss : 0.1334267538965391ITERATION : 99, loss : 0.1334267538965391ITERATION : 100, loss : 0.1334267538965391
ITERATION : 1, loss : 0.04706007363762924ITERATION : 2, loss : 0.04472406579755326ITERATION : 3, loss : 0.04610958422819961ITERATION : 4, loss : 0.04858461577967593ITERATION : 5, loss : 0.050997463567844493ITERATION : 6, loss : 0.05296054748384973ITERATION : 7, loss : 0.05359932366965081ITERATION : 8, loss : 0.053642480356823036ITERATION : 9, loss : 0.05366507191725902ITERATION : 10, loss : 0.05367198433845845ITERATION : 11, loss : 0.053669000654889956ITERATION : 12, loss : 0.053660726959126485ITERATION : 13, loss : 0.0536502861641339ITERATION : 14, loss : 0.053639573807998324ITERATION : 15, loss : 0.053629618756796144ITERATION : 16, loss : 0.053620890639812006ITERATION : 17, loss : 0.05361352508865387ITERATION : 18, loss : 0.05360747400892758ITERATION : 19, loss : 0.05360260020433794ITERATION : 20, loss : 0.05359873334699169ITERATION : 21, loss : 0.05359570133191299ITERATION : 22, loss : 0.05359334652903247ITERATION : 23, loss : 0.05359153150152916ITERATION : 24, loss : 0.05359014159294576ITERATION : 25, loss : 0.05358908294157652ITERATION : 26, loss : 0.05358828020263062ITERATION : 27, loss : 0.05358767392759355ITERATION : 28, loss : 0.05358721755456378ITERATION : 29, loss : 0.05358687490926744ITERATION : 30, loss : 0.05358661851129043ITERATION : 31, loss : 0.05358642704857176ITERATION : 32, loss : 0.053586284383523274ITERATION : 33, loss : 0.053586178205966596ITERATION : 34, loss : 0.05358609928139767ITERATION : 35, loss : 0.05358604072722312ITERATION : 36, loss : 0.05358599733133347ITERATION : 37, loss : 0.05358596519768519ITERATION : 38, loss : 0.05358594143707302ITERATION : 39, loss : 0.053585923917671724ITERATION : 40, loss : 0.053585910980328395ITERATION : 41, loss : 0.05358590142512524ITERATION : 42, loss : 0.05358589437960577ITERATION : 43, loss : 0.05358588919480332ITERATION : 44, loss : 0.05358588537629612ITERATION : 45, loss : 0.05358588264277677ITERATION : 46, loss : 0.053585880577448404ITERATION : 47, loss : 0.05358587884696128ITERATION : 48, loss : 0.05358587778272049ITERATION : 49, loss : 0.05358587698290551ITERATION : 50, loss : 0.053585876411968256ITERATION : 51, loss : 0.05358587604528667ITERATION : 52, loss : 0.0535858757521304ITERATION : 53, loss : 0.05358587555215544ITERATION : 54, loss : 0.05358587546932394ITERATION : 55, loss : 0.05358587538201698ITERATION : 56, loss : 0.05358587532823644ITERATION : 57, loss : 0.05358587532301419ITERATION : 58, loss : 0.05358587532301419ITERATION : 59, loss : 0.05358587532301419ITERATION : 60, loss : 0.05358587532301419ITERATION : 61, loss : 0.05358587532301419ITERATION : 62, loss : 0.05358587532301419ITERATION : 63, loss : 0.05358587532301419ITERATION : 64, loss : 0.05358587532301419ITERATION : 65, loss : 0.05358587532301419ITERATION : 66, loss : 0.05358587532301419ITERATION : 67, loss : 0.05358587532301419ITERATION : 68, loss : 0.05358587532301419ITERATION : 69, loss : 0.05358587532301419ITERATION : 70, loss : 0.05358587532301419ITERATION : 71, loss : 0.05358587532301419ITERATION : 72, loss : 0.05358587532301419ITERATION : 73, loss : 0.05358587532301419ITERATION : 74, loss : 0.05358587532301419ITERATION : 75, loss : 0.05358587532301419ITERATION : 76, loss : 0.05358587532301419ITERATION : 77, loss : 0.05358587532301419ITERATION : 78, loss : 0.05358587532301419ITERATION : 79, loss : 0.05358587532301419ITERATION : 80, loss : 0.05358587532301419ITERATION : 81, loss : 0.05358587532301419ITERATION : 82, loss : 0.05358587532301419ITERATION : 83, loss : 0.05358587532301419ITERATION : 84, loss : 0.05358587532301419ITERATION : 85, loss : 0.05358587532301419ITERATION : 86, loss : 0.05358587532301419ITERATION : 87, loss : 0.05358587532301419ITERATION : 88, loss : 0.05358587532301419ITERATION : 89, loss : 0.05358587532301419ITERATION : 90, loss : 0.05358587532301419ITERATION : 91, loss : 0.05358587532301419ITERATION : 92, loss : 0.05358587532301419ITERATION : 93, loss : 0.05358587532301419ITERATION : 94, loss : 0.05358587532301419ITERATION : 95, loss : 0.05358587532301419ITERATION : 96, loss : 0.05358587532301419ITERATION : 97, loss : 0.05358587532301419ITERATION : 98, loss : 0.05358587532301419ITERATION : 99, loss : 0.05358587532301419ITERATION : 100, loss : 0.05358587532301419
ITERATION : 1, loss : 0.04975209444572248ITERATION : 2, loss : 0.07218643463395395ITERATION : 3, loss : 0.08569151614285025ITERATION : 4, loss : 0.09437091278249525ITERATION : 5, loss : 0.10002528962973033ITERATION : 6, loss : 0.10376251977327086ITERATION : 7, loss : 0.10622608028613739ITERATION : 8, loss : 0.10787045088653144ITERATION : 9, loss : 0.10899141036061497ITERATION : 10, loss : 0.10976052464426178ITERATION : 11, loss : 0.11029091848917884ITERATION : 12, loss : 0.1106581468387712ITERATION : 13, loss : 0.11091320249433483ITERATION : 14, loss : 0.11109078944613214ITERATION : 15, loss : 0.1112146830525242ITERATION : 16, loss : 0.11130125637790135ITERATION : 17, loss : 0.11136183099756536ITERATION : 18, loss : 0.11140426128167433ITERATION : 19, loss : 0.1114340098023806ITERATION : 20, loss : 0.11145488348365928ITERATION : 21, loss : 0.11146954026166547ITERATION : 22, loss : 0.11147983795843575ITERATION : 23, loss : 0.11148707692708139ITERATION : 24, loss : 0.1114921682254261ITERATION : 25, loss : 0.11149575057365027ITERATION : 26, loss : 0.11149827227584225ITERATION : 27, loss : 0.11150004793149237ITERATION : 28, loss : 0.11150129876015766ITERATION : 29, loss : 0.11150218009206511ITERATION : 30, loss : 0.1115028012883891ITERATION : 31, loss : 0.11150323919044479ITERATION : 32, loss : 0.11150354799544336ITERATION : 33, loss : 0.11150376581802203ITERATION : 34, loss : 0.11150391949925643ITERATION : 35, loss : 0.11150402795558391ITERATION : 36, loss : 0.11150410448273401ITERATION : 37, loss : 0.11150415853137112ITERATION : 38, loss : 0.11150419670257807ITERATION : 39, loss : 0.11150422361987297ITERATION : 40, loss : 0.1115042426318365ITERATION : 41, loss : 0.11150425604300633ITERATION : 42, loss : 0.11150426552681944ITERATION : 43, loss : 0.11150427219577148ITERATION : 44, loss : 0.11150427697058225ITERATION : 45, loss : 0.11150428031341753ITERATION : 46, loss : 0.11150428269707763ITERATION : 47, loss : 0.11150428435557252ITERATION : 48, loss : 0.11150428553146677ITERATION : 49, loss : 0.11150428636588938ITERATION : 50, loss : 0.11150428696743525ITERATION : 51, loss : 0.11150428736645362ITERATION : 52, loss : 0.11150428765912017ITERATION : 53, loss : 0.11150428785742497ITERATION : 54, loss : 0.11150428801147827ITERATION : 55, loss : 0.11150428810103391ITERATION : 56, loss : 0.11150428817096121ITERATION : 57, loss : 0.11150428819065986ITERATION : 58, loss : 0.11150428819113516ITERATION : 59, loss : 0.11150428819113516ITERATION : 60, loss : 0.11150428819113516ITERATION : 61, loss : 0.11150428819113516ITERATION : 62, loss : 0.11150428819113516ITERATION : 63, loss : 0.11150428819113516ITERATION : 64, loss : 0.11150428819113516ITERATION : 65, loss : 0.11150428819113516ITERATION : 66, loss : 0.11150428819113516ITERATION : 67, loss : 0.11150428819113516ITERATION : 68, loss : 0.11150428819113516ITERATION : 69, loss : 0.11150428819113516ITERATION : 70, loss : 0.11150428819113516ITERATION : 71, loss : 0.11150428819113516ITERATION : 72, loss : 0.11150428819113516ITERATION : 73, loss : 0.11150428819113516ITERATION : 74, loss : 0.11150428819113516ITERATION : 75, loss : 0.11150428819113516ITERATION : 76, loss : 0.11150428819113516ITERATION : 77, loss : 0.11150428819113516ITERATION : 78, loss : 0.11150428819113516ITERATION : 79, loss : 0.11150428819113516ITERATION : 80, loss : 0.11150428819113516ITERATION : 81, loss : 0.11150428819113516ITERATION : 82, loss : 0.11150428819113516ITERATION : 83, loss : 0.11150428819113516ITERATION : 84, loss : 0.11150428819113516ITERATION : 85, loss : 0.11150428819113516ITERATION : 86, loss : 0.11150428819113516ITERATION : 87, loss : 0.11150428819113516ITERATION : 88, loss : 0.11150428819113516ITERATION : 89, loss : 0.11150428819113516ITERATION : 90, loss : 0.11150428819113516ITERATION : 91, loss : 0.11150428819113516ITERATION : 92, loss : 0.11150428819113516ITERATION : 93, loss : 0.11150428819113516ITERATION : 94, loss : 0.11150428819113516ITERATION : 95, loss : 0.11150428819113516ITERATION : 96, loss : 0.11150428819113516ITERATION : 97, loss : 0.11150428819113516ITERATION : 98, loss : 0.11150428819113516ITERATION : 99, loss : 0.11150428819113516ITERATION : 100, loss : 0.11150428819113516
ITERATION : 1, loss : 0.0748111098871292ITERATION : 2, loss : 0.09411433277728862ITERATION : 3, loss : 0.10781254658688406ITERATION : 4, loss : 0.1170623026231821ITERATION : 5, loss : 0.12337953375894718ITERATION : 6, loss : 0.12773835864978164ITERATION : 7, loss : 0.13076574786974637ITERATION : 8, loss : 0.13287715505095452ITERATION : 9, loss : 0.13435360309043293ITERATION : 10, loss : 0.13538771372532346ITERATION : 11, loss : 0.1361126691715157ITERATION : 12, loss : 0.13662109696454575ITERATION : 13, loss : 0.13697767645129663ITERATION : 14, loss : 0.13722769277518454ITERATION : 15, loss : 0.1374029069811786ITERATION : 16, loss : 0.13752561781846828ITERATION : 17, loss : 0.13761148879941124ITERATION : 18, loss : 0.13767152396008428ITERATION : 19, loss : 0.1377134527138275ITERATION : 20, loss : 0.13774270224966323ITERATION : 21, loss : 0.1377630811350075ITERATION : 22, loss : 0.13777726030218715ITERATION : 23, loss : 0.1377871112091864ITERATION : 24, loss : 0.13779394413935367ITERATION : 25, loss : 0.13779867537996213ITERATION : 26, loss : 0.13780194518529806ITERATION : 27, loss : 0.1378042003595228ITERATION : 28, loss : 0.13780575215697685ITERATION : 29, loss : 0.13780681724534502ITERATION : 30, loss : 0.1378075462786936ITERATION : 31, loss : 0.13780804373736438ITERATION : 32, loss : 0.1378083819866169ITERATION : 33, loss : 0.1378086111020262ITERATION : 34, loss : 0.13780876560473715ITERATION : 35, loss : 0.13780886927889827ITERATION : 36, loss : 0.13780893837687513ITERATION : 37, loss : 0.13780898416447704ITERATION : 38, loss : 0.13780901424903552ITERATION : 39, loss : 0.13780903381419984ITERATION : 40, loss : 0.13780904635644228ITERATION : 41, loss : 0.13780905434734578ITERATION : 42, loss : 0.13780905929180293ITERATION : 43, loss : 0.1378090622263978ITERATION : 44, loss : 0.13780906391630146ITERATION : 45, loss : 0.1378090648939624ITERATION : 46, loss : 0.1378090653594371ITERATION : 47, loss : 0.137809065585169ITERATION : 48, loss : 0.13780906560024295ITERATION : 49, loss : 0.13780906548974003ITERATION : 50, loss : 0.1378090653359791ITERATION : 51, loss : 0.13780906515589858ITERATION : 52, loss : 0.13780906499046267ITERATION : 53, loss : 0.1378090648422949ITERATION : 54, loss : 0.13780906468701157ITERATION : 55, loss : 0.1378090645882204ITERATION : 56, loss : 0.13780906450478034ITERATION : 57, loss : 0.1378090644381888ITERATION : 58, loss : 0.1378090643822936ITERATION : 59, loss : 0.13780906434029608ITERATION : 60, loss : 0.13780906431138862ITERATION : 61, loss : 0.1378090642881456ITERATION : 62, loss : 0.1378090642707983ITERATION : 63, loss : 0.13780906425625628ITERATION : 64, loss : 0.13780906425624265ITERATION : 65, loss : 0.13780906425624265ITERATION : 66, loss : 0.13780906425624265ITERATION : 67, loss : 0.13780906425624265ITERATION : 68, loss : 0.13780906425624265ITERATION : 69, loss : 0.13780906425624265ITERATION : 70, loss : 0.13780906425624265ITERATION : 71, loss : 0.13780906425624265ITERATION : 72, loss : 0.13780906425624265ITERATION : 73, loss : 0.13780906425624265ITERATION : 74, loss : 0.13780906425624265ITERATION : 75, loss : 0.13780906425624265ITERATION : 76, loss : 0.13780906425624265ITERATION : 77, loss : 0.13780906425624265ITERATION : 78, loss : 0.13780906425624265ITERATION : 79, loss : 0.13780906425624265ITERATION : 80, loss : 0.13780906425624265ITERATION : 81, loss : 0.13780906425624265ITERATION : 82, loss : 0.13780906425624265ITERATION : 83, loss : 0.13780906425624265ITERATION : 84, loss : 0.13780906425624265ITERATION : 85, loss : 0.13780906425624265ITERATION : 86, loss : 0.13780906425624265ITERATION : 87, loss : 0.13780906425624265ITERATION : 88, loss : 0.13780906425624265ITERATION : 89, loss : 0.13780906425624265ITERATION : 90, loss : 0.13780906425624265ITERATION : 91, loss : 0.13780906425624265ITERATION : 92, loss : 0.13780906425624265ITERATION : 93, loss : 0.13780906425624265ITERATION : 94, loss : 0.13780906425624265ITERATION : 95, loss : 0.13780906425624265ITERATION : 96, loss : 0.13780906425624265ITERATION : 97, loss : 0.13780906425624265ITERATION : 98, loss : 0.13780906425624265ITERATION : 99, loss : 0.13780906425624265ITERATION : 100, loss : 0.13780906425624265
gradient norm in None layer : 0.0030274068318443758
gradient norm in None layer : 8.107052154967755e-05
gradient norm in None layer : 9.840692701820317e-05
gradient norm in None layer : 0.0013047437497783626
gradient norm in None layer : 8.12601535067486e-05
gradient norm in None layer : 0.00011010321630252289
gradient norm in None layer : 0.0006184078464286618
gradient norm in None layer : 2.778008631986592e-05
gradient norm in None layer : 3.101109295174447e-05
gradient norm in None layer : 0.0006122630054354916
gradient norm in None layer : 3.026037136551927e-05
gradient norm in None layer : 3.044314219289027e-05
gradient norm in None layer : 0.00018869741627448462
gradient norm in None layer : 6.145451314370459e-06
gradient norm in None layer : 5.556235574786643e-06
gradient norm in None layer : 0.00018507114658110026
gradient norm in None layer : 7.394254718300374e-06
gradient norm in None layer : 6.994077712782302e-06
gradient norm in None layer : 0.00030196630331682736
gradient norm in None layer : 4.225188271495319e-06
gradient norm in None layer : 0.0006635015045110939
gradient norm in None layer : 4.7366564740266494e-05
gradient norm in None layer : 2.8442232852565832e-05
gradient norm in None layer : 0.0009044438323090713
gradient norm in None layer : 8.056303944594203e-05
gradient norm in None layer : 8.219386253550143e-05
gradient norm in None layer : 0.0017753521608716676
gradient norm in None layer : 8.931888166722715e-06
gradient norm in None layer : 0.0016880521815669704
gradient norm in None layer : 0.0001512259774866056
gradient norm in None layer : 0.0001349532442641661
gradient norm in None layer : 0.0021840804357769316
gradient norm in None layer : 0.00018724347663268362
gradient norm in None layer : 0.0002942677800131435
gradient norm in None layer : 0.00014429013896389187
gradient norm in None layer : 4.1085358885257074e-05
Total gradient norm: 0.004904702827547851
invariance loss : 4.823690455297707, avg_den : 0.4131317138671875, density loss : 0.3139118645302137, mse loss : 0.07493966235143903, solver time : 116.15948915481567 sec , total loss : 0.08007726467126695, running loss : 0.10607632761696237
Epoch 0/10 , batch 12/12500 
ITERATION : 1, loss : 0.11354480754052165ITERATION : 2, loss : 0.13086153922954766ITERATION : 3, loss : 0.1344942064823603ITERATION : 4, loss : 0.13406207578777182ITERATION : 5, loss : 0.13423045281258383ITERATION : 6, loss : 0.13457646287714978ITERATION : 7, loss : 0.13494126921678712ITERATION : 8, loss : 0.13526743584905762ITERATION : 9, loss : 0.13553876833185124ITERATION : 10, loss : 0.1357558538954868ITERATION : 11, loss : 0.13592548263561874ITERATION : 12, loss : 0.13605600752166597ITERATION : 13, loss : 0.1361553920241748ITERATION : 14, loss : 0.13623049952001162ITERATION : 15, loss : 0.13628694724185933ITERATION : 16, loss : 0.13632919271502267ITERATION : 17, loss : 0.13636070368006406ITERATION : 18, loss : 0.13638414462437437ITERATION : 19, loss : 0.13640154318669706ITERATION : 20, loss : 0.13641443236065945ITERATION : 21, loss : 0.13642396497458298ITERATION : 22, loss : 0.13643100487545232ITERATION : 23, loss : 0.13643619692272715ITERATION : 24, loss : 0.13644002153760984ITERATION : 25, loss : 0.1364428358893029ITERATION : 26, loss : 0.13644490465902354ITERATION : 27, loss : 0.1364464239885797ITERATION : 28, loss : 0.13644753874720794ITERATION : 29, loss : 0.13644835600766908ITERATION : 30, loss : 0.1364489547236251ITERATION : 31, loss : 0.13644939297446898ITERATION : 32, loss : 0.1364497134809684ITERATION : 33, loss : 0.13644994782108208ITERATION : 34, loss : 0.13645011871552065ITERATION : 35, loss : 0.13645024374928272ITERATION : 36, loss : 0.13645033480865698ITERATION : 37, loss : 0.13645040142320375ITERATION : 38, loss : 0.136450449798695ITERATION : 39, loss : 0.13645048499857237ITERATION : 40, loss : 0.13645051088915175ITERATION : 41, loss : 0.13645052968898896ITERATION : 42, loss : 0.1364505435351653ITERATION : 43, loss : 0.1364505535484229ITERATION : 44, loss : 0.1364505609166636ITERATION : 45, loss : 0.13645056626875393ITERATION : 46, loss : 0.1364505701924363ITERATION : 47, loss : 0.13645057306029632ITERATION : 48, loss : 0.13645057506008995ITERATION : 49, loss : 0.13645057656843718ITERATION : 50, loss : 0.13645057769656557ITERATION : 51, loss : 0.13645057841484004ITERATION : 52, loss : 0.13645057896881682ITERATION : 53, loss : 0.13645057927533077ITERATION : 54, loss : 0.13645057960459336ITERATION : 55, loss : 0.13645057983822578ITERATION : 56, loss : 0.13645057988993767ITERATION : 57, loss : 0.13645057997141619ITERATION : 58, loss : 0.13645057997312193ITERATION : 59, loss : 0.13645057997312193ITERATION : 60, loss : 0.13645057997312193ITERATION : 61, loss : 0.13645057997312193ITERATION : 62, loss : 0.13645057997312193ITERATION : 63, loss : 0.13645057997312193ITERATION : 64, loss : 0.13645057997312193ITERATION : 65, loss : 0.13645057997312193ITERATION : 66, loss : 0.13645057997312193ITERATION : 67, loss : 0.13645057997312193ITERATION : 68, loss : 0.13645057997312193ITERATION : 69, loss : 0.13645057997312193ITERATION : 70, loss : 0.13645057997312193ITERATION : 71, loss : 0.13645057997312193ITERATION : 72, loss : 0.13645057997312193ITERATION : 73, loss : 0.13645057997312193ITERATION : 74, loss : 0.13645057997312193ITERATION : 75, loss : 0.13645057997312193ITERATION : 76, loss : 0.13645057997312193ITERATION : 77, loss : 0.13645057997312193ITERATION : 78, loss : 0.13645057997312193ITERATION : 79, loss : 0.13645057997312193ITERATION : 80, loss : 0.13645057997312193ITERATION : 81, loss : 0.13645057997312193ITERATION : 82, loss : 0.13645057997312193ITERATION : 83, loss : 0.13645057997312193ITERATION : 84, loss : 0.13645057997312193ITERATION : 85, loss : 0.13645057997312193ITERATION : 86, loss : 0.13645057997312193ITERATION : 87, loss : 0.13645057997312193ITERATION : 88, loss : 0.13645057997312193ITERATION : 89, loss : 0.13645057997312193ITERATION : 90, loss : 0.13645057997312193ITERATION : 91, loss : 0.13645057997312193ITERATION : 92, loss : 0.13645057997312193ITERATION : 93, loss : 0.13645057997312193ITERATION : 94, loss : 0.13645057997312193ITERATION : 95, loss : 0.13645057997312193ITERATION : 96, loss : 0.13645057997312193ITERATION : 97, loss : 0.13645057997312193ITERATION : 98, loss : 0.13645057997312193ITERATION : 99, loss : 0.13645057997312193ITERATION : 100, loss : 0.13645057997312193
ITERATION : 1, loss : 0.09467713926202517ITERATION : 2, loss : 0.1075392589440171ITERATION : 3, loss : 0.12120439222076279ITERATION : 4, loss : 0.1355787212973478ITERATION : 5, loss : 0.14551771819544693ITERATION : 6, loss : 0.15265646272493186ITERATION : 7, loss : 0.15768161741167125ITERATION : 8, loss : 0.16142260339341058ITERATION : 9, loss : 0.16422954095426182ITERATION : 10, loss : 0.16634600184207624ITERATION : 11, loss : 0.16794704451766948ITERATION : 12, loss : 0.16916096165587088ITERATION : 13, loss : 0.17008290696507003ITERATION : 14, loss : 0.17078400339526933ITERATION : 15, loss : 0.17131769070249972ITERATION : 16, loss : 0.17172427100066112ITERATION : 17, loss : 0.1720342225300853ITERATION : 18, loss : 0.17227064062767106ITERATION : 19, loss : 0.17245105579656195ITERATION : 20, loss : 0.17258879017975728ITERATION : 21, loss : 0.17269397860312524ITERATION : 22, loss : 0.17277433722065108ITERATION : 23, loss : 0.17283574493848392ITERATION : 24, loss : 0.17288268333997617ITERATION : 25, loss : 0.1729185708678679ITERATION : 26, loss : 0.17294601553376132ITERATION : 27, loss : 0.1729670082526074ITERATION : 28, loss : 0.17298306880236722ITERATION : 29, loss : 0.1729953586623367ITERATION : 30, loss : 0.17300476475094761ITERATION : 31, loss : 0.17301196515497158ITERATION : 32, loss : 0.17301747792101752ITERATION : 33, loss : 0.17302169930207434ITERATION : 34, loss : 0.17302493239774797ITERATION : 35, loss : 0.17302740891055834ITERATION : 36, loss : 0.1730293062304128ITERATION : 37, loss : 0.17303076015367264ITERATION : 38, loss : 0.17303187426207922ITERATION : 39, loss : 0.17303272820931817ITERATION : 40, loss : 0.1730333827612829ITERATION : 41, loss : 0.17303388459226274ITERATION : 42, loss : 0.17303426940435973ITERATION : 43, loss : 0.17303456448436794ITERATION : 44, loss : 0.17303479080020057ITERATION : 45, loss : 0.17303496438301721ITERATION : 46, loss : 0.17303509747391788ITERATION : 47, loss : 0.17303519960251862ITERATION : 48, loss : 0.17303527808249894ITERATION : 49, loss : 0.17303533818742106ITERATION : 50, loss : 0.17303538429396992ITERATION : 51, loss : 0.1730354196560215ITERATION : 52, loss : 0.17303544678363525ITERATION : 53, loss : 0.17303546765974778ITERATION : 54, loss : 0.17303548365882016ITERATION : 55, loss : 0.17303549594658688ITERATION : 56, loss : 0.17303550538123566ITERATION : 57, loss : 0.17303551259095926ITERATION : 58, loss : 0.17303551811345538ITERATION : 59, loss : 0.17303552234716035ITERATION : 60, loss : 0.173035525565736ITERATION : 61, loss : 0.17303552805383338ITERATION : 62, loss : 0.17303553001324742ITERATION : 63, loss : 0.17303553151116505ITERATION : 64, loss : 0.1730355326784216ITERATION : 65, loss : 0.1730355335759928ITERATION : 66, loss : 0.17303553421405854ITERATION : 67, loss : 0.17303553476269137ITERATION : 68, loss : 0.17303553504393004ITERATION : 69, loss : 0.1730355353935916ITERATION : 70, loss : 0.17303553558666077ITERATION : 71, loss : 0.17303553578729367ITERATION : 72, loss : 0.17303553585904752ITERATION : 73, loss : 0.1730355358969864ITERATION : 74, loss : 0.17303553593736173ITERATION : 75, loss : 0.17303553593817317ITERATION : 76, loss : 0.17303553593817317ITERATION : 77, loss : 0.17303553593817317ITERATION : 78, loss : 0.17303553593817317ITERATION : 79, loss : 0.17303553593817317ITERATION : 80, loss : 0.17303553593817317ITERATION : 81, loss : 0.17303553593817317ITERATION : 82, loss : 0.17303553593817317ITERATION : 83, loss : 0.17303553593817317ITERATION : 84, loss : 0.17303553593817317ITERATION : 85, loss : 0.17303553593817317ITERATION : 86, loss : 0.17303553593817317ITERATION : 87, loss : 0.17303553593817317ITERATION : 88, loss : 0.17303553593817317ITERATION : 89, loss : 0.17303553593817317ITERATION : 90, loss : 0.17303553593817317ITERATION : 91, loss : 0.17303553593817317ITERATION : 92, loss : 0.17303553593817317ITERATION : 93, loss : 0.17303553593817317ITERATION : 94, loss : 0.17303553593817317ITERATION : 95, loss : 0.17303553593817317ITERATION : 96, loss : 0.17303553593817317ITERATION : 97, loss : 0.17303553593817317ITERATION : 98, loss : 0.17303553593817317ITERATION : 99, loss : 0.17303553593817317ITERATION : 100, loss : 0.17303553593817317
ITERATION : 1, loss : 0.05839240369922468ITERATION : 2, loss : 0.09086873153075434ITERATION : 3, loss : 0.10759454308246959ITERATION : 4, loss : 0.11699487475755996ITERATION : 5, loss : 0.12262573209605551ITERATION : 6, loss : 0.12616258511810347ITERATION : 7, loss : 0.12846136080660306ITERATION : 8, loss : 0.12999269403056884ITERATION : 9, loss : 0.13103131580018082ITERATION : 10, loss : 0.13174527988042928ITERATION : 11, loss : 0.13224113981661867ITERATION : 12, loss : 0.1325883207957258ITERATION : 13, loss : 0.13283300116840052ITERATION : 14, loss : 0.13300638546807328ITERATION : 15, loss : 0.1331298197099724ITERATION : 16, loss : 0.13321804829739972ITERATION : 17, loss : 0.13328133628769206ITERATION : 18, loss : 0.1333268770725604ITERATION : 19, loss : 0.13335973983686286ITERATION : 20, loss : 0.1333835141863064ITERATION : 21, loss : 0.1334007529043915ITERATION : 22, loss : 0.13341327840237235ITERATION : 23, loss : 0.13342239626627556ITERATION : 24, loss : 0.1334290446587882ITERATION : 25, loss : 0.1334338996967553ITERATION : 26, loss : 0.13343744995180348ITERATION : 27, loss : 0.13344004920577016ITERATION : 28, loss : 0.13344195430259534ITERATION : 29, loss : 0.13344335195857715ITERATION : 30, loss : 0.13344437822307695ITERATION : 31, loss : 0.13344513238011135ITERATION : 32, loss : 0.13344568697568918ITERATION : 33, loss : 0.13344609500291515ITERATION : 34, loss : 0.13344639541743308ITERATION : 35, loss : 0.13344661669019495ITERATION : 36, loss : 0.13344677974246308ITERATION : 37, loss : 0.13344689993517939ITERATION : 38, loss : 0.13344698857157217ITERATION : 39, loss : 0.1334470539565995ITERATION : 40, loss : 0.1334471021951716ITERATION : 41, loss : 0.13344713779389736ITERATION : 42, loss : 0.13344716410086424ITERATION : 43, loss : 0.13344718349917958ITERATION : 44, loss : 0.13344719780650433ITERATION : 45, loss : 0.13344720835137439ITERATION : 46, loss : 0.133447216209272ITERATION : 47, loss : 0.13344722197924713ITERATION : 48, loss : 0.1334472262741885ITERATION : 49, loss : 0.1334472294130424ITERATION : 50, loss : 0.13344723172928397ITERATION : 51, loss : 0.13344723344491283ITERATION : 52, loss : 0.133447234706073ITERATION : 53, loss : 0.1334472356334515ITERATION : 54, loss : 0.1334472363273306ITERATION : 55, loss : 0.1334472368374814ITERATION : 56, loss : 0.13344723720397988ITERATION : 57, loss : 0.13344723746782164ITERATION : 58, loss : 0.13344723771261882ITERATION : 59, loss : 0.1334472378631534ITERATION : 60, loss : 0.13344723796704777ITERATION : 61, loss : 0.13344723803829936ITERATION : 62, loss : 0.13344723810095166ITERATION : 63, loss : 0.13344723813254042ITERATION : 64, loss : 0.1334472381467602ITERATION : 65, loss : 0.13344723817646278ITERATION : 66, loss : 0.1334472381765637ITERATION : 67, loss : 0.1334472381765637ITERATION : 68, loss : 0.1334472381765637ITERATION : 69, loss : 0.1334472381765637ITERATION : 70, loss : 0.1334472381765637ITERATION : 71, loss : 0.1334472381765637ITERATION : 72, loss : 0.1334472381765637ITERATION : 73, loss : 0.1334472381765637ITERATION : 74, loss : 0.1334472381765637ITERATION : 75, loss : 0.1334472381765637ITERATION : 76, loss : 0.1334472381765637ITERATION : 77, loss : 0.1334472381765637ITERATION : 78, loss : 0.1334472381765637ITERATION : 79, loss : 0.1334472381765637ITERATION : 80, loss : 0.1334472381765637ITERATION : 81, loss : 0.1334472381765637ITERATION : 82, loss : 0.1334472381765637ITERATION : 83, loss : 0.1334472381765637ITERATION : 84, loss : 0.1334472381765637ITERATION : 85, loss : 0.1334472381765637ITERATION : 86, loss : 0.1334472381765637ITERATION : 87, loss : 0.1334472381765637ITERATION : 88, loss : 0.1334472381765637ITERATION : 89, loss : 0.1334472381765637ITERATION : 90, loss : 0.1334472381765637ITERATION : 91, loss : 0.1334472381765637ITERATION : 92, loss : 0.1334472381765637ITERATION : 93, loss : 0.1334472381765637ITERATION : 94, loss : 0.1334472381765637ITERATION : 95, loss : 0.1334472381765637ITERATION : 96, loss : 0.1334472381765637ITERATION : 97, loss : 0.1334472381765637ITERATION : 98, loss : 0.1334472381765637ITERATION : 99, loss : 0.1334472381765637ITERATION : 100, loss : 0.1334472381765637
ITERATION : 1, loss : 0.12464730973389614ITERATION : 2, loss : 0.13034193874383404ITERATION : 3, loss : 0.14142099545511544ITERATION : 4, loss : 0.15196846821856907ITERATION : 5, loss : 0.1603230306458292ITERATION : 6, loss : 0.166595620299045ITERATION : 7, loss : 0.17121839146323214ITERATION : 8, loss : 0.17460168934070408ITERATION : 9, loss : 0.17707147152761285ITERATION : 10, loss : 0.1788728911582445ITERATION : 11, loss : 0.1801866368939893ITERATION : 12, loss : 0.1811448465630182ITERATION : 13, loss : 0.18184387537622668ITERATION : 14, loss : 0.18235392832842318ITERATION : 15, loss : 0.18272615736907666ITERATION : 16, loss : 0.18299784274773845ITERATION : 17, loss : 0.18319616489507054ITERATION : 18, loss : 0.18334094681707722ITERATION : 19, loss : 0.18344664973114547ITERATION : 20, loss : 0.18352382585069668ITERATION : 21, loss : 0.18358017632068138ITERATION : 22, loss : 0.18362132218906962ITERATION : 23, loss : 0.18365136684283678ITERATION : 24, loss : 0.18367330595708722ITERATION : 25, loss : 0.18368932659628365ITERATION : 26, loss : 0.18370102558677928ITERATION : 27, loss : 0.1837095688139129ITERATION : 28, loss : 0.1837158076680006ITERATION : 29, loss : 0.1837203637636714ITERATION : 30, loss : 0.18372369100101785ITERATION : 31, loss : 0.18372612088900667ITERATION : 32, loss : 0.18372789542465573ITERATION : 33, loss : 0.18372919130569454ITERATION : 34, loss : 0.18373013782730985ITERATION : 35, loss : 0.1837308290098331ITERATION : 36, loss : 0.1837313339265654ITERATION : 37, loss : 0.18373170264333646ITERATION : 38, loss : 0.18373197193648777ITERATION : 39, loss : 0.18373216859825992ITERATION : 40, loss : 0.18373231223574588ITERATION : 41, loss : 0.18373241716348074ITERATION : 42, loss : 0.18373249382327367ITERATION : 43, loss : 0.18373254979879675ITERATION : 44, loss : 0.18373259064280123ITERATION : 45, loss : 0.1837326204721804ITERATION : 46, loss : 0.18373264221338476ITERATION : 47, loss : 0.18373265814989473ITERATION : 48, loss : 0.183732669681183ITERATION : 49, loss : 0.18373267822438774ITERATION : 50, loss : 0.18373268455959407ITERATION : 51, loss : 0.1837326890996538ITERATION : 52, loss : 0.18373269224676822ITERATION : 53, loss : 0.18373269467691958ITERATION : 54, loss : 0.183732696351065ITERATION : 55, loss : 0.18373269768938927ITERATION : 56, loss : 0.1837326985298567ITERATION : 57, loss : 0.1837326992154381ITERATION : 58, loss : 0.18373269969577205ITERATION : 59, loss : 0.18373270004026263ITERATION : 60, loss : 0.18373270026127622ITERATION : 61, loss : 0.18373270049015572ITERATION : 62, loss : 0.18373270055521165ITERATION : 63, loss : 0.1837327006308998ITERATION : 64, loss : 0.1837327006308998ITERATION : 65, loss : 0.1837327006308998ITERATION : 66, loss : 0.1837327006308998ITERATION : 67, loss : 0.1837327006308998ITERATION : 68, loss : 0.1837327006308998ITERATION : 69, loss : 0.1837327006308998ITERATION : 70, loss : 0.1837327006308998ITERATION : 71, loss : 0.1837327006308998ITERATION : 72, loss : 0.1837327006308998ITERATION : 73, loss : 0.1837327006308998ITERATION : 74, loss : 0.1837327006308998ITERATION : 75, loss : 0.1837327006308998ITERATION : 76, loss : 0.1837327006308998ITERATION : 77, loss : 0.1837327006308998ITERATION : 78, loss : 0.1837327006308998ITERATION : 79, loss : 0.1837327006308998ITERATION : 80, loss : 0.1837327006308998ITERATION : 81, loss : 0.1837327006308998ITERATION : 82, loss : 0.1837327006308998ITERATION : 83, loss : 0.1837327006308998ITERATION : 84, loss : 0.1837327006308998ITERATION : 85, loss : 0.1837327006308998ITERATION : 86, loss : 0.1837327006308998ITERATION : 87, loss : 0.1837327006308998ITERATION : 88, loss : 0.1837327006308998ITERATION : 89, loss : 0.1837327006308998ITERATION : 90, loss : 0.1837327006308998ITERATION : 91, loss : 0.1837327006308998ITERATION : 92, loss : 0.1837327006308998ITERATION : 93, loss : 0.1837327006308998ITERATION : 94, loss : 0.1837327006308998ITERATION : 95, loss : 0.1837327006308998ITERATION : 96, loss : 0.1837327006308998ITERATION : 97, loss : 0.1837327006308998ITERATION : 98, loss : 0.1837327006308998ITERATION : 99, loss : 0.1837327006308998ITERATION : 100, loss : 0.1837327006308998
ITERATION : 1, loss : 0.10401899712928639ITERATION : 2, loss : 0.13764033722139815ITERATION : 3, loss : 0.1569841942013051ITERATION : 4, loss : 0.16866218603847985ITERATION : 5, loss : 0.177433137433181ITERATION : 6, loss : 0.18334544961791968ITERATION : 7, loss : 0.18717029871117843ITERATION : 8, loss : 0.1896970225260541ITERATION : 9, loss : 0.19132869535590383ITERATION : 10, loss : 0.19242955923222868ITERATION : 11, loss : 0.19318156521867483ITERATION : 12, loss : 0.19369801610069462ITERATION : 13, loss : 0.1940540394481535ITERATION : 14, loss : 0.19430012599114135ITERATION : 15, loss : 0.19447054415753684ITERATION : 16, loss : 0.19458871779626022ITERATION : 17, loss : 0.19467073997913992ITERATION : 18, loss : 0.19472770763675606ITERATION : 19, loss : 0.19476729222574965ITERATION : 20, loss : 0.19479480697933285ITERATION : 21, loss : 0.19481393656228158ITERATION : 22, loss : 0.194827238547402ITERATION : 23, loss : 0.19483648924965188ITERATION : 24, loss : 0.1948429230563816ITERATION : 25, loss : 0.1948473979955488ITERATION : 26, loss : 0.1948505106064906ITERATION : 27, loss : 0.19485267567109527ITERATION : 28, loss : 0.19485418169765628ITERATION : 29, loss : 0.19485522931199573ITERATION : 30, loss : 0.1948559580501421ITERATION : 31, loss : 0.19485646497117537ITERATION : 32, loss : 0.19485681759835785ITERATION : 33, loss : 0.194857062916078ITERATION : 34, loss : 0.19485723354582205ITERATION : 35, loss : 0.19485735226770862ITERATION : 36, loss : 0.19485743485440543ITERATION : 37, loss : 0.19485749224917606ITERATION : 38, loss : 0.19485753220624363ITERATION : 39, loss : 0.19485756001579804ITERATION : 40, loss : 0.1948575793360173ITERATION : 41, loss : 0.19485759279005857ITERATION : 42, loss : 0.1948576021224748ITERATION : 43, loss : 0.1948576086437275ITERATION : 44, loss : 0.19485761317451675ITERATION : 45, loss : 0.194857616328816ITERATION : 46, loss : 0.19485761852952732ITERATION : 47, loss : 0.1948576200660168ITERATION : 48, loss : 0.1948576211153915ITERATION : 49, loss : 0.19485762183165758ITERATION : 50, loss : 0.19485762233564052ITERATION : 51, loss : 0.19485762268924134ITERATION : 52, loss : 0.19485762293974596ITERATION : 53, loss : 0.19485762310148041ITERATION : 54, loss : 0.1948576232028443ITERATION : 55, loss : 0.19485762327795017ITERATION : 56, loss : 0.19485762334192708ITERATION : 57, loss : 0.19485762338089455ITERATION : 58, loss : 0.19485762340069607ITERATION : 59, loss : 0.1948576234154791ITERATION : 60, loss : 0.19485762341553733ITERATION : 61, loss : 0.19485762341553733ITERATION : 62, loss : 0.19485762341553733ITERATION : 63, loss : 0.19485762341553733ITERATION : 64, loss : 0.19485762341553733ITERATION : 65, loss : 0.19485762341553733ITERATION : 66, loss : 0.19485762341553733ITERATION : 67, loss : 0.19485762341553733ITERATION : 68, loss : 0.19485762341553733ITERATION : 69, loss : 0.19485762341553733ITERATION : 70, loss : 0.19485762341553733ITERATION : 71, loss : 0.19485762341553733ITERATION : 72, loss : 0.19485762341553733ITERATION : 73, loss : 0.19485762341553733ITERATION : 74, loss : 0.19485762341553733ITERATION : 75, loss : 0.19485762341553733ITERATION : 76, loss : 0.19485762341553733ITERATION : 77, loss : 0.19485762341553733ITERATION : 78, loss : 0.19485762341553733ITERATION : 79, loss : 0.19485762341553733ITERATION : 80, loss : 0.19485762341553733ITERATION : 81, loss : 0.19485762341553733ITERATION : 82, loss : 0.19485762341553733ITERATION : 83, loss : 0.19485762341553733ITERATION : 84, loss : 0.19485762341553733ITERATION : 85, loss : 0.19485762341553733ITERATION : 86, loss : 0.19485762341553733ITERATION : 87, loss : 0.19485762341553733ITERATION : 88, loss : 0.19485762341553733ITERATION : 89, loss : 0.19485762341553733ITERATION : 90, loss : 0.19485762341553733ITERATION : 91, loss : 0.19485762341553733ITERATION : 92, loss : 0.19485762341553733ITERATION : 93, loss : 0.19485762341553733ITERATION : 94, loss : 0.19485762341553733ITERATION : 95, loss : 0.19485762341553733ITERATION : 96, loss : 0.19485762341553733ITERATION : 97, loss : 0.19485762341553733ITERATION : 98, loss : 0.19485762341553733ITERATION : 99, loss : 0.19485762341553733ITERATION : 100, loss : 0.19485762341553733
ITERATION : 1, loss : 0.0837930777746841ITERATION : 2, loss : 0.0973284586543909ITERATION : 3, loss : 0.10653582514894444ITERATION : 4, loss : 0.11283824177158287ITERATION : 5, loss : 0.11688949241149392ITERATION : 6, loss : 0.11937589011012487ITERATION : 7, loss : 0.12084084050291122ITERATION : 8, loss : 0.1216640866968518ITERATION : 9, loss : 0.12209607140964118ITERATION : 10, loss : 0.122296662447556ITERATION : 11, loss : 0.1223655519190563ITERATION : 12, loss : 0.12236357579752405ITERATION : 13, loss : 0.12232695402745378ITERATION : 14, loss : 0.122276564471117ITERATION : 15, loss : 0.12222388642812819ITERATION : 16, loss : 0.12217476129357353ITERATION : 17, loss : 0.12213174225421668ITERATION : 18, loss : 0.12209554072917692ITERATION : 19, loss : 0.12206590049212318ITERATION : 20, loss : 0.12204211308542895ITERATION : 21, loss : 0.12202331074314958ITERATION : 22, loss : 0.122008624781097ITERATION : 23, loss : 0.12199726318590397ITERATION : 24, loss : 0.12198854196035847ITERATION : 25, loss : 0.12198189084328129ITERATION : 26, loss : 0.12197684611931175ITERATION : 27, loss : 0.12197303764777452ITERATION : 28, loss : 0.12197017391650343ITERATION : 29, loss : 0.12196802799486971ITERATION : 30, loss : 0.12196642485238496ITERATION : 31, loss : 0.1219652303073237ITERATION : 32, loss : 0.12196434236253292ITERATION : 33, loss : 0.12196368363709574ITERATION : 34, loss : 0.1219631958596251ITERATION : 35, loss : 0.12196283529012521ITERATION : 36, loss : 0.12196256911345403ITERATION : 37, loss : 0.12196237289182588ITERATION : 38, loss : 0.12196222838061833ITERATION : 39, loss : 0.12196212210255865ITERATION : 40, loss : 0.12196204402553737ITERATION : 41, loss : 0.12196198669484402ITERATION : 42, loss : 0.12196194469964844ITERATION : 43, loss : 0.12196191388432065ITERATION : 44, loss : 0.12196189127407221ITERATION : 45, loss : 0.12196187475345437ITERATION : 46, loss : 0.1219618626281838ITERATION : 47, loss : 0.12196185378354167ITERATION : 48, loss : 0.1219618473040184ITERATION : 49, loss : 0.12196184258120504ITERATION : 50, loss : 0.12196183910217145ITERATION : 51, loss : 0.1219618365602595ITERATION : 52, loss : 0.1219618347175004ITERATION : 53, loss : 0.12196183339295992ITERATION : 54, loss : 0.12196183243499006ITERATION : 55, loss : 0.1219618317136049ITERATION : 56, loss : 0.12196183123459482ITERATION : 57, loss : 0.1219618308625407ITERATION : 58, loss : 0.12196183058137522ITERATION : 59, loss : 0.1219618303667318ITERATION : 60, loss : 0.12196183020749142ITERATION : 61, loss : 0.12196183011173259ITERATION : 62, loss : 0.12196183002990535ITERATION : 63, loss : 0.1219618299932969ITERATION : 64, loss : 0.12196182995615203ITERATION : 65, loss : 0.121961829955925ITERATION : 66, loss : 0.121961829955925ITERATION : 67, loss : 0.121961829955925ITERATION : 68, loss : 0.121961829955925ITERATION : 69, loss : 0.121961829955925ITERATION : 70, loss : 0.121961829955925ITERATION : 71, loss : 0.121961829955925ITERATION : 72, loss : 0.121961829955925ITERATION : 73, loss : 0.121961829955925ITERATION : 74, loss : 0.121961829955925ITERATION : 75, loss : 0.121961829955925ITERATION : 76, loss : 0.121961829955925ITERATION : 77, loss : 0.121961829955925ITERATION : 78, loss : 0.121961829955925ITERATION : 79, loss : 0.121961829955925ITERATION : 80, loss : 0.121961829955925ITERATION : 81, loss : 0.121961829955925ITERATION : 82, loss : 0.121961829955925ITERATION : 83, loss : 0.121961829955925ITERATION : 84, loss : 0.121961829955925ITERATION : 85, loss : 0.121961829955925ITERATION : 86, loss : 0.121961829955925ITERATION : 87, loss : 0.121961829955925ITERATION : 88, loss : 0.121961829955925ITERATION : 89, loss : 0.121961829955925ITERATION : 90, loss : 0.121961829955925ITERATION : 91, loss : 0.121961829955925ITERATION : 92, loss : 0.121961829955925ITERATION : 93, loss : 0.121961829955925ITERATION : 94, loss : 0.121961829955925ITERATION : 95, loss : 0.121961829955925ITERATION : 96, loss : 0.121961829955925ITERATION : 97, loss : 0.121961829955925ITERATION : 98, loss : 0.121961829955925ITERATION : 99, loss : 0.121961829955925ITERATION : 100, loss : 0.121961829955925
ITERATION : 1, loss : 0.08595362215174968ITERATION : 2, loss : 0.11657958185975675ITERATION : 3, loss : 0.13722124941257977ITERATION : 4, loss : 0.1508887693579053ITERATION : 5, loss : 0.16001819678503162ITERATION : 6, loss : 0.16619468016650557ITERATION : 7, loss : 0.1704210371730887ITERATION : 8, loss : 0.17333919327175315ITERATION : 9, loss : 0.17536793959637037ITERATION : 10, loss : 0.1767855443650054ITERATION : 11, loss : 0.17777979717943432ITERATION : 12, loss : 0.17847900671883887ITERATION : 13, loss : 0.1789716799598611ITERATION : 14, loss : 0.17931930715476066ITERATION : 15, loss : 0.17956483452506808ITERATION : 16, loss : 0.17973837249744906ITERATION : 17, loss : 0.17986109114939144ITERATION : 18, loss : 0.1799479042078965ITERATION : 19, loss : 0.1800093334437344ITERATION : 20, loss : 0.18005280931288561ITERATION : 21, loss : 0.18008358321910722ITERATION : 22, loss : 0.18010536848427552ITERATION : 23, loss : 0.1801207918376712ITERATION : 24, loss : 0.18013171177457254ITERATION : 25, loss : 0.18013944363093026ITERATION : 26, loss : 0.18014491838726215ITERATION : 27, loss : 0.18014879507645273ITERATION : 28, loss : 0.1801515402505238ITERATION : 29, loss : 0.18015348421874405ITERATION : 30, loss : 0.18015486086988036ITERATION : 31, loss : 0.18015583574964808ITERATION : 32, loss : 0.18015652615939057ITERATION : 33, loss : 0.18015701510496898ITERATION : 34, loss : 0.18015736137300073ITERATION : 35, loss : 0.18015760661441624ITERATION : 36, loss : 0.18015778034702062ITERATION : 37, loss : 0.18015790336066304ITERATION : 38, loss : 0.18015799046471304ITERATION : 39, loss : 0.18015805217208286ITERATION : 40, loss : 0.18015809590927082ITERATION : 41, loss : 0.18015812685421673ITERATION : 42, loss : 0.180158148744679ITERATION : 43, loss : 0.1801581642645319ITERATION : 44, loss : 0.1801581752286832ITERATION : 45, loss : 0.1801581830232244ITERATION : 46, loss : 0.1801581885178065ITERATION : 47, loss : 0.18015819251251797ITERATION : 48, loss : 0.18015819524062748ITERATION : 49, loss : 0.18015819717724782ITERATION : 50, loss : 0.1801581985398215ITERATION : 51, loss : 0.18015819948272122ITERATION : 52, loss : 0.18015820018325795ITERATION : 53, loss : 0.18015820065500945ITERATION : 54, loss : 0.180158201006221ITERATION : 55, loss : 0.18015820121174572ITERATION : 56, loss : 0.18015820139983493ITERATION : 57, loss : 0.18015820159356674ITERATION : 58, loss : 0.18015820164076926ITERATION : 59, loss : 0.1801582016411964ITERATION : 60, loss : 0.1801582016411964ITERATION : 61, loss : 0.1801582016411964ITERATION : 62, loss : 0.1801582016411964ITERATION : 63, loss : 0.1801582016411964ITERATION : 64, loss : 0.1801582016411964ITERATION : 65, loss : 0.1801582016411964ITERATION : 66, loss : 0.1801582016411964ITERATION : 67, loss : 0.1801582016411964ITERATION : 68, loss : 0.1801582016411964ITERATION : 69, loss : 0.1801582016411964ITERATION : 70, loss : 0.1801582016411964ITERATION : 71, loss : 0.1801582016411964ITERATION : 72, loss : 0.1801582016411964ITERATION : 73, loss : 0.1801582016411964ITERATION : 74, loss : 0.1801582016411964ITERATION : 75, loss : 0.1801582016411964ITERATION : 76, loss : 0.1801582016411964ITERATION : 77, loss : 0.1801582016411964ITERATION : 78, loss : 0.1801582016411964ITERATION : 79, loss : 0.1801582016411964ITERATION : 80, loss : 0.1801582016411964ITERATION : 81, loss : 0.1801582016411964ITERATION : 82, loss : 0.1801582016411964ITERATION : 83, loss : 0.1801582016411964ITERATION : 84, loss : 0.1801582016411964ITERATION : 85, loss : 0.1801582016411964ITERATION : 86, loss : 0.1801582016411964ITERATION : 87, loss : 0.1801582016411964ITERATION : 88, loss : 0.1801582016411964ITERATION : 89, loss : 0.1801582016411964ITERATION : 90, loss : 0.1801582016411964ITERATION : 91, loss : 0.1801582016411964ITERATION : 92, loss : 0.1801582016411964ITERATION : 93, loss : 0.1801582016411964ITERATION : 94, loss : 0.1801582016411964ITERATION : 95, loss : 0.1801582016411964ITERATION : 96, loss : 0.1801582016411964ITERATION : 97, loss : 0.1801582016411964ITERATION : 98, loss : 0.1801582016411964ITERATION : 99, loss : 0.1801582016411964ITERATION : 100, loss : 0.1801582016411964
ITERATION : 1, loss : 0.08308848624001942ITERATION : 2, loss : 0.08918961296952109ITERATION : 3, loss : 0.1008322937902768ITERATION : 4, loss : 0.1119009640452936ITERATION : 5, loss : 0.12039896522011344ITERATION : 6, loss : 0.12672650727349344ITERATION : 7, loss : 0.13136007412044232ITERATION : 8, loss : 0.13472187994377088ITERATION : 9, loss : 0.13714791610263377ITERATION : 10, loss : 0.1388929561145184ITERATION : 11, loss : 0.1401455689916728ITERATION : 12, loss : 0.14104349869082927ITERATION : 13, loss : 0.14168659548139798ITERATION : 14, loss : 0.1421469008268572ITERATION : 15, loss : 0.142476235094671ITERATION : 16, loss : 0.14271179883119595ITERATION : 17, loss : 0.14288026092557105ITERATION : 18, loss : 0.14300072223417648ITERATION : 19, loss : 0.14308685453678502ITERATION : 20, loss : 0.14314843924570994ITERATION : 21, loss : 0.14319247236438076ITERATION : 22, loss : 0.1432239566273043ITERATION : 23, loss : 0.1432464689931481ITERATION : 24, loss : 0.14326256670140136ITERATION : 25, loss : 0.14327407828875963ITERATION : 26, loss : 0.14328231049232967ITERATION : 27, loss : 0.1432881978928409ITERATION : 28, loss : 0.14329240868363527ITERATION : 29, loss : 0.14329542045755175ITERATION : 30, loss : 0.14329757476819985ITERATION : 31, loss : 0.1432991158436573ITERATION : 32, loss : 0.14330021830295228ITERATION : 33, loss : 0.14330100703509108ITERATION : 34, loss : 0.14330157133899318ITERATION : 35, loss : 0.14330197510120693ITERATION : 36, loss : 0.14330226402065005ITERATION : 37, loss : 0.14330247078810668ITERATION : 38, loss : 0.1433026187283946ITERATION : 39, loss : 0.14330272463132826ITERATION : 40, loss : 0.14330280040686816ITERATION : 41, loss : 0.14330285466384074ITERATION : 42, loss : 0.14330289351078307ITERATION : 43, loss : 0.14330292129586888ITERATION : 44, loss : 0.1433029411935105ITERATION : 45, loss : 0.143302955448013ITERATION : 46, loss : 0.14330296562322173ITERATION : 47, loss : 0.14330297292662092ITERATION : 48, loss : 0.1433029781348565ITERATION : 49, loss : 0.14330298187860543ITERATION : 50, loss : 0.14330298454095697ITERATION : 51, loss : 0.14330298646685768ITERATION : 52, loss : 0.1433029877983935ITERATION : 53, loss : 0.14330298878224731ITERATION : 54, loss : 0.1433029894744243ITERATION : 55, loss : 0.1433029899949816ITERATION : 56, loss : 0.14330299034100927ITERATION : 57, loss : 0.14330299060158727ITERATION : 58, loss : 0.14330299077676242ITERATION : 59, loss : 0.14330299090503062ITERATION : 60, loss : 0.14330299096962545ITERATION : 61, loss : 0.14330299097201904ITERATION : 62, loss : 0.14330299097201904ITERATION : 63, loss : 0.14330299097201904ITERATION : 64, loss : 0.14330299097201904ITERATION : 65, loss : 0.14330299097201904ITERATION : 66, loss : 0.14330299097201904ITERATION : 67, loss : 0.14330299097201904ITERATION : 68, loss : 0.14330299097201904ITERATION : 69, loss : 0.14330299097201904ITERATION : 70, loss : 0.14330299097201904ITERATION : 71, loss : 0.14330299097201904ITERATION : 72, loss : 0.14330299097201904ITERATION : 73, loss : 0.14330299097201904ITERATION : 74, loss : 0.14330299097201904ITERATION : 75, loss : 0.14330299097201904ITERATION : 76, loss : 0.14330299097201904ITERATION : 77, loss : 0.14330299097201904ITERATION : 78, loss : 0.14330299097201904ITERATION : 79, loss : 0.14330299097201904ITERATION : 80, loss : 0.14330299097201904ITERATION : 81, loss : 0.14330299097201904ITERATION : 82, loss : 0.14330299097201904ITERATION : 83, loss : 0.14330299097201904ITERATION : 84, loss : 0.14330299097201904ITERATION : 85, loss : 0.14330299097201904ITERATION : 86, loss : 0.14330299097201904ITERATION : 87, loss : 0.14330299097201904ITERATION : 88, loss : 0.14330299097201904ITERATION : 89, loss : 0.14330299097201904ITERATION : 90, loss : 0.14330299097201904ITERATION : 91, loss : 0.14330299097201904ITERATION : 92, loss : 0.14330299097201904ITERATION : 93, loss : 0.14330299097201904ITERATION : 94, loss : 0.14330299097201904ITERATION : 95, loss : 0.14330299097201904ITERATION : 96, loss : 0.14330299097201904ITERATION : 97, loss : 0.14330299097201904ITERATION : 98, loss : 0.14330299097201904ITERATION : 99, loss : 0.14330299097201904ITERATION : 100, loss : 0.14330299097201904
gradient norm in None layer : 0.005695050191488087
gradient norm in None layer : 0.00014791909555276033
gradient norm in None layer : 0.0001684105393271774
gradient norm in None layer : 0.00217738390624843
gradient norm in None layer : 0.00012928507258904088
gradient norm in None layer : 0.00012855080753809887
gradient norm in None layer : 0.0010424390165145077
gradient norm in None layer : 4.0027225456600236e-05
gradient norm in None layer : 4.175139389232652e-05
gradient norm in None layer : 0.0008810378650913788
gradient norm in None layer : 3.8213559642125605e-05
gradient norm in None layer : 4.037765428112341e-05
gradient norm in None layer : 0.0002543579441426396
gradient norm in None layer : 9.768289641441194e-06
gradient norm in None layer : 7.775948349391029e-06
gradient norm in None layer : 0.0002683704865833743
gradient norm in None layer : 1.2108437027461749e-05
gradient norm in None layer : 1.0231961460620425e-05
gradient norm in None layer : 0.00041768342229462523
gradient norm in None layer : 3.919767909683234e-06
gradient norm in None layer : 0.0008623157737962926
gradient norm in None layer : 6.0585262930486686e-05
gradient norm in None layer : 4.5065757639470356e-05
gradient norm in None layer : 0.0010471631823570466
gradient norm in None layer : 7.664689965275813e-05
gradient norm in None layer : 7.834085089720508e-05
gradient norm in None layer : 0.0019238119849382435
gradient norm in None layer : 8.750482748465974e-06
gradient norm in None layer : 0.001940508974382227
gradient norm in None layer : 0.00019494599360343385
gradient norm in None layer : 0.0001572974937529307
gradient norm in None layer : 0.002565302300697381
gradient norm in None layer : 0.0002328099747474703
gradient norm in None layer : 0.0003990751147934609
gradient norm in None layer : 0.000199502724755977
gradient norm in None layer : 6.476550081159526e-05
Total gradient norm: 0.007460868689010843
invariance loss : 5.091564348840244, avg_den : 0.410400390625, density loss : 0.30933424710626534, mse loss : 0.15836833758792954, solver time : 121.9217779636383 sec , total loss : 0.16376923618387607, running loss : 0.1108840699975385
Epoch 0/10 , batch 13/12500 
ITERATION : 1, loss : 0.06767489695332775ITERATION : 2, loss : 0.054030618731774005ITERATION : 3, loss : 0.048305654806712ITERATION : 4, loss : 0.04589715663068806ITERATION : 5, loss : 0.044870479494298586ITERATION : 6, loss : 0.044446497609104865ITERATION : 7, loss : 0.044268144292365884ITERATION : 8, loss : 0.044184694639950076ITERATION : 9, loss : 0.04413569480886044ITERATION : 10, loss : 0.044098797817701874ITERATION : 11, loss : 0.044066839911468154ITERATION : 12, loss : 0.04403814655535083ITERATION : 13, loss : 0.04401266761460435ITERATION : 14, loss : 0.04399058730482089ITERATION : 15, loss : 0.04397192443448008ITERATION : 16, loss : 0.043956492831671364ITERATION : 17, loss : 0.0439439651534221ITERATION : 18, loss : 0.04393394742126608ITERATION : 19, loss : 0.04392603580232076ITERATION : 20, loss : 0.04391985110441495ITERATION : 21, loss : 0.04391505721259213ITERATION : 22, loss : 0.043911367764324305ITERATION : 23, loss : 0.0439085451770285ITERATION : 24, loss : 0.04390639670798988ITERATION : 25, loss : 0.043904768437071306ITERATION : 26, loss : 0.043903539003678126ITERATION : 27, loss : 0.04390261363660507ITERATION : 28, loss : 0.04390191910372759ITERATION : 29, loss : 0.04390139907531251ITERATION : 30, loss : 0.04390101057069585ITERATION : 31, loss : 0.04390072083099571ITERATION : 32, loss : 0.043900505139454656ITERATION : 33, loss : 0.04390034478797473ITERATION : 34, loss : 0.04390022570835045ITERATION : 35, loss : 0.04390013740590005ITERATION : 36, loss : 0.04390007195394877ITERATION : 37, loss : 0.0439000235056383ITERATION : 38, loss : 0.04389998766678775ITERATION : 39, loss : 0.04389996118917649ITERATION : 40, loss : 0.043899941636588245ITERATION : 41, loss : 0.04389992719731963ITERATION : 42, loss : 0.04389991656446894ITERATION : 43, loss : 0.043899908724362675ITERATION : 44, loss : 0.043899902932907284ITERATION : 45, loss : 0.04389989866223241ITERATION : 46, loss : 0.04389989552151912ITERATION : 47, loss : 0.04389989319001148ITERATION : 48, loss : 0.04389989149042966ITERATION : 49, loss : 0.04389989023589592ITERATION : 50, loss : 0.04389988935419931ITERATION : 51, loss : 0.04389988871420998ITERATION : 52, loss : 0.043899888160206735ITERATION : 53, loss : 0.0438998878506259ITERATION : 54, loss : 0.04389988756774248ITERATION : 55, loss : 0.0438998873734715ITERATION : 56, loss : 0.04389988723491304ITERATION : 57, loss : 0.04389988713029852ITERATION : 58, loss : 0.04389988708053145ITERATION : 59, loss : 0.04389988701414875ITERATION : 60, loss : 0.04389988701387347ITERATION : 61, loss : 0.04389988701387347ITERATION : 62, loss : 0.04389988701387347ITERATION : 63, loss : 0.04389988701387347ITERATION : 64, loss : 0.04389988701387347ITERATION : 65, loss : 0.04389988701387347ITERATION : 66, loss : 0.04389988701387347ITERATION : 67, loss : 0.04389988701387347ITERATION : 68, loss : 0.04389988701387347ITERATION : 69, loss : 0.04389988701387347ITERATION : 70, loss : 0.04389988701387347ITERATION : 71, loss : 0.04389988701387347ITERATION : 72, loss : 0.04389988701387347ITERATION : 73, loss : 0.04389988701387347ITERATION : 74, loss : 0.04389988701387347ITERATION : 75, loss : 0.04389988701387347ITERATION : 76, loss : 0.04389988701387347ITERATION : 77, loss : 0.04389988701387347ITERATION : 78, loss : 0.04389988701387347ITERATION : 79, loss : 0.04389988701387347ITERATION : 80, loss : 0.04389988701387347ITERATION : 81, loss : 0.04389988701387347ITERATION : 82, loss : 0.04389988701387347ITERATION : 83, loss : 0.04389988701387347ITERATION : 84, loss : 0.04389988701387347ITERATION : 85, loss : 0.04389988701387347ITERATION : 86, loss : 0.04389988701387347ITERATION : 87, loss : 0.04389988701387347ITERATION : 88, loss : 0.04389988701387347ITERATION : 89, loss : 0.04389988701387347ITERATION : 90, loss : 0.04389988701387347ITERATION : 91, loss : 0.04389988701387347ITERATION : 92, loss : 0.04389988701387347ITERATION : 93, loss : 0.04389988701387347ITERATION : 94, loss : 0.04389988701387347ITERATION : 95, loss : 0.04389988701387347ITERATION : 96, loss : 0.04389988701387347ITERATION : 97, loss : 0.04389988701387347ITERATION : 98, loss : 0.04389988701387347ITERATION : 99, loss : 0.04389988701387347ITERATION : 100, loss : 0.04389988701387347
ITERATION : 1, loss : 0.02007935642107984ITERATION : 2, loss : 0.028310027203780425ITERATION : 3, loss : 0.035227552700700555ITERATION : 4, loss : 0.04023572538180493ITERATION : 5, loss : 0.04374751769394726ITERATION : 6, loss : 0.046198508172216955ITERATION : 7, loss : 0.04791204206160913ITERATION : 8, loss : 0.049113644754188585ITERATION : 9, loss : 0.04995866439771903ITERATION : 10, loss : 0.05055427666097319ITERATION : 11, loss : 0.05097480664520295ITERATION : 12, loss : 0.051272077251154624ITERATION : 13, loss : 0.051482387540142185ITERATION : 14, loss : 0.05163125375464722ITERATION : 15, loss : 0.051736660098384096ITERATION : 16, loss : 0.051811305719070305ITERATION : 17, loss : 0.05186416988216108ITERATION : 18, loss : 0.051901607072865494ITERATION : 19, loss : 0.051928116799117255ITERATION : 20, loss : 0.05194688636277397ITERATION : 21, loss : 0.05196017350426231ITERATION : 22, loss : 0.051969577982211516ITERATION : 23, loss : 0.05197623309849816ITERATION : 24, loss : 0.05198094173091523ITERATION : 25, loss : 0.051984272562415744ITERATION : 26, loss : 0.051986628242432616ITERATION : 27, loss : 0.05198829389782699ITERATION : 28, loss : 0.051989471355004625ITERATION : 29, loss : 0.05199030365088766ITERATION : 30, loss : 0.05199089181050258ITERATION : 31, loss : 0.05199130734880348ITERATION : 32, loss : 0.05199160078960921ITERATION : 33, loss : 0.05199180808754124ITERATION : 34, loss : 0.05199195446157253ITERATION : 35, loss : 0.051992057759059386ITERATION : 36, loss : 0.051992130690697566ITERATION : 37, loss : 0.05199218214168388ITERATION : 38, loss : 0.05199221843788789ITERATION : 39, loss : 0.05199224408046329ITERATION : 40, loss : 0.05199226215745519ITERATION : 41, loss : 0.05199227487653543ITERATION : 42, loss : 0.05199228383176157ITERATION : 43, loss : 0.05199229013736996ITERATION : 44, loss : 0.051992294571452276ITERATION : 45, loss : 0.05199229768975047ITERATION : 46, loss : 0.05199229987492671ITERATION : 47, loss : 0.05199230141962921ITERATION : 48, loss : 0.051992302493687186ITERATION : 49, loss : 0.051992303227281ITERATION : 50, loss : 0.051992303784894243ITERATION : 51, loss : 0.05199230416514998ITERATION : 52, loss : 0.05199230439897054ITERATION : 53, loss : 0.05199230452476673ITERATION : 54, loss : 0.05199230468778254ITERATION : 55, loss : 0.051992304748718174ITERATION : 56, loss : 0.051992304774253144ITERATION : 57, loss : 0.05199230486149391ITERATION : 58, loss : 0.05199230486149391ITERATION : 59, loss : 0.05199230486149391ITERATION : 60, loss : 0.05199230486149391ITERATION : 61, loss : 0.05199230486149391ITERATION : 62, loss : 0.05199230486149391ITERATION : 63, loss : 0.05199230486149391ITERATION : 64, loss : 0.05199230486149391ITERATION : 65, loss : 0.05199230486149391ITERATION : 66, loss : 0.05199230486149391ITERATION : 67, loss : 0.05199230486149391ITERATION : 68, loss : 0.05199230486149391ITERATION : 69, loss : 0.05199230486149391ITERATION : 70, loss : 0.05199230486149391ITERATION : 71, loss : 0.05199230486149391ITERATION : 72, loss : 0.05199230486149391ITERATION : 73, loss : 0.05199230486149391ITERATION : 74, loss : 0.05199230486149391ITERATION : 75, loss : 0.05199230486149391ITERATION : 76, loss : 0.05199230486149391ITERATION : 77, loss : 0.05199230486149391ITERATION : 78, loss : 0.05199230486149391ITERATION : 79, loss : 0.05199230486149391ITERATION : 80, loss : 0.05199230486149391ITERATION : 81, loss : 0.05199230486149391ITERATION : 82, loss : 0.05199230486149391ITERATION : 83, loss : 0.05199230486149391ITERATION : 84, loss : 0.05199230486149391ITERATION : 85, loss : 0.05199230486149391ITERATION : 86, loss : 0.05199230486149391ITERATION : 87, loss : 0.05199230486149391ITERATION : 88, loss : 0.05199230486149391ITERATION : 89, loss : 0.05199230486149391ITERATION : 90, loss : 0.05199230486149391ITERATION : 91, loss : 0.05199230486149391ITERATION : 92, loss : 0.05199230486149391ITERATION : 93, loss : 0.05199230486149391ITERATION : 94, loss : 0.05199230486149391ITERATION : 95, loss : 0.05199230486149391ITERATION : 96, loss : 0.05199230486149391ITERATION : 97, loss : 0.05199230486149391ITERATION : 98, loss : 0.05199230486149391ITERATION : 99, loss : 0.05199230486149391ITERATION : 100, loss : 0.05199230486149391
ITERATION : 1, loss : 0.019971371200819498ITERATION : 2, loss : 0.023944328960668858ITERATION : 3, loss : 0.027607348801946187ITERATION : 4, loss : 0.03047640029610213ITERATION : 5, loss : 0.032536843599221016ITERATION : 6, loss : 0.03396911951809988ITERATION : 7, loss : 0.0349469540985666ITERATION : 8, loss : 0.035607472734716816ITERATION : 9, loss : 0.03605070353824062ITERATION : 10, loss : 0.036346835193928465ITERATION : 11, loss : 0.03654408646750557ITERATION : 12, loss : 0.03667517662402026ITERATION : 13, loss : 0.03676213935947806ITERATION : 14, loss : 0.036819739491174744ITERATION : 15, loss : 0.03685783752268916ITERATION : 16, loss : 0.036883002620315276ITERATION : 17, loss : 0.03689960298136115ITERATION : 18, loss : 0.036910538721907714ITERATION : 19, loss : 0.03691773257450277ITERATION : 20, loss : 0.03692245792452781ITERATION : 21, loss : 0.03692555681746923ITERATION : 22, loss : 0.03692758563832662ITERATION : 23, loss : 0.036928911327528346ITERATION : 24, loss : 0.036929775865382264ITERATION : 25, loss : 0.03693033839266799ITERATION : 26, loss : 0.03693070346259479ITERATION : 27, loss : 0.036930939778239184ITERATION : 28, loss : 0.036931092270266516ITERATION : 29, loss : 0.03693119039713445ITERATION : 30, loss : 0.036931253198992396ITERATION : 31, loss : 0.03693129326044451ITERATION : 32, loss : 0.03693131866133306ITERATION : 33, loss : 0.03693133473128786ITERATION : 34, loss : 0.03693134476302847ITERATION : 35, loss : 0.03693135100495268ITERATION : 36, loss : 0.036931354861852256ITERATION : 37, loss : 0.03693135722200783ITERATION : 38, loss : 0.03693135864749679ITERATION : 39, loss : 0.036931359435266445ITERATION : 40, loss : 0.036931359921396276ITERATION : 41, loss : 0.03693136016653769ITERATION : 42, loss : 0.03693136026667476ITERATION : 43, loss : 0.036931360296312735ITERATION : 44, loss : 0.03693136028670781ITERATION : 45, loss : 0.03693136026511254ITERATION : 46, loss : 0.03693136018680252ITERATION : 47, loss : 0.03693136016848365ITERATION : 48, loss : 0.03693136015961767ITERATION : 49, loss : 0.036931360140948215ITERATION : 50, loss : 0.03693136014096831ITERATION : 51, loss : 0.036931360128539065ITERATION : 52, loss : 0.036931360128830165ITERATION : 53, loss : 0.036931360128830165ITERATION : 54, loss : 0.036931360128830165ITERATION : 55, loss : 0.036931360128830165ITERATION : 56, loss : 0.036931360128830165ITERATION : 57, loss : 0.036931360128830165ITERATION : 58, loss : 0.036931360128830165ITERATION : 59, loss : 0.036931360128830165ITERATION : 60, loss : 0.036931360128830165ITERATION : 61, loss : 0.036931360128830165ITERATION : 62, loss : 0.036931360128830165ITERATION : 63, loss : 0.036931360128830165ITERATION : 64, loss : 0.036931360128830165ITERATION : 65, loss : 0.036931360128830165ITERATION : 66, loss : 0.036931360128830165ITERATION : 67, loss : 0.036931360128830165ITERATION : 68, loss : 0.036931360128830165ITERATION : 69, loss : 0.036931360128830165ITERATION : 70, loss : 0.036931360128830165ITERATION : 71, loss : 0.036931360128830165ITERATION : 72, loss : 0.036931360128830165ITERATION : 73, loss : 0.036931360128830165ITERATION : 74, loss : 0.036931360128830165ITERATION : 75, loss : 0.036931360128830165ITERATION : 76, loss : 0.036931360128830165ITERATION : 77, loss : 0.036931360128830165ITERATION : 78, loss : 0.036931360128830165ITERATION : 79, loss : 0.036931360128830165ITERATION : 80, loss : 0.036931360128830165ITERATION : 81, loss : 0.036931360128830165ITERATION : 82, loss : 0.036931360128830165ITERATION : 83, loss : 0.036931360128830165ITERATION : 84, loss : 0.036931360128830165ITERATION : 85, loss : 0.036931360128830165ITERATION : 86, loss : 0.036931360128830165ITERATION : 87, loss : 0.036931360128830165ITERATION : 88, loss : 0.036931360128830165ITERATION : 89, loss : 0.036931360128830165ITERATION : 90, loss : 0.036931360128830165ITERATION : 91, loss : 0.036931360128830165ITERATION : 92, loss : 0.036931360128830165ITERATION : 93, loss : 0.036931360128830165ITERATION : 94, loss : 0.036931360128830165ITERATION : 95, loss : 0.036931360128830165ITERATION : 96, loss : 0.036931360128830165ITERATION : 97, loss : 0.036931360128830165ITERATION : 98, loss : 0.036931360128830165ITERATION : 99, loss : 0.036931360128830165ITERATION : 100, loss : 0.036931360128830165
ITERATION : 1, loss : 0.05896285073238744ITERATION : 2, loss : 0.07805386904902033ITERATION : 3, loss : 0.08852102701652508ITERATION : 4, loss : 0.09512974689435955ITERATION : 5, loss : 0.09927801632429746ITERATION : 6, loss : 0.10198572557945225ITERATION : 7, loss : 0.10389889743736185ITERATION : 8, loss : 0.10527059365570553ITERATION : 9, loss : 0.10626454509884183ITERATION : 10, loss : 0.1069904524423244ITERATION : 11, loss : 0.10752371797576976ITERATION : 12, loss : 0.10791719026212054ITERATION : 13, loss : 0.10820847425732012ITERATION : 14, loss : 0.10842464350031533ITERATION : 15, loss : 0.10858536646207262ITERATION : 16, loss : 0.10870503161769335ITERATION : 17, loss : 0.10879422083686176ITERATION : 18, loss : 0.10886074791471288ITERATION : 19, loss : 0.10891040040108421ITERATION : 20, loss : 0.10894747499578192ITERATION : 21, loss : 0.10897516715845988ITERATION : 22, loss : 0.10899585647190389ITERATION : 23, loss : 0.10901131671847203ITERATION : 24, loss : 0.10902287115936063ITERATION : 25, loss : 0.10903150746591586ITERATION : 26, loss : 0.10903796315358473ITERATION : 27, loss : 0.10904278910286531ITERATION : 28, loss : 0.10904639690680094ITERATION : 29, loss : 0.10904909414967864ITERATION : 30, loss : 0.10905111068218935ITERATION : 31, loss : 0.10905261833189953ITERATION : 32, loss : 0.10905374554830749ITERATION : 33, loss : 0.10905458832654698ITERATION : 34, loss : 0.1090552184654784ITERATION : 35, loss : 0.10905568958938668ITERATION : 36, loss : 0.10905604181147767ITERATION : 37, loss : 0.10905630519137995ITERATION : 38, loss : 0.10905650210940174ITERATION : 39, loss : 0.1090566493520968ITERATION : 40, loss : 0.10905675943986577ITERATION : 41, loss : 0.10905684175431561ITERATION : 42, loss : 0.10905690331537926ITERATION : 43, loss : 0.10905694933090372ITERATION : 44, loss : 0.10905698375376048ITERATION : 45, loss : 0.1090570094569116ITERATION : 46, loss : 0.10905702871354565ITERATION : 47, loss : 0.10905704308092612ITERATION : 48, loss : 0.10905705384417883ITERATION : 49, loss : 0.10905706189757026ITERATION : 50, loss : 0.10905706791563718ITERATION : 51, loss : 0.1090570723999064ITERATION : 52, loss : 0.10905707573894563ITERATION : 53, loss : 0.10905707825573738ITERATION : 54, loss : 0.10905708012774436ITERATION : 55, loss : 0.10905708153821324ITERATION : 56, loss : 0.10905708259408715ITERATION : 57, loss : 0.10905708336753418ITERATION : 58, loss : 0.10905708394983968ITERATION : 59, loss : 0.10905708438269834ITERATION : 60, loss : 0.10905708470227032ITERATION : 61, loss : 0.10905708493377593ITERATION : 62, loss : 0.10905708510250106ITERATION : 63, loss : 0.10905708524092748ITERATION : 64, loss : 0.1090570853341515ITERATION : 65, loss : 0.10905708541560627ITERATION : 66, loss : 0.10905708546870893ITERATION : 67, loss : 0.10905708550535444ITERATION : 68, loss : 0.10905708553033533ITERATION : 69, loss : 0.10905708556786431ITERATION : 70, loss : 0.10905708558287862ITERATION : 71, loss : 0.10905708561036194ITERATION : 72, loss : 0.10905708561061796ITERATION : 73, loss : 0.10905708561061796ITERATION : 74, loss : 0.10905708561061796ITERATION : 75, loss : 0.10905708561061796ITERATION : 76, loss : 0.10905708561061796ITERATION : 77, loss : 0.10905708561061796ITERATION : 78, loss : 0.10905708561061796ITERATION : 79, loss : 0.10905708561061796ITERATION : 80, loss : 0.10905708561061796ITERATION : 81, loss : 0.10905708561061796ITERATION : 82, loss : 0.10905708561061796ITERATION : 83, loss : 0.10905708561061796ITERATION : 84, loss : 0.10905708561061796ITERATION : 85, loss : 0.10905708561061796ITERATION : 86, loss : 0.10905708561061796ITERATION : 87, loss : 0.10905708561061796ITERATION : 88, loss : 0.10905708561061796ITERATION : 89, loss : 0.10905708561061796ITERATION : 90, loss : 0.10905708561061796ITERATION : 91, loss : 0.10905708561061796ITERATION : 92, loss : 0.10905708561061796ITERATION : 93, loss : 0.10905708561061796ITERATION : 94, loss : 0.10905708561061796ITERATION : 95, loss : 0.10905708561061796ITERATION : 96, loss : 0.10905708561061796ITERATION : 97, loss : 0.10905708561061796ITERATION : 98, loss : 0.10905708561061796ITERATION : 99, loss : 0.10905708561061796ITERATION : 100, loss : 0.10905708561061796
ITERATION : 1, loss : 0.12003970396023203ITERATION : 2, loss : 0.1472948987433032ITERATION : 3, loss : 0.1625119727531098ITERATION : 4, loss : 0.1711849006750638ITERATION : 5, loss : 0.17620164372044325ITERATION : 6, loss : 0.178993925642889ITERATION : 7, loss : 0.18076683893455867ITERATION : 8, loss : 0.18190201559223468ITERATION : 9, loss : 0.18262994546646044ITERATION : 10, loss : 0.18309606997335437ITERATION : 11, loss : 0.18339342970200304ITERATION : 12, loss : 0.18358202074114965ITERATION : 13, loss : 0.18370068109579132ITERATION : 14, loss : 0.18377457465119446ITERATION : 15, loss : 0.18381998331420082ITERATION : 16, loss : 0.183847409005555ITERATION : 17, loss : 0.18386359393307344ITERATION : 18, loss : 0.1838728401639971ITERATION : 19, loss : 0.18387787175219947ITERATION : 20, loss : 0.18388039715517104ITERATION : 21, loss : 0.1838814753161423ITERATION : 22, loss : 0.18388175271092352ITERATION : 23, loss : 0.18388161643888531ITERATION : 24, loss : 0.183881292254787ITERATION : 25, loss : 0.18388090683119887ITERATION : 26, loss : 0.18388052717327188ITERATION : 27, loss : 0.18388018521982397ITERATION : 28, loss : 0.18387989298698976ITERATION : 29, loss : 0.18387965173725657ITERATION : 30, loss : 0.18387945737811423ITERATION : 31, loss : 0.1838793036087031ITERATION : 32, loss : 0.18387918365293132ITERATION : 33, loss : 0.183879091104205ITERATION : 34, loss : 0.18387902035812423ITERATION : 35, loss : 0.1838789666628503ITERATION : 36, loss : 0.1838789262000825ITERATION : 37, loss : 0.18387889584530098ITERATION : 38, loss : 0.18387887319052132ITERATION : 39, loss : 0.18387885633791295ITERATION : 40, loss : 0.18387884386836895ITERATION : 41, loss : 0.1838788346328306ITERATION : 42, loss : 0.18387882784187015ITERATION : 43, loss : 0.18387882284733986ITERATION : 44, loss : 0.18387881921842705ITERATION : 45, loss : 0.1838788165545449ITERATION : 46, loss : 0.18387881460222225ITERATION : 47, loss : 0.18387881316002855ITERATION : 48, loss : 0.1838788121120509ITERATION : 49, loss : 0.18387881134939033ITERATION : 50, loss : 0.18387881079190302ITERATION : 51, loss : 0.1838788103881011ITERATION : 52, loss : 0.18387881009506019ITERATION : 53, loss : 0.18387880987518396ITERATION : 54, loss : 0.1838788097256484ITERATION : 55, loss : 0.18387880961271047ITERATION : 56, loss : 0.18387880954651784ITERATION : 57, loss : 0.18387880947781268ITERATION : 58, loss : 0.18387880944415944ITERATION : 59, loss : 0.18387880940836157ITERATION : 60, loss : 0.18387880937242576ITERATION : 61, loss : 0.18387880936208245ITERATION : 62, loss : 0.1838788093622015ITERATION : 63, loss : 0.1838788093622015ITERATION : 64, loss : 0.1838788093622015ITERATION : 65, loss : 0.1838788093622015ITERATION : 66, loss : 0.1838788093622015ITERATION : 67, loss : 0.1838788093622015ITERATION : 68, loss : 0.1838788093622015ITERATION : 69, loss : 0.1838788093622015ITERATION : 70, loss : 0.1838788093622015ITERATION : 71, loss : 0.1838788093622015ITERATION : 72, loss : 0.1838788093622015ITERATION : 73, loss : 0.1838788093622015ITERATION : 74, loss : 0.1838788093622015ITERATION : 75, loss : 0.1838788093622015ITERATION : 76, loss : 0.1838788093622015ITERATION : 77, loss : 0.1838788093622015ITERATION : 78, loss : 0.1838788093622015ITERATION : 79, loss : 0.1838788093622015ITERATION : 80, loss : 0.1838788093622015ITERATION : 81, loss : 0.1838788093622015ITERATION : 82, loss : 0.1838788093622015ITERATION : 83, loss : 0.1838788093622015ITERATION : 84, loss : 0.1838788093622015ITERATION : 85, loss : 0.1838788093622015ITERATION : 86, loss : 0.1838788093622015ITERATION : 87, loss : 0.1838788093622015ITERATION : 88, loss : 0.1838788093622015ITERATION : 89, loss : 0.1838788093622015ITERATION : 90, loss : 0.1838788093622015ITERATION : 91, loss : 0.1838788093622015ITERATION : 92, loss : 0.1838788093622015ITERATION : 93, loss : 0.1838788093622015ITERATION : 94, loss : 0.1838788093622015ITERATION : 95, loss : 0.1838788093622015ITERATION : 96, loss : 0.1838788093622015ITERATION : 97, loss : 0.1838788093622015ITERATION : 98, loss : 0.1838788093622015ITERATION : 99, loss : 0.1838788093622015ITERATION : 100, loss : 0.1838788093622015
ITERATION : 1, loss : 0.22727247465523912ITERATION : 2, loss : 0.2743296107094349ITERATION : 3, loss : 0.29716589774798613ITERATION : 4, loss : 0.3097937581873965ITERATION : 5, loss : 0.3173396528677485ITERATION : 6, loss : 0.3220682086523166ITERATION : 7, loss : 0.32512233521196005ITERATION : 8, loss : 0.32713436369933585ITERATION : 9, loss : 0.3284774311405883ITERATION : 10, loss : 0.32938195838171463ITERATION : 11, loss : 0.3299948548917664ITERATION : 12, loss : 0.3304119035194554ITERATION : 13, loss : 0.33069653419424666ITERATION : 14, loss : 0.33089120969944225ITERATION : 15, loss : 0.3310245719996757ITERATION : 16, loss : 0.33111604299607017ITERATION : 17, loss : 0.33117884185829016ITERATION : 18, loss : 0.33122199002748787ITERATION : 19, loss : 0.3312516563318508ITERATION : 20, loss : 0.3312720651783123ITERATION : 21, loss : 0.33128611278794334ITERATION : 22, loss : 0.3312957865915554ITERATION : 23, loss : 0.3313024513898537ITERATION : 24, loss : 0.331307045091753ITERATION : 25, loss : 0.3313102125794416ITERATION : 26, loss : 0.33131239748993235ITERATION : 27, loss : 0.3313139051935277ITERATION : 28, loss : 0.33131494597118005ITERATION : 29, loss : 0.3313156646627473ITERATION : 30, loss : 0.3313161611258842ITERATION : 31, loss : 0.3313165041750745ITERATION : 32, loss : 0.3313167413011848ITERATION : 33, loss : 0.3313169052640034ITERATION : 34, loss : 0.331317018653909ITERATION : 35, loss : 0.3313170971083351ITERATION : 36, loss : 0.3313171513966881ITERATION : 37, loss : 0.3313171889796325ITERATION : 38, loss : 0.33131721499701ITERATION : 39, loss : 0.33131723301964316ITERATION : 40, loss : 0.33131724550518427ITERATION : 41, loss : 0.3313172541545951ITERATION : 42, loss : 0.3313172601495521ITERATION : 43, loss : 0.33131726430340963ITERATION : 44, loss : 0.3313172671830356ITERATION : 45, loss : 0.33131726919565246ITERATION : 46, loss : 0.33131727057584553ITERATION : 47, loss : 0.3313172715163426ITERATION : 48, loss : 0.3313172721758445ITERATION : 49, loss : 0.3313172726216054ITERATION : 50, loss : 0.3313172729355426ITERATION : 51, loss : 0.331317273149068ITERATION : 52, loss : 0.3313172733186467ITERATION : 53, loss : 0.33131727341659756ITERATION : 54, loss : 0.33131727348986567ITERATION : 55, loss : 0.3313172735377556ITERATION : 56, loss : 0.33131727358194424ITERATION : 57, loss : 0.3313172735970894ITERATION : 58, loss : 0.3313172736093949ITERATION : 59, loss : 0.33131727362292096ITERATION : 60, loss : 0.33131727363901364ITERATION : 61, loss : 0.3313172736431009ITERATION : 62, loss : 0.3313172736432763ITERATION : 63, loss : 0.3313172736432763ITERATION : 64, loss : 0.3313172736432763ITERATION : 65, loss : 0.3313172736432763ITERATION : 66, loss : 0.3313172736432763ITERATION : 67, loss : 0.3313172736432763ITERATION : 68, loss : 0.3313172736432763ITERATION : 69, loss : 0.3313172736432763ITERATION : 70, loss : 0.3313172736432763ITERATION : 71, loss : 0.3313172736432763ITERATION : 72, loss : 0.3313172736432763ITERATION : 73, loss : 0.3313172736432763ITERATION : 74, loss : 0.3313172736432763ITERATION : 75, loss : 0.3313172736432763ITERATION : 76, loss : 0.3313172736432763ITERATION : 77, loss : 0.3313172736432763ITERATION : 78, loss : 0.3313172736432763ITERATION : 79, loss : 0.3313172736432763ITERATION : 80, loss : 0.3313172736432763ITERATION : 81, loss : 0.3313172736432763ITERATION : 82, loss : 0.3313172736432763ITERATION : 83, loss : 0.3313172736432763ITERATION : 84, loss : 0.3313172736432763ITERATION : 85, loss : 0.3313172736432763ITERATION : 86, loss : 0.3313172736432763ITERATION : 87, loss : 0.3313172736432763ITERATION : 88, loss : 0.3313172736432763ITERATION : 89, loss : 0.3313172736432763ITERATION : 90, loss : 0.3313172736432763ITERATION : 91, loss : 0.3313172736432763ITERATION : 92, loss : 0.3313172736432763ITERATION : 93, loss : 0.3313172736432763ITERATION : 94, loss : 0.3313172736432763ITERATION : 95, loss : 0.3313172736432763ITERATION : 96, loss : 0.3313172736432763ITERATION : 97, loss : 0.3313172736432763ITERATION : 98, loss : 0.3313172736432763ITERATION : 99, loss : 0.3313172736432763ITERATION : 100, loss : 0.3313172736432763
ITERATION : 1, loss : 0.052640688756330566ITERATION : 2, loss : 0.07270342616161636ITERATION : 3, loss : 0.08568437541376303ITERATION : 4, loss : 0.09335445882672279ITERATION : 5, loss : 0.09792834388481114ITERATION : 6, loss : 0.10070282482164199ITERATION : 7, loss : 0.10240874376904928ITERATION : 8, loss : 0.10346727140094526ITERATION : 9, loss : 0.1041278177252967ITERATION : 10, loss : 0.10454134515793552ITERATION : 11, loss : 0.10480064536768813ITERATION : 12, loss : 0.10496332977439819ITERATION : 13, loss : 0.10506538733426803ITERATION : 14, loss : 0.10512937834612478ITERATION : 15, loss : 0.10516946968411353ITERATION : 16, loss : 0.1051945633225975ITERATION : 17, loss : 0.1052102525805109ITERATION : 18, loss : 0.10522005023681626ITERATION : 19, loss : 0.10522616086716466ITERATION : 20, loss : 0.10522996674333418ITERATION : 21, loss : 0.1052323336758229ITERATION : 22, loss : 0.10523380341372027ITERATION : 23, loss : 0.10523471447798975ITERATION : 24, loss : 0.10523527819814087ITERATION : 25, loss : 0.10523562629921279ITERATION : 26, loss : 0.10523584081377975ITERATION : 27, loss : 0.10523597268465901ITERATION : 28, loss : 0.10523605353497645ITERATION : 29, loss : 0.10523610296105376ITERATION : 30, loss : 0.1052361330657381ITERATION : 31, loss : 0.10523615134690284ITERATION : 32, loss : 0.10523616239127558ITERATION : 33, loss : 0.10523616904244407ITERATION : 34, loss : 0.10523617302744395ITERATION : 35, loss : 0.10523617540892659ITERATION : 36, loss : 0.10523617682745975ITERATION : 37, loss : 0.10523617764152618ITERATION : 38, loss : 0.1052361781379451ITERATION : 39, loss : 0.10523617840746174ITERATION : 40, loss : 0.10523617854556168ITERATION : 41, loss : 0.10523617861167765ITERATION : 42, loss : 0.10523617863647115ITERATION : 43, loss : 0.10523617865228894ITERATION : 44, loss : 0.10523617865373415ITERATION : 45, loss : 0.10523617865333083ITERATION : 46, loss : 0.10523617865128658ITERATION : 47, loss : 0.10523617865128658ITERATION : 48, loss : 0.10523617865128658ITERATION : 49, loss : 0.10523617865128658ITERATION : 50, loss : 0.10523617865128658ITERATION : 51, loss : 0.10523617865128658ITERATION : 52, loss : 0.10523617865128658ITERATION : 53, loss : 0.10523617865128658ITERATION : 54, loss : 0.10523617865128658ITERATION : 55, loss : 0.10523617865128658ITERATION : 56, loss : 0.10523617865128658ITERATION : 57, loss : 0.10523617865128658ITERATION : 58, loss : 0.10523617865128658ITERATION : 59, loss : 0.10523617865128658ITERATION : 60, loss : 0.10523617865128658ITERATION : 61, loss : 0.10523617865128658ITERATION : 62, loss : 0.10523617865128658ITERATION : 63, loss : 0.10523617865128658ITERATION : 64, loss : 0.10523617865128658ITERATION : 65, loss : 0.10523617865128658ITERATION : 66, loss : 0.10523617865128658ITERATION : 67, loss : 0.10523617865128658ITERATION : 68, loss : 0.10523617865128658ITERATION : 69, loss : 0.10523617865128658ITERATION : 70, loss : 0.10523617865128658ITERATION : 71, loss : 0.10523617865128658ITERATION : 72, loss : 0.10523617865128658ITERATION : 73, loss : 0.10523617865128658ITERATION : 74, loss : 0.10523617865128658ITERATION : 75, loss : 0.10523617865128658ITERATION : 76, loss : 0.10523617865128658ITERATION : 77, loss : 0.10523617865128658ITERATION : 78, loss : 0.10523617865128658ITERATION : 79, loss : 0.10523617865128658ITERATION : 80, loss : 0.10523617865128658ITERATION : 81, loss : 0.10523617865128658ITERATION : 82, loss : 0.10523617865128658ITERATION : 83, loss : 0.10523617865128658ITERATION : 84, loss : 0.10523617865128658ITERATION : 85, loss : 0.10523617865128658ITERATION : 86, loss : 0.10523617865128658ITERATION : 87, loss : 0.10523617865128658ITERATION : 88, loss : 0.10523617865128658ITERATION : 89, loss : 0.10523617865128658ITERATION : 90, loss : 0.10523617865128658ITERATION : 91, loss : 0.10523617865128658ITERATION : 92, loss : 0.10523617865128658ITERATION : 93, loss : 0.10523617865128658ITERATION : 94, loss : 0.10523617865128658ITERATION : 95, loss : 0.10523617865128658ITERATION : 96, loss : 0.10523617865128658ITERATION : 97, loss : 0.10523617865128658ITERATION : 98, loss : 0.10523617865128658ITERATION : 99, loss : 0.10523617865128658ITERATION : 100, loss : 0.10523617865128658
ITERATION : 1, loss : 0.05601906965158779ITERATION : 2, loss : 0.05859660642653993ITERATION : 3, loss : 0.06300669011080129ITERATION : 4, loss : 0.06766912943657927ITERATION : 5, loss : 0.07153011100929582ITERATION : 6, loss : 0.0743341593839134ITERATION : 7, loss : 0.07636418782899884ITERATION : 8, loss : 0.07783140257172445ITERATION : 9, loss : 0.07889079797767817ITERATION : 10, loss : 0.07965525072368083ITERATION : 11, loss : 0.08020664551089242ITERATION : 12, loss : 0.08060425206300006ITERATION : 13, loss : 0.08089091049630077ITERATION : 14, loss : 0.08109755631937503ITERATION : 15, loss : 0.08124651379808277ITERATION : 16, loss : 0.08135388528841672ITERATION : 17, loss : 0.08143128166861686ITERATION : 18, loss : 0.08148707309930002ITERATION : 19, loss : 0.08152729283886782ITERATION : 20, loss : 0.08155628894589455ITERATION : 21, loss : 0.08157719495066784ITERATION : 22, loss : 0.08159226945422808ITERATION : 23, loss : 0.08160314011471541ITERATION : 24, loss : 0.08161097981129149ITERATION : 25, loss : 0.08161663453784006ITERATION : 26, loss : 0.0816207133967705ITERATION : 27, loss : 0.08162365607996828ITERATION : 28, loss : 0.08162577903455347ITERATION : 29, loss : 0.08162731080122894ITERATION : 30, loss : 0.08162841627573565ITERATION : 31, loss : 0.08162921412137304ITERATION : 32, loss : 0.08162978995573276ITERATION : 33, loss : 0.08163020558857914ITERATION : 34, loss : 0.08163050562083776ITERATION : 35, loss : 0.08163072222422724ITERATION : 36, loss : 0.08163087867963001ITERATION : 37, loss : 0.08163099157123398ITERATION : 38, loss : 0.08163107317137167ITERATION : 39, loss : 0.08163113208732611ITERATION : 40, loss : 0.08163117460399538ITERATION : 41, loss : 0.08163120540609627ITERATION : 42, loss : 0.08163122766070488ITERATION : 43, loss : 0.08163124368183428ITERATION : 44, loss : 0.08163125523647731ITERATION : 45, loss : 0.08163126351850926ITERATION : 46, loss : 0.08163126953136678ITERATION : 47, loss : 0.08163127383179199ITERATION : 48, loss : 0.08163127696814826ITERATION : 49, loss : 0.08163127918264593ITERATION : 50, loss : 0.08163128081473674ITERATION : 51, loss : 0.08163128207411174ITERATION : 52, loss : 0.0816312828992367ITERATION : 53, loss : 0.08163128344082807ITERATION : 54, loss : 0.08163128389565943ITERATION : 55, loss : 0.08163128424675112ITERATION : 56, loss : 0.08163128446252814ITERATION : 57, loss : 0.08163128461588537ITERATION : 58, loss : 0.08163128472676916ITERATION : 59, loss : 0.08163128474592884ITERATION : 60, loss : 0.08163128476335012ITERATION : 61, loss : 0.08163128476335012ITERATION : 62, loss : 0.08163128476335012ITERATION : 63, loss : 0.08163128476335012ITERATION : 64, loss : 0.08163128476335012ITERATION : 65, loss : 0.08163128476335012ITERATION : 66, loss : 0.08163128476335012ITERATION : 67, loss : 0.08163128476335012ITERATION : 68, loss : 0.08163128476335012ITERATION : 69, loss : 0.08163128476335012ITERATION : 70, loss : 0.08163128476335012ITERATION : 71, loss : 0.08163128476335012ITERATION : 72, loss : 0.08163128476335012ITERATION : 73, loss : 0.08163128476335012ITERATION : 74, loss : 0.08163128476335012ITERATION : 75, loss : 0.08163128476335012ITERATION : 76, loss : 0.08163128476335012ITERATION : 77, loss : 0.08163128476335012ITERATION : 78, loss : 0.08163128476335012ITERATION : 79, loss : 0.08163128476335012ITERATION : 80, loss : 0.08163128476335012ITERATION : 81, loss : 0.08163128476335012ITERATION : 82, loss : 0.08163128476335012ITERATION : 83, loss : 0.08163128476335012ITERATION : 84, loss : 0.08163128476335012ITERATION : 85, loss : 0.08163128476335012ITERATION : 86, loss : 0.08163128476335012ITERATION : 87, loss : 0.08163128476335012ITERATION : 88, loss : 0.08163128476335012ITERATION : 89, loss : 0.08163128476335012ITERATION : 90, loss : 0.08163128476335012ITERATION : 91, loss : 0.08163128476335012ITERATION : 92, loss : 0.08163128476335012ITERATION : 93, loss : 0.08163128476335012ITERATION : 94, loss : 0.08163128476335012ITERATION : 95, loss : 0.08163128476335012ITERATION : 96, loss : 0.08163128476335012ITERATION : 97, loss : 0.08163128476335012ITERATION : 98, loss : 0.08163128476335012ITERATION : 99, loss : 0.08163128476335012ITERATION : 100, loss : 0.08163128476335012
gradient norm in None layer : 0.002387167582194247
gradient norm in None layer : 5.5003304661013324e-05
gradient norm in None layer : 9.642332147646528e-05
gradient norm in None layer : 0.0009969237141051305
gradient norm in None layer : 7.579367015145002e-05
gradient norm in None layer : 0.0001001283115041655
gradient norm in None layer : 0.0004576190898141584
gradient norm in None layer : 1.6726693098685743e-05
gradient norm in None layer : 2.1044762098548212e-05
gradient norm in None layer : 0.0003506251716914927
gradient norm in None layer : 1.9503746763942525e-05
gradient norm in None layer : 1.9487419857919412e-05
gradient norm in None layer : 0.0001156236468591234
gradient norm in None layer : 3.72228572697929e-06
gradient norm in None layer : 3.317890172605491e-06
gradient norm in None layer : 0.00010571058303196304
gradient norm in None layer : 4.405007443684884e-06
gradient norm in None layer : 3.691283185401335e-06
gradient norm in None layer : 0.00014209015987900675
gradient norm in None layer : 2.4612972784017127e-06
gradient norm in None layer : 0.0003542759174014261
gradient norm in None layer : 2.092553611307708e-05
gradient norm in None layer : 1.703704547920599e-05
gradient norm in None layer : 0.00036605273001773264
gradient norm in None layer : 3.813587384793954e-05
gradient norm in None layer : 5.5382226737583095e-05
gradient norm in None layer : 0.0006678237436286293
gradient norm in None layer : 5.697398674031498e-06
gradient norm in None layer : 0.0009940579256266565
gradient norm in None layer : 5.714946191091712e-05
gradient norm in None layer : 7.51795830562712e-05
gradient norm in None layer : 0.0009750156357733974
gradient norm in None layer : 0.00015471238036327608
gradient norm in None layer : 0.00025819054585874584
gradient norm in None layer : 0.000111307463035594
gradient norm in None layer : 3.898770508850633e-05
Total gradient norm: 0.0031404078373936275
invariance loss : 4.634379213846746, avg_den : 0.43337249755859375, density loss : 0.33327924189274283, mse loss : 0.11799302300436625, solver time : 117.2442877292633 sec , total loss : 0.12296068146010573, running loss : 0.11181304011004367
Epoch 0/10 , batch 14/12500 
ITERATION : 1, loss : 0.03818830344027659ITERATION : 2, loss : 0.052114495262025676ITERATION : 3, loss : 0.0645776941130854ITERATION : 4, loss : 0.07334429185374186ITERATION : 5, loss : 0.07924685216665742ITERATION : 6, loss : 0.08318449798074924ITERATION : 7, loss : 0.0858108785684171ITERATION : 8, loss : 0.0875666295534606ITERATION : 9, loss : 0.08874345457429378ITERATION : 10, loss : 0.08953410006406798ITERATION : 11, loss : 0.09006631020091813ITERATION : 12, loss : 0.09042509569430068ITERATION : 13, loss : 0.09066724717679213ITERATION : 14, loss : 0.09083082467354268ITERATION : 15, loss : 0.09094139980175404ITERATION : 16, loss : 0.09101618678184237ITERATION : 17, loss : 0.09106679067688765ITERATION : 18, loss : 0.09110104377739353ITERATION : 19, loss : 0.09112423657570064ITERATION : 20, loss : 0.09113994492604384ITERATION : 21, loss : 0.09115058683160368ITERATION : 22, loss : 0.09115779820334445ITERATION : 23, loss : 0.09116268611947302ITERATION : 24, loss : 0.09116599998269002ITERATION : 25, loss : 0.09116824724170544ITERATION : 26, loss : 0.09116977156303069ITERATION : 27, loss : 0.09117080581278263ITERATION : 28, loss : 0.09117150771723342ITERATION : 29, loss : 0.09117198421246858ITERATION : 30, loss : 0.09117230775870401ITERATION : 31, loss : 0.0911725275277444ITERATION : 32, loss : 0.09117267682461612ITERATION : 33, loss : 0.09117277833653979ITERATION : 34, loss : 0.09117284734950683ITERATION : 35, loss : 0.09117289428019022ITERATION : 36, loss : 0.09117292621335496ITERATION : 37, loss : 0.0911729479595062ITERATION : 38, loss : 0.09117296279507378ITERATION : 39, loss : 0.09117297287733003ITERATION : 40, loss : 0.09117297977448416ITERATION : 41, loss : 0.09117298445927687ITERATION : 42, loss : 0.09117298772258535ITERATION : 43, loss : 0.09117298993129741ITERATION : 44, loss : 0.09117299138522993ITERATION : 45, loss : 0.09117299239347257ITERATION : 46, loss : 0.09117299306617206ITERATION : 47, loss : 0.09117299352349423ITERATION : 48, loss : 0.09117299382924011ITERATION : 49, loss : 0.09117299404239804ITERATION : 50, loss : 0.09117299415619928ITERATION : 51, loss : 0.09117299426005063ITERATION : 52, loss : 0.09117299433217471ITERATION : 53, loss : 0.09117299436124358ITERATION : 54, loss : 0.09117299439518989ITERATION : 55, loss : 0.09117299441610636ITERATION : 56, loss : 0.09117299442439196ITERATION : 57, loss : 0.09117299443175834ITERATION : 58, loss : 0.0911729944321591ITERATION : 59, loss : 0.0911729944321591ITERATION : 60, loss : 0.0911729944321591ITERATION : 61, loss : 0.0911729944321591ITERATION : 62, loss : 0.0911729944321591ITERATION : 63, loss : 0.0911729944321591ITERATION : 64, loss : 0.0911729944321591ITERATION : 65, loss : 0.0911729944321591ITERATION : 66, loss : 0.0911729944321591ITERATION : 67, loss : 0.0911729944321591ITERATION : 68, loss : 0.0911729944321591ITERATION : 69, loss : 0.0911729944321591ITERATION : 70, loss : 0.0911729944321591ITERATION : 71, loss : 0.0911729944321591ITERATION : 72, loss : 0.0911729944321591ITERATION : 73, loss : 0.0911729944321591ITERATION : 74, loss : 0.0911729944321591ITERATION : 75, loss : 0.0911729944321591ITERATION : 76, loss : 0.0911729944321591ITERATION : 77, loss : 0.0911729944321591ITERATION : 78, loss : 0.0911729944321591ITERATION : 79, loss : 0.0911729944321591ITERATION : 80, loss : 0.0911729944321591ITERATION : 81, loss : 0.0911729944321591ITERATION : 82, loss : 0.0911729944321591ITERATION : 83, loss : 0.0911729944321591ITERATION : 84, loss : 0.0911729944321591ITERATION : 85, loss : 0.0911729944321591ITERATION : 86, loss : 0.0911729944321591ITERATION : 87, loss : 0.0911729944321591ITERATION : 88, loss : 0.0911729944321591ITERATION : 89, loss : 0.0911729944321591ITERATION : 90, loss : 0.0911729944321591ITERATION : 91, loss : 0.0911729944321591ITERATION : 92, loss : 0.0911729944321591ITERATION : 93, loss : 0.0911729944321591ITERATION : 94, loss : 0.0911729944321591ITERATION : 95, loss : 0.0911729944321591ITERATION : 96, loss : 0.0911729944321591ITERATION : 97, loss : 0.0911729944321591ITERATION : 98, loss : 0.0911729944321591ITERATION : 99, loss : 0.0911729944321591ITERATION : 100, loss : 0.0911729944321591
ITERATION : 1, loss : 0.10287354866054019ITERATION : 2, loss : 0.11553357714235203ITERATION : 3, loss : 0.12580929829934417ITERATION : 4, loss : 0.13301158304357624ITERATION : 5, loss : 0.13796870359991387ITERATION : 6, loss : 0.14137881293967636ITERATION : 7, loss : 0.14372980896810963ITERATION : 8, loss : 0.14535424056235757ITERATION : 9, loss : 0.14647870922444986ITERATION : 10, loss : 0.14725820270456325ITERATION : 11, loss : 0.14779913775832187ITERATION : 12, loss : 0.14817482166005394ITERATION : 13, loss : 0.14843588696550727ITERATION : 14, loss : 0.14861737553155102ITERATION : 15, loss : 0.14874357757849266ITERATION : 16, loss : 0.1488313502482954ITERATION : 17, loss : 0.14889240145994728ITERATION : 18, loss : 0.14893486830640928ITERATION : 19, loss : 0.14896440836555308ITERATION : 20, loss : 0.14898495596051647ITERATION : 21, loss : 0.14899924813044493ITERATION : 22, loss : 0.14900918863530696ITERATION : 23, loss : 0.14901610203482415ITERATION : 24, loss : 0.14902090989623828ITERATION : 25, loss : 0.14902425320478846ITERATION : 26, loss : 0.14902657797936128ITERATION : 27, loss : 0.14902819440215095ITERATION : 28, loss : 0.14902931822658538ITERATION : 29, loss : 0.149030099498261ITERATION : 30, loss : 0.14903064259924495ITERATION : 31, loss : 0.14903102011560737ITERATION : 32, loss : 0.14903128252517814ITERATION : 33, loss : 0.1490314648809485ITERATION : 34, loss : 0.14903159163772595ITERATION : 35, loss : 0.149031679699781ITERATION : 36, loss : 0.14903174091197116ITERATION : 37, loss : 0.14903178344050044ITERATION : 38, loss : 0.14903181302621168ITERATION : 39, loss : 0.14903183356883612ITERATION : 40, loss : 0.14903184783552725ITERATION : 41, loss : 0.14903185775327368ITERATION : 42, loss : 0.1490318646124558ITERATION : 43, loss : 0.14903186937728755ITERATION : 44, loss : 0.1490318726740143ITERATION : 45, loss : 0.14903187496605688ITERATION : 46, loss : 0.14903187651245958ITERATION : 47, loss : 0.14903187758618253ITERATION : 48, loss : 0.14903187841361532ITERATION : 49, loss : 0.1490318789098773ITERATION : 50, loss : 0.1490318792163915ITERATION : 51, loss : 0.14903187953860278ITERATION : 52, loss : 0.1490318797357055ITERATION : 53, loss : 0.14903187980637242ITERATION : 54, loss : 0.1490318798071833ITERATION : 55, loss : 0.1490318798071833ITERATION : 56, loss : 0.1490318798071833ITERATION : 57, loss : 0.1490318798071833ITERATION : 58, loss : 0.1490318798071833ITERATION : 59, loss : 0.1490318798071833ITERATION : 60, loss : 0.1490318798071833ITERATION : 61, loss : 0.1490318798071833ITERATION : 62, loss : 0.1490318798071833ITERATION : 63, loss : 0.1490318798071833ITERATION : 64, loss : 0.1490318798071833ITERATION : 65, loss : 0.1490318798071833ITERATION : 66, loss : 0.1490318798071833ITERATION : 67, loss : 0.1490318798071833ITERATION : 68, loss : 0.1490318798071833ITERATION : 69, loss : 0.1490318798071833ITERATION : 70, loss : 0.1490318798071833ITERATION : 71, loss : 0.1490318798071833ITERATION : 72, loss : 0.1490318798071833ITERATION : 73, loss : 0.1490318798071833ITERATION : 74, loss : 0.1490318798071833ITERATION : 75, loss : 0.1490318798071833ITERATION : 76, loss : 0.1490318798071833ITERATION : 77, loss : 0.1490318798071833ITERATION : 78, loss : 0.1490318798071833ITERATION : 79, loss : 0.1490318798071833ITERATION : 80, loss : 0.1490318798071833ITERATION : 81, loss : 0.1490318798071833ITERATION : 82, loss : 0.1490318798071833ITERATION : 83, loss : 0.1490318798071833ITERATION : 84, loss : 0.1490318798071833ITERATION : 85, loss : 0.1490318798071833ITERATION : 86, loss : 0.1490318798071833ITERATION : 87, loss : 0.1490318798071833ITERATION : 88, loss : 0.1490318798071833ITERATION : 89, loss : 0.1490318798071833ITERATION : 90, loss : 0.1490318798071833ITERATION : 91, loss : 0.1490318798071833ITERATION : 92, loss : 0.1490318798071833ITERATION : 93, loss : 0.1490318798071833ITERATION : 94, loss : 0.1490318798071833ITERATION : 95, loss : 0.1490318798071833ITERATION : 96, loss : 0.1490318798071833ITERATION : 97, loss : 0.1490318798071833ITERATION : 98, loss : 0.1490318798071833ITERATION : 99, loss : 0.1490318798071833ITERATION : 100, loss : 0.1490318798071833
ITERATION : 1, loss : 0.030910456284773465ITERATION : 2, loss : 0.03806760484806572ITERATION : 3, loss : 0.04379620932056904ITERATION : 4, loss : 0.04758249073705626ITERATION : 5, loss : 0.05011484515793011ITERATION : 6, loss : 0.05172403450637539ITERATION : 7, loss : 0.052708833529144246ITERATION : 8, loss : 0.053290286222418756ITERATION : 9, loss : 0.0536196908430093ITERATION : 10, loss : 0.05379619210006103ITERATION : 11, loss : 0.053882785093392144ITERATION : 12, loss : 0.05391846544356226ITERATION : 13, loss : 0.05392676508368368ITERATION : 14, loss : 0.053921489337451806ITERATION : 15, loss : 0.05391045953761175ITERATION : 16, loss : 0.05389789826928581ITERATION : 17, loss : 0.05388591953679585ITERATION : 18, loss : 0.05387544119563921ITERATION : 19, loss : 0.053866732979448134ITERATION : 20, loss : 0.05385973704175381ITERATION : 21, loss : 0.053854250457996064ITERATION : 22, loss : 0.05385002419146743ITERATION : 23, loss : 0.0538468136019413ITERATION : 24, loss : 0.05384440147749788ITERATION : 25, loss : 0.05384260533199854ITERATION : 26, loss : 0.05384127778136155ITERATION : 27, loss : 0.05384030266681415ITERATION : 28, loss : 0.053839590188178924ITERATION : 29, loss : 0.05383907202100137ITERATION : 30, loss : 0.05383869663657171ITERATION : 31, loss : 0.053838425637497335ITERATION : 32, loss : 0.053838230577100035ITERATION : 33, loss : 0.053838090564470975ITERATION : 34, loss : 0.053837990274048377ITERATION : 35, loss : 0.0538379185921921ITERATION : 36, loss : 0.05383786745321478ITERATION : 37, loss : 0.053837831065809215ITERATION : 38, loss : 0.05383780520541316ITERATION : 39, loss : 0.053837786860698365ITERATION : 40, loss : 0.05383777384875107ITERATION : 41, loss : 0.053837764640076266ITERATION : 42, loss : 0.053837758117027554ITERATION : 43, loss : 0.05383775352038901ITERATION : 44, loss : 0.05383775028146232ITERATION : 45, loss : 0.05383774797566028ITERATION : 46, loss : 0.053837746309393184ITERATION : 47, loss : 0.05383774518485944ITERATION : 48, loss : 0.053837744412561575ITERATION : 49, loss : 0.053837743851971605ITERATION : 50, loss : 0.05383774347767833ITERATION : 51, loss : 0.05383774319833638ITERATION : 52, loss : 0.05383774304171139ITERATION : 53, loss : 0.053837742898929966ITERATION : 54, loss : 0.05383774283263557ITERATION : 55, loss : 0.05383774276679591ITERATION : 56, loss : 0.05383774274874735ITERATION : 57, loss : 0.053837742747858186ITERATION : 58, loss : 0.053837742747858186ITERATION : 59, loss : 0.053837742747858186ITERATION : 60, loss : 0.053837742747858186ITERATION : 61, loss : 0.053837742747858186ITERATION : 62, loss : 0.053837742747858186ITERATION : 63, loss : 0.053837742747858186ITERATION : 64, loss : 0.053837742747858186ITERATION : 65, loss : 0.053837742747858186ITERATION : 66, loss : 0.053837742747858186ITERATION : 67, loss : 0.053837742747858186ITERATION : 68, loss : 0.053837742747858186ITERATION : 69, loss : 0.053837742747858186ITERATION : 70, loss : 0.053837742747858186ITERATION : 71, loss : 0.053837742747858186ITERATION : 72, loss : 0.053837742747858186ITERATION : 73, loss : 0.053837742747858186ITERATION : 74, loss : 0.053837742747858186ITERATION : 75, loss : 0.053837742747858186ITERATION : 76, loss : 0.053837742747858186ITERATION : 77, loss : 0.053837742747858186ITERATION : 78, loss : 0.053837742747858186ITERATION : 79, loss : 0.053837742747858186ITERATION : 80, loss : 0.053837742747858186ITERATION : 81, loss : 0.053837742747858186ITERATION : 82, loss : 0.053837742747858186ITERATION : 83, loss : 0.053837742747858186ITERATION : 84, loss : 0.053837742747858186ITERATION : 85, loss : 0.053837742747858186ITERATION : 86, loss : 0.053837742747858186ITERATION : 87, loss : 0.053837742747858186ITERATION : 88, loss : 0.053837742747858186ITERATION : 89, loss : 0.053837742747858186ITERATION : 90, loss : 0.053837742747858186ITERATION : 91, loss : 0.053837742747858186ITERATION : 92, loss : 0.053837742747858186ITERATION : 93, loss : 0.053837742747858186ITERATION : 94, loss : 0.053837742747858186ITERATION : 95, loss : 0.053837742747858186ITERATION : 96, loss : 0.053837742747858186ITERATION : 97, loss : 0.053837742747858186ITERATION : 98, loss : 0.053837742747858186ITERATION : 99, loss : 0.053837742747858186ITERATION : 100, loss : 0.053837742747858186
ITERATION : 1, loss : 0.0454018727627723ITERATION : 2, loss : 0.04301692770304153ITERATION : 3, loss : 0.04700168644975084ITERATION : 4, loss : 0.05243043090568181ITERATION : 5, loss : 0.056509872770306ITERATION : 6, loss : 0.05946334284951308ITERATION : 7, loss : 0.061576377044242694ITERATION : 8, loss : 0.06308261747819052ITERATION : 9, loss : 0.06415547927863934ITERATION : 10, loss : 0.06491980809648303ITERATION : 11, loss : 0.06546459269294869ITERATION : 12, loss : 0.06585308953704684ITERATION : 13, loss : 0.06613025707040142ITERATION : 14, loss : 0.06632807054875985ITERATION : 15, loss : 0.06646929061092023ITERATION : 16, loss : 0.06657013175949042ITERATION : 17, loss : 0.06664215275901307ITERATION : 18, loss : 0.06669359775689655ITERATION : 19, loss : 0.06673034950208302ITERATION : 20, loss : 0.06675660696641883ITERATION : 21, loss : 0.0667753682078481ITERATION : 22, loss : 0.06678877410399053ITERATION : 23, loss : 0.0667983539499758ITERATION : 24, loss : 0.0668051999876459ITERATION : 25, loss : 0.06681009256430662ITERATION : 26, loss : 0.06681358917962743ITERATION : 27, loss : 0.066816088176684ITERATION : 28, loss : 0.06681787424989004ITERATION : 29, loss : 0.06681915089080795ITERATION : 30, loss : 0.06682006339840672ITERATION : 31, loss : 0.0668207156253426ITERATION : 32, loss : 0.06682118185238813ITERATION : 33, loss : 0.06682151503344705ITERATION : 34, loss : 0.06682175329614082ITERATION : 35, loss : 0.0668219235307176ITERATION : 36, loss : 0.06682204529290237ITERATION : 37, loss : 0.06682213232155208ITERATION : 38, loss : 0.06682219444981159ITERATION : 39, loss : 0.06682223890665602ITERATION : 40, loss : 0.06682227071169891ITERATION : 41, loss : 0.06682229346751059ITERATION : 42, loss : 0.06682230975772654ITERATION : 43, loss : 0.06682232141693381ITERATION : 44, loss : 0.0668223297240487ITERATION : 45, loss : 0.06682233568333949ITERATION : 46, loss : 0.06682233993199914ITERATION : 47, loss : 0.06682234301948241ITERATION : 48, loss : 0.06682234521919678ITERATION : 49, loss : 0.06682234675901247ITERATION : 50, loss : 0.06682234784335087ITERATION : 51, loss : 0.06682234856563869ITERATION : 52, loss : 0.06682234917988913ITERATION : 53, loss : 0.06682234962638052ITERATION : 54, loss : 0.06682234986968202ITERATION : 55, loss : 0.06682235010985857ITERATION : 56, loss : 0.0668223502002945ITERATION : 57, loss : 0.0668223503318448ITERATION : 58, loss : 0.06682235034801254ITERATION : 59, loss : 0.06682235034801254ITERATION : 60, loss : 0.06682235034801254ITERATION : 61, loss : 0.06682235034801254ITERATION : 62, loss : 0.06682235034801254ITERATION : 63, loss : 0.06682235034801254ITERATION : 64, loss : 0.06682235034801254ITERATION : 65, loss : 0.06682235034801254ITERATION : 66, loss : 0.06682235034801254ITERATION : 67, loss : 0.06682235034801254ITERATION : 68, loss : 0.06682235034801254ITERATION : 69, loss : 0.06682235034801254ITERATION : 70, loss : 0.06682235034801254ITERATION : 71, loss : 0.06682235034801254ITERATION : 72, loss : 0.06682235034801254ITERATION : 73, loss : 0.06682235034801254ITERATION : 74, loss : 0.06682235034801254ITERATION : 75, loss : 0.06682235034801254ITERATION : 76, loss : 0.06682235034801254ITERATION : 77, loss : 0.06682235034801254ITERATION : 78, loss : 0.06682235034801254ITERATION : 79, loss : 0.06682235034801254ITERATION : 80, loss : 0.06682235034801254ITERATION : 81, loss : 0.06682235034801254ITERATION : 82, loss : 0.06682235034801254ITERATION : 83, loss : 0.06682235034801254ITERATION : 84, loss : 0.06682235034801254ITERATION : 85, loss : 0.06682235034801254ITERATION : 86, loss : 0.06682235034801254ITERATION : 87, loss : 0.06682235034801254ITERATION : 88, loss : 0.06682235034801254ITERATION : 89, loss : 0.06682235034801254ITERATION : 90, loss : 0.06682235034801254ITERATION : 91, loss : 0.06682235034801254ITERATION : 92, loss : 0.06682235034801254ITERATION : 93, loss : 0.06682235034801254ITERATION : 94, loss : 0.06682235034801254ITERATION : 95, loss : 0.06682235034801254ITERATION : 96, loss : 0.06682235034801254ITERATION : 97, loss : 0.06682235034801254ITERATION : 98, loss : 0.06682235034801254ITERATION : 99, loss : 0.06682235034801254ITERATION : 100, loss : 0.06682235034801254
ITERATION : 1, loss : 0.04495267070013908ITERATION : 2, loss : 0.06721601742015183ITERATION : 3, loss : 0.07945955195584371ITERATION : 4, loss : 0.08698884660233286ITERATION : 5, loss : 0.0920434534655121ITERATION : 6, loss : 0.09548611354731595ITERATION : 7, loss : 0.09785173512464203ITERATION : 8, loss : 0.09948576704786942ITERATION : 9, loss : 0.10061773303747343ITERATION : 10, loss : 0.10140301264077552ITERATION : 11, loss : 0.10194805313338275ITERATION : 12, loss : 0.10232631450731206ITERATION : 13, loss : 0.10258870967661088ITERATION : 14, loss : 0.10277060551939925ITERATION : 15, loss : 0.10289659488311195ITERATION : 16, loss : 0.10298378228608614ITERATION : 17, loss : 0.10304406053537926ITERATION : 18, loss : 0.10308569394560038ITERATION : 19, loss : 0.10311442075897104ITERATION : 20, loss : 0.1031342218163664ITERATION : 21, loss : 0.10314785626377418ITERATION : 22, loss : 0.10315723460761368ITERATION : 23, loss : 0.10316367839509342ITERATION : 24, loss : 0.10316810087253926ITERATION : 25, loss : 0.10317113266923558ITERATION : 26, loss : 0.10317320855007814ITERATION : 27, loss : 0.10317462820327404ITERATION : 28, loss : 0.10317559778222436ITERATION : 29, loss : 0.10317625901160651ITERATION : 30, loss : 0.10317670939550949ITERATION : 31, loss : 0.10317701571236232ITERATION : 32, loss : 0.10317722366086023ITERATION : 33, loss : 0.10317736459976448ITERATION : 34, loss : 0.10317745996385931ITERATION : 35, loss : 0.10317752438212698ITERATION : 36, loss : 0.10317756781024597ITERATION : 37, loss : 0.10317759700979551ITERATION : 38, loss : 0.10317761657038249ITERATION : 39, loss : 0.10317762964849421ITERATION : 40, loss : 0.10317763835497792ITERATION : 41, loss : 0.10317764414269481ITERATION : 42, loss : 0.10317764797399889ITERATION : 43, loss : 0.10317765052064103ITERATION : 44, loss : 0.1031776521790144ITERATION : 45, loss : 0.10317765330846207ITERATION : 46, loss : 0.10317765403210068ITERATION : 47, loss : 0.10317765448638515ITERATION : 48, loss : 0.10317765475715251ITERATION : 49, loss : 0.10317765492422595ITERATION : 50, loss : 0.10317765503426636ITERATION : 51, loss : 0.10317765509202899ITERATION : 52, loss : 0.10317765511547895ITERATION : 53, loss : 0.10317765513619244ITERATION : 54, loss : 0.10317765513608063ITERATION : 55, loss : 0.10317765514232037ITERATION : 56, loss : 0.10317765513545922ITERATION : 57, loss : 0.10317765514151554ITERATION : 58, loss : 0.1031776551414092ITERATION : 59, loss : 0.1031776551414092ITERATION : 60, loss : 0.1031776551414092ITERATION : 61, loss : 0.1031776551414092ITERATION : 62, loss : 0.1031776551414092ITERATION : 63, loss : 0.1031776551414092ITERATION : 64, loss : 0.1031776551414092ITERATION : 65, loss : 0.1031776551414092ITERATION : 66, loss : 0.1031776551414092ITERATION : 67, loss : 0.1031776551414092ITERATION : 68, loss : 0.1031776551414092ITERATION : 69, loss : 0.1031776551414092ITERATION : 70, loss : 0.1031776551414092ITERATION : 71, loss : 0.1031776551414092ITERATION : 72, loss : 0.1031776551414092ITERATION : 73, loss : 0.1031776551414092ITERATION : 74, loss : 0.1031776551414092ITERATION : 75, loss : 0.1031776551414092ITERATION : 76, loss : 0.1031776551414092ITERATION : 77, loss : 0.1031776551414092ITERATION : 78, loss : 0.1031776551414092ITERATION : 79, loss : 0.1031776551414092ITERATION : 80, loss : 0.1031776551414092ITERATION : 81, loss : 0.1031776551414092ITERATION : 82, loss : 0.1031776551414092ITERATION : 83, loss : 0.1031776551414092ITERATION : 84, loss : 0.1031776551414092ITERATION : 85, loss : 0.1031776551414092ITERATION : 86, loss : 0.1031776551414092ITERATION : 87, loss : 0.1031776551414092ITERATION : 88, loss : 0.1031776551414092ITERATION : 89, loss : 0.1031776551414092ITERATION : 90, loss : 0.1031776551414092ITERATION : 91, loss : 0.1031776551414092ITERATION : 92, loss : 0.1031776551414092ITERATION : 93, loss : 0.1031776551414092ITERATION : 94, loss : 0.1031776551414092ITERATION : 95, loss : 0.1031776551414092ITERATION : 96, loss : 0.1031776551414092ITERATION : 97, loss : 0.1031776551414092ITERATION : 98, loss : 0.1031776551414092ITERATION : 99, loss : 0.1031776551414092ITERATION : 100, loss : 0.1031776551414092
ITERATION : 1, loss : 0.012096019802561453ITERATION : 2, loss : 0.017346856111740826ITERATION : 3, loss : 0.02525361862667957ITERATION : 4, loss : 0.031621601567115125ITERATION : 5, loss : 0.03617334831851581ITERATION : 6, loss : 0.03931797134477303ITERATION : 7, loss : 0.04146539736045974ITERATION : 8, loss : 0.04292569922942752ITERATION : 9, loss : 0.043917310811576356ITERATION : 10, loss : 0.04448861320657116ITERATION : 11, loss : 0.04484529504040937ITERATION : 12, loss : 0.045088314926143645ITERATION : 13, loss : 0.04525392082021025ITERATION : 14, loss : 0.04536681750179377ITERATION : 15, loss : 0.04544382699165788ITERATION : 16, loss : 0.045496397267026055ITERATION : 17, loss : 0.04553231704944114ITERATION : 18, loss : 0.04555688549734921ITERATION : 19, loss : 0.04557370908473515ITERATION : 20, loss : 0.04558524336208249ITERATION : 21, loss : 0.04559316148172939ITERATION : 22, loss : 0.045598604527169344ITERATION : 23, loss : 0.045602351239340616ITERATION : 24, loss : 0.045604933919684267ITERATION : 25, loss : 0.04560671680513279ITERATION : 26, loss : 0.045607949259708604ITERATION : 27, loss : 0.04560880248515369ITERATION : 28, loss : 0.04560939399443899ITERATION : 29, loss : 0.04560980464251546ITERATION : 30, loss : 0.04561009012316123ITERATION : 31, loss : 0.04561028886896971ITERATION : 32, loss : 0.045610427392922016ITERATION : 33, loss : 0.04561052407893731ITERATION : 34, loss : 0.04561059166895036ITERATION : 35, loss : 0.045610638959346674ITERATION : 36, loss : 0.045610672093142166ITERATION : 37, loss : 0.04561069533699123ITERATION : 38, loss : 0.04561071165763368ITERATION : 39, loss : 0.04561072313582885ITERATION : 40, loss : 0.04561073121620952ITERATION : 41, loss : 0.0456107369057954ITERATION : 42, loss : 0.04561074092113288ITERATION : 43, loss : 0.04561074375628945ITERATION : 44, loss : 0.04561074573880529ITERATION : 45, loss : 0.04561074714335452ITERATION : 46, loss : 0.04561074813147195ITERATION : 47, loss : 0.04561074884103285ITERATION : 48, loss : 0.04561074931294162ITERATION : 49, loss : 0.04561074970269689ITERATION : 50, loss : 0.04561074992930457ITERATION : 51, loss : 0.04561075009781557ITERATION : 52, loss : 0.04561075019973148ITERATION : 53, loss : 0.045610750304498925ITERATION : 54, loss : 0.04561075036374044ITERATION : 55, loss : 0.04561075041593755ITERATION : 56, loss : 0.0456107504338986ITERATION : 57, loss : 0.04561075044510603ITERATION : 58, loss : 0.04561075044510603ITERATION : 59, loss : 0.04561075044510603ITERATION : 60, loss : 0.04561075044510603ITERATION : 61, loss : 0.04561075044510603ITERATION : 62, loss : 0.04561075044510603ITERATION : 63, loss : 0.04561075044510603ITERATION : 64, loss : 0.04561075044510603ITERATION : 65, loss : 0.04561075044510603ITERATION : 66, loss : 0.04561075044510603ITERATION : 67, loss : 0.04561075044510603ITERATION : 68, loss : 0.04561075044510603ITERATION : 69, loss : 0.04561075044510603ITERATION : 70, loss : 0.04561075044510603ITERATION : 71, loss : 0.04561075044510603ITERATION : 72, loss : 0.04561075044510603ITERATION : 73, loss : 0.04561075044510603ITERATION : 74, loss : 0.04561075044510603ITERATION : 75, loss : 0.04561075044510603ITERATION : 76, loss : 0.04561075044510603ITERATION : 77, loss : 0.04561075044510603ITERATION : 78, loss : 0.04561075044510603ITERATION : 79, loss : 0.04561075044510603ITERATION : 80, loss : 0.04561075044510603ITERATION : 81, loss : 0.04561075044510603ITERATION : 82, loss : 0.04561075044510603ITERATION : 83, loss : 0.04561075044510603ITERATION : 84, loss : 0.04561075044510603ITERATION : 85, loss : 0.04561075044510603ITERATION : 86, loss : 0.04561075044510603ITERATION : 87, loss : 0.04561075044510603ITERATION : 88, loss : 0.04561075044510603ITERATION : 89, loss : 0.04561075044510603ITERATION : 90, loss : 0.04561075044510603ITERATION : 91, loss : 0.04561075044510603ITERATION : 92, loss : 0.04561075044510603ITERATION : 93, loss : 0.04561075044510603ITERATION : 94, loss : 0.04561075044510603ITERATION : 95, loss : 0.04561075044510603ITERATION : 96, loss : 0.04561075044510603ITERATION : 97, loss : 0.04561075044510603ITERATION : 98, loss : 0.04561075044510603ITERATION : 99, loss : 0.04561075044510603ITERATION : 100, loss : 0.04561075044510603
ITERATION : 1, loss : 0.05485247861043401ITERATION : 2, loss : 0.07084383589105957ITERATION : 3, loss : 0.08496794721997934ITERATION : 4, loss : 0.09441005575922971ITERATION : 5, loss : 0.10042787326018118ITERATION : 6, loss : 0.10424392833743255ITERATION : 7, loss : 0.10665646028745553ITERATION : 8, loss : 0.10817933087697991ITERATION : 9, loss : 0.10913904362961975ITERATION : 10, loss : 0.10974268733751044ITERATION : 11, loss : 0.11012156288196603ITERATION : 12, loss : 0.11035883789127118ITERATION : 13, loss : 0.11050710299489896ITERATION : 14, loss : 0.11059954283861138ITERATION : 15, loss : 0.11065704871450169ITERATION : 16, loss : 0.11069274198193381ITERATION : 17, loss : 0.11071484512865536ITERATION : 18, loss : 0.11072849931259383ITERATION : 19, loss : 0.11073691229683326ITERATION : 20, loss : 0.11074208117489005ITERATION : 21, loss : 0.11074524695099705ITERATION : 22, loss : 0.11074717902160548ITERATION : 23, loss : 0.11074835337082214ITERATION : 24, loss : 0.11074906378131098ITERATION : 25, loss : 0.1107494911878178ITERATION : 26, loss : 0.11074974661632332ITERATION : 27, loss : 0.11074989803263306ITERATION : 28, loss : 0.1107499869328041ITERATION : 29, loss : 0.11075003844325676ITERATION : 30, loss : 0.11075006777930592ITERATION : 31, loss : 0.1107500841513981ITERATION : 32, loss : 0.11075009306225025ITERATION : 33, loss : 0.11075009760048016ITERATION : 34, loss : 0.1107500997608609ITERATION : 35, loss : 0.11075010067481264ITERATION : 36, loss : 0.11075010093375795ITERATION : 37, loss : 0.11075010081771652ITERATION : 38, loss : 0.11075010064628286ITERATION : 39, loss : 0.11075010044938814ITERATION : 40, loss : 0.11075010022081913ITERATION : 41, loss : 0.11075009999474501ITERATION : 42, loss : 0.11075009984024305ITERATION : 43, loss : 0.11075009968759496ITERATION : 44, loss : 0.11075009956807932ITERATION : 45, loss : 0.1107500994729031ITERATION : 46, loss : 0.11075009944686949ITERATION : 47, loss : 0.11075009937588076ITERATION : 48, loss : 0.11075009938305849ITERATION : 49, loss : 0.11075009938305849ITERATION : 50, loss : 0.11075009938305849ITERATION : 51, loss : 0.11075009938305849ITERATION : 52, loss : 0.11075009938305849ITERATION : 53, loss : 0.11075009938305849ITERATION : 54, loss : 0.11075009938305849ITERATION : 55, loss : 0.11075009938305849ITERATION : 56, loss : 0.11075009938305849ITERATION : 57, loss : 0.11075009938305849ITERATION : 58, loss : 0.11075009938305849ITERATION : 59, loss : 0.11075009938305849ITERATION : 60, loss : 0.11075009938305849ITERATION : 61, loss : 0.11075009938305849ITERATION : 62, loss : 0.11075009938305849ITERATION : 63, loss : 0.11075009938305849ITERATION : 64, loss : 0.11075009938305849ITERATION : 65, loss : 0.11075009938305849ITERATION : 66, loss : 0.11075009938305849ITERATION : 67, loss : 0.11075009938305849ITERATION : 68, loss : 0.11075009938305849ITERATION : 69, loss : 0.11075009938305849ITERATION : 70, loss : 0.11075009938305849ITERATION : 71, loss : 0.11075009938305849ITERATION : 72, loss : 0.11075009938305849ITERATION : 73, loss : 0.11075009938305849ITERATION : 74, loss : 0.11075009938305849ITERATION : 75, loss : 0.11075009938305849ITERATION : 76, loss : 0.11075009938305849ITERATION : 77, loss : 0.11075009938305849ITERATION : 78, loss : 0.11075009938305849ITERATION : 79, loss : 0.11075009938305849ITERATION : 80, loss : 0.11075009938305849ITERATION : 81, loss : 0.11075009938305849ITERATION : 82, loss : 0.11075009938305849ITERATION : 83, loss : 0.11075009938305849ITERATION : 84, loss : 0.11075009938305849ITERATION : 85, loss : 0.11075009938305849ITERATION : 86, loss : 0.11075009938305849ITERATION : 87, loss : 0.11075009938305849ITERATION : 88, loss : 0.11075009938305849ITERATION : 89, loss : 0.11075009938305849ITERATION : 90, loss : 0.11075009938305849ITERATION : 91, loss : 0.11075009938305849ITERATION : 92, loss : 0.11075009938305849ITERATION : 93, loss : 0.11075009938305849ITERATION : 94, loss : 0.11075009938305849ITERATION : 95, loss : 0.11075009938305849ITERATION : 96, loss : 0.11075009938305849ITERATION : 97, loss : 0.11075009938305849ITERATION : 98, loss : 0.11075009938305849ITERATION : 99, loss : 0.11075009938305849ITERATION : 100, loss : 0.11075009938305849
ITERATION : 1, loss : 0.010943799524536538ITERATION : 2, loss : 0.012958713993962478ITERATION : 3, loss : 0.016147359881592774ITERATION : 4, loss : 0.018943924473660848ITERATION : 5, loss : 0.02115161947038795ITERATION : 6, loss : 0.022492064427491097ITERATION : 7, loss : 0.02350336544014159ITERATION : 8, loss : 0.02425002260548038ITERATION : 9, loss : 0.02479351530225075ITERATION : 10, loss : 0.02518542816563363ITERATION : 11, loss : 0.025466274279761794ITERATION : 12, loss : 0.02566668622850323ITERATION : 13, loss : 0.025809295801582723ITERATION : 14, loss : 0.02591058037012804ITERATION : 15, loss : 0.02598242253425087ITERATION : 16, loss : 0.026033337092413445ITERATION : 17, loss : 0.026069400016526183ITERATION : 18, loss : 0.026094934335010016ITERATION : 19, loss : 0.026113009963132244ITERATION : 20, loss : 0.026125804166820383ITERATION : 21, loss : 0.026134859625125722ITERATION : 22, loss : 0.02614126890775113ITERATION : 23, loss : 0.026145805390676947ITERATION : 24, loss : 0.02614901646604559ITERATION : 25, loss : 0.02615128956109881ITERATION : 26, loss : 0.026152898711079058ITERATION : 27, loss : 0.02615403797160742ITERATION : 28, loss : 0.026154844673067803ITERATION : 29, loss : 0.02615541592305242ITERATION : 30, loss : 0.026155820488815627ITERATION : 31, loss : 0.026156107023755478ITERATION : 32, loss : 0.02615630998799776ITERATION : 33, loss : 0.02615645376408618ITERATION : 34, loss : 0.026156555627491643ITERATION : 35, loss : 0.026156627790850793ITERATION : 36, loss : 0.026156678908833653ITERATION : 37, loss : 0.026156715149066354ITERATION : 38, loss : 0.026156740815721984ITERATION : 39, loss : 0.026156759014630163ITERATION : 40, loss : 0.02615677188463393ITERATION : 41, loss : 0.026156781027384366ITERATION : 42, loss : 0.02615678750017645ITERATION : 43, loss : 0.02615679209307176ITERATION : 44, loss : 0.02615679535933689ITERATION : 45, loss : 0.026156797674748405ITERATION : 46, loss : 0.026156799273368184ITERATION : 47, loss : 0.026156800419478113ITERATION : 48, loss : 0.026156801233404182ITERATION : 49, loss : 0.026156801823652164ITERATION : 50, loss : 0.026156802247741324ITERATION : 51, loss : 0.026156802554062285ITERATION : 52, loss : 0.02615680274717723ITERATION : 53, loss : 0.026156802910382977ITERATION : 54, loss : 0.026156802981916888ITERATION : 55, loss : 0.026156803037209766ITERATION : 56, loss : 0.02615680303831758ITERATION : 57, loss : 0.02615680303831758ITERATION : 58, loss : 0.02615680303831758ITERATION : 59, loss : 0.02615680303831758ITERATION : 60, loss : 0.02615680303831758ITERATION : 61, loss : 0.02615680303831758ITERATION : 62, loss : 0.02615680303831758ITERATION : 63, loss : 0.02615680303831758ITERATION : 64, loss : 0.02615680303831758ITERATION : 65, loss : 0.02615680303831758ITERATION : 66, loss : 0.02615680303831758ITERATION : 67, loss : 0.02615680303831758ITERATION : 68, loss : 0.02615680303831758ITERATION : 69, loss : 0.02615680303831758ITERATION : 70, loss : 0.02615680303831758ITERATION : 71, loss : 0.02615680303831758ITERATION : 72, loss : 0.02615680303831758ITERATION : 73, loss : 0.02615680303831758ITERATION : 74, loss : 0.02615680303831758ITERATION : 75, loss : 0.02615680303831758ITERATION : 76, loss : 0.02615680303831758ITERATION : 77, loss : 0.02615680303831758ITERATION : 78, loss : 0.02615680303831758ITERATION : 79, loss : 0.02615680303831758ITERATION : 80, loss : 0.02615680303831758ITERATION : 81, loss : 0.02615680303831758ITERATION : 82, loss : 0.02615680303831758ITERATION : 83, loss : 0.02615680303831758ITERATION : 84, loss : 0.02615680303831758ITERATION : 85, loss : 0.02615680303831758ITERATION : 86, loss : 0.02615680303831758ITERATION : 87, loss : 0.02615680303831758ITERATION : 88, loss : 0.02615680303831758ITERATION : 89, loss : 0.02615680303831758ITERATION : 90, loss : 0.02615680303831758ITERATION : 91, loss : 0.02615680303831758ITERATION : 92, loss : 0.02615680303831758ITERATION : 93, loss : 0.02615680303831758ITERATION : 94, loss : 0.02615680303831758ITERATION : 95, loss : 0.02615680303831758ITERATION : 96, loss : 0.02615680303831758ITERATION : 97, loss : 0.02615680303831758ITERATION : 98, loss : 0.02615680303831758ITERATION : 99, loss : 0.02615680303831758ITERATION : 100, loss : 0.02615680303831758
gradient norm in None layer : 0.0018766123646949348
gradient norm in None layer : 4.448883907455688e-05
gradient norm in None layer : 4.024374586294142e-05
gradient norm in None layer : 0.000761188256867937
gradient norm in None layer : 5.3285224557461204e-05
gradient norm in None layer : 6.033574294810978e-05
gradient norm in None layer : 0.0003296156119802265
gradient norm in None layer : 1.3059691885706434e-05
gradient norm in None layer : 1.1474177717033166e-05
gradient norm in None layer : 0.00029178645431189305
gradient norm in None layer : 1.3106720473682453e-05
gradient norm in None layer : 9.721594759486685e-06
gradient norm in None layer : 0.00010282192361185246
gradient norm in None layer : 3.545861626977997e-06
gradient norm in None layer : 3.2939258906396763e-06
gradient norm in None layer : 0.00010740250030066368
gradient norm in None layer : 4.091289416441451e-06
gradient norm in None layer : 4.045755627208503e-06
gradient norm in None layer : 0.00015121623772429362
gradient norm in None layer : 2.208816452986192e-06
gradient norm in None layer : 0.00028219290084161724
gradient norm in None layer : 2.0413573218645208e-05
gradient norm in None layer : 1.5849840222918616e-05
gradient norm in None layer : 0.0003105390048177908
gradient norm in None layer : 3.307050466683077e-05
gradient norm in None layer : 4.40312583162927e-05
gradient norm in None layer : 0.0005584808402826351
gradient norm in None layer : 6.96697012741079e-06
gradient norm in None layer : 0.0007282842731204129
gradient norm in None layer : 4.712336701500831e-05
gradient norm in None layer : 4.9732354118080295e-05
gradient norm in None layer : 0.0008272054574712758
gradient norm in None layer : 0.0001426059610561879
gradient norm in None layer : 0.00021837085098523723
gradient norm in None layer : 0.00010458859364683912
gradient norm in None layer : 3.456652072572094e-05
Total gradient norm: 0.002478262188367261
invariance loss : 4.603079159717658, avg_den : 0.45076751708984375, density loss : 0.35082497352486564, mse loss : 0.08082003441788806, solver time : 114.35855674743652 sec , total loss : 0.08577393855113058, running loss : 0.10995310428440701
Epoch 0/10 , batch 15/12500 
ITERATION : 1, loss : 0.06800574265781456ITERATION : 2, loss : 0.051405931515511365ITERATION : 3, loss : 0.049389606821948644ITERATION : 4, loss : 0.04878143793761576ITERATION : 5, loss : 0.04890055054120813ITERATION : 6, loss : 0.04922474321548125ITERATION : 7, loss : 0.049571925246563155ITERATION : 8, loss : 0.0498805164895922ITERATION : 9, loss : 0.05013473201093049ITERATION : 10, loss : 0.05033602369534895ITERATION : 11, loss : 0.0504916377066711ITERATION : 12, loss : 0.05061000907803745ITERATION : 13, loss : 0.05069898731953491ITERATION : 14, loss : 0.05076525309815986ITERATION : 15, loss : 0.05081423162629057ITERATION : 16, loss : 0.05085020318657158ITERATION : 17, loss : 0.050876478510585896ITERATION : 18, loss : 0.05089558062781363ITERATION : 19, loss : 0.05090941048645123ITERATION : 20, loss : 0.050919386832103286ITERATION : 21, loss : 0.050926560010570616ITERATION : 22, loss : 0.05093170285480648ITERATION : 23, loss : 0.050935380503962104ITERATION : 24, loss : 0.05093800429771937ITERATION : 25, loss : 0.05093987238353476ITERATION : 26, loss : 0.05094119975709397ITERATION : 27, loss : 0.05094214138696407ITERATION : 28, loss : 0.05094280830438877ITERATION : 29, loss : 0.05094327999834893ITERATION : 30, loss : 0.05094361324007179ITERATION : 31, loss : 0.0509438483706822ITERATION : 32, loss : 0.050944014118221255ITERATION : 33, loss : 0.05094413080691544ITERATION : 34, loss : 0.05094421287590724ITERATION : 35, loss : 0.0509442705594743ITERATION : 36, loss : 0.05094431103813247ITERATION : 37, loss : 0.05094433945672938ITERATION : 38, loss : 0.05094435938969323ITERATION : 39, loss : 0.05094437336038365ITERATION : 40, loss : 0.050944383140429435ITERATION : 41, loss : 0.05094438999731576ITERATION : 42, loss : 0.05094439476525863ITERATION : 43, loss : 0.050944398139191986ITERATION : 44, loss : 0.05094440043682724ITERATION : 45, loss : 0.05094440210851085ITERATION : 46, loss : 0.05094440318929033ITERATION : 47, loss : 0.05094440397075394ITERATION : 48, loss : 0.05094440450994324ITERATION : 49, loss : 0.05094440494607568ITERATION : 50, loss : 0.05094440518362362ITERATION : 51, loss : 0.05094440539601202ITERATION : 52, loss : 0.050944405516261275ITERATION : 53, loss : 0.0509444056007181ITERATION : 54, loss : 0.05094440561572246ITERATION : 55, loss : 0.0509444056161422ITERATION : 56, loss : 0.0509444056161422ITERATION : 57, loss : 0.0509444056161422ITERATION : 58, loss : 0.0509444056161422ITERATION : 59, loss : 0.0509444056161422ITERATION : 60, loss : 0.0509444056161422ITERATION : 61, loss : 0.0509444056161422ITERATION : 62, loss : 0.0509444056161422ITERATION : 63, loss : 0.0509444056161422ITERATION : 64, loss : 0.0509444056161422ITERATION : 65, loss : 0.0509444056161422ITERATION : 66, loss : 0.0509444056161422ITERATION : 67, loss : 0.0509444056161422ITERATION : 68, loss : 0.0509444056161422ITERATION : 69, loss : 0.0509444056161422ITERATION : 70, loss : 0.0509444056161422ITERATION : 71, loss : 0.0509444056161422ITERATION : 72, loss : 0.0509444056161422ITERATION : 73, loss : 0.0509444056161422ITERATION : 74, loss : 0.0509444056161422ITERATION : 75, loss : 0.0509444056161422ITERATION : 76, loss : 0.0509444056161422ITERATION : 77, loss : 0.0509444056161422ITERATION : 78, loss : 0.0509444056161422ITERATION : 79, loss : 0.0509444056161422ITERATION : 80, loss : 0.0509444056161422ITERATION : 81, loss : 0.0509444056161422ITERATION : 82, loss : 0.0509444056161422ITERATION : 83, loss : 0.0509444056161422ITERATION : 84, loss : 0.0509444056161422ITERATION : 85, loss : 0.0509444056161422ITERATION : 86, loss : 0.0509444056161422ITERATION : 87, loss : 0.0509444056161422ITERATION : 88, loss : 0.0509444056161422ITERATION : 89, loss : 0.0509444056161422ITERATION : 90, loss : 0.0509444056161422ITERATION : 91, loss : 0.0509444056161422ITERATION : 92, loss : 0.0509444056161422ITERATION : 93, loss : 0.0509444056161422ITERATION : 94, loss : 0.0509444056161422ITERATION : 95, loss : 0.0509444056161422ITERATION : 96, loss : 0.0509444056161422ITERATION : 97, loss : 0.0509444056161422ITERATION : 98, loss : 0.0509444056161422ITERATION : 99, loss : 0.0509444056161422ITERATION : 100, loss : 0.0509444056161422
ITERATION : 1, loss : 0.020946852860472185ITERATION : 2, loss : 0.02469554911378864ITERATION : 3, loss : 0.030365513911678643ITERATION : 4, loss : 0.034884872570032484ITERATION : 5, loss : 0.038097357945481414ITERATION : 6, loss : 0.04029901233807257ITERATION : 7, loss : 0.04178770972406194ITERATION : 8, loss : 0.04278877569875476ITERATION : 9, loss : 0.043460137117513835ITERATION : 10, loss : 0.043909617681815685ITERATION : 11, loss : 0.04421011425130056ITERATION : 12, loss : 0.04441071775813659ITERATION : 13, loss : 0.044544427048588006ITERATION : 14, loss : 0.04463339791667092ITERATION : 15, loss : 0.04469248992400682ITERATION : 16, loss : 0.044731657796431006ITERATION : 17, loss : 0.044757561858431945ITERATION : 18, loss : 0.04477465216210043ITERATION : 19, loss : 0.044785897373653676ITERATION : 20, loss : 0.04479327456899317ITERATION : 21, loss : 0.044798098089852995ITERATION : 22, loss : 0.04480124010595307ITERATION : 23, loss : 0.04480327802078515ITERATION : 24, loss : 0.044804593342856996ITERATION : 25, loss : 0.044805437405473617ITERATION : 26, loss : 0.044805975424876916ITERATION : 27, loss : 0.04480631560916362ITERATION : 28, loss : 0.044806528613689486ITERATION : 29, loss : 0.04480666039259005ITERATION : 30, loss : 0.04480674064679427ITERATION : 31, loss : 0.04480678855153865ITERATION : 32, loss : 0.04480681637363468ITERATION : 33, loss : 0.04480683188833915ITERATION : 34, loss : 0.04480684002974873ITERATION : 35, loss : 0.04480684382279731ITERATION : 36, loss : 0.044806845196862254ITERATION : 37, loss : 0.044806845230098605ITERATION : 38, loss : 0.04480684461596555ITERATION : 39, loss : 0.04480684372797057ITERATION : 40, loss : 0.044806842834735065ITERATION : 41, loss : 0.044806842024251115ITERATION : 42, loss : 0.04480684123252586ITERATION : 43, loss : 0.04480684067073701ITERATION : 44, loss : 0.04480684018362873ITERATION : 45, loss : 0.044806839772546375ITERATION : 46, loss : 0.04480683943471506ITERATION : 47, loss : 0.044806839181188336ITERATION : 48, loss : 0.04480683896726272ITERATION : 49, loss : 0.044806838811948954ITERATION : 50, loss : 0.0448068386841569ITERATION : 51, loss : 0.04480683860734853ITERATION : 52, loss : 0.044806838536758396ITERATION : 53, loss : 0.0448068384892921ITERATION : 54, loss : 0.0448068384414548ITERATION : 55, loss : 0.04480683841896898ITERATION : 56, loss : 0.044806838419618085ITERATION : 57, loss : 0.044806838419618085ITERATION : 58, loss : 0.044806838419618085ITERATION : 59, loss : 0.044806838419618085ITERATION : 60, loss : 0.044806838419618085ITERATION : 61, loss : 0.044806838419618085ITERATION : 62, loss : 0.044806838419618085ITERATION : 63, loss : 0.044806838419618085ITERATION : 64, loss : 0.044806838419618085ITERATION : 65, loss : 0.044806838419618085ITERATION : 66, loss : 0.044806838419618085ITERATION : 67, loss : 0.044806838419618085ITERATION : 68, loss : 0.044806838419618085ITERATION : 69, loss : 0.044806838419618085ITERATION : 70, loss : 0.044806838419618085ITERATION : 71, loss : 0.044806838419618085ITERATION : 72, loss : 0.044806838419618085ITERATION : 73, loss : 0.044806838419618085ITERATION : 74, loss : 0.044806838419618085ITERATION : 75, loss : 0.044806838419618085ITERATION : 76, loss : 0.044806838419618085ITERATION : 77, loss : 0.044806838419618085ITERATION : 78, loss : 0.044806838419618085ITERATION : 79, loss : 0.044806838419618085ITERATION : 80, loss : 0.044806838419618085ITERATION : 81, loss : 0.044806838419618085ITERATION : 82, loss : 0.044806838419618085ITERATION : 83, loss : 0.044806838419618085ITERATION : 84, loss : 0.044806838419618085ITERATION : 85, loss : 0.044806838419618085ITERATION : 86, loss : 0.044806838419618085ITERATION : 87, loss : 0.044806838419618085ITERATION : 88, loss : 0.044806838419618085ITERATION : 89, loss : 0.044806838419618085ITERATION : 90, loss : 0.044806838419618085ITERATION : 91, loss : 0.044806838419618085ITERATION : 92, loss : 0.044806838419618085ITERATION : 93, loss : 0.044806838419618085ITERATION : 94, loss : 0.044806838419618085ITERATION : 95, loss : 0.044806838419618085ITERATION : 96, loss : 0.044806838419618085ITERATION : 97, loss : 0.044806838419618085ITERATION : 98, loss : 0.044806838419618085ITERATION : 99, loss : 0.044806838419618085ITERATION : 100, loss : 0.044806838419618085
ITERATION : 1, loss : 0.12349026578001784ITERATION : 2, loss : 0.15720797377946333ITERATION : 3, loss : 0.17742661406385107ITERATION : 4, loss : 0.1897933232459407ITERATION : 5, loss : 0.19720947834991878ITERATION : 6, loss : 0.20210109634462498ITERATION : 7, loss : 0.20545645232437112ITERATION : 8, loss : 0.20779041967097506ITERATION : 9, loss : 0.2094292626758855ITERATION : 10, loss : 0.21058732859514306ITERATION : 11, loss : 0.21140915435869595ITERATION : 12, loss : 0.21199402519255706ITERATION : 13, loss : 0.21241103823175522ITERATION : 14, loss : 0.21270872203758648ITERATION : 15, loss : 0.2129213759176615ITERATION : 16, loss : 0.21307334747007264ITERATION : 17, loss : 0.21318197094126437ITERATION : 18, loss : 0.2132596116423261ITERATION : 19, loss : 0.21331510124901304ITERATION : 20, loss : 0.21335475255239153ITERATION : 21, loss : 0.21338307976363702ITERATION : 22, loss : 0.21340331155067194ITERATION : 23, loss : 0.2134177572717703ITERATION : 24, loss : 0.2134280684875811ITERATION : 25, loss : 0.21343542614396216ITERATION : 26, loss : 0.21344067456059668ITERATION : 27, loss : 0.2134444170831289ITERATION : 28, loss : 0.21344708490433204ITERATION : 29, loss : 0.2134489859940258ITERATION : 30, loss : 0.21345034020034323ITERATION : 31, loss : 0.2134513045023885ITERATION : 32, loss : 0.2134519909814768ITERATION : 33, loss : 0.21345247943348294ITERATION : 34, loss : 0.2134528268965965ITERATION : 35, loss : 0.21345307395422805ITERATION : 36, loss : 0.2134532495635738ITERATION : 37, loss : 0.21345337435540285ITERATION : 38, loss : 0.21345346296316867ITERATION : 39, loss : 0.21345352589207817ITERATION : 40, loss : 0.21345357054507333ITERATION : 41, loss : 0.2134536022262453ITERATION : 42, loss : 0.21345362470855592ITERATION : 43, loss : 0.21345364062833833ITERATION : 44, loss : 0.21345365192632382ITERATION : 45, loss : 0.21345365990738782ITERATION : 46, loss : 0.21345366556579295ITERATION : 47, loss : 0.21345366957944306ITERATION : 48, loss : 0.2134536724248942ITERATION : 49, loss : 0.21345367442376015ITERATION : 50, loss : 0.21345367582353317ITERATION : 51, loss : 0.213453676811553ITERATION : 52, loss : 0.2134536775121643ITERATION : 53, loss : 0.21345367799181358ITERATION : 54, loss : 0.21345367833302908ITERATION : 55, loss : 0.21345367857468406ITERATION : 56, loss : 0.21345367874268753ITERATION : 57, loss : 0.21345367886593727ITERATION : 58, loss : 0.21345367893168857ITERATION : 59, loss : 0.2134536789923333ITERATION : 60, loss : 0.21345367902852005ITERATION : 61, loss : 0.21345367905121487ITERATION : 62, loss : 0.21345367907312401ITERATION : 63, loss : 0.21345367907573004ITERATION : 64, loss : 0.21345367907573004ITERATION : 65, loss : 0.21345367907573004ITERATION : 66, loss : 0.21345367907573004ITERATION : 67, loss : 0.21345367907573004ITERATION : 68, loss : 0.21345367907573004ITERATION : 69, loss : 0.21345367907573004ITERATION : 70, loss : 0.21345367907573004ITERATION : 71, loss : 0.21345367907573004ITERATION : 72, loss : 0.21345367907573004ITERATION : 73, loss : 0.21345367907573004ITERATION : 74, loss : 0.21345367907573004ITERATION : 75, loss : 0.21345367907573004ITERATION : 76, loss : 0.21345367907573004ITERATION : 77, loss : 0.21345367907573004ITERATION : 78, loss : 0.21345367907573004ITERATION : 79, loss : 0.21345367907573004ITERATION : 80, loss : 0.21345367907573004ITERATION : 81, loss : 0.21345367907573004ITERATION : 82, loss : 0.21345367907573004ITERATION : 83, loss : 0.21345367907573004ITERATION : 84, loss : 0.21345367907573004ITERATION : 85, loss : 0.21345367907573004ITERATION : 86, loss : 0.21345367907573004ITERATION : 87, loss : 0.21345367907573004ITERATION : 88, loss : 0.21345367907573004ITERATION : 89, loss : 0.21345367907573004ITERATION : 90, loss : 0.21345367907573004ITERATION : 91, loss : 0.21345367907573004ITERATION : 92, loss : 0.21345367907573004ITERATION : 93, loss : 0.21345367907573004ITERATION : 94, loss : 0.21345367907573004ITERATION : 95, loss : 0.21345367907573004ITERATION : 96, loss : 0.21345367907573004ITERATION : 97, loss : 0.21345367907573004ITERATION : 98, loss : 0.21345367907573004ITERATION : 99, loss : 0.21345367907573004ITERATION : 100, loss : 0.21345367907573004
ITERATION : 1, loss : 0.0033135680550891084ITERATION : 2, loss : 0.0038239908676499963ITERATION : 3, loss : 0.005129443463831502ITERATION : 4, loss : 0.005793121879569799ITERATION : 5, loss : 0.006264579941091114ITERATION : 6, loss : 0.0066317733824372335ITERATION : 7, loss : 0.006904163837824005ITERATION : 8, loss : 0.007100825317570397ITERATION : 9, loss : 0.007240484640812832ITERATION : 10, loss : 0.007338612796096908ITERATION : 11, loss : 0.007407070297259223ITERATION : 12, loss : 0.007454595045807933ITERATION : 13, loss : 0.0074874744932209684ITERATION : 14, loss : 0.007510165915037613ITERATION : 15, loss : 0.007525798251679283ITERATION : 16, loss : 0.007536553308632434ITERATION : 17, loss : 0.0075439454501022ITERATION : 18, loss : 0.007549022298964772ITERATION : 19, loss : 0.007552506923196275ITERATION : 20, loss : 0.007554897495738221ITERATION : 21, loss : 0.007556536832084497ITERATION : 22, loss : 0.007557660619024648ITERATION : 23, loss : 0.007558430718101846ITERATION : 24, loss : 0.007558958309666438ITERATION : 25, loss : 0.007559319646999422ITERATION : 26, loss : 0.007559567055699287ITERATION : 27, loss : 0.007559736412975719ITERATION : 28, loss : 0.007559852305674649ITERATION : 29, loss : 0.0075599315947142585ITERATION : 30, loss : 0.007559985840409829ITERATION : 31, loss : 0.007560022919064189ITERATION : 32, loss : 0.007560048285223327ITERATION : 33, loss : 0.0075600656024721ITERATION : 34, loss : 0.007560077443125402ITERATION : 35, loss : 0.007560085537670222ITERATION : 36, loss : 0.007560091036985804ITERATION : 37, loss : 0.007560094818893041ITERATION : 38, loss : 0.007560097396815015ITERATION : 39, loss : 0.007560099163253526ITERATION : 40, loss : 0.0075601003653167085ITERATION : 41, loss : 0.007560101174842574ITERATION : 42, loss : 0.007560101722714188ITERATION : 43, loss : 0.00756010209176006ITERATION : 44, loss : 0.007560102341676737ITERATION : 45, loss : 0.007560102501012649ITERATION : 46, loss : 0.007560102635174617ITERATION : 47, loss : 0.0075601027021451675ITERATION : 48, loss : 0.0075601027490521ITERATION : 49, loss : 0.00756010277461046ITERATION : 50, loss : 0.007560102795702201ITERATION : 51, loss : 0.007560102802972957ITERATION : 52, loss : 0.007560102802972957ITERATION : 53, loss : 0.007560102802972957ITERATION : 54, loss : 0.007560102802972957ITERATION : 55, loss : 0.007560102802972957ITERATION : 56, loss : 0.007560102802972957ITERATION : 57, loss : 0.007560102802972957ITERATION : 58, loss : 0.007560102802972957ITERATION : 59, loss : 0.007560102802972957ITERATION : 60, loss : 0.007560102802972957ITERATION : 61, loss : 0.007560102802972957ITERATION : 62, loss : 0.007560102802972957ITERATION : 63, loss : 0.007560102802972957ITERATION : 64, loss : 0.007560102802972957ITERATION : 65, loss : 0.007560102802972957ITERATION : 66, loss : 0.007560102802972957ITERATION : 67, loss : 0.007560102802972957ITERATION : 68, loss : 0.007560102802972957ITERATION : 69, loss : 0.007560102802972957ITERATION : 70, loss : 0.007560102802972957ITERATION : 71, loss : 0.007560102802972957ITERATION : 72, loss : 0.007560102802972957ITERATION : 73, loss : 0.007560102802972957ITERATION : 74, loss : 0.007560102802972957ITERATION : 75, loss : 0.007560102802972957ITERATION : 76, loss : 0.007560102802972957ITERATION : 77, loss : 0.007560102802972957ITERATION : 78, loss : 0.007560102802972957ITERATION : 79, loss : 0.007560102802972957ITERATION : 80, loss : 0.007560102802972957ITERATION : 81, loss : 0.007560102802972957ITERATION : 82, loss : 0.007560102802972957ITERATION : 83, loss : 0.007560102802972957ITERATION : 84, loss : 0.007560102802972957ITERATION : 85, loss : 0.007560102802972957ITERATION : 86, loss : 0.007560102802972957ITERATION : 87, loss : 0.007560102802972957ITERATION : 88, loss : 0.007560102802972957ITERATION : 89, loss : 0.007560102802972957ITERATION : 90, loss : 0.007560102802972957ITERATION : 91, loss : 0.007560102802972957ITERATION : 92, loss : 0.007560102802972957ITERATION : 93, loss : 0.007560102802972957ITERATION : 94, loss : 0.007560102802972957ITERATION : 95, loss : 0.007560102802972957ITERATION : 96, loss : 0.007560102802972957ITERATION : 97, loss : 0.007560102802972957ITERATION : 98, loss : 0.007560102802972957ITERATION : 99, loss : 0.007560102802972957ITERATION : 100, loss : 0.007560102802972957
ITERATION : 1, loss : 0.06724835514622297ITERATION : 2, loss : 0.08142353515652503ITERATION : 3, loss : 0.09699431708119438ITERATION : 4, loss : 0.10848250810231409ITERATION : 5, loss : 0.11707810392609155ITERATION : 6, loss : 0.1234713360564923ITERATION : 7, loss : 0.12805538397103358ITERATION : 8, loss : 0.1314630247774056ITERATION : 9, loss : 0.1340019993547578ITERATION : 10, loss : 0.1358971516403764ITERATION : 11, loss : 0.1373136368732115ITERATION : 12, loss : 0.1383733748585659ITERATION : 13, loss : 0.13916674305878868ITERATION : 14, loss : 0.13976095645341102ITERATION : 15, loss : 0.14020612836447857ITERATION : 16, loss : 0.14053968994127614ITERATION : 17, loss : 0.1407896368847556ITERATION : 18, loss : 0.14097692641175746ITERATION : 19, loss : 0.1411172568619266ITERATION : 20, loss : 0.14122239222543237ITERATION : 21, loss : 0.14130114979156272ITERATION : 22, loss : 0.1413601393402414ITERATION : 23, loss : 0.14140431580313045ITERATION : 24, loss : 0.14143739355603058ITERATION : 25, loss : 0.1414621567269611ITERATION : 26, loss : 0.14148069198050042ITERATION : 27, loss : 0.14149456304107014ITERATION : 28, loss : 0.14150494169226854ITERATION : 29, loss : 0.14151270574057884ITERATION : 30, loss : 0.1415185126553694ITERATION : 31, loss : 0.14152285481691715ITERATION : 32, loss : 0.14152610106942526ITERATION : 33, loss : 0.14152852747879457ITERATION : 34, loss : 0.1415303406966985ITERATION : 35, loss : 0.1415316953684917ITERATION : 36, loss : 0.14153270727781966ITERATION : 37, loss : 0.1415334629204783ITERATION : 38, loss : 0.1415340270526592ITERATION : 39, loss : 0.14153444805732357ITERATION : 40, loss : 0.14153476219952094ITERATION : 41, loss : 0.14153499652368076ITERATION : 42, loss : 0.14153517125193368ITERATION : 43, loss : 0.14153530150199883ITERATION : 44, loss : 0.1415353985723581ITERATION : 45, loss : 0.1415354708846572ITERATION : 46, loss : 0.14153552475548425ITERATION : 47, loss : 0.1415355648918889ITERATION : 48, loss : 0.14153559472329616ITERATION : 49, loss : 0.14153561695389663ITERATION : 50, loss : 0.14153563349553153ITERATION : 51, loss : 0.141535645765741ITERATION : 52, loss : 0.14153565490784578ITERATION : 53, loss : 0.14153566161395786ITERATION : 54, loss : 0.14153566663174333ITERATION : 55, loss : 0.14153567032416817ITERATION : 56, loss : 0.14153567309909995ITERATION : 57, loss : 0.1415356750993025ITERATION : 58, loss : 0.14153567661713862ITERATION : 59, loss : 0.1415356777288019ITERATION : 60, loss : 0.14153567849300477ITERATION : 61, loss : 0.14153567912973558ITERATION : 62, loss : 0.14153567957955465ITERATION : 63, loss : 0.14153567990668978ITERATION : 64, loss : 0.14153568013310014ITERATION : 65, loss : 0.141535680298956ITERATION : 66, loss : 0.1415356803847249ITERATION : 67, loss : 0.14153568044319537ITERATION : 68, loss : 0.14153568044737427ITERATION : 69, loss : 0.14153568051288024ITERATION : 70, loss : 0.14153568051580262ITERATION : 71, loss : 0.14153568051580262ITERATION : 72, loss : 0.14153568051580262ITERATION : 73, loss : 0.14153568051580262ITERATION : 74, loss : 0.14153568051580262ITERATION : 75, loss : 0.14153568051580262ITERATION : 76, loss : 0.14153568051580262ITERATION : 77, loss : 0.14153568051580262ITERATION : 78, loss : 0.14153568051580262ITERATION : 79, loss : 0.14153568051580262ITERATION : 80, loss : 0.14153568051580262ITERATION : 81, loss : 0.14153568051580262ITERATION : 82, loss : 0.14153568051580262ITERATION : 83, loss : 0.14153568051580262ITERATION : 84, loss : 0.14153568051580262ITERATION : 85, loss : 0.14153568051580262ITERATION : 86, loss : 0.14153568051580262ITERATION : 87, loss : 0.14153568051580262ITERATION : 88, loss : 0.14153568051580262ITERATION : 89, loss : 0.14153568051580262ITERATION : 90, loss : 0.14153568051580262ITERATION : 91, loss : 0.14153568051580262ITERATION : 92, loss : 0.14153568051580262ITERATION : 93, loss : 0.14153568051580262ITERATION : 94, loss : 0.14153568051580262ITERATION : 95, loss : 0.14153568051580262ITERATION : 96, loss : 0.14153568051580262ITERATION : 97, loss : 0.14153568051580262ITERATION : 98, loss : 0.14153568051580262ITERATION : 99, loss : 0.14153568051580262ITERATION : 100, loss : 0.14153568051580262
ITERATION : 1, loss : 0.03778232178343629ITERATION : 2, loss : 0.035971426114893075ITERATION : 3, loss : 0.0363875591742103ITERATION : 4, loss : 0.037156107741762846ITERATION : 5, loss : 0.03784358567613932ITERATION : 6, loss : 0.03836823549065704ITERATION : 7, loss : 0.03874330687577487ITERATION : 8, loss : 0.039002414767240735ITERATION : 9, loss : 0.039177685764234485ITERATION : 10, loss : 0.03936944465061841ITERATION : 11, loss : 0.03952427935994119ITERATION : 12, loss : 0.03962913196409444ITERATION : 13, loss : 0.03969988876763817ITERATION : 14, loss : 0.03974749737258409ITERATION : 15, loss : 0.03977944750990356ITERATION : 16, loss : 0.03980083734224816ITERATION : 17, loss : 0.039815123425502384ITERATION : 18, loss : 0.039824641765539ITERATION : 19, loss : 0.03983096759015753ITERATION : 20, loss : 0.03983516030005302ITERATION : 21, loss : 0.03983793102339257ITERATION : 22, loss : 0.03983975608261098ITERATION : 23, loss : 0.039840953866882356ITERATION : 24, loss : 0.03984173675522251ITERATION : 25, loss : 0.039842246076297064ITERATION : 26, loss : 0.039842575640195776ITERATION : 27, loss : 0.039842787564285116ITERATION : 28, loss : 0.03984292280461541ITERATION : 29, loss : 0.03984300838801118ITERATION : 30, loss : 0.03984306190695139ITERATION : 31, loss : 0.03984309494869712ITERATION : 32, loss : 0.03984311498080865ITERATION : 33, loss : 0.03984312687044367ITERATION : 34, loss : 0.03984313376740273ITERATION : 35, loss : 0.03984313745550498ITERATION : 36, loss : 0.03984313933870369ITERATION : 37, loss : 0.03984314011961437ITERATION : 38, loss : 0.03984314037462428ITERATION : 39, loss : 0.03984314024668884ITERATION : 40, loss : 0.039843140062230366ITERATION : 41, loss : 0.0398431398106092ITERATION : 42, loss : 0.03984313952899914ITERATION : 43, loss : 0.03984313919178853ITERATION : 44, loss : 0.03984313895111356ITERATION : 45, loss : 0.03984313875429393ITERATION : 46, loss : 0.03984313858048784ITERATION : 47, loss : 0.03984313843860051ITERATION : 48, loss : 0.0398431383260779ITERATION : 49, loss : 0.03984313823681566ITERATION : 50, loss : 0.03984313817269121ITERATION : 51, loss : 0.039843138136010786ITERATION : 52, loss : 0.03984313809473084ITERATION : 53, loss : 0.03984313807651828ITERATION : 54, loss : 0.03984313807576661ITERATION : 55, loss : 0.03984313807576661ITERATION : 56, loss : 0.03984313807576661ITERATION : 57, loss : 0.03984313807576661ITERATION : 58, loss : 0.03984313807576661ITERATION : 59, loss : 0.03984313807576661ITERATION : 60, loss : 0.03984313807576661ITERATION : 61, loss : 0.03984313807576661ITERATION : 62, loss : 0.03984313807576661ITERATION : 63, loss : 0.03984313807576661ITERATION : 64, loss : 0.03984313807576661ITERATION : 65, loss : 0.03984313807576661ITERATION : 66, loss : 0.03984313807576661ITERATION : 67, loss : 0.03984313807576661ITERATION : 68, loss : 0.03984313807576661ITERATION : 69, loss : 0.03984313807576661ITERATION : 70, loss : 0.03984313807576661ITERATION : 71, loss : 0.03984313807576661ITERATION : 72, loss : 0.03984313807576661ITERATION : 73, loss : 0.03984313807576661ITERATION : 74, loss : 0.03984313807576661ITERATION : 75, loss : 0.03984313807576661ITERATION : 76, loss : 0.03984313807576661ITERATION : 77, loss : 0.03984313807576661ITERATION : 78, loss : 0.03984313807576661ITERATION : 79, loss : 0.03984313807576661ITERATION : 80, loss : 0.03984313807576661ITERATION : 81, loss : 0.03984313807576661ITERATION : 82, loss : 0.03984313807576661ITERATION : 83, loss : 0.03984313807576661ITERATION : 84, loss : 0.03984313807576661ITERATION : 85, loss : 0.03984313807576661ITERATION : 86, loss : 0.03984313807576661ITERATION : 87, loss : 0.03984313807576661ITERATION : 88, loss : 0.03984313807576661ITERATION : 89, loss : 0.03984313807576661ITERATION : 90, loss : 0.03984313807576661ITERATION : 91, loss : 0.03984313807576661ITERATION : 92, loss : 0.03984313807576661ITERATION : 93, loss : 0.03984313807576661ITERATION : 94, loss : 0.03984313807576661ITERATION : 95, loss : 0.03984313807576661ITERATION : 96, loss : 0.03984313807576661ITERATION : 97, loss : 0.03984313807576661ITERATION : 98, loss : 0.03984313807576661ITERATION : 99, loss : 0.03984313807576661ITERATION : 100, loss : 0.03984313807576661
ITERATION : 1, loss : 0.01803147206930672ITERATION : 2, loss : 0.019330789995840487ITERATION : 3, loss : 0.023152246353037878ITERATION : 4, loss : 0.026667819906422013ITERATION : 5, loss : 0.029510353131448577ITERATION : 6, loss : 0.031622719835077535ITERATION : 7, loss : 0.033165356953533155ITERATION : 8, loss : 0.0342798213692678ITERATION : 9, loss : 0.035079391484718ITERATION : 10, loss : 0.035650412643904165ITERATION : 11, loss : 0.036056940520367835ITERATION : 12, loss : 0.036345730267486526ITERATION : 13, loss : 0.0365505625469227ITERATION : 14, loss : 0.03669568150408677ITERATION : 15, loss : 0.036798408753220734ITERATION : 16, loss : 0.03687108143763001ITERATION : 17, loss : 0.03692246733634878ITERATION : 18, loss : 0.036958787558034625ITERATION : 19, loss : 0.036984451052516285ITERATION : 20, loss : 0.03700257988748231ITERATION : 21, loss : 0.03701538339596555ITERATION : 22, loss : 0.037024424054157914ITERATION : 23, loss : 0.03703080665281315ITERATION : 24, loss : 0.03703531196407258ITERATION : 25, loss : 0.037038491681528ITERATION : 26, loss : 0.03704073551594759ITERATION : 27, loss : 0.03704231863999184ITERATION : 28, loss : 0.03704343545735225ITERATION : 29, loss : 0.03704422322458413ITERATION : 30, loss : 0.03704477882496848ITERATION : 31, loss : 0.03704517062807761ITERATION : 32, loss : 0.03704544690328375ITERATION : 33, loss : 0.03704564166812116ITERATION : 34, loss : 0.03704577893563338ITERATION : 35, loss : 0.03704587570945834ITERATION : 36, loss : 0.037045943933059114ITERATION : 37, loss : 0.03704599194974215ITERATION : 38, loss : 0.0370460257706622ITERATION : 39, loss : 0.037046049570932306ITERATION : 40, loss : 0.03704606636380846ITERATION : 41, loss : 0.03704607819297988ITERATION : 42, loss : 0.037046086503892184ITERATION : 43, loss : 0.037046092428926494ITERATION : 44, loss : 0.03704609658504339ITERATION : 45, loss : 0.037046099448703795ITERATION : 46, loss : 0.037046101474926305ITERATION : 47, loss : 0.03704610287953977ITERATION : 48, loss : 0.037046103830477066ITERATION : 49, loss : 0.037046104493779035ITERATION : 50, loss : 0.03704610497465512ITERATION : 51, loss : 0.03704610531185023ITERATION : 52, loss : 0.03704610547438242ITERATION : 53, loss : 0.037046105656666085ITERATION : 54, loss : 0.03704610571927534ITERATION : 55, loss : 0.03704610578526602ITERATION : 56, loss : 0.03704610588034554ITERATION : 57, loss : 0.03704610587599874ITERATION : 58, loss : 0.03704610588780494ITERATION : 59, loss : 0.03704610588780494ITERATION : 60, loss : 0.03704610588780494ITERATION : 61, loss : 0.03704610588780494ITERATION : 62, loss : 0.03704610588780494ITERATION : 63, loss : 0.03704610588780494ITERATION : 64, loss : 0.03704610588780494ITERATION : 65, loss : 0.03704610588780494ITERATION : 66, loss : 0.03704610588780494ITERATION : 67, loss : 0.03704610588780494ITERATION : 68, loss : 0.03704610588780494ITERATION : 69, loss : 0.03704610588780494ITERATION : 70, loss : 0.03704610588780494ITERATION : 71, loss : 0.03704610588780494ITERATION : 72, loss : 0.03704610588780494ITERATION : 73, loss : 0.03704610588780494ITERATION : 74, loss : 0.03704610588780494ITERATION : 75, loss : 0.03704610588780494ITERATION : 76, loss : 0.03704610588780494ITERATION : 77, loss : 0.03704610588780494ITERATION : 78, loss : 0.03704610588780494ITERATION : 79, loss : 0.03704610588780494ITERATION : 80, loss : 0.03704610588780494ITERATION : 81, loss : 0.03704610588780494ITERATION : 82, loss : 0.03704610588780494ITERATION : 83, loss : 0.03704610588780494ITERATION : 84, loss : 0.03704610588780494ITERATION : 85, loss : 0.03704610588780494ITERATION : 86, loss : 0.03704610588780494ITERATION : 87, loss : 0.03704610588780494ITERATION : 88, loss : 0.03704610588780494ITERATION : 89, loss : 0.03704610588780494ITERATION : 90, loss : 0.03704610588780494ITERATION : 91, loss : 0.03704610588780494ITERATION : 92, loss : 0.03704610588780494ITERATION : 93, loss : 0.03704610588780494ITERATION : 94, loss : 0.03704610588780494ITERATION : 95, loss : 0.03704610588780494ITERATION : 96, loss : 0.03704610588780494ITERATION : 97, loss : 0.03704610588780494ITERATION : 98, loss : 0.03704610588780494ITERATION : 99, loss : 0.03704610588780494ITERATION : 100, loss : 0.03704610588780494
ITERATION : 1, loss : 0.03785339189846364ITERATION : 2, loss : 0.05180221102204033ITERATION : 3, loss : 0.060890595976335435ITERATION : 4, loss : 0.06838322884562666ITERATION : 5, loss : 0.07387082515623432ITERATION : 6, loss : 0.07778641723624918ITERATION : 7, loss : 0.08057567797703126ITERATION : 8, loss : 0.08256251406984932ITERATION : 9, loss : 0.08397852680672434ITERATION : 10, loss : 0.08498834800972939ITERATION : 11, loss : 0.08570888604396032ITERATION : 12, loss : 0.08622322471283793ITERATION : 13, loss : 0.08659048185268094ITERATION : 14, loss : 0.08685277066380029ITERATION : 15, loss : 0.08704011812745091ITERATION : 16, loss : 0.08717394809749805ITERATION : 17, loss : 0.08726955338362179ITERATION : 18, loss : 0.08733785383382363ITERATION : 19, loss : 0.0873866484233991ITERATION : 20, loss : 0.08742150808141216ITERATION : 21, loss : 0.08744641240553885ITERATION : 22, loss : 0.0874642044382045ITERATION : 23, loss : 0.08747691519561131ITERATION : 24, loss : 0.08748599582792674ITERATION : 25, loss : 0.08749248306573915ITERATION : 26, loss : 0.08749711750107926ITERATION : 27, loss : 0.0875004282537265ITERATION : 28, loss : 0.08750279338121775ITERATION : 29, loss : 0.08750448297223873ITERATION : 30, loss : 0.08750569000851993ITERATION : 31, loss : 0.08750655225949053ITERATION : 32, loss : 0.08750716822192303ITERATION : 33, loss : 0.08750760825315376ITERATION : 34, loss : 0.08750792257626773ITERATION : 35, loss : 0.08750814710639884ITERATION : 36, loss : 0.08750830750783932ITERATION : 37, loss : 0.08750842206638323ITERATION : 38, loss : 0.08750850391454791ITERATION : 39, loss : 0.08750856239787547ITERATION : 40, loss : 0.08750860417656695ITERATION : 41, loss : 0.08750863402619297ITERATION : 42, loss : 0.08750865534804544ITERATION : 43, loss : 0.08750867057734071ITERATION : 44, loss : 0.08750868145763427ITERATION : 45, loss : 0.08750868923938457ITERATION : 46, loss : 0.08750869479220681ITERATION : 47, loss : 0.0875086987576215ITERATION : 48, loss : 0.08750870158746317ITERATION : 49, loss : 0.08750870362194484ITERATION : 50, loss : 0.08750870498945769ITERATION : 51, loss : 0.08750870605385079ITERATION : 52, loss : 0.08750870679453328ITERATION : 53, loss : 0.08750870734124058ITERATION : 54, loss : 0.08750870762475053ITERATION : 55, loss : 0.08750870792799352ITERATION : 56, loss : 0.0875087080744843ITERATION : 57, loss : 0.08750870823888574ITERATION : 58, loss : 0.08750870828022572ITERATION : 59, loss : 0.0875087083605964ITERATION : 60, loss : 0.08750870836202647ITERATION : 61, loss : 0.08750870836202647ITERATION : 62, loss : 0.08750870836202647ITERATION : 63, loss : 0.08750870836202647ITERATION : 64, loss : 0.08750870836202647ITERATION : 65, loss : 0.08750870836202647ITERATION : 66, loss : 0.08750870836202647ITERATION : 67, loss : 0.08750870836202647ITERATION : 68, loss : 0.08750870836202647ITERATION : 69, loss : 0.08750870836202647ITERATION : 70, loss : 0.08750870836202647ITERATION : 71, loss : 0.08750870836202647ITERATION : 72, loss : 0.08750870836202647ITERATION : 73, loss : 0.08750870836202647ITERATION : 74, loss : 0.08750870836202647ITERATION : 75, loss : 0.08750870836202647ITERATION : 76, loss : 0.08750870836202647ITERATION : 77, loss : 0.08750870836202647ITERATION : 78, loss : 0.08750870836202647ITERATION : 79, loss : 0.08750870836202647ITERATION : 80, loss : 0.08750870836202647ITERATION : 81, loss : 0.08750870836202647ITERATION : 82, loss : 0.08750870836202647ITERATION : 83, loss : 0.08750870836202647ITERATION : 84, loss : 0.08750870836202647ITERATION : 85, loss : 0.08750870836202647ITERATION : 86, loss : 0.08750870836202647ITERATION : 87, loss : 0.08750870836202647ITERATION : 88, loss : 0.08750870836202647ITERATION : 89, loss : 0.08750870836202647ITERATION : 90, loss : 0.08750870836202647ITERATION : 91, loss : 0.08750870836202647ITERATION : 92, loss : 0.08750870836202647ITERATION : 93, loss : 0.08750870836202647ITERATION : 94, loss : 0.08750870836202647ITERATION : 95, loss : 0.08750870836202647ITERATION : 96, loss : 0.08750870836202647ITERATION : 97, loss : 0.08750870836202647ITERATION : 98, loss : 0.08750870836202647ITERATION : 99, loss : 0.08750870836202647ITERATION : 100, loss : 0.08750870836202647
gradient norm in None layer : 0.002805648525345112
gradient norm in None layer : 9.94518618448346e-05
gradient norm in None layer : 8.732055175260552e-05
gradient norm in None layer : 0.0015379861787908763
gradient norm in None layer : 0.00011186714611927684
gradient norm in None layer : 0.00010297077751012064
gradient norm in None layer : 0.0006665459492005153
gradient norm in None layer : 3.155466888463571e-05
gradient norm in None layer : 3.116423029653015e-05
gradient norm in None layer : 0.0006134786220071132
gradient norm in None layer : 3.053875202543116e-05
gradient norm in None layer : 3.06243000745471e-05
gradient norm in None layer : 0.00020937626853827752
gradient norm in None layer : 6.910846723145904e-06
gradient norm in None layer : 7.367944211039624e-06
gradient norm in None layer : 0.0002082069123311068
gradient norm in None layer : 7.870826289110607e-06
gradient norm in None layer : 7.949538724586382e-06
gradient norm in None layer : 0.00027585347520800386
gradient norm in None layer : 3.305711930312387e-06
gradient norm in None layer : 0.0006354563868910509
gradient norm in None layer : 3.3147513103491654e-05
gradient norm in None layer : 3.117206945714057e-05
gradient norm in None layer : 0.0006206099447418861
gradient norm in None layer : 6.288129753326827e-05
gradient norm in None layer : 8.679389745288892e-05
gradient norm in None layer : 0.0010486262358228838
gradient norm in None layer : 7.894605416382755e-06
gradient norm in None layer : 0.0014580977271669347
gradient norm in None layer : 0.00013121162930919119
gradient norm in None layer : 0.0001276333885372948
gradient norm in None layer : 0.0017418916978936129
gradient norm in None layer : 0.00015499097366938286
gradient norm in None layer : 0.00016686111596724785
gradient norm in None layer : 0.00011587516542899067
gradient norm in None layer : 1.338264611894185e-05
Total gradient norm: 0.004292680813385498
invariance loss : 5.054588225631887, avg_den : 0.44527435302734375, density loss : 0.34455140920413196, mse loss : 0.07783733234448298, solver time : 117.34230256080627 sec , total loss : 0.083236471979319, running loss : 0.10817199546406782
Epoch 0/10 , batch 16/12500 
ITERATION : 1, loss : 0.08517998095274455ITERATION : 2, loss : 0.09489172866277237ITERATION : 3, loss : 0.10309017995944306ITERATION : 4, loss : 0.10890809457394461ITERATION : 5, loss : 0.11288419381920642ITERATION : 6, loss : 0.11555832254584245ITERATION : 7, loss : 0.11734076378816508ITERATION : 8, loss : 0.11852254067677248ITERATION : 9, loss : 0.11930362393257192ITERATION : 10, loss : 0.11968083282085436ITERATION : 11, loss : 0.1198906233705322ITERATION : 12, loss : 0.1200278892556314ITERATION : 13, loss : 0.12011776374677088ITERATION : 14, loss : 0.1201766496193986ITERATION : 15, loss : 0.12021525593843063ITERATION : 16, loss : 0.12024057988637132ITERATION : 17, loss : 0.12025719784391638ITERATION : 18, loss : 0.12026810578560335ITERATION : 19, loss : 0.1202752667871794ITERATION : 20, loss : 0.12027996821361309ITERATION : 21, loss : 0.12028305458917028ITERATION : 22, loss : 0.12028508043302928ITERATION : 23, loss : 0.1202864098138499ITERATION : 24, loss : 0.120287281895756ITERATION : 25, loss : 0.12028785378714132ITERATION : 26, loss : 0.12028822857351418ITERATION : 27, loss : 0.12028847411138968ITERATION : 28, loss : 0.12028863485680727ITERATION : 29, loss : 0.1202887400456018ITERATION : 30, loss : 0.12028880877915242ITERATION : 31, loss : 0.12028885361995079ITERATION : 32, loss : 0.12028888289709581ITERATION : 33, loss : 0.12028890195953623ITERATION : 34, loss : 0.12028891439380418ITERATION : 35, loss : 0.12028892237077636ITERATION : 36, loss : 0.12028892763574335ITERATION : 37, loss : 0.12028893111783641ITERATION : 38, loss : 0.12028893335746701ITERATION : 39, loss : 0.12028893478614108ITERATION : 40, loss : 0.12028893571008792ITERATION : 41, loss : 0.12028893626021493ITERATION : 42, loss : 0.1202889366117948ITERATION : 43, loss : 0.12028893682112932ITERATION : 44, loss : 0.12028893696274735ITERATION : 45, loss : 0.12028893704481294ITERATION : 46, loss : 0.12028893709128012ITERATION : 47, loss : 0.12028893711764549ITERATION : 48, loss : 0.12028893713259296ITERATION : 49, loss : 0.1202889371360177ITERATION : 50, loss : 0.1202889371360177ITERATION : 51, loss : 0.1202889371360177ITERATION : 52, loss : 0.1202889371360177ITERATION : 53, loss : 0.1202889371360177ITERATION : 54, loss : 0.1202889371360177ITERATION : 55, loss : 0.1202889371360177ITERATION : 56, loss : 0.1202889371360177ITERATION : 57, loss : 0.1202889371360177ITERATION : 58, loss : 0.1202889371360177ITERATION : 59, loss : 0.1202889371360177ITERATION : 60, loss : 0.1202889371360177ITERATION : 61, loss : 0.1202889371360177ITERATION : 62, loss : 0.1202889371360177ITERATION : 63, loss : 0.1202889371360177ITERATION : 64, loss : 0.1202889371360177ITERATION : 65, loss : 0.1202889371360177ITERATION : 66, loss : 0.1202889371360177ITERATION : 67, loss : 0.1202889371360177ITERATION : 68, loss : 0.1202889371360177ITERATION : 69, loss : 0.1202889371360177ITERATION : 70, loss : 0.1202889371360177ITERATION : 71, loss : 0.1202889371360177ITERATION : 72, loss : 0.1202889371360177ITERATION : 73, loss : 0.1202889371360177ITERATION : 74, loss : 0.1202889371360177ITERATION : 75, loss : 0.1202889371360177ITERATION : 76, loss : 0.1202889371360177ITERATION : 77, loss : 0.1202889371360177ITERATION : 78, loss : 0.1202889371360177ITERATION : 79, loss : 0.1202889371360177ITERATION : 80, loss : 0.1202889371360177ITERATION : 81, loss : 0.1202889371360177ITERATION : 82, loss : 0.1202889371360177ITERATION : 83, loss : 0.1202889371360177ITERATION : 84, loss : 0.1202889371360177ITERATION : 85, loss : 0.1202889371360177ITERATION : 86, loss : 0.1202889371360177ITERATION : 87, loss : 0.1202889371360177ITERATION : 88, loss : 0.1202889371360177ITERATION : 89, loss : 0.1202889371360177ITERATION : 90, loss : 0.1202889371360177ITERATION : 91, loss : 0.1202889371360177ITERATION : 92, loss : 0.1202889371360177ITERATION : 93, loss : 0.1202889371360177ITERATION : 94, loss : 0.1202889371360177ITERATION : 95, loss : 0.1202889371360177ITERATION : 96, loss : 0.1202889371360177ITERATION : 97, loss : 0.1202889371360177ITERATION : 98, loss : 0.1202889371360177ITERATION : 99, loss : 0.1202889371360177ITERATION : 100, loss : 0.1202889371360177
ITERATION : 1, loss : 0.03229502609277418ITERATION : 2, loss : 0.03542694664574599ITERATION : 3, loss : 0.040004354873902984ITERATION : 4, loss : 0.043464939289095436ITERATION : 5, loss : 0.0458762678887914ITERATION : 6, loss : 0.047521170738855256ITERATION : 7, loss : 0.0486354393226827ITERATION : 8, loss : 0.04938823350989847ITERATION : 9, loss : 0.049896189554671046ITERATION : 10, loss : 0.05023868982306613ITERATION : 11, loss : 0.05046950418243038ITERATION : 12, loss : 0.05062498090351791ITERATION : 13, loss : 0.05072966566313512ITERATION : 14, loss : 0.050800122788723596ITERATION : 15, loss : 0.05084752453253308ITERATION : 16, loss : 0.05087940261909591ITERATION : 17, loss : 0.05090083245869617ITERATION : 18, loss : 0.05091523273920323ITERATION : 19, loss : 0.05092490535543731ITERATION : 20, loss : 0.05093139964978557ITERATION : 21, loss : 0.0509357579926515ITERATION : 22, loss : 0.05093868153697421ITERATION : 23, loss : 0.05094064166180775ITERATION : 24, loss : 0.05094195516532756ITERATION : 25, loss : 0.05094283485028673ITERATION : 26, loss : 0.05094342365200978ITERATION : 27, loss : 0.050943817519190866ITERATION : 28, loss : 0.0509440808181051ITERATION : 29, loss : 0.05094425667010856ITERATION : 30, loss : 0.05094437406301527ITERATION : 31, loss : 0.050944452326980735ITERATION : 32, loss : 0.05094450450174784ITERATION : 33, loss : 0.05094453920670883ITERATION : 34, loss : 0.050944562287020836ITERATION : 35, loss : 0.05094457760942137ITERATION : 36, loss : 0.05094458779104072ITERATION : 37, loss : 0.05094459454484067ITERATION : 38, loss : 0.05094459900842637ITERATION : 39, loss : 0.05094460194063707ITERATION : 40, loss : 0.05094460388277222ITERATION : 41, loss : 0.050944605122937116ITERATION : 42, loss : 0.050944605960691135ITERATION : 43, loss : 0.05094460648667224ITERATION : 44, loss : 0.05094460682348927ITERATION : 45, loss : 0.050944607038820436ITERATION : 46, loss : 0.050944607162189424ITERATION : 47, loss : 0.05094460725117473ITERATION : 48, loss : 0.05094460731084462ITERATION : 49, loss : 0.05094460736162017ITERATION : 50, loss : 0.050944607377975196ITERATION : 51, loss : 0.050944607378676114ITERATION : 52, loss : 0.050944607378676114ITERATION : 53, loss : 0.050944607378676114ITERATION : 54, loss : 0.050944607378676114ITERATION : 55, loss : 0.050944607378676114ITERATION : 56, loss : 0.050944607378676114ITERATION : 57, loss : 0.050944607378676114ITERATION : 58, loss : 0.050944607378676114ITERATION : 59, loss : 0.050944607378676114ITERATION : 60, loss : 0.050944607378676114ITERATION : 61, loss : 0.050944607378676114ITERATION : 62, loss : 0.050944607378676114ITERATION : 63, loss : 0.050944607378676114ITERATION : 64, loss : 0.050944607378676114ITERATION : 65, loss : 0.050944607378676114ITERATION : 66, loss : 0.050944607378676114ITERATION : 67, loss : 0.050944607378676114ITERATION : 68, loss : 0.050944607378676114ITERATION : 69, loss : 0.050944607378676114ITERATION : 70, loss : 0.050944607378676114ITERATION : 71, loss : 0.050944607378676114ITERATION : 72, loss : 0.050944607378676114ITERATION : 73, loss : 0.050944607378676114ITERATION : 74, loss : 0.050944607378676114ITERATION : 75, loss : 0.050944607378676114ITERATION : 76, loss : 0.050944607378676114ITERATION : 77, loss : 0.050944607378676114ITERATION : 78, loss : 0.050944607378676114ITERATION : 79, loss : 0.050944607378676114ITERATION : 80, loss : 0.050944607378676114ITERATION : 81, loss : 0.050944607378676114ITERATION : 82, loss : 0.050944607378676114ITERATION : 83, loss : 0.050944607378676114ITERATION : 84, loss : 0.050944607378676114ITERATION : 85, loss : 0.050944607378676114ITERATION : 86, loss : 0.050944607378676114ITERATION : 87, loss : 0.050944607378676114ITERATION : 88, loss : 0.050944607378676114ITERATION : 89, loss : 0.050944607378676114ITERATION : 90, loss : 0.050944607378676114ITERATION : 91, loss : 0.050944607378676114ITERATION : 92, loss : 0.050944607378676114ITERATION : 93, loss : 0.050944607378676114ITERATION : 94, loss : 0.050944607378676114ITERATION : 95, loss : 0.050944607378676114ITERATION : 96, loss : 0.050944607378676114ITERATION : 97, loss : 0.050944607378676114ITERATION : 98, loss : 0.050944607378676114ITERATION : 99, loss : 0.050944607378676114ITERATION : 100, loss : 0.050944607378676114
ITERATION : 1, loss : 0.035400401336152375ITERATION : 2, loss : 0.023291835887204883ITERATION : 3, loss : 0.01893712374612089ITERATION : 4, loss : 0.017511049360585017ITERATION : 5, loss : 0.017204065899228346ITERATION : 6, loss : 0.01729282782977108ITERATION : 7, loss : 0.017489572383874406ITERATION : 8, loss : 0.017686726833020517ITERATION : 9, loss : 0.01785050587922248ITERATION : 10, loss : 0.017975896931148785ITERATION : 11, loss : 0.018067805763761713ITERATION : 12, loss : 0.018133456753875538ITERATION : 13, loss : 0.018179594686714935ITERATION : 14, loss : 0.018211674233376957ITERATION : 15, loss : 0.018233817745359752ITERATION : 16, loss : 0.018249025642037627ITERATION : 17, loss : 0.018259432586101996ITERATION : 18, loss : 0.018266535372929313ITERATION : 19, loss : 0.01827137333189442ITERATION : 20, loss : 0.018274663437775094ITERATION : 21, loss : 0.01827689806054706ITERATION : 22, loss : 0.018278414123659915ITERATION : 23, loss : 0.018279441670401672ITERATION : 24, loss : 0.018280137491828764ITERATION : 25, loss : 0.018280608260467027ITERATION : 26, loss : 0.018280926502401783ITERATION : 27, loss : 0.018281141446368086ITERATION : 28, loss : 0.018281286466168518ITERATION : 29, loss : 0.01828138423300952ITERATION : 30, loss : 0.01828145008094652ITERATION : 31, loss : 0.018281494381878493ITERATION : 32, loss : 0.018281524165993813ITERATION : 33, loss : 0.018281544149852703ITERATION : 34, loss : 0.01828155756172792ITERATION : 35, loss : 0.01828156654500817ITERATION : 36, loss : 0.01828157252517249ITERATION : 37, loss : 0.01828157651804262ITERATION : 38, loss : 0.018281579191089428ITERATION : 39, loss : 0.018281580977312632ITERATION : 40, loss : 0.018281582142897164ITERATION : 41, loss : 0.01828158293916545ITERATION : 42, loss : 0.01828158345741089ITERATION : 43, loss : 0.01828158379469682ITERATION : 44, loss : 0.01828158400584224ITERATION : 45, loss : 0.018281584145140386ITERATION : 46, loss : 0.01828158422754432ITERATION : 47, loss : 0.01828158428301066ITERATION : 48, loss : 0.01828158431401792ITERATION : 49, loss : 0.018281584334716446ITERATION : 50, loss : 0.018281584344290943ITERATION : 51, loss : 0.01828158435448405ITERATION : 52, loss : 0.018281584353848045ITERATION : 53, loss : 0.018281584359446105ITERATION : 54, loss : 0.018281584355718517ITERATION : 55, loss : 0.018281584360439133ITERATION : 56, loss : 0.018281584360439133ITERATION : 57, loss : 0.018281584360439133ITERATION : 58, loss : 0.018281584360439133ITERATION : 59, loss : 0.018281584360439133ITERATION : 60, loss : 0.018281584360439133ITERATION : 61, loss : 0.018281584360439133ITERATION : 62, loss : 0.018281584360439133ITERATION : 63, loss : 0.018281584360439133ITERATION : 64, loss : 0.018281584360439133ITERATION : 65, loss : 0.018281584360439133ITERATION : 66, loss : 0.018281584360439133ITERATION : 67, loss : 0.018281584360439133ITERATION : 68, loss : 0.018281584360439133ITERATION : 69, loss : 0.018281584360439133ITERATION : 70, loss : 0.018281584360439133ITERATION : 71, loss : 0.018281584360439133ITERATION : 72, loss : 0.018281584360439133ITERATION : 73, loss : 0.018281584360439133ITERATION : 74, loss : 0.018281584360439133ITERATION : 75, loss : 0.018281584360439133ITERATION : 76, loss : 0.018281584360439133ITERATION : 77, loss : 0.018281584360439133ITERATION : 78, loss : 0.018281584360439133ITERATION : 79, loss : 0.018281584360439133ITERATION : 80, loss : 0.018281584360439133ITERATION : 81, loss : 0.018281584360439133ITERATION : 82, loss : 0.018281584360439133ITERATION : 83, loss : 0.018281584360439133ITERATION : 84, loss : 0.018281584360439133ITERATION : 85, loss : 0.018281584360439133ITERATION : 86, loss : 0.018281584360439133ITERATION : 87, loss : 0.018281584360439133ITERATION : 88, loss : 0.018281584360439133ITERATION : 89, loss : 0.018281584360439133ITERATION : 90, loss : 0.018281584360439133ITERATION : 91, loss : 0.018281584360439133ITERATION : 92, loss : 0.018281584360439133ITERATION : 93, loss : 0.018281584360439133ITERATION : 94, loss : 0.018281584360439133ITERATION : 95, loss : 0.018281584360439133ITERATION : 96, loss : 0.018281584360439133ITERATION : 97, loss : 0.018281584360439133ITERATION : 98, loss : 0.018281584360439133ITERATION : 99, loss : 0.018281584360439133ITERATION : 100, loss : 0.018281584360439133
ITERATION : 1, loss : 0.03489858331648585ITERATION : 2, loss : 0.04620786753729293ITERATION : 3, loss : 0.05597277319496602ITERATION : 4, loss : 0.06312652071388884ITERATION : 5, loss : 0.06832735196664522ITERATION : 6, loss : 0.07213156833734856ITERATION : 7, loss : 0.07489016836451874ITERATION : 8, loss : 0.07688188546541783ITERATION : 9, loss : 0.07831641406353965ITERATION : 10, loss : 0.07932034973945717ITERATION : 11, loss : 0.08002006744345094ITERATION : 12, loss : 0.08052272895392959ITERATION : 13, loss : 0.08088351615470155ITERATION : 14, loss : 0.08114228404208315ITERATION : 15, loss : 0.08132775909564685ITERATION : 16, loss : 0.08146061938446766ITERATION : 17, loss : 0.0815557344464332ITERATION : 18, loss : 0.0816237882337792ITERATION : 19, loss : 0.08167245228510961ITERATION : 20, loss : 0.08170723135550098ITERATION : 21, loss : 0.08173207323935007ITERATION : 22, loss : 0.08174980724223048ITERATION : 23, loss : 0.08176245997542222ITERATION : 24, loss : 0.08177148229677986ITERATION : 25, loss : 0.08177791231964596ITERATION : 26, loss : 0.08178249212303629ITERATION : 27, loss : 0.08178575238093704ITERATION : 28, loss : 0.08178807176543934ITERATION : 29, loss : 0.08178972110403479ITERATION : 30, loss : 0.08179089326250377ITERATION : 31, loss : 0.08179172567310386ITERATION : 32, loss : 0.08179231652203446ITERATION : 33, loss : 0.0817927356826233ITERATION : 34, loss : 0.08179303286442813ITERATION : 35, loss : 0.08179324342745214ITERATION : 36, loss : 0.08179339250897791ITERATION : 37, loss : 0.08179349799900146ITERATION : 38, loss : 0.08179357257955423ITERATION : 39, loss : 0.08179362527434712ITERATION : 40, loss : 0.08179366248024371ITERATION : 41, loss : 0.08179368875860202ITERATION : 42, loss : 0.08179370726889852ITERATION : 43, loss : 0.0817937203193767ITERATION : 44, loss : 0.0817937295066168ITERATION : 45, loss : 0.0817937359787964ITERATION : 46, loss : 0.08179374054932571ITERATION : 47, loss : 0.08179374375742268ITERATION : 48, loss : 0.08179374599877447ITERATION : 49, loss : 0.08179374755715914ITERATION : 50, loss : 0.0817937486476038ITERATION : 51, loss : 0.08179374940656008ITERATION : 52, loss : 0.08179374992012928ITERATION : 53, loss : 0.08179375027398267ITERATION : 54, loss : 0.08179375053256566ITERATION : 55, loss : 0.0817937506604901ITERATION : 56, loss : 0.08179375074463853ITERATION : 57, loss : 0.08179375082244293ITERATION : 58, loss : 0.08179375090197649ITERATION : 59, loss : 0.08179375091132833ITERATION : 60, loss : 0.08179375094362111ITERATION : 61, loss : 0.0817937509482677ITERATION : 62, loss : 0.0817937509482677ITERATION : 63, loss : 0.0817937509482677ITERATION : 64, loss : 0.0817937509482677ITERATION : 65, loss : 0.0817937509482677ITERATION : 66, loss : 0.0817937509482677ITERATION : 67, loss : 0.0817937509482677ITERATION : 68, loss : 0.0817937509482677ITERATION : 69, loss : 0.0817937509482677ITERATION : 70, loss : 0.0817937509482677ITERATION : 71, loss : 0.0817937509482677ITERATION : 72, loss : 0.0817937509482677ITERATION : 73, loss : 0.0817937509482677ITERATION : 74, loss : 0.0817937509482677ITERATION : 75, loss : 0.0817937509482677ITERATION : 76, loss : 0.0817937509482677ITERATION : 77, loss : 0.0817937509482677ITERATION : 78, loss : 0.0817937509482677ITERATION : 79, loss : 0.0817937509482677ITERATION : 80, loss : 0.0817937509482677ITERATION : 81, loss : 0.0817937509482677ITERATION : 82, loss : 0.0817937509482677ITERATION : 83, loss : 0.0817937509482677ITERATION : 84, loss : 0.0817937509482677ITERATION : 85, loss : 0.0817937509482677ITERATION : 86, loss : 0.0817937509482677ITERATION : 87, loss : 0.0817937509482677ITERATION : 88, loss : 0.0817937509482677ITERATION : 89, loss : 0.0817937509482677ITERATION : 90, loss : 0.0817937509482677ITERATION : 91, loss : 0.0817937509482677ITERATION : 92, loss : 0.0817937509482677ITERATION : 93, loss : 0.0817937509482677ITERATION : 94, loss : 0.0817937509482677ITERATION : 95, loss : 0.0817937509482677ITERATION : 96, loss : 0.0817937509482677ITERATION : 97, loss : 0.0817937509482677ITERATION : 98, loss : 0.0817937509482677ITERATION : 99, loss : 0.0817937509482677ITERATION : 100, loss : 0.0817937509482677
ITERATION : 1, loss : 0.022429941782217976ITERATION : 2, loss : 0.031033946287710758ITERATION : 3, loss : 0.03817963391935025ITERATION : 4, loss : 0.04335716400260677ITERATION : 5, loss : 0.04701553409717035ITERATION : 6, loss : 0.049592426990526614ITERATION : 7, loss : 0.05140932360400288ITERATION : 8, loss : 0.05269233558455788ITERATION : 9, loss : 0.0535995687229209ITERATION : 10, loss : 0.05424175920478677ITERATION : 11, loss : 0.054696690055286414ITERATION : 12, loss : 0.055019142659957374ITERATION : 13, loss : 0.05524778155053472ITERATION : 14, loss : 0.055409940560964835ITERATION : 15, loss : 0.05552496656079477ITERATION : 16, loss : 0.05560656556355946ITERATION : 17, loss : 0.055664452952403395ITERATION : 18, loss : 0.05570551840056659ITERATION : 19, loss : 0.055734648955190626ITERATION : 20, loss : 0.05575531203333573ITERATION : 21, loss : 0.055769967821672144ITERATION : 22, loss : 0.05578036183126667ITERATION : 23, loss : 0.05578773261023913ITERATION : 24, loss : 0.055792959013367095ITERATION : 25, loss : 0.0557966645743559ITERATION : 26, loss : 0.055799291508361576ITERATION : 27, loss : 0.05580115359389427ITERATION : 28, loss : 0.05580247348548531ITERATION : 29, loss : 0.05580340886364199ITERATION : 30, loss : 0.05580407166037935ITERATION : 31, loss : 0.05580454127695978ITERATION : 32, loss : 0.055804873986441794ITERATION : 33, loss : 0.05580510964964851ITERATION : 34, loss : 0.055805276574769705ITERATION : 35, loss : 0.05580539481653234ITERATION : 36, loss : 0.05580547854036845ITERATION : 37, loss : 0.055805537816934876ITERATION : 38, loss : 0.055805579733084705ITERATION : 39, loss : 0.05580560946596246ITERATION : 40, loss : 0.05580563053954935ITERATION : 41, loss : 0.05580564543224522ITERATION : 42, loss : 0.05580565597154883ITERATION : 43, loss : 0.05580566343636361ITERATION : 44, loss : 0.05580566869963753ITERATION : 45, loss : 0.05580567242430319ITERATION : 46, loss : 0.05580567505503066ITERATION : 47, loss : 0.05580567693406913ITERATION : 48, loss : 0.055805678243365346ITERATION : 49, loss : 0.0558056791719563ITERATION : 50, loss : 0.05580567982071166ITERATION : 51, loss : 0.05580568021108087ITERATION : 52, loss : 0.055805680494731884ITERATION : 53, loss : 0.055805680745952504ITERATION : 54, loss : 0.05580568091535332ITERATION : 55, loss : 0.05580568101709981ITERATION : 56, loss : 0.05580568108632561ITERATION : 57, loss : 0.0558056811152725ITERATION : 58, loss : 0.0558056811152725ITERATION : 59, loss : 0.0558056811152725ITERATION : 60, loss : 0.0558056811152725ITERATION : 61, loss : 0.0558056811152725ITERATION : 62, loss : 0.0558056811152725ITERATION : 63, loss : 0.0558056811152725ITERATION : 64, loss : 0.0558056811152725ITERATION : 65, loss : 0.0558056811152725ITERATION : 66, loss : 0.0558056811152725ITERATION : 67, loss : 0.0558056811152725ITERATION : 68, loss : 0.0558056811152725ITERATION : 69, loss : 0.0558056811152725ITERATION : 70, loss : 0.0558056811152725ITERATION : 71, loss : 0.0558056811152725ITERATION : 72, loss : 0.0558056811152725ITERATION : 73, loss : 0.0558056811152725ITERATION : 74, loss : 0.0558056811152725ITERATION : 75, loss : 0.0558056811152725ITERATION : 76, loss : 0.0558056811152725ITERATION : 77, loss : 0.0558056811152725ITERATION : 78, loss : 0.0558056811152725ITERATION : 79, loss : 0.0558056811152725ITERATION : 80, loss : 0.0558056811152725ITERATION : 81, loss : 0.0558056811152725ITERATION : 82, loss : 0.0558056811152725ITERATION : 83, loss : 0.0558056811152725ITERATION : 84, loss : 0.0558056811152725ITERATION : 85, loss : 0.0558056811152725ITERATION : 86, loss : 0.0558056811152725ITERATION : 87, loss : 0.0558056811152725ITERATION : 88, loss : 0.0558056811152725ITERATION : 89, loss : 0.0558056811152725ITERATION : 90, loss : 0.0558056811152725ITERATION : 91, loss : 0.0558056811152725ITERATION : 92, loss : 0.0558056811152725ITERATION : 93, loss : 0.0558056811152725ITERATION : 94, loss : 0.0558056811152725ITERATION : 95, loss : 0.0558056811152725ITERATION : 96, loss : 0.0558056811152725ITERATION : 97, loss : 0.0558056811152725ITERATION : 98, loss : 0.0558056811152725ITERATION : 99, loss : 0.0558056811152725ITERATION : 100, loss : 0.0558056811152725
ITERATION : 1, loss : 0.01084540478703095ITERATION : 2, loss : 0.013115759425432699ITERATION : 3, loss : 0.015514139503417923ITERATION : 4, loss : 0.017670369417192293ITERATION : 5, loss : 0.019376308385956276ITERATION : 6, loss : 0.02065353054658492ITERATION : 7, loss : 0.021581983230015166ITERATION : 8, loss : 0.022245294561419143ITERATION : 9, loss : 0.022714105230961755ITERATION : 10, loss : 0.023043163382374318ITERATION : 11, loss : 0.023273080062557263ITERATION : 12, loss : 0.02343323604361588ITERATION : 13, loss : 0.023544566591702686ITERATION : 14, loss : 0.023621846026833423ITERATION : 15, loss : 0.023675435526756217ITERATION : 16, loss : 0.023712570924201034ITERATION : 17, loss : 0.023738291139214284ITERATION : 18, loss : 0.023756098414048455ITERATION : 19, loss : 0.023768423759719746ITERATION : 20, loss : 0.023776952834311714ITERATION : 21, loss : 0.023782853914826124ITERATION : 22, loss : 0.02378693611047806ITERATION : 23, loss : 0.0237897597034039ITERATION : 24, loss : 0.02379171257119078ITERATION : 25, loss : 0.023793063034475103ITERATION : 26, loss : 0.023793996796172907ITERATION : 27, loss : 0.023794642406671423ITERATION : 28, loss : 0.0237950886920722ITERATION : 29, loss : 0.02379539724980073ITERATION : 30, loss : 0.02379561052023804ITERATION : 31, loss : 0.023795757951999407ITERATION : 32, loss : 0.02379585981199718ITERATION : 33, loss : 0.023795930217138685ITERATION : 34, loss : 0.023795978848958874ITERATION : 35, loss : 0.023796012452893305ITERATION : 36, loss : 0.02379603567610706ITERATION : 37, loss : 0.023796051719097107ITERATION : 38, loss : 0.023796062796382796ITERATION : 39, loss : 0.023796070450416915ITERATION : 40, loss : 0.023796075730331302ITERATION : 41, loss : 0.023796079418390445ITERATION : 42, loss : 0.02379608189585921ITERATION : 43, loss : 0.02379608363317661ITERATION : 44, loss : 0.023796084796239736ITERATION : 45, loss : 0.02379608560619052ITERATION : 46, loss : 0.023796086117491653ITERATION : 47, loss : 0.023796086526376846ITERATION : 48, loss : 0.023796086773159128ITERATION : 49, loss : 0.02379608698280444ITERATION : 50, loss : 0.023796087070303656ITERATION : 51, loss : 0.02379608714073957ITERATION : 52, loss : 0.02379608719611713ITERATION : 53, loss : 0.023796087242016078ITERATION : 54, loss : 0.02379608725723203ITERATION : 55, loss : 0.02379608725859385ITERATION : 56, loss : 0.02379608725859385ITERATION : 57, loss : 0.02379608725859385ITERATION : 58, loss : 0.02379608725859385ITERATION : 59, loss : 0.02379608725859385ITERATION : 60, loss : 0.02379608725859385ITERATION : 61, loss : 0.02379608725859385ITERATION : 62, loss : 0.02379608725859385ITERATION : 63, loss : 0.02379608725859385ITERATION : 64, loss : 0.02379608725859385ITERATION : 65, loss : 0.02379608725859385ITERATION : 66, loss : 0.02379608725859385ITERATION : 67, loss : 0.02379608725859385ITERATION : 68, loss : 0.02379608725859385ITERATION : 69, loss : 0.02379608725859385ITERATION : 70, loss : 0.02379608725859385ITERATION : 71, loss : 0.02379608725859385ITERATION : 72, loss : 0.02379608725859385ITERATION : 73, loss : 0.02379608725859385ITERATION : 74, loss : 0.02379608725859385ITERATION : 75, loss : 0.02379608725859385ITERATION : 76, loss : 0.02379608725859385ITERATION : 77, loss : 0.02379608725859385ITERATION : 78, loss : 0.02379608725859385ITERATION : 79, loss : 0.02379608725859385ITERATION : 80, loss : 0.02379608725859385ITERATION : 81, loss : 0.02379608725859385ITERATION : 82, loss : 0.02379608725859385ITERATION : 83, loss : 0.02379608725859385ITERATION : 84, loss : 0.02379608725859385ITERATION : 85, loss : 0.02379608725859385ITERATION : 86, loss : 0.02379608725859385ITERATION : 87, loss : 0.02379608725859385ITERATION : 88, loss : 0.02379608725859385ITERATION : 89, loss : 0.02379608725859385ITERATION : 90, loss : 0.02379608725859385ITERATION : 91, loss : 0.02379608725859385ITERATION : 92, loss : 0.02379608725859385ITERATION : 93, loss : 0.02379608725859385ITERATION : 94, loss : 0.02379608725859385ITERATION : 95, loss : 0.02379608725859385ITERATION : 96, loss : 0.02379608725859385ITERATION : 97, loss : 0.02379608725859385ITERATION : 98, loss : 0.02379608725859385ITERATION : 99, loss : 0.02379608725859385ITERATION : 100, loss : 0.02379608725859385
ITERATION : 1, loss : 0.32899631065420154ITERATION : 2, loss : 0.3581797060942581ITERATION : 3, loss : 0.368353878996852ITERATION : 4, loss : 0.3730704321181826ITERATION : 5, loss : 0.3755969750411168ITERATION : 6, loss : 0.37704762735609465ITERATION : 7, loss : 0.3779161874450638ITERATION : 8, loss : 0.3784503872468399ITERATION : 9, loss : 0.3787849103022476ITERATION : 10, loss : 0.37899702886873776ITERATION : 11, loss : 0.37913274094625155ITERATION : 12, loss : 0.3792201415972959ITERATION : 13, loss : 0.37927670804432584ITERATION : 14, loss : 0.37931345761058055ITERATION : 15, loss : 0.3793374037372279ITERATION : 16, loss : 0.37935304313549373ITERATION : 17, loss : 0.37936327691950467ITERATION : 18, loss : 0.37936998424611235ITERATION : 19, loss : 0.37937438591192185ITERATION : 20, loss : 0.3793772775560802ITERATION : 21, loss : 0.3793791788905174ITERATION : 22, loss : 0.37938042999408866ITERATION : 23, loss : 0.3793812537588476ITERATION : 24, loss : 0.3793817964350124ITERATION : 25, loss : 0.3793821541039799ITERATION : 26, loss : 0.3793823899207768ITERATION : 27, loss : 0.3793825454503099ITERATION : 28, loss : 0.3793826480583519ITERATION : 29, loss : 0.3793827157680539ITERATION : 30, loss : 0.3793827604563033ITERATION : 31, loss : 0.37938278995513053ITERATION : 32, loss : 0.379382809433348ITERATION : 33, loss : 0.3793828222952868ITERATION : 34, loss : 0.3793828307867989ITERATION : 35, loss : 0.37938283639612225ITERATION : 36, loss : 0.3793828401023432ITERATION : 37, loss : 0.3793828425497533ITERATION : 38, loss : 0.3793828441639407ITERATION : 39, loss : 0.37938284523179383ITERATION : 40, loss : 0.37938284593381033ITERATION : 41, loss : 0.37938284640153175ITERATION : 42, loss : 0.3793828467097084ITERATION : 43, loss : 0.3793828469135143ITERATION : 44, loss : 0.3793828470457989ITERATION : 45, loss : 0.37938284713462506ITERATION : 46, loss : 0.3793828471937448ITERATION : 47, loss : 0.37938284723242993ITERATION : 48, loss : 0.37938284726086213ITERATION : 49, loss : 0.3793828472779308ITERATION : 50, loss : 0.3793828472864256ITERATION : 51, loss : 0.37938284729301835ITERATION : 52, loss : 0.3793828472981876ITERATION : 53, loss : 0.37938284729989596ITERATION : 54, loss : 0.37938284730093297ITERATION : 55, loss : 0.37938284730424876ITERATION : 56, loss : 0.37938284730424876ITERATION : 57, loss : 0.37938284730424876ITERATION : 58, loss : 0.37938284730424876ITERATION : 59, loss : 0.37938284730424876ITERATION : 60, loss : 0.37938284730424876ITERATION : 61, loss : 0.37938284730424876ITERATION : 62, loss : 0.37938284730424876ITERATION : 63, loss : 0.37938284730424876ITERATION : 64, loss : 0.37938284730424876ITERATION : 65, loss : 0.37938284730424876ITERATION : 66, loss : 0.37938284730424876ITERATION : 67, loss : 0.37938284730424876ITERATION : 68, loss : 0.37938284730424876ITERATION : 69, loss : 0.37938284730424876ITERATION : 70, loss : 0.37938284730424876ITERATION : 71, loss : 0.37938284730424876ITERATION : 72, loss : 0.37938284730424876ITERATION : 73, loss : 0.37938284730424876ITERATION : 74, loss : 0.37938284730424876ITERATION : 75, loss : 0.37938284730424876ITERATION : 76, loss : 0.37938284730424876ITERATION : 77, loss : 0.37938284730424876ITERATION : 78, loss : 0.37938284730424876ITERATION : 79, loss : 0.37938284730424876ITERATION : 80, loss : 0.37938284730424876ITERATION : 81, loss : 0.37938284730424876ITERATION : 82, loss : 0.37938284730424876ITERATION : 83, loss : 0.37938284730424876ITERATION : 84, loss : 0.37938284730424876ITERATION : 85, loss : 0.37938284730424876ITERATION : 86, loss : 0.37938284730424876ITERATION : 87, loss : 0.37938284730424876ITERATION : 88, loss : 0.37938284730424876ITERATION : 89, loss : 0.37938284730424876ITERATION : 90, loss : 0.37938284730424876ITERATION : 91, loss : 0.37938284730424876ITERATION : 92, loss : 0.37938284730424876ITERATION : 93, loss : 0.37938284730424876ITERATION : 94, loss : 0.37938284730424876ITERATION : 95, loss : 0.37938284730424876ITERATION : 96, loss : 0.37938284730424876ITERATION : 97, loss : 0.37938284730424876ITERATION : 98, loss : 0.37938284730424876ITERATION : 99, loss : 0.37938284730424876ITERATION : 100, loss : 0.37938284730424876
ITERATION : 1, loss : 0.13691946233058697ITERATION : 2, loss : 0.1663081450594476ITERATION : 3, loss : 0.18122022809828567ITERATION : 4, loss : 0.1890514637242193ITERATION : 5, loss : 0.19363345773365276ITERATION : 6, loss : 0.19649353851345705ITERATION : 7, loss : 0.19835249824824946ITERATION : 8, loss : 0.19959264678465544ITERATION : 9, loss : 0.20043418932762072ITERATION : 10, loss : 0.20093831525450018ITERATION : 11, loss : 0.20127780771589268ITERATION : 12, loss : 0.20151358433326602ITERATION : 13, loss : 0.20167774706518088ITERATION : 14, loss : 0.2017922140276259ITERATION : 15, loss : 0.20187208722628447ITERATION : 16, loss : 0.20192783486979474ITERATION : 17, loss : 0.20196674102939496ITERATION : 18, loss : 0.2019938858764176ITERATION : 19, loss : 0.20201281707361074ITERATION : 20, loss : 0.20202601356757813ITERATION : 21, loss : 0.20203520769037622ITERATION : 22, loss : 0.202041609848635ITERATION : 23, loss : 0.20204606545471446ITERATION : 24, loss : 0.202049164701467ITERATION : 25, loss : 0.20205131937611362ITERATION : 26, loss : 0.20205281658106852ITERATION : 27, loss : 0.20205385646919538ITERATION : 28, loss : 0.20205457837848384ITERATION : 29, loss : 0.20205507932245817ITERATION : 30, loss : 0.2020554267842385ITERATION : 31, loss : 0.20205566769544883ITERATION : 32, loss : 0.2020558346551861ITERATION : 33, loss : 0.20205595034430557ITERATION : 34, loss : 0.2020560304659196ITERATION : 35, loss : 0.20205608593589722ITERATION : 36, loss : 0.20205612433473052ITERATION : 37, loss : 0.20205615090801532ITERATION : 38, loss : 0.2020561692844897ITERATION : 39, loss : 0.20205618199680264ITERATION : 40, loss : 0.20205619078578665ITERATION : 41, loss : 0.20205619686330656ITERATION : 42, loss : 0.20205620105979744ITERATION : 43, loss : 0.20205620395440158ITERATION : 44, loss : 0.2020562059518911ITERATION : 45, loss : 0.2020562073276102ITERATION : 46, loss : 0.2020562082824121ITERATION : 47, loss : 0.20205620893437418ITERATION : 48, loss : 0.2020562093892612ITERATION : 49, loss : 0.20205620969320334ITERATION : 50, loss : 0.20205620991101253ITERATION : 51, loss : 0.20205621006978614ITERATION : 52, loss : 0.20205621017231007ITERATION : 53, loss : 0.20205621024273024ITERATION : 54, loss : 0.20205621028917745ITERATION : 55, loss : 0.20205621031675644ITERATION : 56, loss : 0.2020562103377879ITERATION : 57, loss : 0.202056210356538ITERATION : 58, loss : 0.2020562103603543ITERATION : 59, loss : 0.2020562103705085ITERATION : 60, loss : 0.20205621037187432ITERATION : 61, loss : 0.20205621037187432ITERATION : 62, loss : 0.20205621037187432ITERATION : 63, loss : 0.20205621037187432ITERATION : 64, loss : 0.20205621037187432ITERATION : 65, loss : 0.20205621037187432ITERATION : 66, loss : 0.20205621037187432ITERATION : 67, loss : 0.20205621037187432ITERATION : 68, loss : 0.20205621037187432ITERATION : 69, loss : 0.20205621037187432ITERATION : 70, loss : 0.20205621037187432ITERATION : 71, loss : 0.20205621037187432ITERATION : 72, loss : 0.20205621037187432ITERATION : 73, loss : 0.20205621037187432ITERATION : 74, loss : 0.20205621037187432ITERATION : 75, loss : 0.20205621037187432ITERATION : 76, loss : 0.20205621037187432ITERATION : 77, loss : 0.20205621037187432ITERATION : 78, loss : 0.20205621037187432ITERATION : 79, loss : 0.20205621037187432ITERATION : 80, loss : 0.20205621037187432ITERATION : 81, loss : 0.20205621037187432ITERATION : 82, loss : 0.20205621037187432ITERATION : 83, loss : 0.20205621037187432ITERATION : 84, loss : 0.20205621037187432ITERATION : 85, loss : 0.20205621037187432ITERATION : 86, loss : 0.20205621037187432ITERATION : 87, loss : 0.20205621037187432ITERATION : 88, loss : 0.20205621037187432ITERATION : 89, loss : 0.20205621037187432ITERATION : 90, loss : 0.20205621037187432ITERATION : 91, loss : 0.20205621037187432ITERATION : 92, loss : 0.20205621037187432ITERATION : 93, loss : 0.20205621037187432ITERATION : 94, loss : 0.20205621037187432ITERATION : 95, loss : 0.20205621037187432ITERATION : 96, loss : 0.20205621037187432ITERATION : 97, loss : 0.20205621037187432ITERATION : 98, loss : 0.20205621037187432ITERATION : 99, loss : 0.20205621037187432ITERATION : 100, loss : 0.20205621037187432
gradient norm in None layer : 0.00631527055291921
gradient norm in None layer : 0.00014489056796667874
gradient norm in None layer : 0.00015371334019570942
gradient norm in None layer : 0.0024853912820671314
gradient norm in None layer : 0.00015292560818850858
gradient norm in None layer : 0.0001548909675132588
gradient norm in None layer : 0.0008466661819111119
gradient norm in None layer : 3.631181418595123e-05
gradient norm in None layer : 3.212785326858321e-05
gradient norm in None layer : 0.0007900430799545528
gradient norm in None layer : 3.6540236455167904e-05
gradient norm in None layer : 3.142164816587392e-05
gradient norm in None layer : 0.00020401397636787724
gradient norm in None layer : 7.925127890766236e-06
gradient norm in None layer : 6.955729504592964e-06
gradient norm in None layer : 0.00023998456090789687
gradient norm in None layer : 9.33078646163411e-06
gradient norm in None layer : 8.771369056409022e-06
gradient norm in None layer : 0.0003479714949883137
gradient norm in None layer : 2.779782345665902e-06
gradient norm in None layer : 0.0007258110428616536
gradient norm in None layer : 5.22729244256099e-05
gradient norm in None layer : 4.104691427532495e-05
gradient norm in None layer : 0.0008232535459067233
gradient norm in None layer : 7.728192510091942e-05
gradient norm in None layer : 7.929347555802586e-05
gradient norm in None layer : 0.0014236101864634008
gradient norm in None layer : 5.2668290110794166e-06
gradient norm in None layer : 0.002112327350861334
gradient norm in None layer : 0.00014587260572264223
gradient norm in None layer : 0.00012881021299300006
gradient norm in None layer : 0.0021359746315133088
gradient norm in None layer : 0.0002758021537814106
gradient norm in None layer : 0.0003643306723693014
gradient norm in None layer : 0.00021808075703078227
gradient norm in None layer : 5.835773684749107e-05
Total gradient norm: 0.007764531519277772
invariance loss : 4.776805910801856, avg_den : 0.4319305419921875, density loss : 0.33173648940172135, mse loss : 0.11654371323417376, solver time : 118.52101588249207 sec , total loss : 0.12165225563437734, running loss : 0.10901451172471216
Epoch 0/10 , batch 17/12500 
ITERATION : 1, loss : 0.054926846602083884ITERATION : 2, loss : 0.06551777012381124ITERATION : 3, loss : 0.07630075662035336ITERATION : 4, loss : 0.08439732034722536ITERATION : 5, loss : 0.0915458496481766ITERATION : 6, loss : 0.09689962917135926ITERATION : 7, loss : 0.10059186112181849ITERATION : 8, loss : 0.1031676565896923ITERATION : 9, loss : 0.10498096173230768ITERATION : 10, loss : 0.1062661440702298ITERATION : 11, loss : 0.10718150588048926ITERATION : 12, loss : 0.10783576854971876ITERATION : 13, loss : 0.10830458044820879ITERATION : 14, loss : 0.10864109860576206ITERATION : 15, loss : 0.10888294893339773ITERATION : 16, loss : 0.10905690626277441ITERATION : 17, loss : 0.10918209740270857ITERATION : 18, loss : 0.10927222344884714ITERATION : 19, loss : 0.10933711794614702ITERATION : 20, loss : 0.10938384845102263ITERATION : 21, loss : 0.10941749926047739ITERATION : 22, loss : 0.10944173010790921ITERATION : 23, loss : 0.10945917631955371ITERATION : 24, loss : 0.10947173613224552ITERATION : 25, loss : 0.10948077692493481ITERATION : 26, loss : 0.10948728361679806ITERATION : 27, loss : 0.10949196580295349ITERATION : 28, loss : 0.10949533454894603ITERATION : 29, loss : 0.109497757860297ITERATION : 30, loss : 0.10949950076484333ITERATION : 31, loss : 0.10950075413328257ITERATION : 32, loss : 0.1095016552722068ITERATION : 33, loss : 0.10950230308254379ITERATION : 34, loss : 0.10950276869457445ITERATION : 35, loss : 0.1095031032764979ITERATION : 36, loss : 0.10950334363794316ITERATION : 37, loss : 0.10950351631687559ITERATION : 38, loss : 0.1095036403401489ITERATION : 39, loss : 0.1095037294029393ITERATION : 40, loss : 0.10950379335555044ITERATION : 41, loss : 0.10950383925484855ITERATION : 42, loss : 0.10950387220496328ITERATION : 43, loss : 0.1095038958473528ITERATION : 44, loss : 0.10950391283186724ITERATION : 45, loss : 0.10950392499942556ITERATION : 46, loss : 0.10950393370341731ITERATION : 47, loss : 0.10950393996620758ITERATION : 48, loss : 0.10950394445638988ITERATION : 49, loss : 0.10950394767171392ITERATION : 50, loss : 0.10950395000145664ITERATION : 51, loss : 0.10950395170139349ITERATION : 52, loss : 0.10950395285924762ITERATION : 53, loss : 0.10950395369389923ITERATION : 54, loss : 0.10950395430919364ITERATION : 55, loss : 0.109503954734206ITERATION : 56, loss : 0.1095039550500892ITERATION : 57, loss : 0.10950395522891852ITERATION : 58, loss : 0.10950395539333514ITERATION : 59, loss : 0.1095039555110771ITERATION : 60, loss : 0.10950395559872889ITERATION : 61, loss : 0.10950395562759425ITERATION : 62, loss : 0.10950395564414611ITERATION : 63, loss : 0.1095039556523815ITERATION : 64, loss : 0.1095039556523815ITERATION : 65, loss : 0.1095039556523815ITERATION : 66, loss : 0.1095039556523815ITERATION : 67, loss : 0.1095039556523815ITERATION : 68, loss : 0.1095039556523815ITERATION : 69, loss : 0.1095039556523815ITERATION : 70, loss : 0.1095039556523815ITERATION : 71, loss : 0.1095039556523815ITERATION : 72, loss : 0.1095039556523815ITERATION : 73, loss : 0.1095039556523815ITERATION : 74, loss : 0.1095039556523815ITERATION : 75, loss : 0.1095039556523815ITERATION : 76, loss : 0.1095039556523815ITERATION : 77, loss : 0.1095039556523815ITERATION : 78, loss : 0.1095039556523815ITERATION : 79, loss : 0.1095039556523815ITERATION : 80, loss : 0.1095039556523815ITERATION : 81, loss : 0.1095039556523815ITERATION : 82, loss : 0.1095039556523815ITERATION : 83, loss : 0.1095039556523815ITERATION : 84, loss : 0.1095039556523815ITERATION : 85, loss : 0.1095039556523815ITERATION : 86, loss : 0.1095039556523815ITERATION : 87, loss : 0.1095039556523815ITERATION : 88, loss : 0.1095039556523815ITERATION : 89, loss : 0.1095039556523815ITERATION : 90, loss : 0.1095039556523815ITERATION : 91, loss : 0.1095039556523815ITERATION : 92, loss : 0.1095039556523815ITERATION : 93, loss : 0.1095039556523815ITERATION : 94, loss : 0.1095039556523815ITERATION : 95, loss : 0.1095039556523815ITERATION : 96, loss : 0.1095039556523815ITERATION : 97, loss : 0.1095039556523815ITERATION : 98, loss : 0.1095039556523815ITERATION : 99, loss : 0.1095039556523815ITERATION : 100, loss : 0.1095039556523815
ITERATION : 1, loss : 0.0480606645296582ITERATION : 2, loss : 0.06270916015503121ITERATION : 3, loss : 0.0726744910229979ITERATION : 4, loss : 0.07960950699908137ITERATION : 5, loss : 0.08452594334568117ITERATION : 6, loss : 0.08805799850599663ITERATION : 7, loss : 0.090620333209306ITERATION : 8, loss : 0.09249255372540452ITERATION : 9, loss : 0.09386774111715043ITERATION : 10, loss : 0.09488173921458498ITERATION : 11, loss : 0.09563151463578975ITERATION : 12, loss : 0.09618704983230464ITERATION : 13, loss : 0.09659927461782938ITERATION : 14, loss : 0.09690548399567935ITERATION : 15, loss : 0.09713311505661885ITERATION : 16, loss : 0.0973024221824951ITERATION : 17, loss : 0.09742839525904877ITERATION : 18, loss : 0.09752214838583562ITERATION : 19, loss : 0.09759193292200764ITERATION : 20, loss : 0.09764388096386091ITERATION : 21, loss : 0.09768255308080155ITERATION : 22, loss : 0.09771134217737037ITERATION : 23, loss : 0.09773277352848775ITERATION : 24, loss : 0.09774872703433109ITERATION : 25, loss : 0.09776060222690382ITERATION : 26, loss : 0.09776944116094588ITERATION : 27, loss : 0.09777601982312548ITERATION : 28, loss : 0.09778091581153796ITERATION : 29, loss : 0.09778455932840716ITERATION : 30, loss : 0.09778727058802186ITERATION : 31, loss : 0.09778928802668736ITERATION : 32, loss : 0.09779078914106543ITERATION : 33, loss : 0.09779190598857478ITERATION : 34, loss : 0.09779273688545188ITERATION : 35, loss : 0.09779335500911539ITERATION : 36, loss : 0.09779381483067955ITERATION : 37, loss : 0.097794156874155ITERATION : 38, loss : 0.09779441127539062ITERATION : 39, loss : 0.09779460049348451ITERATION : 40, loss : 0.09779474123027648ITERATION : 41, loss : 0.09779484589786985ITERATION : 42, loss : 0.09779492374747252ITERATION : 43, loss : 0.09779498163259695ITERATION : 44, loss : 0.09779502467288294ITERATION : 45, loss : 0.09779505666512849ITERATION : 46, loss : 0.09779508043495684ITERATION : 47, loss : 0.0977950981539054ITERATION : 48, loss : 0.0977951113178707ITERATION : 49, loss : 0.09779512106195772ITERATION : 50, loss : 0.09779512833268912ITERATION : 51, loss : 0.0977951338005407ITERATION : 52, loss : 0.09779513778216004ITERATION : 53, loss : 0.09779514074403202ITERATION : 54, loss : 0.09779514306381396ITERATION : 55, loss : 0.09779514461630257ITERATION : 56, loss : 0.09779514582152708ITERATION : 57, loss : 0.09779514669134858ITERATION : 58, loss : 0.09779514732859278ITERATION : 59, loss : 0.09779514778211561ITERATION : 60, loss : 0.09779514810804518ITERATION : 61, loss : 0.0977951484408242ITERATION : 62, loss : 0.09779514869087856ITERATION : 63, loss : 0.09779514881611796ITERATION : 64, loss : 0.09779514896057336ITERATION : 65, loss : 0.09779514899543347ITERATION : 66, loss : 0.09779514908234359ITERATION : 67, loss : 0.09779514911613275ITERATION : 68, loss : 0.09779514911650973ITERATION : 69, loss : 0.09779514911650973ITERATION : 70, loss : 0.09779514911650973ITERATION : 71, loss : 0.09779514911650973ITERATION : 72, loss : 0.09779514911650973ITERATION : 73, loss : 0.09779514911650973ITERATION : 74, loss : 0.09779514911650973ITERATION : 75, loss : 0.09779514911650973ITERATION : 76, loss : 0.09779514911650973ITERATION : 77, loss : 0.09779514911650973ITERATION : 78, loss : 0.09779514911650973ITERATION : 79, loss : 0.09779514911650973ITERATION : 80, loss : 0.09779514911650973ITERATION : 81, loss : 0.09779514911650973ITERATION : 82, loss : 0.09779514911650973ITERATION : 83, loss : 0.09779514911650973ITERATION : 84, loss : 0.09779514911650973ITERATION : 85, loss : 0.09779514911650973ITERATION : 86, loss : 0.09779514911650973ITERATION : 87, loss : 0.09779514911650973ITERATION : 88, loss : 0.09779514911650973ITERATION : 89, loss : 0.09779514911650973ITERATION : 90, loss : 0.09779514911650973ITERATION : 91, loss : 0.09779514911650973ITERATION : 92, loss : 0.09779514911650973ITERATION : 93, loss : 0.09779514911650973ITERATION : 94, loss : 0.09779514911650973ITERATION : 95, loss : 0.09779514911650973ITERATION : 96, loss : 0.09779514911650973ITERATION : 97, loss : 0.09779514911650973ITERATION : 98, loss : 0.09779514911650973ITERATION : 99, loss : 0.09779514911650973ITERATION : 100, loss : 0.09779514911650973
ITERATION : 1, loss : 0.040284917202398136ITERATION : 2, loss : 0.05486810375127318ITERATION : 3, loss : 0.0664890786158404ITERATION : 4, loss : 0.0742466046034531ITERATION : 5, loss : 0.07926614087534203ITERATION : 6, loss : 0.0825010208531917ITERATION : 7, loss : 0.08468188044187121ITERATION : 8, loss : 0.08616330021086961ITERATION : 9, loss : 0.08717535789842094ITERATION : 10, loss : 0.08786941932533082ITERATION : 11, loss : 0.0883464897271568ITERATION : 12, loss : 0.08867476538863117ITERATION : 13, loss : 0.08890069809012266ITERATION : 14, loss : 0.08905612258961866ITERATION : 15, loss : 0.08916294281519015ITERATION : 16, loss : 0.08923626424096648ITERATION : 17, loss : 0.08928651467079708ITERATION : 18, loss : 0.0893208938736318ITERATION : 19, loss : 0.08934437035125414ITERATION : 20, loss : 0.0893603693958797ITERATION : 21, loss : 0.08937124943664218ITERATION : 22, loss : 0.08937863172089199ITERATION : 23, loss : 0.08938362893313428ITERATION : 24, loss : 0.0893870032786215ITERATION : 25, loss : 0.08938927585161513ITERATION : 26, loss : 0.0893908021457862ITERATION : 27, loss : 0.08939182420450557ITERATION : 28, loss : 0.08939250651240681ITERATION : 29, loss : 0.08939296046259874ITERATION : 30, loss : 0.08939326139777981ITERATION : 31, loss : 0.089393460123975ITERATION : 32, loss : 0.08939359075278701ITERATION : 33, loss : 0.08939367622802495ITERATION : 34, loss : 0.08939373184702373ITERATION : 35, loss : 0.08939376781188423ITERATION : 36, loss : 0.08939379091621291ITERATION : 37, loss : 0.08939380565392571ITERATION : 38, loss : 0.08939381495229841ITERATION : 39, loss : 0.08939382075012288ITERATION : 40, loss : 0.08939382431521604ITERATION : 41, loss : 0.08939382649307895ITERATION : 42, loss : 0.08939382779328948ITERATION : 43, loss : 0.08939382852129348ITERATION : 44, loss : 0.08939382890484018ITERATION : 45, loss : 0.08939382913368663ITERATION : 46, loss : 0.08939382923167656ITERATION : 47, loss : 0.08939382925011087ITERATION : 48, loss : 0.08939382923572954ITERATION : 49, loss : 0.08939382919433925ITERATION : 50, loss : 0.08939382915028557ITERATION : 51, loss : 0.08939382910560424ITERATION : 52, loss : 0.08939382908707902ITERATION : 53, loss : 0.08939382905881042ITERATION : 54, loss : 0.0893938290550017ITERATION : 55, loss : 0.08939382905480811ITERATION : 56, loss : 0.08939382905480811ITERATION : 57, loss : 0.08939382905480811ITERATION : 58, loss : 0.08939382905480811ITERATION : 59, loss : 0.08939382905480811ITERATION : 60, loss : 0.08939382905480811ITERATION : 61, loss : 0.08939382905480811ITERATION : 62, loss : 0.08939382905480811ITERATION : 63, loss : 0.08939382905480811ITERATION : 64, loss : 0.08939382905480811ITERATION : 65, loss : 0.08939382905480811ITERATION : 66, loss : 0.08939382905480811ITERATION : 67, loss : 0.08939382905480811ITERATION : 68, loss : 0.08939382905480811ITERATION : 69, loss : 0.08939382905480811ITERATION : 70, loss : 0.08939382905480811ITERATION : 71, loss : 0.08939382905480811ITERATION : 72, loss : 0.08939382905480811ITERATION : 73, loss : 0.08939382905480811ITERATION : 74, loss : 0.08939382905480811ITERATION : 75, loss : 0.08939382905480811ITERATION : 76, loss : 0.08939382905480811ITERATION : 77, loss : 0.08939382905480811ITERATION : 78, loss : 0.08939382905480811ITERATION : 79, loss : 0.08939382905480811ITERATION : 80, loss : 0.08939382905480811ITERATION : 81, loss : 0.08939382905480811ITERATION : 82, loss : 0.08939382905480811ITERATION : 83, loss : 0.08939382905480811ITERATION : 84, loss : 0.08939382905480811ITERATION : 85, loss : 0.08939382905480811ITERATION : 86, loss : 0.08939382905480811ITERATION : 87, loss : 0.08939382905480811ITERATION : 88, loss : 0.08939382905480811ITERATION : 89, loss : 0.08939382905480811ITERATION : 90, loss : 0.08939382905480811ITERATION : 91, loss : 0.08939382905480811ITERATION : 92, loss : 0.08939382905480811ITERATION : 93, loss : 0.08939382905480811ITERATION : 94, loss : 0.08939382905480811ITERATION : 95, loss : 0.08939382905480811ITERATION : 96, loss : 0.08939382905480811ITERATION : 97, loss : 0.08939382905480811ITERATION : 98, loss : 0.08939382905480811ITERATION : 99, loss : 0.08939382905480811ITERATION : 100, loss : 0.08939382905480811
ITERATION : 1, loss : 0.08628261205149215ITERATION : 2, loss : 0.09722385970852306ITERATION : 3, loss : 0.10823980774578275ITERATION : 4, loss : 0.11695402195903544ITERATION : 5, loss : 0.12339385191752297ITERATION : 6, loss : 0.12800109251136027ITERATION : 7, loss : 0.1312358402755844ITERATION : 8, loss : 0.1334802293078822ITERATION : 9, loss : 0.13502537061522887ITERATION : 10, loss : 0.13608346334292284ITERATION : 11, loss : 0.136805298887689ITERATION : 12, loss : 0.13729636378278676ITERATION : 13, loss : 0.1376297113439398ITERATION : 14, loss : 0.13785559465803263ITERATION : 15, loss : 0.13800842353140685ITERATION : 16, loss : 0.13811168146171396ITERATION : 17, loss : 0.13818135521243374ITERATION : 18, loss : 0.13822830757271007ITERATION : 19, loss : 0.13825990699176433ITERATION : 20, loss : 0.13828114524037574ITERATION : 21, loss : 0.13829540040606564ITERATION : 22, loss : 0.1383049542508204ITERATION : 23, loss : 0.13831134765334213ITERATION : 24, loss : 0.1383156189333125ITERATION : 25, loss : 0.13831846747107884ITERATION : 26, loss : 0.1383203634624708ITERATION : 27, loss : 0.13832162297915004ITERATION : 28, loss : 0.13832245774345503ITERATION : 29, loss : 0.13832300957013932ITERATION : 30, loss : 0.13832337345505075ITERATION : 31, loss : 0.13832361263371112ITERATION : 32, loss : 0.13832376935711935ITERATION : 33, loss : 0.13832387140139699ITERATION : 34, loss : 0.13832393812570296ITERATION : 35, loss : 0.13832398122574857ITERATION : 36, loss : 0.13832400895176372ITERATION : 37, loss : 0.13832402655111828ITERATION : 38, loss : 0.13832403784289135ITERATION : 39, loss : 0.1383240450604314ITERATION : 40, loss : 0.1383240493792065ITERATION : 41, loss : 0.13832405217993704ITERATION : 42, loss : 0.13832405381474044ITERATION : 43, loss : 0.1383240546536342ITERATION : 44, loss : 0.13832405510504997ITERATION : 45, loss : 0.13832405526066105ITERATION : 46, loss : 0.1383240553169918ITERATION : 47, loss : 0.13832405530317204ITERATION : 48, loss : 0.13832405524174723ITERATION : 49, loss : 0.13832405516130092ITERATION : 50, loss : 0.1383240550984679ITERATION : 51, loss : 0.13832405505799328ITERATION : 52, loss : 0.13832405503399067ITERATION : 53, loss : 0.13832405500897177ITERATION : 54, loss : 0.1383240550061702ITERATION : 55, loss : 0.1383240550061702ITERATION : 56, loss : 0.1383240550061702ITERATION : 57, loss : 0.1383240550061702ITERATION : 58, loss : 0.1383240550061702ITERATION : 59, loss : 0.1383240550061702ITERATION : 60, loss : 0.1383240550061702ITERATION : 61, loss : 0.1383240550061702ITERATION : 62, loss : 0.1383240550061702ITERATION : 63, loss : 0.1383240550061702ITERATION : 64, loss : 0.1383240550061702ITERATION : 65, loss : 0.1383240550061702ITERATION : 66, loss : 0.1383240550061702ITERATION : 67, loss : 0.1383240550061702ITERATION : 68, loss : 0.1383240550061702ITERATION : 69, loss : 0.1383240550061702ITERATION : 70, loss : 0.1383240550061702ITERATION : 71, loss : 0.1383240550061702ITERATION : 72, loss : 0.1383240550061702ITERATION : 73, loss : 0.1383240550061702ITERATION : 74, loss : 0.1383240550061702ITERATION : 75, loss : 0.1383240550061702ITERATION : 76, loss : 0.1383240550061702ITERATION : 77, loss : 0.1383240550061702ITERATION : 78, loss : 0.1383240550061702ITERATION : 79, loss : 0.1383240550061702ITERATION : 80, loss : 0.1383240550061702ITERATION : 81, loss : 0.1383240550061702ITERATION : 82, loss : 0.1383240550061702ITERATION : 83, loss : 0.1383240550061702ITERATION : 84, loss : 0.1383240550061702ITERATION : 85, loss : 0.1383240550061702ITERATION : 86, loss : 0.1383240550061702ITERATION : 87, loss : 0.1383240550061702ITERATION : 88, loss : 0.1383240550061702ITERATION : 89, loss : 0.1383240550061702ITERATION : 90, loss : 0.1383240550061702ITERATION : 91, loss : 0.1383240550061702ITERATION : 92, loss : 0.1383240550061702ITERATION : 93, loss : 0.1383240550061702ITERATION : 94, loss : 0.1383240550061702ITERATION : 95, loss : 0.1383240550061702ITERATION : 96, loss : 0.1383240550061702ITERATION : 97, loss : 0.1383240550061702ITERATION : 98, loss : 0.1383240550061702ITERATION : 99, loss : 0.1383240550061702ITERATION : 100, loss : 0.1383240550061702
ITERATION : 1, loss : 0.024434609418760307ITERATION : 2, loss : 0.023861196581844477ITERATION : 3, loss : 0.023936288312002618ITERATION : 4, loss : 0.02455823743773434ITERATION : 5, loss : 0.025172225578708917ITERATION : 6, loss : 0.025657833234480905ITERATION : 7, loss : 0.02601237940786499ITERATION : 8, loss : 0.026261658906779214ITERATION : 9, loss : 0.026433442865999142ITERATION : 10, loss : 0.026550478315693543ITERATION : 11, loss : 0.026629676353864346ITERATION : 12, loss : 0.026683050377032145ITERATION : 13, loss : 0.02671893042053334ITERATION : 14, loss : 0.026743013093727803ITERATION : 15, loss : 0.02675916225901213ITERATION : 16, loss : 0.026769985500687123ITERATION : 17, loss : 0.02677723717097163ITERATION : 18, loss : 0.026782095184429176ITERATION : 19, loss : 0.026785349569676726ITERATION : 20, loss : 0.02678752976954838ITERATION : 21, loss : 0.02678899046306363ITERATION : 22, loss : 0.026789969238288185ITERATION : 23, loss : 0.026790625164795114ITERATION : 24, loss : 0.026791064837001347ITERATION : 25, loss : 0.026791359577922755ITERATION : 26, loss : 0.026791557242393807ITERATION : 27, loss : 0.02679168975978666ITERATION : 28, loss : 0.026791778660139442ITERATION : 29, loss : 0.02679183829184154ITERATION : 30, loss : 0.026791878306873407ITERATION : 31, loss : 0.026791905190347017ITERATION : 32, loss : 0.026791923214300937ITERATION : 33, loss : 0.026791935356723903ITERATION : 34, loss : 0.026791943525359296ITERATION : 35, loss : 0.026791948993034276ITERATION : 36, loss : 0.026791952660448037ITERATION : 37, loss : 0.026791955096842162ITERATION : 38, loss : 0.026791956724122913ITERATION : 39, loss : 0.02679195781252909ITERATION : 40, loss : 0.026791958543420938ITERATION : 41, loss : 0.026791959031456567ITERATION : 42, loss : 0.02679195936225042ITERATION : 43, loss : 0.0267919595853409ITERATION : 44, loss : 0.026791959733128208ITERATION : 45, loss : 0.026791959830654136ITERATION : 46, loss : 0.02679195988879888ITERATION : 47, loss : 0.026791959921378398ITERATION : 48, loss : 0.026791959949635888ITERATION : 49, loss : 0.02679195997122645ITERATION : 50, loss : 0.026791959979749775ITERATION : 51, loss : 0.026791959979784084ITERATION : 52, loss : 0.026791959979784084ITERATION : 53, loss : 0.026791959979784084ITERATION : 54, loss : 0.026791959979784084ITERATION : 55, loss : 0.026791959979784084ITERATION : 56, loss : 0.026791959979784084ITERATION : 57, loss : 0.026791959979784084ITERATION : 58, loss : 0.026791959979784084ITERATION : 59, loss : 0.026791959979784084ITERATION : 60, loss : 0.026791959979784084ITERATION : 61, loss : 0.026791959979784084ITERATION : 62, loss : 0.026791959979784084ITERATION : 63, loss : 0.026791959979784084ITERATION : 64, loss : 0.026791959979784084ITERATION : 65, loss : 0.026791959979784084ITERATION : 66, loss : 0.026791959979784084ITERATION : 67, loss : 0.026791959979784084ITERATION : 68, loss : 0.026791959979784084ITERATION : 69, loss : 0.026791959979784084ITERATION : 70, loss : 0.026791959979784084ITERATION : 71, loss : 0.026791959979784084ITERATION : 72, loss : 0.026791959979784084ITERATION : 73, loss : 0.026791959979784084ITERATION : 74, loss : 0.026791959979784084ITERATION : 75, loss : 0.026791959979784084ITERATION : 76, loss : 0.026791959979784084ITERATION : 77, loss : 0.026791959979784084ITERATION : 78, loss : 0.026791959979784084ITERATION : 79, loss : 0.026791959979784084ITERATION : 80, loss : 0.026791959979784084ITERATION : 81, loss : 0.026791959979784084ITERATION : 82, loss : 0.026791959979784084ITERATION : 83, loss : 0.026791959979784084ITERATION : 84, loss : 0.026791959979784084ITERATION : 85, loss : 0.026791959979784084ITERATION : 86, loss : 0.026791959979784084ITERATION : 87, loss : 0.026791959979784084ITERATION : 88, loss : 0.026791959979784084ITERATION : 89, loss : 0.026791959979784084ITERATION : 90, loss : 0.026791959979784084ITERATION : 91, loss : 0.026791959979784084ITERATION : 92, loss : 0.026791959979784084ITERATION : 93, loss : 0.026791959979784084ITERATION : 94, loss : 0.026791959979784084ITERATION : 95, loss : 0.026791959979784084ITERATION : 96, loss : 0.026791959979784084ITERATION : 97, loss : 0.026791959979784084ITERATION : 98, loss : 0.026791959979784084ITERATION : 99, loss : 0.026791959979784084ITERATION : 100, loss : 0.026791959979784084
ITERATION : 1, loss : 0.026334978468334543ITERATION : 2, loss : 0.04260279134006696ITERATION : 3, loss : 0.05638889878980009ITERATION : 4, loss : 0.06721956824014616ITERATION : 5, loss : 0.07555799273117557ITERATION : 6, loss : 0.08187003002268033ITERATION : 7, loss : 0.08661835450679634ITERATION : 8, loss : 0.0901811702427754ITERATION : 9, loss : 0.09285159183730769ITERATION : 10, loss : 0.09485230017908382ITERATION : 11, loss : 0.09635104955177264ITERATION : 12, loss : 0.097473755939389ITERATION : 13, loss : 0.09831478697666421ITERATION : 14, loss : 0.09894482761682886ITERATION : 15, loss : 0.09941681682563235ITERATION : 16, loss : 0.09977040416198585ITERATION : 17, loss : 0.1000352893319748ITERATION : 18, loss : 0.10023372050949168ITERATION : 19, loss : 0.10038236561733437ITERATION : 20, loss : 0.1004937121732463ITERATION : 21, loss : 0.10057711623684767ITERATION : 22, loss : 0.10063958761660917ITERATION : 23, loss : 0.10068637820740421ITERATION : 24, loss : 0.10072142252056569ITERATION : 25, loss : 0.10074766838595979ITERATION : 26, loss : 0.10076732400883623ITERATION : 27, loss : 0.1007820435636159ITERATION : 28, loss : 0.10079306633124008ITERATION : 29, loss : 0.10080132054099122ITERATION : 30, loss : 0.10080750126202487ITERATION : 31, loss : 0.10081212922619621ITERATION : 32, loss : 0.10081559439383479ITERATION : 33, loss : 0.10081818885486596ITERATION : 34, loss : 0.10082013138836346ITERATION : 35, loss : 0.10082158572907193ITERATION : 36, loss : 0.1008226745565586ITERATION : 37, loss : 0.10082348970213136ITERATION : 38, loss : 0.10082409996164594ITERATION : 39, loss : 0.1008245568226836ITERATION : 40, loss : 0.10082489882572854ITERATION : 41, loss : 0.10082515482807779ITERATION : 42, loss : 0.10082534649584432ITERATION : 43, loss : 0.10082548998721805ITERATION : 44, loss : 0.10082559741413803ITERATION : 45, loss : 0.10082567781605627ITERATION : 46, loss : 0.10082573802548664ITERATION : 47, loss : 0.1008257830842491ITERATION : 48, loss : 0.10082581682885401ITERATION : 49, loss : 0.10082584205582144ITERATION : 50, loss : 0.10082586092904655ITERATION : 51, loss : 0.10082587502559959ITERATION : 52, loss : 0.1008258856731221ITERATION : 53, loss : 0.10082589368289874ITERATION : 54, loss : 0.10082589957868329ITERATION : 55, loss : 0.10082590393842256ITERATION : 56, loss : 0.10082590726429214ITERATION : 57, loss : 0.10082590972981617ITERATION : 58, loss : 0.10082591156072848ITERATION : 59, loss : 0.10082591292937293ITERATION : 60, loss : 0.1008259139770628ITERATION : 61, loss : 0.10082591471220889ITERATION : 62, loss : 0.10082591526932651ITERATION : 63, loss : 0.10082591570122702ITERATION : 64, loss : 0.10082591604108379ITERATION : 65, loss : 0.10082591625027869ITERATION : 66, loss : 0.10082591643582105ITERATION : 67, loss : 0.10082591652657111ITERATION : 68, loss : 0.1008259166393019ITERATION : 69, loss : 0.10082591664133839ITERATION : 70, loss : 0.10082591670868037ITERATION : 71, loss : 0.10082591671067401ITERATION : 72, loss : 0.10082591671067401ITERATION : 73, loss : 0.10082591671067401ITERATION : 74, loss : 0.10082591671067401ITERATION : 75, loss : 0.10082591671067401ITERATION : 76, loss : 0.10082591671067401ITERATION : 77, loss : 0.10082591671067401ITERATION : 78, loss : 0.10082591671067401ITERATION : 79, loss : 0.10082591671067401ITERATION : 80, loss : 0.10082591671067401ITERATION : 81, loss : 0.10082591671067401ITERATION : 82, loss : 0.10082591671067401ITERATION : 83, loss : 0.10082591671067401ITERATION : 84, loss : 0.10082591671067401ITERATION : 85, loss : 0.10082591671067401ITERATION : 86, loss : 0.10082591671067401ITERATION : 87, loss : 0.10082591671067401ITERATION : 88, loss : 0.10082591671067401ITERATION : 89, loss : 0.10082591671067401ITERATION : 90, loss : 0.10082591671067401ITERATION : 91, loss : 0.10082591671067401ITERATION : 92, loss : 0.10082591671067401ITERATION : 93, loss : 0.10082591671067401ITERATION : 94, loss : 0.10082591671067401ITERATION : 95, loss : 0.10082591671067401ITERATION : 96, loss : 0.10082591671067401ITERATION : 97, loss : 0.10082591671067401ITERATION : 98, loss : 0.10082591671067401ITERATION : 99, loss : 0.10082591671067401ITERATION : 100, loss : 0.10082591671067401
ITERATION : 1, loss : 0.03949453196868705ITERATION : 2, loss : 0.059561687501954834ITERATION : 3, loss : 0.07167932206985729ITERATION : 4, loss : 0.07961810353549098ITERATION : 5, loss : 0.08502601959853176ITERATION : 6, loss : 0.08878949084282124ITERATION : 7, loss : 0.09144290718006877ITERATION : 8, loss : 0.09332970158579851ITERATION : 9, loss : 0.0948204765547391ITERATION : 10, loss : 0.09594360289649408ITERATION : 11, loss : 0.09675046285946912ITERATION : 12, loss : 0.09733153579231721ITERATION : 13, loss : 0.09775075344380489ITERATION : 14, loss : 0.09805359955399566ITERATION : 15, loss : 0.0982725944861667ITERATION : 16, loss : 0.09843107407689022ITERATION : 17, loss : 0.09854582777023023ITERATION : 18, loss : 0.09862895869535213ITERATION : 19, loss : 0.09868920398437711ITERATION : 20, loss : 0.09873287787928187ITERATION : 21, loss : 0.0987645472886966ITERATION : 22, loss : 0.0987875173677578ITERATION : 23, loss : 0.09880418138812729ITERATION : 24, loss : 0.09881627301948674ITERATION : 25, loss : 0.09882504851241337ITERATION : 26, loss : 0.0988314183509419ITERATION : 27, loss : 0.0988360428771396ITERATION : 28, loss : 0.09883940079850688ITERATION : 29, loss : 0.09884183944258763ITERATION : 30, loss : 0.09884361068561372ITERATION : 31, loss : 0.09884489743241419ITERATION : 32, loss : 0.0988458322772484ITERATION : 33, loss : 0.09884651157682622ITERATION : 34, loss : 0.0988470052839371ITERATION : 35, loss : 0.09884736415199728ITERATION : 36, loss : 0.09884762502967044ITERATION : 37, loss : 0.09884781471073607ITERATION : 38, loss : 0.09884795262873632ITERATION : 39, loss : 0.09884805293140803ITERATION : 40, loss : 0.09884812587772854ITERATION : 41, loss : 0.09884817892719243ITERATION : 42, loss : 0.09884821751328439ITERATION : 43, loss : 0.09884824561026229ITERATION : 44, loss : 0.09884826604940354ITERATION : 45, loss : 0.09884828092556212ITERATION : 46, loss : 0.0988482917259623ITERATION : 47, loss : 0.09884829959185516ITERATION : 48, loss : 0.09884830540319464ITERATION : 49, loss : 0.09884830956497867ITERATION : 50, loss : 0.09884831258899578ITERATION : 51, loss : 0.09884831478358702ITERATION : 52, loss : 0.0988483164130707ITERATION : 53, loss : 0.09884831757175455ITERATION : 54, loss : 0.09884831839245097ITERATION : 55, loss : 0.098848319022402ITERATION : 56, loss : 0.09884831940534114ITERATION : 57, loss : 0.09884831976272396ITERATION : 58, loss : 0.09884832001377516ITERATION : 59, loss : 0.09884832017748271ITERATION : 60, loss : 0.0988483203117153ITERATION : 61, loss : 0.09884832038376785ITERATION : 62, loss : 0.09884832043728561ITERATION : 63, loss : 0.09884832043789565ITERATION : 64, loss : 0.09884832043789565ITERATION : 65, loss : 0.09884832043789565ITERATION : 66, loss : 0.09884832043789565ITERATION : 67, loss : 0.09884832043789565ITERATION : 68, loss : 0.09884832043789565ITERATION : 69, loss : 0.09884832043789565ITERATION : 70, loss : 0.09884832043789565ITERATION : 71, loss : 0.09884832043789565ITERATION : 72, loss : 0.09884832043789565ITERATION : 73, loss : 0.09884832043789565ITERATION : 74, loss : 0.09884832043789565ITERATION : 75, loss : 0.09884832043789565ITERATION : 76, loss : 0.09884832043789565ITERATION : 77, loss : 0.09884832043789565ITERATION : 78, loss : 0.09884832043789565ITERATION : 79, loss : 0.09884832043789565ITERATION : 80, loss : 0.09884832043789565ITERATION : 81, loss : 0.09884832043789565ITERATION : 82, loss : 0.09884832043789565ITERATION : 83, loss : 0.09884832043789565ITERATION : 84, loss : 0.09884832043789565ITERATION : 85, loss : 0.09884832043789565ITERATION : 86, loss : 0.09884832043789565ITERATION : 87, loss : 0.09884832043789565ITERATION : 88, loss : 0.09884832043789565ITERATION : 89, loss : 0.09884832043789565ITERATION : 90, loss : 0.09884832043789565ITERATION : 91, loss : 0.09884832043789565ITERATION : 92, loss : 0.09884832043789565ITERATION : 93, loss : 0.09884832043789565ITERATION : 94, loss : 0.09884832043789565ITERATION : 95, loss : 0.09884832043789565ITERATION : 96, loss : 0.09884832043789565ITERATION : 97, loss : 0.09884832043789565ITERATION : 98, loss : 0.09884832043789565ITERATION : 99, loss : 0.09884832043789565ITERATION : 100, loss : 0.09884832043789565
ITERATION : 1, loss : 0.030990095169095507ITERATION : 2, loss : 0.034682403730404ITERATION : 3, loss : 0.040373284426959546ITERATION : 4, loss : 0.045091249097430994ITERATION : 5, loss : 0.04829808514211935ITERATION : 6, loss : 0.05046804415643201ITERATION : 7, loss : 0.05193553242991233ITERATION : 8, loss : 0.05292884521832075ITERATION : 9, loss : 0.05360215286227638ITERATION : 10, loss : 0.05405924726980489ITERATION : 11, loss : 0.054370018115018605ITERATION : 12, loss : 0.05458159673085784ITERATION : 13, loss : 0.054725826642152954ITERATION : 14, loss : 0.05482426254499395ITERATION : 15, loss : 0.05489151950160851ITERATION : 16, loss : 0.05493752226946275ITERATION : 17, loss : 0.05496901997818289ITERATION : 18, loss : 0.05499060767196999ITERATION : 19, loss : 0.055005417868001436ITERATION : 20, loss : 0.05501558810921758ITERATION : 21, loss : 0.055022578495306264ITERATION : 22, loss : 0.055027387709357396ITERATION : 23, loss : 0.055030699252465905ITERATION : 24, loss : 0.055032981523708356ITERATION : 25, loss : 0.0550345557871527ITERATION : 26, loss : 0.05503564254198094ITERATION : 27, loss : 0.05503639338103966ITERATION : 28, loss : 0.05503691255409158ITERATION : 29, loss : 0.055037271762328636ITERATION : 30, loss : 0.055037520552690976ITERATION : 31, loss : 0.05503769291779326ITERATION : 32, loss : 0.055037812430992886ITERATION : 33, loss : 0.055037895323227344ITERATION : 34, loss : 0.05503795289143538ITERATION : 35, loss : 0.05503799287226922ITERATION : 36, loss : 0.055038020680647746ITERATION : 37, loss : 0.0550380400157725ITERATION : 38, loss : 0.05503805350072279ITERATION : 39, loss : 0.055038062863631346ITERATION : 40, loss : 0.055038069394641176ITERATION : 41, loss : 0.055038073907908686ITERATION : 42, loss : 0.05503807705716969ITERATION : 43, loss : 0.055038079215941976ITERATION : 44, loss : 0.05503808074610651ITERATION : 45, loss : 0.05503808181368345ITERATION : 46, loss : 0.055038082548251654ITERATION : 47, loss : 0.05503808307612305ITERATION : 48, loss : 0.055038083417784336ITERATION : 49, loss : 0.055038083674115126ITERATION : 50, loss : 0.055038083839804276ITERATION : 51, loss : 0.055038083947065976ITERATION : 52, loss : 0.05503808401997604ITERATION : 53, loss : 0.055038084088185314ITERATION : 54, loss : 0.05503808411763106ITERATION : 55, loss : 0.05503808415722579ITERATION : 56, loss : 0.05503808415970052ITERATION : 57, loss : 0.05503808415970052ITERATION : 58, loss : 0.05503808415970052ITERATION : 59, loss : 0.05503808415970052ITERATION : 60, loss : 0.05503808415970052ITERATION : 61, loss : 0.05503808415970052ITERATION : 62, loss : 0.05503808415970052ITERATION : 63, loss : 0.05503808415970052ITERATION : 64, loss : 0.05503808415970052ITERATION : 65, loss : 0.05503808415970052ITERATION : 66, loss : 0.05503808415970052ITERATION : 67, loss : 0.05503808415970052ITERATION : 68, loss : 0.05503808415970052ITERATION : 69, loss : 0.05503808415970052ITERATION : 70, loss : 0.05503808415970052ITERATION : 71, loss : 0.05503808415970052ITERATION : 72, loss : 0.05503808415970052ITERATION : 73, loss : 0.05503808415970052ITERATION : 74, loss : 0.05503808415970052ITERATION : 75, loss : 0.05503808415970052ITERATION : 76, loss : 0.05503808415970052ITERATION : 77, loss : 0.05503808415970052ITERATION : 78, loss : 0.05503808415970052ITERATION : 79, loss : 0.05503808415970052ITERATION : 80, loss : 0.05503808415970052ITERATION : 81, loss : 0.05503808415970052ITERATION : 82, loss : 0.05503808415970052ITERATION : 83, loss : 0.05503808415970052ITERATION : 84, loss : 0.05503808415970052ITERATION : 85, loss : 0.05503808415970052ITERATION : 86, loss : 0.05503808415970052ITERATION : 87, loss : 0.05503808415970052ITERATION : 88, loss : 0.05503808415970052ITERATION : 89, loss : 0.05503808415970052ITERATION : 90, loss : 0.05503808415970052ITERATION : 91, loss : 0.05503808415970052ITERATION : 92, loss : 0.05503808415970052ITERATION : 93, loss : 0.05503808415970052ITERATION : 94, loss : 0.05503808415970052ITERATION : 95, loss : 0.05503808415970052ITERATION : 96, loss : 0.05503808415970052ITERATION : 97, loss : 0.05503808415970052ITERATION : 98, loss : 0.05503808415970052ITERATION : 99, loss : 0.05503808415970052ITERATION : 100, loss : 0.05503808415970052
gradient norm in None layer : 0.0014917447007659396
gradient norm in None layer : 3.4595171655512826e-05
gradient norm in None layer : 4.295711695724959e-05
gradient norm in None layer : 0.0005336147365096589
gradient norm in None layer : 3.989676630036337e-05
gradient norm in None layer : 3.866038676707239e-05
gradient norm in None layer : 0.00033735814772344654
gradient norm in None layer : 1.1633304611496236e-05
gradient norm in None layer : 1.1534316199607091e-05
gradient norm in None layer : 0.00026343046486828605
gradient norm in None layer : 1.2338975320268934e-05
gradient norm in None layer : 1.0927353630158619e-05
gradient norm in None layer : 0.00010187136768100524
gradient norm in None layer : 3.1899843889529676e-06
gradient norm in None layer : 2.7850114378182265e-06
gradient norm in None layer : 9.362564155793431e-05
gradient norm in None layer : 3.4824963682542493e-06
gradient norm in None layer : 3.5613380236894006e-06
gradient norm in None layer : 0.00012432548616311327
gradient norm in None layer : 3.846846942369231e-06
gradient norm in None layer : 0.00025326418179974557
gradient norm in None layer : 1.4944289499525265e-05
gradient norm in None layer : 1.2215581886910025e-05
gradient norm in None layer : 0.00027331378068185853
gradient norm in None layer : 2.6557963468301834e-05
gradient norm in None layer : 3.412844285875243e-05
gradient norm in None layer : 0.0004947612345395895
gradient norm in None layer : 9.399947991708732e-06
gradient norm in None layer : 0.0005753511931867035
gradient norm in None layer : 3.5681522011842355e-05
gradient norm in None layer : 3.8038546436103855e-05
gradient norm in None layer : 0.0006618106944223936
gradient norm in None layer : 0.00010642522512902634
gradient norm in None layer : 0.00015574435263016317
gradient norm in None layer : 7.362588732698955e-05
gradient norm in None layer : 2.2489873483098088e-05
Total gradient norm: 0.001983379451196775
invariance loss : 4.646870492234161, avg_den : 0.45882415771484375, density loss : 0.35756076424749783, mse loss : 0.08956515876474047, solver time : 118.92820525169373 sec , total loss : 0.09456959002122212, running loss : 0.10816481044803626
Epoch 0/10 , batch 18/12500 
ITERATION : 1, loss : 0.08878687733796833ITERATION : 2, loss : 0.11946950408409907ITERATION : 3, loss : 0.13539355981116974ITERATION : 4, loss : 0.14458542078527958ITERATION : 5, loss : 0.1502394087594945ITERATION : 6, loss : 0.1540034573917896ITERATION : 7, loss : 0.1566218713147572ITERATION : 8, loss : 0.15835408108452909ITERATION : 9, loss : 0.15951622209629845ITERATION : 10, loss : 0.16030347802025838ITERATION : 11, loss : 0.16084040268902303ITERATION : 12, loss : 0.16120837102284125ITERATION : 13, loss : 0.16146144253837105ITERATION : 14, loss : 0.16163595720943671ITERATION : 15, loss : 0.16175654951256466ITERATION : 16, loss : 0.16184001974650064ITERATION : 17, loss : 0.16189787564839606ITERATION : 18, loss : 0.16193802551714695ITERATION : 19, loss : 0.16196591763599605ITERATION : 20, loss : 0.16198531287676796ITERATION : 21, loss : 0.16199881158971957ITERATION : 22, loss : 0.16200821414694577ITERATION : 23, loss : 0.16201476857879613ITERATION : 24, loss : 0.1620193409080865ITERATION : 25, loss : 0.16202253276463088ITERATION : 26, loss : 0.16202476236782803ITERATION : 27, loss : 0.16202632078636853ITERATION : 28, loss : 0.16202741069375767ITERATION : 29, loss : 0.16202817337200923ITERATION : 30, loss : 0.16202870734562938ITERATION : 31, loss : 0.1620290813775351ITERATION : 32, loss : 0.16202934350319612ITERATION : 33, loss : 0.1620295272817347ITERATION : 34, loss : 0.162029656174274ITERATION : 35, loss : 0.16202974663432676ITERATION : 36, loss : 0.16202981011881157ITERATION : 37, loss : 0.16202985470380657ITERATION : 38, loss : 0.1620298860248126ITERATION : 39, loss : 0.1620299080326659ITERATION : 40, loss : 0.1620299235056321ITERATION : 41, loss : 0.16202993439325084ITERATION : 42, loss : 0.16202994204260246ITERATION : 43, loss : 0.16202994741888044ITERATION : 44, loss : 0.16202995119855756ITERATION : 45, loss : 0.16202995386452243ITERATION : 46, loss : 0.16202995573257625ITERATION : 47, loss : 0.16202995704325507ITERATION : 48, loss : 0.16202995796848724ITERATION : 49, loss : 0.1620299586183437ITERATION : 50, loss : 0.16202995907170167ITERATION : 51, loss : 0.16202995939066142ITERATION : 52, loss : 0.1620299596118815ITERATION : 53, loss : 0.16202995976114098ITERATION : 54, loss : 0.16202995988611707ITERATION : 55, loss : 0.16202995995559977ITERATION : 56, loss : 0.16202996000345915ITERATION : 57, loss : 0.16202996005405243ITERATION : 58, loss : 0.16202996007568315ITERATION : 59, loss : 0.16202996009508855ITERATION : 60, loss : 0.16202996011599494ITERATION : 61, loss : 0.1620299601234129ITERATION : 62, loss : 0.1620299601234129ITERATION : 63, loss : 0.1620299601234129ITERATION : 64, loss : 0.1620299601234129ITERATION : 65, loss : 0.1620299601234129ITERATION : 66, loss : 0.1620299601234129ITERATION : 67, loss : 0.1620299601234129ITERATION : 68, loss : 0.1620299601234129ITERATION : 69, loss : 0.1620299601234129ITERATION : 70, loss : 0.1620299601234129ITERATION : 71, loss : 0.1620299601234129ITERATION : 72, loss : 0.1620299601234129ITERATION : 73, loss : 0.1620299601234129ITERATION : 74, loss : 0.1620299601234129ITERATION : 75, loss : 0.1620299601234129ITERATION : 76, loss : 0.1620299601234129ITERATION : 77, loss : 0.1620299601234129ITERATION : 78, loss : 0.1620299601234129ITERATION : 79, loss : 0.1620299601234129ITERATION : 80, loss : 0.1620299601234129ITERATION : 81, loss : 0.1620299601234129ITERATION : 82, loss : 0.1620299601234129ITERATION : 83, loss : 0.1620299601234129ITERATION : 84, loss : 0.1620299601234129ITERATION : 85, loss : 0.1620299601234129ITERATION : 86, loss : 0.1620299601234129ITERATION : 87, loss : 0.1620299601234129ITERATION : 88, loss : 0.1620299601234129ITERATION : 89, loss : 0.1620299601234129ITERATION : 90, loss : 0.1620299601234129ITERATION : 91, loss : 0.1620299601234129ITERATION : 92, loss : 0.1620299601234129ITERATION : 93, loss : 0.1620299601234129ITERATION : 94, loss : 0.1620299601234129ITERATION : 95, loss : 0.1620299601234129ITERATION : 96, loss : 0.1620299601234129ITERATION : 97, loss : 0.1620299601234129ITERATION : 98, loss : 0.1620299601234129ITERATION : 99, loss : 0.1620299601234129ITERATION : 100, loss : 0.1620299601234129
ITERATION : 1, loss : 0.017439574071611577ITERATION : 2, loss : 0.02462973381104978ITERATION : 3, loss : 0.02524256136585596ITERATION : 4, loss : 0.026019296943634383ITERATION : 5, loss : 0.026674327751042906ITERATION : 6, loss : 0.027184525888980144ITERATION : 7, loss : 0.027564809429320863ITERATION : 8, loss : 0.027838615892466497ITERATION : 9, loss : 0.028030372433369685ITERATION : 10, loss : 0.028161784766558187ITERATION : 11, loss : 0.02825034578849655ITERATION : 12, loss : 0.028309262710606117ITERATION : 13, loss : 0.028348068541160485ITERATION : 14, loss : 0.028373428914885642ITERATION : 15, loss : 0.028389899381904716ITERATION : 16, loss : 0.02840054199743752ITERATION : 17, loss : 0.02840738976772566ITERATION : 18, loss : 0.028411779445396194ITERATION : 19, loss : 0.0284145840364359ITERATION : 20, loss : 0.028416370376213224ITERATION : 21, loss : 0.028417504746848895ITERATION : 22, loss : 0.028418222870874393ITERATION : 23, loss : 0.02841867610398553ITERATION : 24, loss : 0.028418961196797873ITERATION : 25, loss : 0.028419139919226167ITERATION : 26, loss : 0.028419251489522776ITERATION : 27, loss : 0.028419320819941585ITERATION : 28, loss : 0.028419363700846894ITERATION : 29, loss : 0.028419389988128433ITERATION : 30, loss : 0.028419406167537464ITERATION : 31, loss : 0.028419415934923344ITERATION : 32, loss : 0.028419421756688202ITERATION : 33, loss : 0.02841942529557336ITERATION : 34, loss : 0.02841942735444611ITERATION : 35, loss : 0.028419428473721788ITERATION : 36, loss : 0.028419429153408428ITERATION : 37, loss : 0.028419429500635553ITERATION : 38, loss : 0.028419429627275044ITERATION : 39, loss : 0.028419429694716093ITERATION : 40, loss : 0.028419429668232403ITERATION : 41, loss : 0.028419429634286168ITERATION : 42, loss : 0.028419429598249515ITERATION : 43, loss : 0.028419429572971555ITERATION : 44, loss : 0.028419429524490693ITERATION : 45, loss : 0.028419429524432906ITERATION : 46, loss : 0.028419429498826615ITERATION : 47, loss : 0.028419429488232482ITERATION : 48, loss : 0.028419429486501346ITERATION : 49, loss : 0.028419429486501346ITERATION : 50, loss : 0.028419429486501346ITERATION : 51, loss : 0.028419429486501346ITERATION : 52, loss : 0.028419429486501346ITERATION : 53, loss : 0.028419429486501346ITERATION : 54, loss : 0.028419429486501346ITERATION : 55, loss : 0.028419429486501346ITERATION : 56, loss : 0.028419429486501346ITERATION : 57, loss : 0.028419429486501346ITERATION : 58, loss : 0.028419429486501346ITERATION : 59, loss : 0.028419429486501346ITERATION : 60, loss : 0.028419429486501346ITERATION : 61, loss : 0.028419429486501346ITERATION : 62, loss : 0.028419429486501346ITERATION : 63, loss : 0.028419429486501346ITERATION : 64, loss : 0.028419429486501346ITERATION : 65, loss : 0.028419429486501346ITERATION : 66, loss : 0.028419429486501346ITERATION : 67, loss : 0.028419429486501346ITERATION : 68, loss : 0.028419429486501346ITERATION : 69, loss : 0.028419429486501346ITERATION : 70, loss : 0.028419429486501346ITERATION : 71, loss : 0.028419429486501346ITERATION : 72, loss : 0.028419429486501346ITERATION : 73, loss : 0.028419429486501346ITERATION : 74, loss : 0.028419429486501346ITERATION : 75, loss : 0.028419429486501346ITERATION : 76, loss : 0.028419429486501346ITERATION : 77, loss : 0.028419429486501346ITERATION : 78, loss : 0.028419429486501346ITERATION : 79, loss : 0.028419429486501346ITERATION : 80, loss : 0.028419429486501346ITERATION : 81, loss : 0.028419429486501346ITERATION : 82, loss : 0.028419429486501346ITERATION : 83, loss : 0.028419429486501346ITERATION : 84, loss : 0.028419429486501346ITERATION : 85, loss : 0.028419429486501346ITERATION : 86, loss : 0.028419429486501346ITERATION : 87, loss : 0.028419429486501346ITERATION : 88, loss : 0.028419429486501346ITERATION : 89, loss : 0.028419429486501346ITERATION : 90, loss : 0.028419429486501346ITERATION : 91, loss : 0.028419429486501346ITERATION : 92, loss : 0.028419429486501346ITERATION : 93, loss : 0.028419429486501346ITERATION : 94, loss : 0.028419429486501346ITERATION : 95, loss : 0.028419429486501346ITERATION : 96, loss : 0.028419429486501346ITERATION : 97, loss : 0.028419429486501346ITERATION : 98, loss : 0.028419429486501346ITERATION : 99, loss : 0.028419429486501346ITERATION : 100, loss : 0.028419429486501346
ITERATION : 1, loss : 0.02516201115263431ITERATION : 2, loss : 0.023173470594139687ITERATION : 3, loss : 0.023904040054434805ITERATION : 4, loss : 0.023824974231914414ITERATION : 5, loss : 0.024151412296813224ITERATION : 6, loss : 0.024564350648412ITERATION : 7, loss : 0.02494388585809312ITERATION : 8, loss : 0.02525491511283501ITERATION : 9, loss : 0.025496291058449687ITERATION : 10, loss : 0.025678286939073328ITERATION : 11, loss : 0.02581332683623185ITERATION : 12, loss : 0.025912619396740236ITERATION : 13, loss : 0.025985254889629283ITERATION : 14, loss : 0.026038242012999303ITERATION : 15, loss : 0.026076841923605182ITERATION : 16, loss : 0.02610494512731493ITERATION : 17, loss : 0.02612540458748488ITERATION : 18, loss : 0.026140302419952025ITERATION : 19, loss : 0.026151154499346168ITERATION : 20, loss : 0.026159062968335832ITERATION : 21, loss : 0.026164828971838472ITERATION : 22, loss : 0.026169034838784046ITERATION : 23, loss : 0.026172104036174294ITERATION : 24, loss : 0.026174344713726927ITERATION : 25, loss : 0.02617598109226182ITERATION : 26, loss : 0.026177176600887978ITERATION : 27, loss : 0.02617805022504755ITERATION : 28, loss : 0.02617868883499985ITERATION : 29, loss : 0.026179155764456525ITERATION : 30, loss : 0.026179497248384417ITERATION : 31, loss : 0.026179747008352745ITERATION : 32, loss : 0.026179929727974758ITERATION : 33, loss : 0.026180063420888806ITERATION : 34, loss : 0.026180161242660167ITERATION : 35, loss : 0.026180232833429695ITERATION : 36, loss : 0.026180285239724176ITERATION : 37, loss : 0.02618032358275037ITERATION : 38, loss : 0.02618035164276934ITERATION : 39, loss : 0.026180372166013207ITERATION : 40, loss : 0.026180387207587457ITERATION : 41, loss : 0.026180398222150954ITERATION : 42, loss : 0.026180406287363473ITERATION : 43, loss : 0.026180412172767717ITERATION : 44, loss : 0.026180416470418202ITERATION : 45, loss : 0.026180419626137565ITERATION : 46, loss : 0.02618042194695463ITERATION : 47, loss : 0.026180423642779383ITERATION : 48, loss : 0.02618042488775839ITERATION : 49, loss : 0.026180425776505464ITERATION : 50, loss : 0.026180426486246744ITERATION : 51, loss : 0.026180426949830502ITERATION : 52, loss : 0.026180427289432776ITERATION : 53, loss : 0.026180427550653706ITERATION : 54, loss : 0.02618042770909007ITERATION : 55, loss : 0.026180427866285275ITERATION : 56, loss : 0.02618042796694014ITERATION : 57, loss : 0.026180428050553997ITERATION : 58, loss : 0.026180428102175888ITERATION : 59, loss : 0.026180428144890626ITERATION : 60, loss : 0.026180428157372634ITERATION : 61, loss : 0.026180428157372634ITERATION : 62, loss : 0.026180428157372634ITERATION : 63, loss : 0.026180428157372634ITERATION : 64, loss : 0.026180428157372634ITERATION : 65, loss : 0.026180428157372634ITERATION : 66, loss : 0.026180428157372634ITERATION : 67, loss : 0.026180428157372634ITERATION : 68, loss : 0.026180428157372634ITERATION : 69, loss : 0.026180428157372634ITERATION : 70, loss : 0.026180428157372634ITERATION : 71, loss : 0.026180428157372634ITERATION : 72, loss : 0.026180428157372634ITERATION : 73, loss : 0.026180428157372634ITERATION : 74, loss : 0.026180428157372634ITERATION : 75, loss : 0.026180428157372634ITERATION : 76, loss : 0.026180428157372634ITERATION : 77, loss : 0.026180428157372634ITERATION : 78, loss : 0.026180428157372634ITERATION : 79, loss : 0.026180428157372634ITERATION : 80, loss : 0.026180428157372634ITERATION : 81, loss : 0.026180428157372634ITERATION : 82, loss : 0.026180428157372634ITERATION : 83, loss : 0.026180428157372634ITERATION : 84, loss : 0.026180428157372634ITERATION : 85, loss : 0.026180428157372634ITERATION : 86, loss : 0.026180428157372634ITERATION : 87, loss : 0.026180428157372634ITERATION : 88, loss : 0.026180428157372634ITERATION : 89, loss : 0.026180428157372634ITERATION : 90, loss : 0.026180428157372634ITERATION : 91, loss : 0.026180428157372634ITERATION : 92, loss : 0.026180428157372634ITERATION : 93, loss : 0.026180428157372634ITERATION : 94, loss : 0.026180428157372634ITERATION : 95, loss : 0.026180428157372634ITERATION : 96, loss : 0.026180428157372634ITERATION : 97, loss : 0.026180428157372634ITERATION : 98, loss : 0.026180428157372634ITERATION : 99, loss : 0.026180428157372634ITERATION : 100, loss : 0.026180428157372634
ITERATION : 1, loss : 0.02670544389603905ITERATION : 2, loss : 0.03993040797336103ITERATION : 3, loss : 0.049656439882451386ITERATION : 4, loss : 0.056459293736404646ITERATION : 5, loss : 0.061200852507567426ITERATION : 6, loss : 0.0645429172900238ITERATION : 7, loss : 0.06692744739431414ITERATION : 8, loss : 0.06864621803293386ITERATION : 9, loss : 0.0698948849450918ITERATION : 10, loss : 0.07080735402161069ITERATION : 11, loss : 0.07147700697011904ITERATION : 12, loss : 0.07196998174748169ITERATION : 13, loss : 0.07233369399860727ITERATION : 14, loss : 0.07260245419154587ITERATION : 15, loss : 0.07280126306440132ITERATION : 16, loss : 0.07294843199414323ITERATION : 17, loss : 0.07305742346799095ITERATION : 18, loss : 0.07313816228180925ITERATION : 19, loss : 0.07319797931541266ITERATION : 20, loss : 0.07324229696146059ITERATION : 21, loss : 0.07327512954246704ITERATION : 22, loss : 0.07329945088487413ITERATION : 23, loss : 0.07331746484931395ITERATION : 24, loss : 0.07333080481191454ITERATION : 25, loss : 0.07334068179896436ITERATION : 26, loss : 0.07334799326801406ITERATION : 27, loss : 0.07335340453954542ITERATION : 28, loss : 0.07335740860575744ITERATION : 29, loss : 0.07336037079299901ITERATION : 30, loss : 0.07336256172209077ITERATION : 31, loss : 0.07336418187933749ITERATION : 32, loss : 0.07336537970812129ITERATION : 33, loss : 0.07336626515137973ITERATION : 34, loss : 0.07336691952001391ITERATION : 35, loss : 0.07336740302092035ITERATION : 36, loss : 0.0733677601929874ITERATION : 37, loss : 0.07336802399325326ITERATION : 38, loss : 0.07336821880988288ITERATION : 39, loss : 0.0733683626493561ITERATION : 40, loss : 0.07336846881173295ITERATION : 41, loss : 0.07336854716661029ITERATION : 42, loss : 0.07336860499440871ITERATION : 43, loss : 0.07336864765466321ITERATION : 44, loss : 0.07336867910832691ITERATION : 45, loss : 0.07336870234875269ITERATION : 46, loss : 0.07336871943441284ITERATION : 47, loss : 0.07336873208298045ITERATION : 48, loss : 0.07336874139545926ITERATION : 49, loss : 0.07336874822666455ITERATION : 50, loss : 0.07336875321636468ITERATION : 51, loss : 0.07336875693850133ITERATION : 52, loss : 0.07336875967552954ITERATION : 53, loss : 0.07336876171218164ITERATION : 54, loss : 0.07336876320059857ITERATION : 55, loss : 0.07336876428071493ITERATION : 56, loss : 0.07336876507262899ITERATION : 57, loss : 0.07336876562463815ITERATION : 58, loss : 0.07336876607310669ITERATION : 59, loss : 0.0733687664052716ITERATION : 60, loss : 0.07336876663265966ITERATION : 61, loss : 0.07336876680398181ITERATION : 62, loss : 0.07336876694490323ITERATION : 63, loss : 0.07336876703973719ITERATION : 64, loss : 0.0733687671182554ITERATION : 65, loss : 0.07336876713297884ITERATION : 66, loss : 0.07336876719500292ITERATION : 67, loss : 0.07336876720924806ITERATION : 68, loss : 0.07336876720924806ITERATION : 69, loss : 0.07336876720924806ITERATION : 70, loss : 0.07336876720924806ITERATION : 71, loss : 0.07336876720924806ITERATION : 72, loss : 0.07336876720924806ITERATION : 73, loss : 0.07336876720924806ITERATION : 74, loss : 0.07336876720924806ITERATION : 75, loss : 0.07336876720924806ITERATION : 76, loss : 0.07336876720924806ITERATION : 77, loss : 0.07336876720924806ITERATION : 78, loss : 0.07336876720924806ITERATION : 79, loss : 0.07336876720924806ITERATION : 80, loss : 0.07336876720924806ITERATION : 81, loss : 0.07336876720924806ITERATION : 82, loss : 0.07336876720924806ITERATION : 83, loss : 0.07336876720924806ITERATION : 84, loss : 0.07336876720924806ITERATION : 85, loss : 0.07336876720924806ITERATION : 86, loss : 0.07336876720924806ITERATION : 87, loss : 0.07336876720924806ITERATION : 88, loss : 0.07336876720924806ITERATION : 89, loss : 0.07336876720924806ITERATION : 90, loss : 0.07336876720924806ITERATION : 91, loss : 0.07336876720924806ITERATION : 92, loss : 0.07336876720924806ITERATION : 93, loss : 0.07336876720924806ITERATION : 94, loss : 0.07336876720924806ITERATION : 95, loss : 0.07336876720924806ITERATION : 96, loss : 0.07336876720924806ITERATION : 97, loss : 0.07336876720924806ITERATION : 98, loss : 0.07336876720924806ITERATION : 99, loss : 0.07336876720924806ITERATION : 100, loss : 0.07336876720924806
ITERATION : 1, loss : 0.09991954611228983ITERATION : 2, loss : 0.11141362168635577ITERATION : 3, loss : 0.1224964220041282ITERATION : 4, loss : 0.1303858288919082ITERATION : 5, loss : 0.13560786850206394ITERATION : 6, loss : 0.13899454221941543ITERATION : 7, loss : 0.14118416125177716ITERATION : 8, loss : 0.14260486820961846ITERATION : 9, loss : 0.1435323600814272ITERATION : 10, loss : 0.14414212746854632ITERATION : 11, loss : 0.14454586240239428ITERATION : 12, loss : 0.14481499556717708ITERATION : 13, loss : 0.14499552867922558ITERATION : 14, loss : 0.14511731786962723ITERATION : 15, loss : 0.14519989510584871ITERATION : 16, loss : 0.14525613620751499ITERATION : 17, loss : 0.1452945900290196ITERATION : 18, loss : 0.1453209715313235ITERATION : 19, loss : 0.14533912326661413ITERATION : 20, loss : 0.14535164383465696ITERATION : 21, loss : 0.14536029852581855ITERATION : 22, loss : 0.14536629175782992ITERATION : 23, loss : 0.14537044816377548ITERATION : 24, loss : 0.14537333447464185ITERATION : 25, loss : 0.14537534087979787ITERATION : 26, loss : 0.1453767369422803ITERATION : 27, loss : 0.1453777089550601ITERATION : 28, loss : 0.1453783861539024ITERATION : 29, loss : 0.14537885828788472ITERATION : 30, loss : 0.14537918757298465ITERATION : 31, loss : 0.14537941735938867ITERATION : 32, loss : 0.1453795777017472ITERATION : 33, loss : 0.1453796896843956ITERATION : 34, loss : 0.14537976780300702ITERATION : 35, loss : 0.14537982233342572ITERATION : 36, loss : 0.14537986045819984ITERATION : 37, loss : 0.14537988702302118ITERATION : 38, loss : 0.145379905625542ITERATION : 39, loss : 0.14537991857085833ITERATION : 40, loss : 0.1453799276744344ITERATION : 41, loss : 0.14537993397042925ITERATION : 42, loss : 0.14537993842562588ITERATION : 43, loss : 0.1453799415146774ITERATION : 44, loss : 0.1453799436745207ITERATION : 45, loss : 0.14537994518708833ITERATION : 46, loss : 0.14537994624859554ITERATION : 47, loss : 0.14537994699904166ITERATION : 48, loss : 0.1453799474975305ITERATION : 49, loss : 0.14537994786380126ITERATION : 50, loss : 0.14537994803518572ITERATION : 51, loss : 0.14537994820477093ITERATION : 52, loss : 0.14537994832766016ITERATION : 53, loss : 0.1453799483795781ITERATION : 54, loss : 0.14537994838860563ITERATION : 55, loss : 0.14537994838860563ITERATION : 56, loss : 0.14537994838860563ITERATION : 57, loss : 0.14537994838860563ITERATION : 58, loss : 0.14537994838860563ITERATION : 59, loss : 0.14537994838860563ITERATION : 60, loss : 0.14537994838860563ITERATION : 61, loss : 0.14537994838860563ITERATION : 62, loss : 0.14537994838860563ITERATION : 63, loss : 0.14537994838860563ITERATION : 64, loss : 0.14537994838860563ITERATION : 65, loss : 0.14537994838860563ITERATION : 66, loss : 0.14537994838860563ITERATION : 67, loss : 0.14537994838860563ITERATION : 68, loss : 0.14537994838860563ITERATION : 69, loss : 0.14537994838860563ITERATION : 70, loss : 0.14537994838860563ITERATION : 71, loss : 0.14537994838860563ITERATION : 72, loss : 0.14537994838860563ITERATION : 73, loss : 0.14537994838860563ITERATION : 74, loss : 0.14537994838860563ITERATION : 75, loss : 0.14537994838860563ITERATION : 76, loss : 0.14537994838860563ITERATION : 77, loss : 0.14537994838860563ITERATION : 78, loss : 0.14537994838860563ITERATION : 79, loss : 0.14537994838860563ITERATION : 80, loss : 0.14537994838860563ITERATION : 81, loss : 0.14537994838860563ITERATION : 82, loss : 0.14537994838860563ITERATION : 83, loss : 0.14537994838860563ITERATION : 84, loss : 0.14537994838860563ITERATION : 85, loss : 0.14537994838860563ITERATION : 86, loss : 0.14537994838860563ITERATION : 87, loss : 0.14537994838860563ITERATION : 88, loss : 0.14537994838860563ITERATION : 89, loss : 0.14537994838860563ITERATION : 90, loss : 0.14537994838860563ITERATION : 91, loss : 0.14537994838860563ITERATION : 92, loss : 0.14537994838860563ITERATION : 93, loss : 0.14537994838860563ITERATION : 94, loss : 0.14537994838860563ITERATION : 95, loss : 0.14537994838860563ITERATION : 96, loss : 0.14537994838860563ITERATION : 97, loss : 0.14537994838860563ITERATION : 98, loss : 0.14537994838860563ITERATION : 99, loss : 0.14537994838860563ITERATION : 100, loss : 0.14537994838860563
ITERATION : 1, loss : 0.016996706217345142ITERATION : 2, loss : 0.01599411074002295ITERATION : 3, loss : 0.016289131727258353ITERATION : 4, loss : 0.017205229632062658ITERATION : 5, loss : 0.018094676801397487ITERATION : 6, loss : 0.0188202584764269ITERATION : 7, loss : 0.019378346200760597ITERATION : 8, loss : 0.019796975538935495ITERATION : 9, loss : 0.020107237940432777ITERATION : 10, loss : 0.02033577364424582ITERATION : 11, loss : 0.020503572674607614ITERATION : 12, loss : 0.020626582319363145ITERATION : 13, loss : 0.020716700958264868ITERATION : 14, loss : 0.020782719666585393ITERATION : 15, loss : 0.020831098210836303ITERATION : 16, loss : 0.020866568986228688ITERATION : 17, loss : 0.020892593361894713ITERATION : 18, loss : 0.020911701346422232ITERATION : 19, loss : 0.020925742131287166ITERATION : 20, loss : 0.02093606766155882ITERATION : 21, loss : 0.020943666948213528ITERATION : 22, loss : 0.020949264041743656ITERATION : 23, loss : 0.02095338946790317ITERATION : 24, loss : 0.02095643225008266ITERATION : 25, loss : 0.02095867795539537ITERATION : 26, loss : 0.020960336354842703ITERATION : 27, loss : 0.0209615617377039ITERATION : 28, loss : 0.02096246764301229ITERATION : 29, loss : 0.020963137659819425ITERATION : 30, loss : 0.020963633434813535ITERATION : 31, loss : 0.020964000423136227ITERATION : 32, loss : 0.020964272188211605ITERATION : 33, loss : 0.020964473494954897ITERATION : 34, loss : 0.020964622659386187ITERATION : 35, loss : 0.020964733221958697ITERATION : 36, loss : 0.020964815199173238ITERATION : 37, loss : 0.020964875993275123ITERATION : 38, loss : 0.020964921073439606ITERATION : 39, loss : 0.020964954516803257ITERATION : 40, loss : 0.020964979335513636ITERATION : 41, loss : 0.02096499774458253ITERATION : 42, loss : 0.020965011400576383ITERATION : 43, loss : 0.020965021533714254ITERATION : 44, loss : 0.02096502905540475ITERATION : 45, loss : 0.020965034629138755ITERATION : 46, loss : 0.020965038774527562ITERATION : 47, loss : 0.020965041852200314ITERATION : 48, loss : 0.02096504413860384ITERATION : 49, loss : 0.020965045825446064ITERATION : 50, loss : 0.020965047090742855ITERATION : 51, loss : 0.020965048022361713ITERATION : 52, loss : 0.02096504872031716ITERATION : 53, loss : 0.02096504923085868ITERATION : 54, loss : 0.020965049622733073ITERATION : 55, loss : 0.02096504993295491ITERATION : 56, loss : 0.020965050134389013ITERATION : 57, loss : 0.020965050289185026ITERATION : 58, loss : 0.020965050394137822ITERATION : 59, loss : 0.02096505046605712ITERATION : 60, loss : 0.020965050548815346ITERATION : 61, loss : 0.02096505059329733ITERATION : 62, loss : 0.020965050618788804ITERATION : 63, loss : 0.020965050627648762ITERATION : 64, loss : 0.020965050627756426ITERATION : 65, loss : 0.020965050627756426ITERATION : 66, loss : 0.020965050627756426ITERATION : 67, loss : 0.020965050627756426ITERATION : 68, loss : 0.020965050627756426ITERATION : 69, loss : 0.020965050627756426ITERATION : 70, loss : 0.020965050627756426ITERATION : 71, loss : 0.020965050627756426ITERATION : 72, loss : 0.020965050627756426ITERATION : 73, loss : 0.020965050627756426ITERATION : 74, loss : 0.020965050627756426ITERATION : 75, loss : 0.020965050627756426ITERATION : 76, loss : 0.020965050627756426ITERATION : 77, loss : 0.020965050627756426ITERATION : 78, loss : 0.020965050627756426ITERATION : 79, loss : 0.020965050627756426ITERATION : 80, loss : 0.020965050627756426ITERATION : 81, loss : 0.020965050627756426ITERATION : 82, loss : 0.020965050627756426ITERATION : 83, loss : 0.020965050627756426ITERATION : 84, loss : 0.020965050627756426ITERATION : 85, loss : 0.020965050627756426ITERATION : 86, loss : 0.020965050627756426ITERATION : 87, loss : 0.020965050627756426ITERATION : 88, loss : 0.020965050627756426ITERATION : 89, loss : 0.020965050627756426ITERATION : 90, loss : 0.020965050627756426ITERATION : 91, loss : 0.020965050627756426ITERATION : 92, loss : 0.020965050627756426ITERATION : 93, loss : 0.020965050627756426ITERATION : 94, loss : 0.020965050627756426ITERATION : 95, loss : 0.020965050627756426ITERATION : 96, loss : 0.020965050627756426ITERATION : 97, loss : 0.020965050627756426ITERATION : 98, loss : 0.020965050627756426ITERATION : 99, loss : 0.020965050627756426ITERATION : 100, loss : 0.020965050627756426
ITERATION : 1, loss : 0.057612679972517906ITERATION : 2, loss : 0.07167751032198644ITERATION : 3, loss : 0.08563891957769543ITERATION : 4, loss : 0.09573499606978288ITERATION : 5, loss : 0.10508379371989165ITERATION : 6, loss : 0.11166521090568433ITERATION : 7, loss : 0.11619871334218998ITERATION : 8, loss : 0.11933214942023673ITERATION : 9, loss : 0.12150499850292804ITERATION : 10, loss : 0.12301612509718782ITERATION : 11, loss : 0.1240697278407745ITERATION : 12, loss : 0.12480598800775433ITERATION : 13, loss : 0.1253215399533361ITERATION : 14, loss : 0.1256832319123097ITERATION : 15, loss : 0.12593744152650516ITERATION : 16, loss : 0.126116423744985ITERATION : 17, loss : 0.12624265946876484ITERATION : 18, loss : 0.12633184748512172ITERATION : 19, loss : 0.12639496971242561ITERATION : 20, loss : 0.12643972185960825ITERATION : 21, loss : 0.1264715055483031ITERATION : 22, loss : 0.12649411846397368ITERATION : 23, loss : 0.12651023499730255ITERATION : 24, loss : 0.12652174167865185ITERATION : 25, loss : 0.12652997150503395ITERATION : 26, loss : 0.1265358678731668ITERATION : 27, loss : 0.12654009963529367ITERATION : 28, loss : 0.12654314201483044ITERATION : 29, loss : 0.12654533281956581ITERATION : 30, loss : 0.12654691305953794ITERATION : 31, loss : 0.12654805471068453ITERATION : 32, loss : 0.12654888083032412ITERATION : 33, loss : 0.1265494794509985ITERATION : 34, loss : 0.12654991396477944ITERATION : 35, loss : 0.12655022973079447ITERATION : 36, loss : 0.12655045954849647ITERATION : 37, loss : 0.12655062702575892ITERATION : 38, loss : 0.12655074922073165ITERATION : 39, loss : 0.12655083850767318ITERATION : 40, loss : 0.12655090380950512ITERATION : 41, loss : 0.12655095163911673ITERATION : 42, loss : 0.12655098667530376ITERATION : 43, loss : 0.12655101238407135ITERATION : 44, loss : 0.12655103126990813ITERATION : 45, loss : 0.12655104514631804ITERATION : 46, loss : 0.12655105536610203ITERATION : 47, loss : 0.12655106290226992ITERATION : 48, loss : 0.12655106842892672ITERATION : 49, loss : 0.12655107249461636ITERATION : 50, loss : 0.12655107546332786ITERATION : 51, loss : 0.12655107767504445ITERATION : 52, loss : 0.12655107928860154ITERATION : 53, loss : 0.1265510804942512ITERATION : 54, loss : 0.12655108135859705ITERATION : 55, loss : 0.12655108202143145ITERATION : 56, loss : 0.12655108247087954ITERATION : 57, loss : 0.12655108282968305ITERATION : 58, loss : 0.12655108310214136ITERATION : 59, loss : 0.12655108330934067ITERATION : 60, loss : 0.1265510834238834ITERATION : 61, loss : 0.12655108352293057ITERATION : 62, loss : 0.12655108358970685ITERATION : 63, loss : 0.12655108365946788ITERATION : 64, loss : 0.1265510837175292ITERATION : 65, loss : 0.12655108376744245ITERATION : 66, loss : 0.12655108376756383ITERATION : 67, loss : 0.12655108376756383ITERATION : 68, loss : 0.12655108376756383ITERATION : 69, loss : 0.12655108376756383ITERATION : 70, loss : 0.12655108376756383ITERATION : 71, loss : 0.12655108376756383ITERATION : 72, loss : 0.12655108376756383ITERATION : 73, loss : 0.12655108376756383ITERATION : 74, loss : 0.12655108376756383ITERATION : 75, loss : 0.12655108376756383ITERATION : 76, loss : 0.12655108376756383ITERATION : 77, loss : 0.12655108376756383ITERATION : 78, loss : 0.12655108376756383ITERATION : 79, loss : 0.12655108376756383ITERATION : 80, loss : 0.12655108376756383ITERATION : 81, loss : 0.12655108376756383ITERATION : 82, loss : 0.12655108376756383ITERATION : 83, loss : 0.12655108376756383ITERATION : 84, loss : 0.12655108376756383ITERATION : 85, loss : 0.12655108376756383ITERATION : 86, loss : 0.12655108376756383ITERATION : 87, loss : 0.12655108376756383ITERATION : 88, loss : 0.12655108376756383ITERATION : 89, loss : 0.12655108376756383ITERATION : 90, loss : 0.12655108376756383ITERATION : 91, loss : 0.12655108376756383ITERATION : 92, loss : 0.12655108376756383ITERATION : 93, loss : 0.12655108376756383ITERATION : 94, loss : 0.12655108376756383ITERATION : 95, loss : 0.12655108376756383ITERATION : 96, loss : 0.12655108376756383ITERATION : 97, loss : 0.12655108376756383ITERATION : 98, loss : 0.12655108376756383ITERATION : 99, loss : 0.12655108376756383ITERATION : 100, loss : 0.12655108376756383
ITERATION : 1, loss : 0.05716010475823141ITERATION : 2, loss : 0.08057103895363919ITERATION : 3, loss : 0.0955177213197651ITERATION : 4, loss : 0.10550597019904659ITERATION : 5, loss : 0.11220648404559629ITERATION : 6, loss : 0.1161544135147562ITERATION : 7, loss : 0.1187014961166474ITERATION : 8, loss : 0.12045911319564333ITERATION : 9, loss : 0.12168061741358634ITERATION : 10, loss : 0.12253389542530735ITERATION : 11, loss : 0.1231321076977241ITERATION : 12, loss : 0.12355255279899709ITERATION : 13, loss : 0.12384856331211991ITERATION : 14, loss : 0.12405720585173063ITERATION : 15, loss : 0.12420437698904625ITERATION : 16, loss : 0.12430823586405591ITERATION : 17, loss : 0.12438154862211113ITERATION : 18, loss : 0.12443330591735335ITERATION : 19, loss : 0.124469846517968ITERATION : 20, loss : 0.12449564316586136ITERATION : 21, loss : 0.12451385340999956ITERATION : 22, loss : 0.12452670676428694ITERATION : 23, loss : 0.1245357778281961ITERATION : 24, loss : 0.12454217862995422ITERATION : 25, loss : 0.12454669447290956ITERATION : 26, loss : 0.1245498799228255ITERATION : 27, loss : 0.12455212650373187ITERATION : 28, loss : 0.1245537106463297ITERATION : 29, loss : 0.12455482751638547ITERATION : 30, loss : 0.12455561476270832ITERATION : 31, loss : 0.1245561695737922ITERATION : 32, loss : 0.12455656057056233ITERATION : 33, loss : 0.12455683601393416ITERATION : 34, loss : 0.12455703002002402ITERATION : 35, loss : 0.12455716664075048ITERATION : 36, loss : 0.12455726283747424ITERATION : 37, loss : 0.12455733055542464ITERATION : 38, loss : 0.12455737820673364ITERATION : 39, loss : 0.12455741175071844ITERATION : 40, loss : 0.12455743535171374ITERATION : 41, loss : 0.12455745195264739ITERATION : 42, loss : 0.1245574635918836ITERATION : 43, loss : 0.12455747179863602ITERATION : 44, loss : 0.12455747759249332ITERATION : 45, loss : 0.12455748165351244ITERATION : 46, loss : 0.12455748450879152ITERATION : 47, loss : 0.12455748651446592ITERATION : 48, loss : 0.12455748790106129ITERATION : 49, loss : 0.1245574888823518ITERATION : 50, loss : 0.12455748956071144ITERATION : 51, loss : 0.12455749001810233ITERATION : 52, loss : 0.12455749035022821ITERATION : 53, loss : 0.12455749057874974ITERATION : 54, loss : 0.12455749072670429ITERATION : 55, loss : 0.12455749082640259ITERATION : 56, loss : 0.12455749094642446ITERATION : 57, loss : 0.12455749099305853ITERATION : 58, loss : 0.12455749104576919ITERATION : 59, loss : 0.12455749104878085ITERATION : 60, loss : 0.12455749107527744ITERATION : 61, loss : 0.12455749107574646ITERATION : 62, loss : 0.12455749107574646ITERATION : 63, loss : 0.12455749107574646ITERATION : 64, loss : 0.12455749107574646ITERATION : 65, loss : 0.12455749107574646ITERATION : 66, loss : 0.12455749107574646ITERATION : 67, loss : 0.12455749107574646ITERATION : 68, loss : 0.12455749107574646ITERATION : 69, loss : 0.12455749107574646ITERATION : 70, loss : 0.12455749107574646ITERATION : 71, loss : 0.12455749107574646ITERATION : 72, loss : 0.12455749107574646ITERATION : 73, loss : 0.12455749107574646ITERATION : 74, loss : 0.12455749107574646ITERATION : 75, loss : 0.12455749107574646ITERATION : 76, loss : 0.12455749107574646ITERATION : 77, loss : 0.12455749107574646ITERATION : 78, loss : 0.12455749107574646ITERATION : 79, loss : 0.12455749107574646ITERATION : 80, loss : 0.12455749107574646ITERATION : 81, loss : 0.12455749107574646ITERATION : 82, loss : 0.12455749107574646ITERATION : 83, loss : 0.12455749107574646ITERATION : 84, loss : 0.12455749107574646ITERATION : 85, loss : 0.12455749107574646ITERATION : 86, loss : 0.12455749107574646ITERATION : 87, loss : 0.12455749107574646ITERATION : 88, loss : 0.12455749107574646ITERATION : 89, loss : 0.12455749107574646ITERATION : 90, loss : 0.12455749107574646ITERATION : 91, loss : 0.12455749107574646ITERATION : 92, loss : 0.12455749107574646ITERATION : 93, loss : 0.12455749107574646ITERATION : 94, loss : 0.12455749107574646ITERATION : 95, loss : 0.12455749107574646ITERATION : 96, loss : 0.12455749107574646ITERATION : 97, loss : 0.12455749107574646ITERATION : 98, loss : 0.12455749107574646ITERATION : 99, loss : 0.12455749107574646ITERATION : 100, loss : 0.12455749107574646
gradient norm in None layer : 0.0011025448737891261
gradient norm in None layer : 4.561683055299183e-05
gradient norm in None layer : 3.654120221140227e-05
gradient norm in None layer : 0.0006971012444315732
gradient norm in None layer : 4.680303217221986e-05
gradient norm in None layer : 4.767573711150945e-05
gradient norm in None layer : 0.00041435728685566104
gradient norm in None layer : 1.5155440123287992e-05
gradient norm in None layer : 1.5511716435382385e-05
gradient norm in None layer : 0.00032819659715173924
gradient norm in None layer : 1.6349208643552683e-05
gradient norm in None layer : 1.288688174148996e-05
gradient norm in None layer : 0.00012089626420473992
gradient norm in None layer : 3.635606770268179e-06
gradient norm in None layer : 3.5964792102319604e-06
gradient norm in None layer : 0.0001192367702989463
gradient norm in None layer : 4.446541765189322e-06
gradient norm in None layer : 4.0738436930353745e-06
gradient norm in None layer : 0.00014803595010248792
gradient norm in None layer : 4.936129333224015e-06
gradient norm in None layer : 0.00031123137175898224
gradient norm in None layer : 2.0694828900817412e-05
gradient norm in None layer : 1.4535351788178338e-05
gradient norm in None layer : 0.00032615509713713445
gradient norm in None layer : 2.8960715395130583e-05
gradient norm in None layer : 3.307121040633609e-05
gradient norm in None layer : 0.0005238226019720085
gradient norm in None layer : 9.264926809679099e-06
gradient norm in None layer : 0.0007110024502700087
gradient norm in None layer : 4.87229106391686e-05
gradient norm in None layer : 4.82974397537948e-05
gradient norm in None layer : 0.0008344286192517565
gradient norm in None layer : 0.00014067829052840592
gradient norm in None layer : 0.0002112175088676315
gradient norm in None layer : 0.00010703003338476065
gradient norm in None layer : 3.422283375219706e-05
Total gradient norm: 0.0019504872586621788
invariance loss : 4.839501804614311, avg_den : 0.44423675537109375, density loss : 0.34364949707019077, mse loss : 0.08843151985452591, solver time : 122.90615963935852 sec , total loss : 0.09361467115621042, running loss : 0.10735646937626817
Epoch 0/10 , batch 19/12500 
ITERATION : 1, loss : 0.016517526817581413ITERATION : 2, loss : 0.014522572455029468ITERATION : 3, loss : 0.0159488555775088ITERATION : 4, loss : 0.017879046585457414ITERATION : 5, loss : 0.01959226678816116ITERATION : 6, loss : 0.020985845932020217ITERATION : 7, loss : 0.022065909701989083ITERATION : 8, loss : 0.022870204102072373ITERATION : 9, loss : 0.0234562705711581ITERATION : 10, loss : 0.02387780760457469ITERATION : 11, loss : 0.0241785052229017ITERATION : 12, loss : 0.024391826338615318ITERATION : 13, loss : 0.024542590942805885ITERATION : 14, loss : 0.02464886249194516ITERATION : 15, loss : 0.024723630551531036ITERATION : 16, loss : 0.024776162383799115ITERATION : 17, loss : 0.02481303414473335ITERATION : 18, loss : 0.02483889483870802ITERATION : 19, loss : 0.02485702238187141ITERATION : 20, loss : 0.024869723552686514ITERATION : 21, loss : 0.024878619559974214ITERATION : 22, loss : 0.024884848571941662ITERATION : 23, loss : 0.024889209087483707ITERATION : 24, loss : 0.02489226089236358ITERATION : 25, loss : 0.024894396354911103ITERATION : 26, loss : 0.024895890330617735ITERATION : 27, loss : 0.02489693534620476ITERATION : 28, loss : 0.024897666199134052ITERATION : 29, loss : 0.024898177238897746ITERATION : 30, loss : 0.02489853451210518ITERATION : 31, loss : 0.02489878424245598ITERATION : 32, loss : 0.024898958752402608ITERATION : 33, loss : 0.024899080710219828ITERATION : 34, loss : 0.02489916591526901ITERATION : 35, loss : 0.024899225414371604ITERATION : 36, loss : 0.02489926696551676ITERATION : 37, loss : 0.02489929597660791ITERATION : 38, loss : 0.02489931623548659ITERATION : 39, loss : 0.024899330380201135ITERATION : 40, loss : 0.024899340238246997ITERATION : 41, loss : 0.024899347124203046ITERATION : 42, loss : 0.02489935192733449ITERATION : 43, loss : 0.024899355278358595ITERATION : 44, loss : 0.024899357596210918ITERATION : 45, loss : 0.024899359206915685ITERATION : 46, loss : 0.024899360359261047ITERATION : 47, loss : 0.024899361125076312ITERATION : 48, loss : 0.024899361666080273ITERATION : 49, loss : 0.024899362004136054ITERATION : 50, loss : 0.024899362271569598ITERATION : 51, loss : 0.02489936245671638ITERATION : 52, loss : 0.02489936256163118ITERATION : 53, loss : 0.024899362649361743ITERATION : 54, loss : 0.024899362715846623ITERATION : 55, loss : 0.024899362735420417ITERATION : 56, loss : 0.024899362772160143ITERATION : 57, loss : 0.02489936277353244ITERATION : 58, loss : 0.02489936277446272ITERATION : 59, loss : 0.02489936277446272ITERATION : 60, loss : 0.02489936277446272ITERATION : 61, loss : 0.02489936277446272ITERATION : 62, loss : 0.02489936277446272ITERATION : 63, loss : 0.02489936277446272ITERATION : 64, loss : 0.02489936277446272ITERATION : 65, loss : 0.02489936277446272ITERATION : 66, loss : 0.02489936277446272ITERATION : 67, loss : 0.02489936277446272ITERATION : 68, loss : 0.02489936277446272ITERATION : 69, loss : 0.02489936277446272ITERATION : 70, loss : 0.02489936277446272ITERATION : 71, loss : 0.02489936277446272ITERATION : 72, loss : 0.02489936277446272ITERATION : 73, loss : 0.02489936277446272ITERATION : 74, loss : 0.02489936277446272ITERATION : 75, loss : 0.02489936277446272ITERATION : 76, loss : 0.02489936277446272ITERATION : 77, loss : 0.02489936277446272ITERATION : 78, loss : 0.02489936277446272ITERATION : 79, loss : 0.02489936277446272ITERATION : 80, loss : 0.02489936277446272ITERATION : 81, loss : 0.02489936277446272ITERATION : 82, loss : 0.02489936277446272ITERATION : 83, loss : 0.02489936277446272ITERATION : 84, loss : 0.02489936277446272ITERATION : 85, loss : 0.02489936277446272ITERATION : 86, loss : 0.02489936277446272ITERATION : 87, loss : 0.02489936277446272ITERATION : 88, loss : 0.02489936277446272ITERATION : 89, loss : 0.02489936277446272ITERATION : 90, loss : 0.02489936277446272ITERATION : 91, loss : 0.02489936277446272ITERATION : 92, loss : 0.02489936277446272ITERATION : 93, loss : 0.02489936277446272ITERATION : 94, loss : 0.02489936277446272ITERATION : 95, loss : 0.02489936277446272ITERATION : 96, loss : 0.02489936277446272ITERATION : 97, loss : 0.02489936277446272ITERATION : 98, loss : 0.02489936277446272ITERATION : 99, loss : 0.02489936277446272ITERATION : 100, loss : 0.02489936277446272
ITERATION : 1, loss : 0.03954305284916347ITERATION : 2, loss : 0.0346252646338423ITERATION : 3, loss : 0.03611490123460551ITERATION : 4, loss : 0.038462657389859566ITERATION : 5, loss : 0.04080349490953811ITERATION : 6, loss : 0.04312240102709133ITERATION : 7, loss : 0.04454084297371468ITERATION : 8, loss : 0.045450136356774244ITERATION : 9, loss : 0.046028283051978855ITERATION : 10, loss : 0.04638761105634272ITERATION : 11, loss : 0.0466054360062353ITERATION : 12, loss : 0.04673354238771731ITERATION : 13, loss : 0.046805903104246524ITERATION : 14, loss : 0.046844407350356373ITERATION : 15, loss : 0.04686291416915594ITERATION : 16, loss : 0.04687003745417836ITERATION : 17, loss : 0.04687101967839407ITERATION : 18, loss : 0.04686896993037507ITERATION : 19, loss : 0.04686567064873514ITERATION : 20, loss : 0.04686209494043688ITERATION : 21, loss : 0.04685873426683743ITERATION : 22, loss : 0.04685580224161624ITERATION : 23, loss : 0.046853358795486924ITERATION : 24, loss : 0.04685138518427473ITERATION : 25, loss : 0.046849826753099705ITERATION : 26, loss : 0.046848617189846016ITERATION : 27, loss : 0.046847690901142144ITERATION : 28, loss : 0.04684698928594313ITERATION : 29, loss : 0.04684646261000508ITERATION : 30, loss : 0.04684607023762872ITERATION : 31, loss : 0.04684577979402349ITERATION : 32, loss : 0.046845565969124224ITERATION : 33, loss : 0.04684540933184056ITERATION : 34, loss : 0.04684529506628675ITERATION : 35, loss : 0.046845212016055904ITERATION : 36, loss : 0.04684515185970123ITERATION : 37, loss : 0.046845108412105756ITERATION : 38, loss : 0.04684507714184476ITERATION : 39, loss : 0.046845054728000254ITERATION : 40, loss : 0.046845038629197394ITERATION : 41, loss : 0.0468450271628326ITERATION : 42, loss : 0.04684501895165445ITERATION : 43, loss : 0.04684501312208478ITERATION : 44, loss : 0.04684500895621518ITERATION : 45, loss : 0.04684500602291014ITERATION : 46, loss : 0.04684500393300725ITERATION : 47, loss : 0.04684500247813355ITERATION : 48, loss : 0.04684500141466343ITERATION : 49, loss : 0.046845000694241995ITERATION : 50, loss : 0.04684500016617558ITERATION : 51, loss : 0.046844999827413566ITERATION : 52, loss : 0.04684499954489063ITERATION : 53, loss : 0.046844999348422865ITERATION : 54, loss : 0.046844999238334024ITERATION : 55, loss : 0.046844999138303096ITERATION : 56, loss : 0.046844999057436505ITERATION : 57, loss : 0.046844999057124116ITERATION : 58, loss : 0.046844999057124116ITERATION : 59, loss : 0.046844999057124116ITERATION : 60, loss : 0.046844999057124116ITERATION : 61, loss : 0.046844999057124116ITERATION : 62, loss : 0.046844999057124116ITERATION : 63, loss : 0.046844999057124116ITERATION : 64, loss : 0.046844999057124116ITERATION : 65, loss : 0.046844999057124116ITERATION : 66, loss : 0.046844999057124116ITERATION : 67, loss : 0.046844999057124116ITERATION : 68, loss : 0.046844999057124116ITERATION : 69, loss : 0.046844999057124116ITERATION : 70, loss : 0.046844999057124116ITERATION : 71, loss : 0.046844999057124116ITERATION : 72, loss : 0.046844999057124116ITERATION : 73, loss : 0.046844999057124116ITERATION : 74, loss : 0.046844999057124116ITERATION : 75, loss : 0.046844999057124116ITERATION : 76, loss : 0.046844999057124116ITERATION : 77, loss : 0.046844999057124116ITERATION : 78, loss : 0.046844999057124116ITERATION : 79, loss : 0.046844999057124116ITERATION : 80, loss : 0.046844999057124116ITERATION : 81, loss : 0.046844999057124116ITERATION : 82, loss : 0.046844999057124116ITERATION : 83, loss : 0.046844999057124116ITERATION : 84, loss : 0.046844999057124116ITERATION : 85, loss : 0.046844999057124116ITERATION : 86, loss : 0.046844999057124116ITERATION : 87, loss : 0.046844999057124116ITERATION : 88, loss : 0.046844999057124116ITERATION : 89, loss : 0.046844999057124116ITERATION : 90, loss : 0.046844999057124116ITERATION : 91, loss : 0.046844999057124116ITERATION : 92, loss : 0.046844999057124116ITERATION : 93, loss : 0.046844999057124116ITERATION : 94, loss : 0.046844999057124116ITERATION : 95, loss : 0.046844999057124116ITERATION : 96, loss : 0.046844999057124116ITERATION : 97, loss : 0.046844999057124116ITERATION : 98, loss : 0.046844999057124116ITERATION : 99, loss : 0.046844999057124116ITERATION : 100, loss : 0.046844999057124116
ITERATION : 1, loss : 0.04541770169404564ITERATION : 2, loss : 0.0493699914026675ITERATION : 3, loss : 0.05248134623226939ITERATION : 4, loss : 0.05478471655699245ITERATION : 5, loss : 0.056480208175802744ITERATION : 6, loss : 0.05771279454686959ITERATION : 7, loss : 0.05859727622335638ITERATION : 8, loss : 0.059225328770759006ITERATION : 9, loss : 0.05966782722662637ITERATION : 10, loss : 0.05997783704433218ITERATION : 11, loss : 0.0601941463167376ITERATION : 12, loss : 0.06034463328994936ITERATION : 13, loss : 0.06044910253382986ITERATION : 14, loss : 0.06052150971676908ITERATION : 15, loss : 0.06057163351927797ITERATION : 16, loss : 0.06060629841046438ITERATION : 17, loss : 0.06063025360005912ITERATION : 18, loss : 0.06064679719901364ITERATION : 19, loss : 0.06065821578128975ITERATION : 20, loss : 0.060666093073225014ITERATION : 21, loss : 0.06067152480430709ITERATION : 22, loss : 0.060675268527408954ITERATION : 23, loss : 0.0606778476886971ITERATION : 24, loss : 0.06067962380019305ITERATION : 25, loss : 0.060680846378739585ITERATION : 26, loss : 0.060681687519212824ITERATION : 27, loss : 0.0606822660067656ITERATION : 28, loss : 0.060682663699476745ITERATION : 29, loss : 0.06068293692946729ITERATION : 30, loss : 0.060683124553305955ITERATION : 31, loss : 0.060683253336661615ITERATION : 32, loss : 0.06068334168624686ITERATION : 33, loss : 0.06068340224316828ITERATION : 34, loss : 0.06068344374639893ITERATION : 35, loss : 0.060683472160466684ITERATION : 36, loss : 0.06068349161664663ITERATION : 37, loss : 0.06068350493779536ITERATION : 38, loss : 0.0606835140418676ITERATION : 39, loss : 0.06068352024135325ITERATION : 40, loss : 0.060683524501491316ITERATION : 41, loss : 0.06068352740613795ITERATION : 42, loss : 0.06068352935236893ITERATION : 43, loss : 0.060683530690326795ITERATION : 44, loss : 0.06068353157087251ITERATION : 45, loss : 0.06068353219261686ITERATION : 46, loss : 0.06068353258328545ITERATION : 47, loss : 0.06068353286436554ITERATION : 48, loss : 0.060683533033636945ITERATION : 49, loss : 0.06068353315074582ITERATION : 50, loss : 0.06068353323224029ITERATION : 51, loss : 0.06068353326970024ITERATION : 52, loss : 0.0606835333027007ITERATION : 53, loss : 0.060683533336013326ITERATION : 54, loss : 0.06068353333346708ITERATION : 55, loss : 0.06068353334587603ITERATION : 56, loss : 0.06068353334574497ITERATION : 57, loss : 0.06068353334574497ITERATION : 58, loss : 0.06068353334574497ITERATION : 59, loss : 0.06068353334574497ITERATION : 60, loss : 0.06068353334574497ITERATION : 61, loss : 0.06068353334574497ITERATION : 62, loss : 0.06068353334574497ITERATION : 63, loss : 0.06068353334574497ITERATION : 64, loss : 0.06068353334574497ITERATION : 65, loss : 0.06068353334574497ITERATION : 66, loss : 0.06068353334574497ITERATION : 67, loss : 0.06068353334574497ITERATION : 68, loss : 0.06068353334574497ITERATION : 69, loss : 0.06068353334574497ITERATION : 70, loss : 0.06068353334574497ITERATION : 71, loss : 0.06068353334574497ITERATION : 72, loss : 0.06068353334574497ITERATION : 73, loss : 0.06068353334574497ITERATION : 74, loss : 0.06068353334574497ITERATION : 75, loss : 0.06068353334574497ITERATION : 76, loss : 0.06068353334574497ITERATION : 77, loss : 0.06068353334574497ITERATION : 78, loss : 0.06068353334574497ITERATION : 79, loss : 0.06068353334574497ITERATION : 80, loss : 0.06068353334574497ITERATION : 81, loss : 0.06068353334574497ITERATION : 82, loss : 0.06068353334574497ITERATION : 83, loss : 0.06068353334574497ITERATION : 84, loss : 0.06068353334574497ITERATION : 85, loss : 0.06068353334574497ITERATION : 86, loss : 0.06068353334574497ITERATION : 87, loss : 0.06068353334574497ITERATION : 88, loss : 0.06068353334574497ITERATION : 89, loss : 0.06068353334574497ITERATION : 90, loss : 0.06068353334574497ITERATION : 91, loss : 0.06068353334574497ITERATION : 92, loss : 0.06068353334574497ITERATION : 93, loss : 0.06068353334574497ITERATION : 94, loss : 0.06068353334574497ITERATION : 95, loss : 0.06068353334574497ITERATION : 96, loss : 0.06068353334574497ITERATION : 97, loss : 0.06068353334574497ITERATION : 98, loss : 0.06068353334574497ITERATION : 99, loss : 0.06068353334574497ITERATION : 100, loss : 0.06068353334574497
ITERATION : 1, loss : 0.04439948874232574ITERATION : 2, loss : 0.03228690181525893ITERATION : 3, loss : 0.02787281787386067ITERATION : 4, loss : 0.025934633647680713ITERATION : 5, loss : 0.02499322174260911ITERATION : 6, loss : 0.024496356408114362ITERATION : 7, loss : 0.024215887961256625ITERATION : 8, loss : 0.024049298035889295ITERATION : 9, loss : 0.02394665239473408ITERATION : 10, loss : 0.023881784434952348ITERATION : 11, loss : 0.02384009622470755ITERATION : 12, loss : 0.023813019023965838ITERATION : 13, loss : 0.023795321569121276ITERATION : 14, loss : 0.023783717343789885ITERATION : 15, loss : 0.023776099829337236ITERATION : 16, loss : 0.023771101102370035ITERATION : 17, loss : 0.02376782530666988ITERATION : 18, loss : 0.02376568331629109ITERATION : 19, loss : 0.023764286684880787ITERATION : 20, loss : 0.023763379094249654ITERATION : 21, loss : 0.023762791705128888ITERATION : 22, loss : 0.02376241332502241ITERATION : 23, loss : 0.02376217090426559ITERATION : 24, loss : 0.023762016565185867ITERATION : 25, loss : 0.023761919049828193ITERATION : 26, loss : 0.023761857986217518ITERATION : 27, loss : 0.023761820164764823ITERATION : 28, loss : 0.023761797066998165ITERATION : 29, loss : 0.02376178320916496ITERATION : 30, loss : 0.023761775086042367ITERATION : 31, loss : 0.023761770483829152ITERATION : 32, loss : 0.023761768018832608ITERATION : 33, loss : 0.023761766792788876ITERATION : 34, loss : 0.02376176628776984ITERATION : 35, loss : 0.023761766173102026ITERATION : 36, loss : 0.02376176623081743ITERATION : 37, loss : 0.02376176640203146ITERATION : 38, loss : 0.023761766604032016ITERATION : 39, loss : 0.023761766773313008ITERATION : 40, loss : 0.02376176694049342ITERATION : 41, loss : 0.02376176709007528ITERATION : 42, loss : 0.023761767232267937ITERATION : 43, loss : 0.02376176733462047ITERATION : 44, loss : 0.023761767427644395ITERATION : 45, loss : 0.023761767490011368ITERATION : 46, loss : 0.023761767553312836ITERATION : 47, loss : 0.02376176758506128ITERATION : 48, loss : 0.023761767617482596ITERATION : 49, loss : 0.023761767637873573ITERATION : 50, loss : 0.023761767653280082ITERATION : 51, loss : 0.02376176765531808ITERATION : 52, loss : 0.02376176765531808ITERATION : 53, loss : 0.02376176765531808ITERATION : 54, loss : 0.02376176765531808ITERATION : 55, loss : 0.02376176765531808ITERATION : 56, loss : 0.02376176765531808ITERATION : 57, loss : 0.02376176765531808ITERATION : 58, loss : 0.02376176765531808ITERATION : 59, loss : 0.02376176765531808ITERATION : 60, loss : 0.02376176765531808ITERATION : 61, loss : 0.02376176765531808ITERATION : 62, loss : 0.02376176765531808ITERATION : 63, loss : 0.02376176765531808ITERATION : 64, loss : 0.02376176765531808ITERATION : 65, loss : 0.02376176765531808ITERATION : 66, loss : 0.02376176765531808ITERATION : 67, loss : 0.02376176765531808ITERATION : 68, loss : 0.02376176765531808ITERATION : 69, loss : 0.02376176765531808ITERATION : 70, loss : 0.02376176765531808ITERATION : 71, loss : 0.02376176765531808ITERATION : 72, loss : 0.02376176765531808ITERATION : 73, loss : 0.02376176765531808ITERATION : 74, loss : 0.02376176765531808ITERATION : 75, loss : 0.02376176765531808ITERATION : 76, loss : 0.02376176765531808ITERATION : 77, loss : 0.02376176765531808ITERATION : 78, loss : 0.02376176765531808ITERATION : 79, loss : 0.02376176765531808ITERATION : 80, loss : 0.02376176765531808ITERATION : 81, loss : 0.02376176765531808ITERATION : 82, loss : 0.02376176765531808ITERATION : 83, loss : 0.02376176765531808ITERATION : 84, loss : 0.02376176765531808ITERATION : 85, loss : 0.02376176765531808ITERATION : 86, loss : 0.02376176765531808ITERATION : 87, loss : 0.02376176765531808ITERATION : 88, loss : 0.02376176765531808ITERATION : 89, loss : 0.02376176765531808ITERATION : 90, loss : 0.02376176765531808ITERATION : 91, loss : 0.02376176765531808ITERATION : 92, loss : 0.02376176765531808ITERATION : 93, loss : 0.02376176765531808ITERATION : 94, loss : 0.02376176765531808ITERATION : 95, loss : 0.02376176765531808ITERATION : 96, loss : 0.02376176765531808ITERATION : 97, loss : 0.02376176765531808ITERATION : 98, loss : 0.02376176765531808ITERATION : 99, loss : 0.02376176765531808ITERATION : 100, loss : 0.02376176765531808
ITERATION : 1, loss : 0.005157230232828503ITERATION : 2, loss : 0.0027176938815236954ITERATION : 3, loss : 0.002996333406087635ITERATION : 4, loss : 0.003192619765457169ITERATION : 5, loss : 0.0033797978013956937ITERATION : 6, loss : 0.003541249537964839ITERATION : 7, loss : 0.0036725962088349773ITERATION : 8, loss : 0.0037752482269841603ITERATION : 9, loss : 0.0038533073951338365ITERATION : 10, loss : 0.003911569433184856ITERATION : 11, loss : 0.003954503650664891ITERATION : 12, loss : 0.003985864695095637ITERATION : 13, loss : 0.0040086318429625265ITERATION : 14, loss : 0.004025089045604844ITERATION : 15, loss : 0.004036949122570335ITERATION : 16, loss : 0.004045477991717822ITERATION : 17, loss : 0.004051602157515129ITERATION : 18, loss : 0.004055994996793079ITERATION : 19, loss : 0.004059143670818644ITERATION : 20, loss : 0.004061399450969814ITERATION : 21, loss : 0.004063015004494477ITERATION : 22, loss : 0.00406417179136975ITERATION : 23, loss : 0.004064999983320663ITERATION : 24, loss : 0.004065592918343182ITERATION : 25, loss : 0.0040660173661469685ITERATION : 26, loss : 0.0040663212283939465ITERATION : 27, loss : 0.004066538777488691ITERATION : 28, loss : 0.004066694553519352ITERATION : 29, loss : 0.004066806096561777ITERATION : 30, loss : 0.0040668859672432466ITERATION : 31, loss : 0.0040669431740373825ITERATION : 32, loss : 0.004066984142547553ITERATION : 33, loss : 0.004067013495251017ITERATION : 34, loss : 0.004067034524374988ITERATION : 35, loss : 0.0040670495984373185ITERATION : 36, loss : 0.004067060395709162ITERATION : 37, loss : 0.004067068111611459ITERATION : 38, loss : 0.004067073671004833ITERATION : 39, loss : 0.004067077652166779ITERATION : 40, loss : 0.0040670804985415586ITERATION : 41, loss : 0.004067082535951878ITERATION : 42, loss : 0.004067083992215277ITERATION : 43, loss : 0.00406708503797957ITERATION : 44, loss : 0.004067085785570579ITERATION : 45, loss : 0.004067086326524145ITERATION : 46, loss : 0.004067086702811671ITERATION : 47, loss : 0.004067086982618839ITERATION : 48, loss : 0.004067087181381491ITERATION : 49, loss : 0.004067087321527862ITERATION : 50, loss : 0.004067087422983225ITERATION : 51, loss : 0.004067087490005943ITERATION : 52, loss : 0.004067087541935338ITERATION : 53, loss : 0.004067087575356329ITERATION : 54, loss : 0.0040670875912472435ITERATION : 55, loss : 0.004067087592713759ITERATION : 56, loss : 0.004067087592713759ITERATION : 57, loss : 0.004067087592713759ITERATION : 58, loss : 0.004067087592713759ITERATION : 59, loss : 0.004067087592713759ITERATION : 60, loss : 0.004067087592713759ITERATION : 61, loss : 0.004067087592713759ITERATION : 62, loss : 0.004067087592713759ITERATION : 63, loss : 0.004067087592713759ITERATION : 64, loss : 0.004067087592713759ITERATION : 65, loss : 0.004067087592713759ITERATION : 66, loss : 0.004067087592713759ITERATION : 67, loss : 0.004067087592713759ITERATION : 68, loss : 0.004067087592713759ITERATION : 69, loss : 0.004067087592713759ITERATION : 70, loss : 0.004067087592713759ITERATION : 71, loss : 0.004067087592713759ITERATION : 72, loss : 0.004067087592713759ITERATION : 73, loss : 0.004067087592713759ITERATION : 74, loss : 0.004067087592713759ITERATION : 75, loss : 0.004067087592713759ITERATION : 76, loss : 0.004067087592713759ITERATION : 77, loss : 0.004067087592713759ITERATION : 78, loss : 0.004067087592713759ITERATION : 79, loss : 0.004067087592713759ITERATION : 80, loss : 0.004067087592713759ITERATION : 81, loss : 0.004067087592713759ITERATION : 82, loss : 0.004067087592713759ITERATION : 83, loss : 0.004067087592713759ITERATION : 84, loss : 0.004067087592713759ITERATION : 85, loss : 0.004067087592713759ITERATION : 86, loss : 0.004067087592713759ITERATION : 87, loss : 0.004067087592713759ITERATION : 88, loss : 0.004067087592713759ITERATION : 89, loss : 0.004067087592713759ITERATION : 90, loss : 0.004067087592713759ITERATION : 91, loss : 0.004067087592713759ITERATION : 92, loss : 0.004067087592713759ITERATION : 93, loss : 0.004067087592713759ITERATION : 94, loss : 0.004067087592713759ITERATION : 95, loss : 0.004067087592713759ITERATION : 96, loss : 0.004067087592713759ITERATION : 97, loss : 0.004067087592713759ITERATION : 98, loss : 0.004067087592713759ITERATION : 99, loss : 0.004067087592713759ITERATION : 100, loss : 0.004067087592713759
ITERATION : 1, loss : 0.06936267957455097ITERATION : 2, loss : 0.08962860524549006ITERATION : 3, loss : 0.10282331887650856ITERATION : 4, loss : 0.11124674846187088ITERATION : 5, loss : 0.11670180458386134ITERATION : 6, loss : 0.12004775344396107ITERATION : 7, loss : 0.12225176172092145ITERATION : 8, loss : 0.12374272496049658ITERATION : 9, loss : 0.12475979399530146ITERATION : 10, loss : 0.12545769755423009ITERATION : 11, loss : 0.12593856032434442ITERATION : 12, loss : 0.12627081327632791ITERATION : 13, loss : 0.12650082160304585ITERATION : 14, loss : 0.12666025087053437ITERATION : 15, loss : 0.12677084960028556ITERATION : 16, loss : 0.126847613227394ITERATION : 17, loss : 0.12690090892035533ITERATION : 18, loss : 0.12693791701084123ITERATION : 19, loss : 0.12696361651878132ITERATION : 20, loss : 0.126981462799513ITERATION : 21, loss : 0.12699385490650592ITERATION : 22, loss : 0.127002458957511ITERATION : 23, loss : 0.12700843222153932ITERATION : 24, loss : 0.1270125785910304ITERATION : 25, loss : 0.1270154564219679ITERATION : 26, loss : 0.12701745353451743ITERATION : 27, loss : 0.12701883919512907ITERATION : 28, loss : 0.12701980056064383ITERATION : 29, loss : 0.12702046745586584ITERATION : 30, loss : 0.12702092999008072ITERATION : 31, loss : 0.12702125071234138ITERATION : 32, loss : 0.1270214730816357ITERATION : 33, loss : 0.12702162722861976ITERATION : 34, loss : 0.12702173406931003ITERATION : 35, loss : 0.12702180811054553ITERATION : 36, loss : 0.12702185941359623ITERATION : 37, loss : 0.1270218949423824ITERATION : 38, loss : 0.12702191957673292ITERATION : 39, loss : 0.127021936649313ITERATION : 40, loss : 0.12702194845602116ITERATION : 41, loss : 0.1270219566613403ITERATION : 42, loss : 0.1270219623094391ITERATION : 43, loss : 0.1270219662175085ITERATION : 44, loss : 0.12702196897574286ITERATION : 45, loss : 0.12702197082888886ITERATION : 46, loss : 0.12702197210575722ITERATION : 47, loss : 0.12702197296062492ITERATION : 48, loss : 0.12702197357783296ITERATION : 49, loss : 0.1270219739804239ITERATION : 50, loss : 0.12702197423951866ITERATION : 51, loss : 0.12702197445875943ITERATION : 52, loss : 0.12702197460804945ITERATION : 53, loss : 0.12702197471064308ITERATION : 54, loss : 0.1270219747759879ITERATION : 55, loss : 0.1270219748244661ITERATION : 56, loss : 0.12702197483164945ITERATION : 57, loss : 0.12702197483164945ITERATION : 58, loss : 0.12702197483164945ITERATION : 59, loss : 0.12702197483164945ITERATION : 60, loss : 0.12702197483164945ITERATION : 61, loss : 0.12702197483164945ITERATION : 62, loss : 0.12702197483164945ITERATION : 63, loss : 0.12702197483164945ITERATION : 64, loss : 0.12702197483164945ITERATION : 65, loss : 0.12702197483164945ITERATION : 66, loss : 0.12702197483164945ITERATION : 67, loss : 0.12702197483164945ITERATION : 68, loss : 0.12702197483164945ITERATION : 69, loss : 0.12702197483164945ITERATION : 70, loss : 0.12702197483164945ITERATION : 71, loss : 0.12702197483164945ITERATION : 72, loss : 0.12702197483164945ITERATION : 73, loss : 0.12702197483164945ITERATION : 74, loss : 0.12702197483164945ITERATION : 75, loss : 0.12702197483164945ITERATION : 76, loss : 0.12702197483164945ITERATION : 77, loss : 0.12702197483164945ITERATION : 78, loss : 0.12702197483164945ITERATION : 79, loss : 0.12702197483164945ITERATION : 80, loss : 0.12702197483164945ITERATION : 81, loss : 0.12702197483164945ITERATION : 82, loss : 0.12702197483164945ITERATION : 83, loss : 0.12702197483164945ITERATION : 84, loss : 0.12702197483164945ITERATION : 85, loss : 0.12702197483164945ITERATION : 86, loss : 0.12702197483164945ITERATION : 87, loss : 0.12702197483164945ITERATION : 88, loss : 0.12702197483164945ITERATION : 89, loss : 0.12702197483164945ITERATION : 90, loss : 0.12702197483164945ITERATION : 91, loss : 0.12702197483164945ITERATION : 92, loss : 0.12702197483164945ITERATION : 93, loss : 0.12702197483164945ITERATION : 94, loss : 0.12702197483164945ITERATION : 95, loss : 0.12702197483164945ITERATION : 96, loss : 0.12702197483164945ITERATION : 97, loss : 0.12702197483164945ITERATION : 98, loss : 0.12702197483164945ITERATION : 99, loss : 0.12702197483164945ITERATION : 100, loss : 0.12702197483164945
ITERATION : 1, loss : 0.026087598298603226ITERATION : 2, loss : 0.038200389180463874ITERATION : 3, loss : 0.044544119098111866ITERATION : 4, loss : 0.04948413434847925ITERATION : 5, loss : 0.05337343099022989ITERATION : 6, loss : 0.05641274133480004ITERATION : 7, loss : 0.058789102595393154ITERATION : 8, loss : 0.060653185838196594ITERATION : 9, loss : 0.06212083228561151ITERATION : 10, loss : 0.06328010418939645ITERATION : 11, loss : 0.06419812542425289ITERATION : 12, loss : 0.06492645213962563ITERATION : 13, loss : 0.06550502222788863ITERATION : 14, loss : 0.06596501079017149ITERATION : 15, loss : 0.0663309039968511ITERATION : 16, loss : 0.06662202446730414ITERATION : 17, loss : 0.06685367278924474ITERATION : 18, loss : 0.06703799299375977ITERATION : 19, loss : 0.06718463925892212ITERATION : 20, loss : 0.0673012941915175ITERATION : 21, loss : 0.06739407469826242ITERATION : 22, loss : 0.06746785256971385ITERATION : 23, loss : 0.06752650827605368ITERATION : 24, loss : 0.06757313227996498ITERATION : 25, loss : 0.06761018566747132ITERATION : 26, loss : 0.06763962783169714ITERATION : 27, loss : 0.06766301833657243ITERATION : 28, loss : 0.06768159821556889ITERATION : 29, loss : 0.06769635472364796ITERATION : 30, loss : 0.06770807315984025ITERATION : 31, loss : 0.06771737786729645ITERATION : 32, loss : 0.06772476527870464ITERATION : 33, loss : 0.06773062986710465ITERATION : 34, loss : 0.06773528513774878ITERATION : 35, loss : 0.06773898019108564ITERATION : 36, loss : 0.06774191283661632ITERATION : 37, loss : 0.06774424025085378ITERATION : 38, loss : 0.06774608723405284ITERATION : 39, loss : 0.06774755287003921ITERATION : 40, loss : 0.06774871586419326ITERATION : 41, loss : 0.06774963864965772ITERATION : 42, loss : 0.06775037084220971ITERATION : 43, loss : 0.06775095177727483ITERATION : 44, loss : 0.06775141267116548ITERATION : 45, loss : 0.06775177833270309ITERATION : 46, loss : 0.06775206843207943ITERATION : 47, loss : 0.06775229861968501ITERATION : 48, loss : 0.06775248119470863ITERATION : 49, loss : 0.06775262601740363ITERATION : 50, loss : 0.06775274093922079ITERATION : 51, loss : 0.067752832120036ITERATION : 52, loss : 0.06775290444394044ITERATION : 53, loss : 0.06775296177470638ITERATION : 54, loss : 0.06775300728190639ITERATION : 55, loss : 0.0677530433464273ITERATION : 56, loss : 0.0677530719892555ITERATION : 57, loss : 0.06775309467998569ITERATION : 58, loss : 0.06775311270360297ITERATION : 59, loss : 0.06775312698622034ITERATION : 60, loss : 0.06775313832597628ITERATION : 61, loss : 0.06775314730469022ITERATION : 62, loss : 0.06775315441530903ITERATION : 63, loss : 0.06775316005776993ITERATION : 64, loss : 0.0677531645416048ITERATION : 65, loss : 0.06775316812418039ITERATION : 66, loss : 0.06775317093363027ITERATION : 67, loss : 0.06775317316335536ITERATION : 68, loss : 0.0677531749433968ITERATION : 69, loss : 0.06775317635077295ITERATION : 70, loss : 0.06775317744997018ITERATION : 71, loss : 0.06775317832716593ITERATION : 72, loss : 0.0677531790294408ITERATION : 73, loss : 0.06775317959110402ITERATION : 74, loss : 0.06775317997799093ITERATION : 75, loss : 0.0677531803129527ITERATION : 76, loss : 0.06775318060545715ITERATION : 77, loss : 0.06775318083343071ITERATION : 78, loss : 0.0677531809653093ITERATION : 79, loss : 0.06775318110360447ITERATION : 80, loss : 0.06775318121301532ITERATION : 81, loss : 0.06775318130307399ITERATION : 82, loss : 0.06775318136201607ITERATION : 83, loss : 0.06775318139243586ITERATION : 84, loss : 0.06775318139301074ITERATION : 85, loss : 0.06775318139301074ITERATION : 86, loss : 0.06775318139301074ITERATION : 87, loss : 0.06775318139301074ITERATION : 88, loss : 0.06775318139301074ITERATION : 89, loss : 0.06775318139301074ITERATION : 90, loss : 0.06775318139301074ITERATION : 91, loss : 0.06775318139301074ITERATION : 92, loss : 0.06775318139301074ITERATION : 93, loss : 0.06775318139301074ITERATION : 94, loss : 0.06775318139301074ITERATION : 95, loss : 0.06775318139301074ITERATION : 96, loss : 0.06775318139301074ITERATION : 97, loss : 0.06775318139301074ITERATION : 98, loss : 0.06775318139301074ITERATION : 99, loss : 0.06775318139301074ITERATION : 100, loss : 0.06775318139301074
ITERATION : 1, loss : 0.09262045885952185ITERATION : 2, loss : 0.12881635401817426ITERATION : 3, loss : 0.15373825073480618ITERATION : 4, loss : 0.16944897097912878ITERATION : 5, loss : 0.17932374020845523ITERATION : 6, loss : 0.18559476172990283ITERATION : 7, loss : 0.18962995020875192ITERATION : 8, loss : 0.19226134685733773ITERATION : 9, loss : 0.19399966930513293ITERATION : 10, loss : 0.19516248676420803ITERATION : 11, loss : 0.19594986077103987ITERATION : 12, loss : 0.19648938095217394ITERATION : 13, loss : 0.1968633602429848ITERATION : 14, loss : 0.19712549170189023ITERATION : 15, loss : 0.19731118289177635ITERATION : 16, loss : 0.1974440385453083ITERATION : 17, loss : 0.19753996902518672ITERATION : 18, loss : 0.1976098180974663ITERATION : 19, loss : 0.19766105900964753ITERATION : 20, loss : 0.19769889859806464ITERATION : 21, loss : 0.19772700366816423ITERATION : 22, loss : 0.19774798298281046ITERATION : 23, loss : 0.1977637100129613ITERATION : 24, loss : 0.19777554244908957ITERATION : 25, loss : 0.19778447191817564ITERATION : 26, loss : 0.19779122778011834ITERATION : 27, loss : 0.197796349897042ITERATION : 28, loss : 0.1978002403189768ITERATION : 29, loss : 0.19780319942400681ITERATION : 30, loss : 0.19780545286273107ITERATION : 31, loss : 0.19780717064287384ITERATION : 32, loss : 0.197808481125468ITERATION : 33, loss : 0.1978094815050338ITERATION : 34, loss : 0.1978102456419088ITERATION : 35, loss : 0.1978108295475118ITERATION : 36, loss : 0.1978112759233468ITERATION : 37, loss : 0.19781161722317245ITERATION : 38, loss : 0.19781187828077187ITERATION : 39, loss : 0.1978120779912457ITERATION : 40, loss : 0.19781223079951482ITERATION : 41, loss : 0.1978123477042109ITERATION : 42, loss : 0.19781243714065302ITERATION : 43, loss : 0.1978125055888301ITERATION : 44, loss : 0.19781255798052202ITERATION : 45, loss : 0.19781259809742527ITERATION : 46, loss : 0.197812628784954ITERATION : 47, loss : 0.19781265229650333ITERATION : 48, loss : 0.19781267027040567ITERATION : 49, loss : 0.1978126840434552ITERATION : 50, loss : 0.19781269457661352ITERATION : 51, loss : 0.19781270261884104ITERATION : 52, loss : 0.19781270879001525ITERATION : 53, loss : 0.19781271352297797ITERATION : 54, loss : 0.19781271713301252ITERATION : 55, loss : 0.19781271989005222ITERATION : 56, loss : 0.19781272201350264ITERATION : 57, loss : 0.19781272365361685ITERATION : 58, loss : 0.19781272490977247ITERATION : 59, loss : 0.19781272586616414ITERATION : 60, loss : 0.19781272656289758ITERATION : 61, loss : 0.1978127271395218ITERATION : 62, loss : 0.19781272758751606ITERATION : 63, loss : 0.1978127278977948ITERATION : 64, loss : 0.19781272812456796ITERATION : 65, loss : 0.19781272832048138ITERATION : 66, loss : 0.1978127284812786ITERATION : 67, loss : 0.19781272856456478ITERATION : 68, loss : 0.19781272865434754ITERATION : 69, loss : 0.1978127287211241ITERATION : 70, loss : 0.1978127287630653ITERATION : 71, loss : 0.1978127287630653ITERATION : 72, loss : 0.1978127287630653ITERATION : 73, loss : 0.1978127287630653ITERATION : 74, loss : 0.1978127287630653ITERATION : 75, loss : 0.1978127287630653ITERATION : 76, loss : 0.1978127287630653ITERATION : 77, loss : 0.1978127287630653ITERATION : 78, loss : 0.1978127287630653ITERATION : 79, loss : 0.1978127287630653ITERATION : 80, loss : 0.1978127287630653ITERATION : 81, loss : 0.1978127287630653ITERATION : 82, loss : 0.1978127287630653ITERATION : 83, loss : 0.1978127287630653ITERATION : 84, loss : 0.1978127287630653ITERATION : 85, loss : 0.1978127287630653ITERATION : 86, loss : 0.1978127287630653ITERATION : 87, loss : 0.1978127287630653ITERATION : 88, loss : 0.1978127287630653ITERATION : 89, loss : 0.1978127287630653ITERATION : 90, loss : 0.1978127287630653ITERATION : 91, loss : 0.1978127287630653ITERATION : 92, loss : 0.1978127287630653ITERATION : 93, loss : 0.1978127287630653ITERATION : 94, loss : 0.1978127287630653ITERATION : 95, loss : 0.1978127287630653ITERATION : 96, loss : 0.1978127287630653ITERATION : 97, loss : 0.1978127287630653ITERATION : 98, loss : 0.1978127287630653ITERATION : 99, loss : 0.1978127287630653ITERATION : 100, loss : 0.1978127287630653
gradient norm in None layer : 0.0012773852496284581
gradient norm in None layer : 4.3135381713714646e-05
gradient norm in None layer : 3.0895234222793594e-05
gradient norm in None layer : 0.000676150661449026
gradient norm in None layer : 5.209574110792899e-05
gradient norm in None layer : 5.072258365903819e-05
gradient norm in None layer : 0.00035234099958395075
gradient norm in None layer : 1.3032690160996485e-05
gradient norm in None layer : 1.70722226049328e-05
gradient norm in None layer : 0.00028942872787162126
gradient norm in None layer : 1.2994442036865874e-05
gradient norm in None layer : 1.2078096916131435e-05
gradient norm in None layer : 9.128886598801976e-05
gradient norm in None layer : 2.8796836041287274e-06
gradient norm in None layer : 2.5780066474592267e-06
gradient norm in None layer : 9.114204813369686e-05
gradient norm in None layer : 3.3554914307228473e-06
gradient norm in None layer : 3.1669330078775487e-06
gradient norm in None layer : 0.00012835492684366829
gradient norm in None layer : 2.62452515267055e-06
gradient norm in None layer : 0.0002694938567550018
gradient norm in None layer : 1.576179824323833e-05
gradient norm in None layer : 1.3639878578256814e-05
gradient norm in None layer : 0.00030199342779511354
gradient norm in None layer : 2.9112684613812413e-05
gradient norm in None layer : 3.4334214251201944e-05
gradient norm in None layer : 0.000588273966327509
gradient norm in None layer : 6.746621671726383e-06
gradient norm in None layer : 0.0007481574054267296
gradient norm in None layer : 6.229573993784576e-05
gradient norm in None layer : 6.183231435455369e-05
gradient norm in None layer : 0.0008716218325952204
gradient norm in None layer : 0.00011358616339238593
gradient norm in None layer : 0.00017052274952037815
gradient norm in None layer : 8.573369641846853e-05
gradient norm in None layer : 2.588810016050116e-05
Total gradient norm: 0.002056309942596191
invariance loss : 4.507736158584265, avg_den : 0.45087432861328125, density loss : 0.3502713533622658, mse loss : 0.06910557942663614, solver time : 121.34117078781128 sec , total loss : 0.07396358693858268, running loss : 0.10559894924796893
Epoch 0/10 , batch 20/12500 
ITERATION : 1, loss : 0.014469261255644612ITERATION : 2, loss : 0.021972011042957824ITERATION : 3, loss : 0.028840166621219045ITERATION : 4, loss : 0.033774154918957626ITERATION : 5, loss : 0.037466637715208334ITERATION : 6, loss : 0.0401056377180887ITERATION : 7, loss : 0.04195915068684023ITERATION : 8, loss : 0.043252081495836904ITERATION : 9, loss : 0.04415180520092142ITERATION : 10, loss : 0.04476788739805473ITERATION : 11, loss : 0.04519605342619228ITERATION : 12, loss : 0.045494415111816955ITERATION : 13, loss : 0.0457026148977153ITERATION : 14, loss : 0.04584811947300262ITERATION : 15, loss : 0.04594996870798708ITERATION : 16, loss : 0.04602137506024203ITERATION : 17, loss : 0.04607151878387254ITERATION : 18, loss : 0.046106788090718165ITERATION : 19, loss : 0.046131635026484555ITERATION : 20, loss : 0.04614916736930534ITERATION : 21, loss : 0.04616155777500597ITERATION : 22, loss : 0.046170327873313086ITERATION : 23, loss : 0.04617654477679094ITERATION : 24, loss : 0.0461809584994481ITERATION : 25, loss : 0.04618409639943427ITERATION : 26, loss : 0.04618633049164204ITERATION : 27, loss : 0.04618792335302015ITERATION : 28, loss : 0.046189060500123925ITERATION : 29, loss : 0.046189873448179056ITERATION : 30, loss : 0.046190455207835414ITERATION : 31, loss : 0.046190872040410155ITERATION : 32, loss : 0.046191171098446976ITERATION : 33, loss : 0.04619138589195651ITERATION : 34, loss : 0.04619154029445906ITERATION : 35, loss : 0.04619165143787032ITERATION : 36, loss : 0.04619173150231127ITERATION : 37, loss : 0.04619178922950521ITERATION : 38, loss : 0.046191830888669995ITERATION : 39, loss : 0.04619186097357551ITERATION : 40, loss : 0.04619188273352922ITERATION : 41, loss : 0.04619189847984087ITERATION : 42, loss : 0.04619190986727983ITERATION : 43, loss : 0.04619191813002767ITERATION : 44, loss : 0.04619192411123437ITERATION : 45, loss : 0.046191928437462136ITERATION : 46, loss : 0.04619193157747342ITERATION : 47, loss : 0.04619193385412343ITERATION : 48, loss : 0.04619193559058306ITERATION : 49, loss : 0.046191936764627295ITERATION : 50, loss : 0.046191937586775644ITERATION : 51, loss : 0.0461919382079186ITERATION : 52, loss : 0.046191938663944214ITERATION : 53, loss : 0.04619193896385121ITERATION : 54, loss : 0.04619193921671681ITERATION : 55, loss : 0.046191939403760596ITERATION : 56, loss : 0.04619193952727882ITERATION : 57, loss : 0.046191939618749414ITERATION : 58, loss : 0.046191939661715704ITERATION : 59, loss : 0.04619193969088295ITERATION : 60, loss : 0.04619193969088295ITERATION : 61, loss : 0.04619193969088295ITERATION : 62, loss : 0.04619193969088295ITERATION : 63, loss : 0.04619193969088295ITERATION : 64, loss : 0.04619193969088295ITERATION : 65, loss : 0.04619193969088295ITERATION : 66, loss : 0.04619193969088295ITERATION : 67, loss : 0.04619193969088295ITERATION : 68, loss : 0.04619193969088295ITERATION : 69, loss : 0.04619193969088295ITERATION : 70, loss : 0.04619193969088295ITERATION : 71, loss : 0.04619193969088295ITERATION : 72, loss : 0.04619193969088295ITERATION : 73, loss : 0.04619193969088295ITERATION : 74, loss : 0.04619193969088295ITERATION : 75, loss : 0.04619193969088295ITERATION : 76, loss : 0.04619193969088295ITERATION : 77, loss : 0.04619193969088295ITERATION : 78, loss : 0.04619193969088295ITERATION : 79, loss : 0.04619193969088295ITERATION : 80, loss : 0.04619193969088295ITERATION : 81, loss : 0.04619193969088295ITERATION : 82, loss : 0.04619193969088295ITERATION : 83, loss : 0.04619193969088295ITERATION : 84, loss : 0.04619193969088295ITERATION : 85, loss : 0.04619193969088295ITERATION : 86, loss : 0.04619193969088295ITERATION : 87, loss : 0.04619193969088295ITERATION : 88, loss : 0.04619193969088295ITERATION : 89, loss : 0.04619193969088295ITERATION : 90, loss : 0.04619193969088295ITERATION : 91, loss : 0.04619193969088295ITERATION : 92, loss : 0.04619193969088295ITERATION : 93, loss : 0.04619193969088295ITERATION : 94, loss : 0.04619193969088295ITERATION : 95, loss : 0.04619193969088295ITERATION : 96, loss : 0.04619193969088295ITERATION : 97, loss : 0.04619193969088295ITERATION : 98, loss : 0.04619193969088295ITERATION : 99, loss : 0.04619193969088295ITERATION : 100, loss : 0.04619193969088295
ITERATION : 1, loss : 0.043697848826470286ITERATION : 2, loss : 0.04791469607670367ITERATION : 3, loss : 0.04905494397669129ITERATION : 4, loss : 0.049451315294588565ITERATION : 5, loss : 0.04965392655991275ITERATION : 6, loss : 0.049798795158663804ITERATION : 7, loss : 0.04991486928713239ITERATION : 8, loss : 0.050008378475224026ITERATION : 9, loss : 0.0500823387410464ITERATION : 10, loss : 0.050139813555672146ITERATION : 11, loss : 0.050183897251908145ITERATION : 12, loss : 0.05021739483194799ITERATION : 13, loss : 0.05024267101980888ITERATION : 14, loss : 0.05026163724198097ITERATION : 15, loss : 0.05027580108563873ITERATION : 16, loss : 0.05028633392273791ITERATION : 17, loss : 0.050294136672492336ITERATION : 18, loss : 0.050299896746634154ITERATION : 19, loss : 0.05030413547017438ITERATION : 20, loss : 0.05030724548110247ITERATION : 21, loss : 0.05030952134892292ITERATION : 22, loss : 0.0503111829439937ITERATION : 23, loss : 0.05031239342422036ITERATION : 24, loss : 0.050313273584182784ITERATION : 25, loss : 0.05031391243614633ITERATION : 26, loss : 0.05031437543923193ITERATION : 27, loss : 0.05031471055411767ITERATION : 28, loss : 0.05031495279317129ITERATION : 29, loss : 0.050315127703216625ITERATION : 30, loss : 0.05031525387669779ITERATION : 31, loss : 0.05031534481974352ITERATION : 32, loss : 0.05031541029529816ITERATION : 33, loss : 0.050315457432891855ITERATION : 34, loss : 0.0503154913272703ITERATION : 35, loss : 0.05031551570808035ITERATION : 36, loss : 0.05031553320591388ITERATION : 37, loss : 0.050315545787535144ITERATION : 38, loss : 0.05031555479976935ITERATION : 39, loss : 0.05031556129476061ITERATION : 40, loss : 0.05031556592422114ITERATION : 41, loss : 0.05031556923025416ITERATION : 42, loss : 0.050315571631604966ITERATION : 43, loss : 0.05031557335435449ITERATION : 44, loss : 0.050315574627842446ITERATION : 45, loss : 0.05031557550210562ITERATION : 46, loss : 0.05031557610598864ITERATION : 47, loss : 0.0503155765527822ITERATION : 48, loss : 0.05031557688551214ITERATION : 49, loss : 0.05031557712920578ITERATION : 50, loss : 0.05031557726665228ITERATION : 51, loss : 0.05031557738282155ITERATION : 52, loss : 0.05031557747973348ITERATION : 53, loss : 0.05031557753431366ITERATION : 54, loss : 0.05031557757237135ITERATION : 55, loss : 0.05031557762063696ITERATION : 56, loss : 0.05031557762372151ITERATION : 57, loss : 0.05031557762372151ITERATION : 58, loss : 0.05031557762372151ITERATION : 59, loss : 0.05031557762372151ITERATION : 60, loss : 0.05031557762372151ITERATION : 61, loss : 0.05031557762372151ITERATION : 62, loss : 0.05031557762372151ITERATION : 63, loss : 0.05031557762372151ITERATION : 64, loss : 0.05031557762372151ITERATION : 65, loss : 0.05031557762372151ITERATION : 66, loss : 0.05031557762372151ITERATION : 67, loss : 0.05031557762372151ITERATION : 68, loss : 0.05031557762372151ITERATION : 69, loss : 0.05031557762372151ITERATION : 70, loss : 0.05031557762372151ITERATION : 71, loss : 0.05031557762372151ITERATION : 72, loss : 0.05031557762372151ITERATION : 73, loss : 0.05031557762372151ITERATION : 74, loss : 0.05031557762372151ITERATION : 75, loss : 0.05031557762372151ITERATION : 76, loss : 0.05031557762372151ITERATION : 77, loss : 0.05031557762372151ITERATION : 78, loss : 0.05031557762372151ITERATION : 79, loss : 0.05031557762372151ITERATION : 80, loss : 0.05031557762372151ITERATION : 81, loss : 0.05031557762372151ITERATION : 82, loss : 0.05031557762372151ITERATION : 83, loss : 0.05031557762372151ITERATION : 84, loss : 0.05031557762372151ITERATION : 85, loss : 0.05031557762372151ITERATION : 86, loss : 0.05031557762372151ITERATION : 87, loss : 0.05031557762372151ITERATION : 88, loss : 0.05031557762372151ITERATION : 89, loss : 0.05031557762372151ITERATION : 90, loss : 0.05031557762372151ITERATION : 91, loss : 0.05031557762372151ITERATION : 92, loss : 0.05031557762372151ITERATION : 93, loss : 0.05031557762372151ITERATION : 94, loss : 0.05031557762372151ITERATION : 95, loss : 0.05031557762372151ITERATION : 96, loss : 0.05031557762372151ITERATION : 97, loss : 0.05031557762372151ITERATION : 98, loss : 0.05031557762372151ITERATION : 99, loss : 0.05031557762372151ITERATION : 100, loss : 0.05031557762372151
ITERATION : 1, loss : 0.03145885487353243ITERATION : 2, loss : 0.03386964284418022ITERATION : 3, loss : 0.03699679311021808ITERATION : 4, loss : 0.03964325878830744ITERATION : 5, loss : 0.0417712160907982ITERATION : 6, loss : 0.043411576398183015ITERATION : 7, loss : 0.04464821468116129ITERATION : 8, loss : 0.045568606213136534ITERATION : 9, loss : 0.046248232156627274ITERATION : 10, loss : 0.04674750664841422ITERATION : 11, loss : 0.04711301780053133ITERATION : 12, loss : 0.04737995173006383ITERATION : 13, loss : 0.04757455098382207ITERATION : 14, loss : 0.047716230429269514ITERATION : 15, loss : 0.04781927695236559ITERATION : 16, loss : 0.047894165068542736ITERATION : 17, loss : 0.04794855388346366ITERATION : 18, loss : 0.04798803321939012ITERATION : 19, loss : 0.04801667685901668ITERATION : 20, loss : 0.04803745025977886ITERATION : 21, loss : 0.048052510381049435ITERATION : 22, loss : 0.048063424837837286ITERATION : 23, loss : 0.04807133231886643ITERATION : 24, loss : 0.048077059698697994ITERATION : 25, loss : 0.04808120684527212ITERATION : 26, loss : 0.04808420899212186ITERATION : 27, loss : 0.04808638165841435ITERATION : 28, loss : 0.04808795369854831ITERATION : 29, loss : 0.048089090853680995ITERATION : 30, loss : 0.048089913266589034ITERATION : 31, loss : 0.04809050789884834ITERATION : 32, loss : 0.04809093778271517ITERATION : 33, loss : 0.04809124845249785ITERATION : 34, loss : 0.04809147292252728ITERATION : 35, loss : 0.04809163507133363ITERATION : 36, loss : 0.0480917521894786ITERATION : 37, loss : 0.04809183676103282ITERATION : 38, loss : 0.04809189781202996ITERATION : 39, loss : 0.04809194188203571ITERATION : 40, loss : 0.048091973684109966ITERATION : 41, loss : 0.04809199664910935ITERATION : 42, loss : 0.04809201320992476ITERATION : 43, loss : 0.04809202515689311ITERATION : 44, loss : 0.048092033779867184ITERATION : 45, loss : 0.048092039986239475ITERATION : 46, loss : 0.04809204446323772ITERATION : 47, loss : 0.048092047689442266ITERATION : 48, loss : 0.048092049996159264ITERATION : 49, loss : 0.04809205167131259ITERATION : 50, loss : 0.048092052866006485ITERATION : 51, loss : 0.0480920537374708ITERATION : 52, loss : 0.04809205433106182ITERATION : 53, loss : 0.04809205482082308ITERATION : 54, loss : 0.048092055098377225ITERATION : 55, loss : 0.04809205531717102ITERATION : 56, loss : 0.04809205546131704ITERATION : 57, loss : 0.04809205559251708ITERATION : 58, loss : 0.04809205566722114ITERATION : 59, loss : 0.04809205566722114ITERATION : 60, loss : 0.04809205566722114ITERATION : 61, loss : 0.04809205566722114ITERATION : 62, loss : 0.04809205566722114ITERATION : 63, loss : 0.04809205566722114ITERATION : 64, loss : 0.04809205566722114ITERATION : 65, loss : 0.04809205566722114ITERATION : 66, loss : 0.04809205566722114ITERATION : 67, loss : 0.04809205566722114ITERATION : 68, loss : 0.04809205566722114ITERATION : 69, loss : 0.04809205566722114ITERATION : 70, loss : 0.04809205566722114ITERATION : 71, loss : 0.04809205566722114ITERATION : 72, loss : 0.04809205566722114ITERATION : 73, loss : 0.04809205566722114ITERATION : 74, loss : 0.04809205566722114ITERATION : 75, loss : 0.04809205566722114ITERATION : 76, loss : 0.04809205566722114ITERATION : 77, loss : 0.04809205566722114ITERATION : 78, loss : 0.04809205566722114ITERATION : 79, loss : 0.04809205566722114ITERATION : 80, loss : 0.04809205566722114ITERATION : 81, loss : 0.04809205566722114ITERATION : 82, loss : 0.04809205566722114ITERATION : 83, loss : 0.04809205566722114ITERATION : 84, loss : 0.04809205566722114ITERATION : 85, loss : 0.04809205566722114ITERATION : 86, loss : 0.04809205566722114ITERATION : 87, loss : 0.04809205566722114ITERATION : 88, loss : 0.04809205566722114ITERATION : 89, loss : 0.04809205566722114ITERATION : 90, loss : 0.04809205566722114ITERATION : 91, loss : 0.04809205566722114ITERATION : 92, loss : 0.04809205566722114ITERATION : 93, loss : 0.04809205566722114ITERATION : 94, loss : 0.04809205566722114ITERATION : 95, loss : 0.04809205566722114ITERATION : 96, loss : 0.04809205566722114ITERATION : 97, loss : 0.04809205566722114ITERATION : 98, loss : 0.04809205566722114ITERATION : 99, loss : 0.04809205566722114ITERATION : 100, loss : 0.04809205566722114
ITERATION : 1, loss : 0.03638172563686968ITERATION : 2, loss : 0.0597001961288857ITERATION : 3, loss : 0.07754775388629748ITERATION : 4, loss : 0.09033342582954161ITERATION : 5, loss : 0.09894833498438299ITERATION : 6, loss : 0.10487182870732334ITERATION : 7, loss : 0.10901624921191262ITERATION : 8, loss : 0.11195521815768417ITERATION : 9, loss : 0.11406041270187484ITERATION : 10, loss : 0.11557957397898412ITERATION : 11, loss : 0.11668178711829426ITERATION : 12, loss : 0.11748464440629051ITERATION : 13, loss : 0.11807112380049004ITERATION : 14, loss : 0.11850042843428596ITERATION : 15, loss : 0.11881514941581624ITERATION : 16, loss : 0.11904611651363209ITERATION : 17, loss : 0.11921574763623687ITERATION : 18, loss : 0.11934039843979244ITERATION : 19, loss : 0.11943203057268297ITERATION : 20, loss : 0.11949940775531516ITERATION : 21, loss : 0.11954895883868978ITERATION : 22, loss : 0.11958540413674795ITERATION : 23, loss : 0.11961221182760678ITERATION : 24, loss : 0.11963193119486683ITERATION : 25, loss : 0.11964643669665005ITERATION : 26, loss : 0.11965710686400695ITERATION : 27, loss : 0.11966495571735937ITERATION : 28, loss : 0.11967072906574663ITERATION : 29, loss : 0.11967497567510939ITERATION : 30, loss : 0.11967809921645733ITERATION : 31, loss : 0.11968039659319787ITERATION : 32, loss : 0.11968208630865425ITERATION : 33, loss : 0.11968332902472914ITERATION : 34, loss : 0.11968424296085355ITERATION : 35, loss : 0.1196849150826881ITERATION : 36, loss : 0.11968540935739685ITERATION : 37, loss : 0.11968577283231382ITERATION : 38, loss : 0.1196860401166682ITERATION : 39, loss : 0.11968623666210024ITERATION : 40, loss : 0.11968638118214033ITERATION : 41, loss : 0.11968648745076356ITERATION : 42, loss : 0.11968656559193004ITERATION : 43, loss : 0.11968662305233424ITERATION : 44, loss : 0.11968666525782909ITERATION : 45, loss : 0.11968669629465ITERATION : 46, loss : 0.1196867191744119ITERATION : 47, loss : 0.11968673597500655ITERATION : 48, loss : 0.11968674835592191ITERATION : 49, loss : 0.11968675743408033ITERATION : 50, loss : 0.11968676413588047ITERATION : 51, loss : 0.11968676908786156ITERATION : 52, loss : 0.11968677266467326ITERATION : 53, loss : 0.11968677529620225ITERATION : 54, loss : 0.11968677718111327ITERATION : 55, loss : 0.11968677860523463ITERATION : 56, loss : 0.1196867796428564ITERATION : 57, loss : 0.11968678041938033ITERATION : 58, loss : 0.11968678096507679ITERATION : 59, loss : 0.11968678137783612ITERATION : 60, loss : 0.1196867816772642ITERATION : 61, loss : 0.11968678186401688ITERATION : 62, loss : 0.11968678202191506ITERATION : 63, loss : 0.11968678214761841ITERATION : 64, loss : 0.11968678224125168ITERATION : 65, loss : 0.11968678231976705ITERATION : 66, loss : 0.11968678233919187ITERATION : 67, loss : 0.11968678233953159ITERATION : 68, loss : 0.11968678233953159ITERATION : 69, loss : 0.11968678233953159ITERATION : 70, loss : 0.11968678233953159ITERATION : 71, loss : 0.11968678233953159ITERATION : 72, loss : 0.11968678233953159ITERATION : 73, loss : 0.11968678233953159ITERATION : 74, loss : 0.11968678233953159ITERATION : 75, loss : 0.11968678233953159ITERATION : 76, loss : 0.11968678233953159ITERATION : 77, loss : 0.11968678233953159ITERATION : 78, loss : 0.11968678233953159ITERATION : 79, loss : 0.11968678233953159ITERATION : 80, loss : 0.11968678233953159ITERATION : 81, loss : 0.11968678233953159ITERATION : 82, loss : 0.11968678233953159ITERATION : 83, loss : 0.11968678233953159ITERATION : 84, loss : 0.11968678233953159ITERATION : 85, loss : 0.11968678233953159ITERATION : 86, loss : 0.11968678233953159ITERATION : 87, loss : 0.11968678233953159ITERATION : 88, loss : 0.11968678233953159ITERATION : 89, loss : 0.11968678233953159ITERATION : 90, loss : 0.11968678233953159ITERATION : 91, loss : 0.11968678233953159ITERATION : 92, loss : 0.11968678233953159ITERATION : 93, loss : 0.11968678233953159ITERATION : 94, loss : 0.11968678233953159ITERATION : 95, loss : 0.11968678233953159ITERATION : 96, loss : 0.11968678233953159ITERATION : 97, loss : 0.11968678233953159ITERATION : 98, loss : 0.11968678233953159ITERATION : 99, loss : 0.11968678233953159ITERATION : 100, loss : 0.11968678233953159
ITERATION : 1, loss : 0.08707508802826597ITERATION : 2, loss : 0.11074683489915577ITERATION : 3, loss : 0.12440461527570386ITERATION : 4, loss : 0.13269194174569954ITERATION : 5, loss : 0.1385707775590762ITERATION : 6, loss : 0.1428324991401014ITERATION : 7, loss : 0.14596206875664566ITERATION : 8, loss : 0.14828000397595445ITERATION : 9, loss : 0.15000695895585023ITERATION : 10, loss : 0.15129890407689467ITERATION : 11, loss : 0.15226816560004724ITERATION : 12, loss : 0.1529967460977183ITERATION : 13, loss : 0.15354511098999052ITERATION : 14, loss : 0.1539581703984036ITERATION : 15, loss : 0.1542694563453063ITERATION : 16, loss : 0.15450409834294698ITERATION : 17, loss : 0.1546809781581654ITERATION : 18, loss : 0.15481430787004558ITERATION : 19, loss : 0.1549147967254312ITERATION : 20, loss : 0.1549905199118717ITERATION : 21, loss : 0.15504756889590265ITERATION : 22, loss : 0.155090539134964ITERATION : 23, loss : 0.15512289755540587ITERATION : 24, loss : 0.1551472591358949ITERATION : 25, loss : 0.15516559607975133ITERATION : 26, loss : 0.1551793952932701ITERATION : 27, loss : 0.15518977761030242ITERATION : 28, loss : 0.15519758762867988ITERATION : 29, loss : 0.15520346161705378ITERATION : 30, loss : 0.15520787875209005ITERATION : 31, loss : 0.15521119986175783ITERATION : 32, loss : 0.15521369656397543ITERATION : 33, loss : 0.15521557326985663ITERATION : 34, loss : 0.1552169837387977ITERATION : 35, loss : 0.1552180437484801ITERATION : 36, loss : 0.1552188402615439ITERATION : 37, loss : 0.1552194387420162ITERATION : 38, loss : 0.15521988836590223ITERATION : 39, loss : 0.15522022614624698ITERATION : 40, loss : 0.15522047988170198ITERATION : 41, loss : 0.1552206704849138ITERATION : 42, loss : 0.15522081363346665ITERATION : 43, loss : 0.15522092116730596ITERATION : 44, loss : 0.1552210019020222ITERATION : 45, loss : 0.15522106255840476ITERATION : 46, loss : 0.15522110814091705ITERATION : 47, loss : 0.15522114237241944ITERATION : 48, loss : 0.15522116808662814ITERATION : 49, loss : 0.15522118737923146ITERATION : 50, loss : 0.155221201861834ITERATION : 51, loss : 0.15522121271503975ITERATION : 52, loss : 0.15522122088231602ITERATION : 53, loss : 0.1552212270209061ITERATION : 54, loss : 0.15522123159934245ITERATION : 55, loss : 0.15522123499796875ITERATION : 56, loss : 0.15522123757482478ITERATION : 57, loss : 0.15522123950925626ITERATION : 58, loss : 0.15522124095611006ITERATION : 59, loss : 0.15522124204847806ITERATION : 60, loss : 0.15522124286127323ITERATION : 61, loss : 0.15522124344040192ITERATION : 62, loss : 0.1552212438662551ITERATION : 63, loss : 0.15522124423275688ITERATION : 64, loss : 0.15522124448827024ITERATION : 65, loss : 0.15522124470082468ITERATION : 66, loss : 0.15522124485239666ITERATION : 67, loss : 0.155221244942208ITERATION : 68, loss : 0.15522124504364498ITERATION : 69, loss : 0.15522124507621413ITERATION : 70, loss : 0.15522124507726096ITERATION : 71, loss : 0.15522124507726096ITERATION : 72, loss : 0.15522124507726096ITERATION : 73, loss : 0.15522124507726096ITERATION : 74, loss : 0.15522124507726096ITERATION : 75, loss : 0.15522124507726096ITERATION : 76, loss : 0.15522124507726096ITERATION : 77, loss : 0.15522124507726096ITERATION : 78, loss : 0.15522124507726096ITERATION : 79, loss : 0.15522124507726096ITERATION : 80, loss : 0.15522124507726096ITERATION : 81, loss : 0.15522124507726096ITERATION : 82, loss : 0.15522124507726096ITERATION : 83, loss : 0.15522124507726096ITERATION : 84, loss : 0.15522124507726096ITERATION : 85, loss : 0.15522124507726096ITERATION : 86, loss : 0.15522124507726096ITERATION : 87, loss : 0.15522124507726096ITERATION : 88, loss : 0.15522124507726096ITERATION : 89, loss : 0.15522124507726096ITERATION : 90, loss : 0.15522124507726096ITERATION : 91, loss : 0.15522124507726096ITERATION : 92, loss : 0.15522124507726096ITERATION : 93, loss : 0.15522124507726096ITERATION : 94, loss : 0.15522124507726096ITERATION : 95, loss : 0.15522124507726096ITERATION : 96, loss : 0.15522124507726096ITERATION : 97, loss : 0.15522124507726096ITERATION : 98, loss : 0.15522124507726096ITERATION : 99, loss : 0.15522124507726096ITERATION : 100, loss : 0.15522124507726096
ITERATION : 1, loss : 0.015478029911042324ITERATION : 2, loss : 0.01903886580860213ITERATION : 3, loss : 0.02377370762742278ITERATION : 4, loss : 0.028837504007572533ITERATION : 5, loss : 0.0332379049321051ITERATION : 6, loss : 0.036783300579228795ITERATION : 7, loss : 0.03954760369945794ITERATION : 8, loss : 0.04166862821106674ITERATION : 9, loss : 0.04328255502650468ITERATION : 10, loss : 0.04450510716014452ITERATION : 11, loss : 0.04542890996188017ITERATION : 12, loss : 0.04612603236139051ITERATION : 13, loss : 0.04665173238822383ITERATION : 14, loss : 0.04704804019354482ITERATION : 15, loss : 0.04734678057320991ITERATION : 16, loss : 0.04757198836433901ITERATION : 17, loss : 0.04774178856079044ITERATION : 18, loss : 0.04786983930809228ITERATION : 19, loss : 0.04796642803797016ITERATION : 20, loss : 0.04803930312009681ITERATION : 21, loss : 0.04809430056741783ITERATION : 22, loss : 0.04813581660317646ITERATION : 23, loss : 0.0481671636817806ITERATION : 24, loss : 0.04819083826406599ITERATION : 25, loss : 0.04820872239361861ITERATION : 26, loss : 0.04822223526185847ITERATION : 27, loss : 0.04823244744211101ITERATION : 28, loss : 0.04824016680805598ITERATION : 29, loss : 0.048246002757635215ITERATION : 30, loss : 0.048250415704884454ITERATION : 31, loss : 0.048253753111345325ITERATION : 32, loss : 0.04825627746985857ITERATION : 33, loss : 0.04825818716374912ITERATION : 34, loss : 0.048259631991383764ITERATION : 35, loss : 0.048260725307148644ITERATION : 36, loss : 0.04826155272899436ITERATION : 37, loss : 0.048262179008119806ITERATION : 38, loss : 0.04826265306943825ITERATION : 39, loss : 0.048263011944588506ITERATION : 40, loss : 0.048263283657709925ITERATION : 41, loss : 0.04826348938306908ITERATION : 42, loss : 0.048263645159122646ITERATION : 43, loss : 0.04826376311981987ITERATION : 44, loss : 0.04826385245984755ITERATION : 45, loss : 0.04826392011449841ITERATION : 46, loss : 0.04826397134621898ITERATION : 47, loss : 0.04826401014723279ITERATION : 48, loss : 0.04826403954966796ITERATION : 49, loss : 0.04826406181590141ITERATION : 50, loss : 0.04826407868829828ITERATION : 51, loss : 0.04826409146222366ITERATION : 52, loss : 0.04826410121105908ITERATION : 53, loss : 0.048264108598262995ITERATION : 54, loss : 0.04826411413745851ITERATION : 55, loss : 0.048264118322035546ITERATION : 56, loss : 0.04826412149149012ITERATION : 57, loss : 0.04826412389986013ITERATION : 58, loss : 0.048264125692296926ITERATION : 59, loss : 0.04826412707927127ITERATION : 60, loss : 0.04826412813422691ITERATION : 61, loss : 0.04826412892789692ITERATION : 62, loss : 0.04826412952952095ITERATION : 63, loss : 0.04826412996838652ITERATION : 64, loss : 0.04826413031711574ITERATION : 65, loss : 0.048264130573536075ITERATION : 66, loss : 0.04826413075121489ITERATION : 67, loss : 0.048264130889423607ITERATION : 68, loss : 0.0482641310002635ITERATION : 69, loss : 0.0482641310031677ITERATION : 70, loss : 0.0482641310031677ITERATION : 71, loss : 0.0482641310031677ITERATION : 72, loss : 0.0482641310031677ITERATION : 73, loss : 0.0482641310031677ITERATION : 74, loss : 0.0482641310031677ITERATION : 75, loss : 0.0482641310031677ITERATION : 76, loss : 0.0482641310031677ITERATION : 77, loss : 0.0482641310031677ITERATION : 78, loss : 0.0482641310031677ITERATION : 79, loss : 0.0482641310031677ITERATION : 80, loss : 0.0482641310031677ITERATION : 81, loss : 0.0482641310031677ITERATION : 82, loss : 0.0482641310031677ITERATION : 83, loss : 0.0482641310031677ITERATION : 84, loss : 0.0482641310031677ITERATION : 85, loss : 0.0482641310031677ITERATION : 86, loss : 0.0482641310031677ITERATION : 87, loss : 0.0482641310031677ITERATION : 88, loss : 0.0482641310031677ITERATION : 89, loss : 0.0482641310031677ITERATION : 90, loss : 0.0482641310031677ITERATION : 91, loss : 0.0482641310031677ITERATION : 92, loss : 0.0482641310031677ITERATION : 93, loss : 0.0482641310031677ITERATION : 94, loss : 0.0482641310031677ITERATION : 95, loss : 0.0482641310031677ITERATION : 96, loss : 0.0482641310031677ITERATION : 97, loss : 0.0482641310031677ITERATION : 98, loss : 0.0482641310031677ITERATION : 99, loss : 0.0482641310031677ITERATION : 100, loss : 0.0482641310031677
ITERATION : 1, loss : 0.06162491012660693ITERATION : 2, loss : 0.05215555051311933ITERATION : 3, loss : 0.05225510813099514ITERATION : 4, loss : 0.05402885050432417ITERATION : 5, loss : 0.055749345197128194ITERATION : 6, loss : 0.05705723625622505ITERATION : 7, loss : 0.05796233711132237ITERATION : 8, loss : 0.0585596528445371ITERATION : 9, loss : 0.058943490063483886ITERATION : 10, loss : 0.05918633739671811ITERATION : 11, loss : 0.059338579102622906ITERATION : 12, loss : 0.05943351215621912ITERATION : 13, loss : 0.0594925348464053ITERATION : 14, loss : 0.059529178127906895ITERATION : 15, loss : 0.05955191715195176ITERATION : 16, loss : 0.05956603079103647ITERATION : 17, loss : 0.0595747970270031ITERATION : 18, loss : 0.05958024744909501ITERATION : 19, loss : 0.05958364080164899ITERATION : 20, loss : 0.059585756936035905ITERATION : 21, loss : 0.05958707895854828ITERATION : 22, loss : 0.05958790667148949ITERATION : 23, loss : 0.05958842603691137ITERATION : 24, loss : 0.05958875274122731ITERATION : 25, loss : 0.05958895890784664ITERATION : 26, loss : 0.05958908936527748ITERATION : 27, loss : 0.059589172223446826ITERATION : 28, loss : 0.05958922498667799ITERATION : 29, loss : 0.05958925875411469ITERATION : 30, loss : 0.05958928043639235ITERATION : 31, loss : 0.05958929449884388ITERATION : 32, loss : 0.0595893034766568ITERATION : 33, loss : 0.059589309463783165ITERATION : 34, loss : 0.05958931342612742ITERATION : 35, loss : 0.059589316041770805ITERATION : 36, loss : 0.05958931772761795ITERATION : 37, loss : 0.0595893188064043ITERATION : 38, loss : 0.05958931950454069ITERATION : 39, loss : 0.059589319959604294ITERATION : 40, loss : 0.059589320267432085ITERATION : 41, loss : 0.0595893204809131ITERATION : 42, loss : 0.05958932060334131ITERATION : 43, loss : 0.059589320679820305ITERATION : 44, loss : 0.05958932076766681ITERATION : 45, loss : 0.05958932079901355ITERATION : 46, loss : 0.05958932082728848ITERATION : 47, loss : 0.05958932086595322ITERATION : 48, loss : 0.05958932086492466ITERATION : 49, loss : 0.05958932086492466ITERATION : 50, loss : 0.05958932086492466ITERATION : 51, loss : 0.05958932086492466ITERATION : 52, loss : 0.05958932086492466ITERATION : 53, loss : 0.05958932086492466ITERATION : 54, loss : 0.05958932086492466ITERATION : 55, loss : 0.05958932086492466ITERATION : 56, loss : 0.05958932086492466ITERATION : 57, loss : 0.05958932086492466ITERATION : 58, loss : 0.05958932086492466ITERATION : 59, loss : 0.05958932086492466ITERATION : 60, loss : 0.05958932086492466ITERATION : 61, loss : 0.05958932086492466ITERATION : 62, loss : 0.05958932086492466ITERATION : 63, loss : 0.05958932086492466ITERATION : 64, loss : 0.05958932086492466ITERATION : 65, loss : 0.05958932086492466ITERATION : 66, loss : 0.05958932086492466ITERATION : 67, loss : 0.05958932086492466ITERATION : 68, loss : 0.05958932086492466ITERATION : 69, loss : 0.05958932086492466ITERATION : 70, loss : 0.05958932086492466ITERATION : 71, loss : 0.05958932086492466ITERATION : 72, loss : 0.05958932086492466ITERATION : 73, loss : 0.05958932086492466ITERATION : 74, loss : 0.05958932086492466ITERATION : 75, loss : 0.05958932086492466ITERATION : 76, loss : 0.05958932086492466ITERATION : 77, loss : 0.05958932086492466ITERATION : 78, loss : 0.05958932086492466ITERATION : 79, loss : 0.05958932086492466ITERATION : 80, loss : 0.05958932086492466ITERATION : 81, loss : 0.05958932086492466ITERATION : 82, loss : 0.05958932086492466ITERATION : 83, loss : 0.05958932086492466ITERATION : 84, loss : 0.05958932086492466ITERATION : 85, loss : 0.05958932086492466ITERATION : 86, loss : 0.05958932086492466ITERATION : 87, loss : 0.05958932086492466ITERATION : 88, loss : 0.05958932086492466ITERATION : 89, loss : 0.05958932086492466ITERATION : 90, loss : 0.05958932086492466ITERATION : 91, loss : 0.05958932086492466ITERATION : 92, loss : 0.05958932086492466ITERATION : 93, loss : 0.05958932086492466ITERATION : 94, loss : 0.05958932086492466ITERATION : 95, loss : 0.05958932086492466ITERATION : 96, loss : 0.05958932086492466ITERATION : 97, loss : 0.05958932086492466ITERATION : 98, loss : 0.05958932086492466ITERATION : 99, loss : 0.05958932086492466ITERATION : 100, loss : 0.05958932086492466
ITERATION : 1, loss : 0.0390360254954487ITERATION : 2, loss : 0.04341640411572389ITERATION : 3, loss : 0.047103200939344586ITERATION : 4, loss : 0.05028732982293957ITERATION : 5, loss : 0.05288910206147551ITERATION : 6, loss : 0.054923736540996554ITERATION : 7, loss : 0.056469257445981ITERATION : 8, loss : 0.0576212136312583ITERATION : 9, loss : 0.05846913390457728ITERATION : 10, loss : 0.05908800751521623ITERATION : 11, loss : 0.05953709005180523ITERATION : 12, loss : 0.05986164677069763ITERATION : 13, loss : 0.06009553792146584ITERATION : 14, loss : 0.06026374815941761ITERATION : 15, loss : 0.06038454497681745ITERATION : 16, loss : 0.060471201274204744ITERATION : 17, loss : 0.06053331799519404ITERATION : 18, loss : 0.06057781940868846ITERATION : 19, loss : 0.06060968773285103ITERATION : 20, loss : 0.060632502313208415ITERATION : 21, loss : 0.06064883150654537ITERATION : 22, loss : 0.060660516854810866ITERATION : 23, loss : 0.060668877942918184ITERATION : 24, loss : 0.06067485987219571ITERATION : 25, loss : 0.06067913927790262ITERATION : 26, loss : 0.06068220055079513ITERATION : 27, loss : 0.06068439031010513ITERATION : 28, loss : 0.06068595655157901ITERATION : 29, loss : 0.06068707683032396ITERATION : 30, loss : 0.060687878074734636ITERATION : 31, loss : 0.06068845118184622ITERATION : 32, loss : 0.06068886105278288ITERATION : 33, loss : 0.060689154186636576ITERATION : 34, loss : 0.060689363782235606ITERATION : 35, loss : 0.060689513709433776ITERATION : 36, loss : 0.0606896209549657ITERATION : 37, loss : 0.060689697651099016ITERATION : 38, loss : 0.060689752558015515ITERATION : 39, loss : 0.06068979176541135ITERATION : 40, loss : 0.060689819845782395ITERATION : 41, loss : 0.06068983988105467ITERATION : 42, loss : 0.06068985424129389ITERATION : 43, loss : 0.060689864479612485ITERATION : 44, loss : 0.060689871894571645ITERATION : 45, loss : 0.0606898771025393ITERATION : 46, loss : 0.060689880868560774ITERATION : 47, loss : 0.06068988351542194ITERATION : 48, loss : 0.060689885423553844ITERATION : 49, loss : 0.06068988676206901ITERATION : 50, loss : 0.060689887778143534ITERATION : 51, loss : 0.060689888415945865ITERATION : 52, loss : 0.06068988893788711ITERATION : 53, loss : 0.06068988926383345ITERATION : 54, loss : 0.06068988951470319ITERATION : 55, loss : 0.06068988963399474ITERATION : 56, loss : 0.06068988974432412ITERATION : 57, loss : 0.06068988978436333ITERATION : 58, loss : 0.06068988978779971ITERATION : 59, loss : 0.06068988978779971ITERATION : 60, loss : 0.06068988978779971ITERATION : 61, loss : 0.06068988978779971ITERATION : 62, loss : 0.06068988978779971ITERATION : 63, loss : 0.06068988978779971ITERATION : 64, loss : 0.06068988978779971ITERATION : 65, loss : 0.06068988978779971ITERATION : 66, loss : 0.06068988978779971ITERATION : 67, loss : 0.06068988978779971ITERATION : 68, loss : 0.06068988978779971ITERATION : 69, loss : 0.06068988978779971ITERATION : 70, loss : 0.06068988978779971ITERATION : 71, loss : 0.06068988978779971ITERATION : 72, loss : 0.06068988978779971ITERATION : 73, loss : 0.06068988978779971ITERATION : 74, loss : 0.06068988978779971ITERATION : 75, loss : 0.06068988978779971ITERATION : 76, loss : 0.06068988978779971ITERATION : 77, loss : 0.06068988978779971ITERATION : 78, loss : 0.06068988978779971ITERATION : 79, loss : 0.06068988978779971ITERATION : 80, loss : 0.06068988978779971ITERATION : 81, loss : 0.06068988978779971ITERATION : 82, loss : 0.06068988978779971ITERATION : 83, loss : 0.06068988978779971ITERATION : 84, loss : 0.06068988978779971ITERATION : 85, loss : 0.06068988978779971ITERATION : 86, loss : 0.06068988978779971ITERATION : 87, loss : 0.06068988978779971ITERATION : 88, loss : 0.06068988978779971ITERATION : 89, loss : 0.06068988978779971ITERATION : 90, loss : 0.06068988978779971ITERATION : 91, loss : 0.06068988978779971ITERATION : 92, loss : 0.06068988978779971ITERATION : 93, loss : 0.06068988978779971ITERATION : 94, loss : 0.06068988978779971ITERATION : 95, loss : 0.06068988978779971ITERATION : 96, loss : 0.06068988978779971ITERATION : 97, loss : 0.06068988978779971ITERATION : 98, loss : 0.06068988978779971ITERATION : 99, loss : 0.06068988978779971ITERATION : 100, loss : 0.06068988978779971
gradient norm in None layer : 0.0011914012291348846
gradient norm in None layer : 3.486955959104418e-05
gradient norm in None layer : 3.7932528213931594e-05
gradient norm in None layer : 0.0005237090011386792
gradient norm in None layer : 3.794852474454436e-05
gradient norm in None layer : 3.7853661936373944e-05
gradient norm in None layer : 0.000318195315819832
gradient norm in None layer : 1.2676915695642784e-05
gradient norm in None layer : 1.4090294550390488e-05
gradient norm in None layer : 0.00024753247625818297
gradient norm in None layer : 1.142257612029328e-05
gradient norm in None layer : 9.737590604507525e-06
gradient norm in None layer : 7.816245601979831e-05
gradient norm in None layer : 2.4323510298638884e-06
gradient norm in None layer : 2.0306078105563726e-06
gradient norm in None layer : 7.330224884988678e-05
gradient norm in None layer : 3.0223362761468265e-06
gradient norm in None layer : 2.3957833479472457e-06
gradient norm in None layer : 9.790381032783511e-05
gradient norm in None layer : 1.7889805653493668e-06
gradient norm in None layer : 0.00021450407230843527
gradient norm in None layer : 1.3442067544837488e-05
gradient norm in None layer : 1.0539277297741669e-05
gradient norm in None layer : 0.00025816688678871613
gradient norm in None layer : 2.2880530216900385e-05
gradient norm in None layer : 2.8951236122841875e-05
gradient norm in None layer : 0.0005024131082102917
gradient norm in None layer : 5.0945991164041966e-06
gradient norm in None layer : 0.0005289177293761061
gradient norm in None layer : 5.3438082299837815e-05
gradient norm in None layer : 5.321671896432009e-05
gradient norm in None layer : 0.0007045947190643066
gradient norm in None layer : 8.327848204874311e-05
gradient norm in None layer : 0.00012963917560601147
gradient norm in None layer : 6.314866291721937e-05
gradient norm in None layer : 1.989588663108616e-05
Total gradient norm: 0.0017493478146450956
invariance loss : 4.594848322839555, avg_den : 0.44690704345703125, density loss : 0.34651042546013877, mse loss : 0.07350636775681377, solver time : 121.51777338981628 sec , total loss : 0.07844772650511346, running loss : 0.10424138811082617
saving checkpoint
Epoch 0/10 , batch 21/12500 
ITERATION : 1, loss : 0.02537107889052985ITERATION : 2, loss : 0.02180154564711549ITERATION : 3, loss : 0.024108654146573077ITERATION : 4, loss : 0.02684590415574718ITERATION : 5, loss : 0.02911322267129313ITERATION : 6, loss : 0.030806022824826546ITERATION : 7, loss : 0.0320114143986273ITERATION : 8, loss : 0.03284906228966072ITERATION : 9, loss : 0.033423869948329805ITERATION : 10, loss : 0.033815939879195255ITERATION : 11, loss : 0.034082762838655606ITERATION : 12, loss : 0.03426433650122945ITERATION : 13, loss : 0.03438804639860928ITERATION : 14, loss : 0.03447249550958349ITERATION : 15, loss : 0.034530278649494296ITERATION : 16, loss : 0.03456991685876034ITERATION : 17, loss : 0.03459717978887715ITERATION : 18, loss : 0.03461598080960974ITERATION : 19, loss : 0.03462898022892275ITERATION : 20, loss : 0.03463799114054736ITERATION : 21, loss : 0.03464425258256286ITERATION : 22, loss : 0.0346486137207643ITERATION : 23, loss : 0.03465165790594205ITERATION : 24, loss : 0.03465378729643725ITERATION : 25, loss : 0.034655279713476914ITERATION : 26, loss : 0.034656327610572794ITERATION : 27, loss : 0.03465706463822326ITERATION : 28, loss : 0.034657583833841744ITERATION : 29, loss : 0.034657950081531996ITERATION : 30, loss : 0.03465820880872427ITERATION : 31, loss : 0.03465839177675316ITERATION : 32, loss : 0.0346585213774317ITERATION : 33, loss : 0.034658613221705326ITERATION : 34, loss : 0.03465867839019215ITERATION : 35, loss : 0.03465872465857924ITERATION : 36, loss : 0.03465875754127278ITERATION : 37, loss : 0.034658780922190206ITERATION : 38, loss : 0.03465879758480924ITERATION : 39, loss : 0.034658809441653914ITERATION : 40, loss : 0.03465881788454884ITERATION : 41, loss : 0.034658823899632304ITERATION : 42, loss : 0.03465882817098499ITERATION : 43, loss : 0.034658831208879526ITERATION : 44, loss : 0.034658833359696634ITERATION : 45, loss : 0.03465883493776703ITERATION : 46, loss : 0.03465883603826661ITERATION : 47, loss : 0.034658836846250755ITERATION : 48, loss : 0.034658837437946045ITERATION : 49, loss : 0.03465883782653863ITERATION : 50, loss : 0.034658838081619736ITERATION : 51, loss : 0.03465883828497556ITERATION : 52, loss : 0.0346588383887351ITERATION : 53, loss : 0.034658838454286446ITERATION : 54, loss : 0.03465883854649623ITERATION : 55, loss : 0.034658838595702145ITERATION : 56, loss : 0.03465883862339841ITERATION : 57, loss : 0.03465883863257005ITERATION : 58, loss : 0.034658838639085386ITERATION : 59, loss : 0.034658838639085386ITERATION : 60, loss : 0.034658838639085386ITERATION : 61, loss : 0.034658838639085386ITERATION : 62, loss : 0.034658838639085386ITERATION : 63, loss : 0.034658838639085386ITERATION : 64, loss : 0.034658838639085386ITERATION : 65, loss : 0.034658838639085386ITERATION : 66, loss : 0.034658838639085386ITERATION : 67, loss : 0.034658838639085386ITERATION : 68, loss : 0.034658838639085386ITERATION : 69, loss : 0.034658838639085386ITERATION : 70, loss : 0.034658838639085386ITERATION : 71, loss : 0.034658838639085386ITERATION : 72, loss : 0.034658838639085386ITERATION : 73, loss : 0.034658838639085386ITERATION : 74, loss : 0.034658838639085386ITERATION : 75, loss : 0.034658838639085386ITERATION : 76, loss : 0.034658838639085386ITERATION : 77, loss : 0.034658838639085386ITERATION : 78, loss : 0.034658838639085386ITERATION : 79, loss : 0.034658838639085386ITERATION : 80, loss : 0.034658838639085386ITERATION : 81, loss : 0.034658838639085386ITERATION : 82, loss : 0.034658838639085386ITERATION : 83, loss : 0.034658838639085386ITERATION : 84, loss : 0.034658838639085386ITERATION : 85, loss : 0.034658838639085386ITERATION : 86, loss : 0.034658838639085386ITERATION : 87, loss : 0.034658838639085386ITERATION : 88, loss : 0.034658838639085386ITERATION : 89, loss : 0.034658838639085386ITERATION : 90, loss : 0.034658838639085386ITERATION : 91, loss : 0.034658838639085386ITERATION : 92, loss : 0.034658838639085386ITERATION : 93, loss : 0.034658838639085386ITERATION : 94, loss : 0.034658838639085386ITERATION : 95, loss : 0.034658838639085386ITERATION : 96, loss : 0.034658838639085386ITERATION : 97, loss : 0.034658838639085386ITERATION : 98, loss : 0.034658838639085386ITERATION : 99, loss : 0.034658838639085386ITERATION : 100, loss : 0.034658838639085386
ITERATION : 1, loss : 0.020097336388877236ITERATION : 2, loss : 0.022425480721551302ITERATION : 3, loss : 0.026032177572110536ITERATION : 4, loss : 0.02918189316056014ITERATION : 5, loss : 0.03164282091819593ITERATION : 6, loss : 0.03350515873420514ITERATION : 7, loss : 0.03484288053611722ITERATION : 8, loss : 0.03578897722464875ITERATION : 9, loss : 0.03645060252796115ITERATION : 10, loss : 0.03690957534861301ITERATION : 11, loss : 0.03722618954010107ITERATION : 12, loss : 0.037443793736454296ITERATION : 13, loss : 0.03759301599731651ITERATION : 14, loss : 0.037695232412064815ITERATION : 15, loss : 0.037765233189533ITERATION : 16, loss : 0.03781319261700337ITERATION : 17, loss : 0.037846082051079595ITERATION : 18, loss : 0.037868667044511764ITERATION : 19, loss : 0.03788420148636787ITERATION : 20, loss : 0.037894906253032566ITERATION : 21, loss : 0.037902297804368976ITERATION : 22, loss : 0.03790741243489244ITERATION : 23, loss : 0.03791095950872333ITERATION : 24, loss : 0.03791342487690637ITERATION : 25, loss : 0.03791514213736811ITERATION : 26, loss : 0.03791634105411015ITERATION : 27, loss : 0.03791717991023339ITERATION : 28, loss : 0.03791776816255004ITERATION : 29, loss : 0.0379181814721771ITERATION : 30, loss : 0.0379184724643295ITERATION : 31, loss : 0.03791867777451675ITERATION : 32, loss : 0.03791882278722566ITERATION : 33, loss : 0.037918925484661456ITERATION : 34, loss : 0.03791899836225879ITERATION : 35, loss : 0.037919050156279464ITERATION : 36, loss : 0.03791908697854037ITERATION : 37, loss : 0.03791911323646423ITERATION : 38, loss : 0.03791913199266373ITERATION : 39, loss : 0.03791914534565309ITERATION : 40, loss : 0.03791915493395139ITERATION : 41, loss : 0.037919161761403704ITERATION : 42, loss : 0.03791916669095627ITERATION : 43, loss : 0.03791917017142628ITERATION : 44, loss : 0.03791917267569184ITERATION : 45, loss : 0.03791917440957397ITERATION : 46, loss : 0.03791917565960216ITERATION : 47, loss : 0.03791917656331218ITERATION : 48, loss : 0.03791917724941951ITERATION : 49, loss : 0.03791917771459851ITERATION : 50, loss : 0.037919178092489396ITERATION : 51, loss : 0.037919178264195524ITERATION : 52, loss : 0.0379191783929449ITERATION : 53, loss : 0.03791917855965641ITERATION : 54, loss : 0.037919178597154664ITERATION : 55, loss : 0.03791917868330143ITERATION : 56, loss : 0.03791917869520928ITERATION : 57, loss : 0.037919178695629684ITERATION : 58, loss : 0.037919178695629684ITERATION : 59, loss : 0.037919178695629684ITERATION : 60, loss : 0.037919178695629684ITERATION : 61, loss : 0.037919178695629684ITERATION : 62, loss : 0.037919178695629684ITERATION : 63, loss : 0.037919178695629684ITERATION : 64, loss : 0.037919178695629684ITERATION : 65, loss : 0.037919178695629684ITERATION : 66, loss : 0.037919178695629684ITERATION : 67, loss : 0.037919178695629684ITERATION : 68, loss : 0.037919178695629684ITERATION : 69, loss : 0.037919178695629684ITERATION : 70, loss : 0.037919178695629684ITERATION : 71, loss : 0.037919178695629684ITERATION : 72, loss : 0.037919178695629684ITERATION : 73, loss : 0.037919178695629684ITERATION : 74, loss : 0.037919178695629684ITERATION : 75, loss : 0.037919178695629684ITERATION : 76, loss : 0.037919178695629684ITERATION : 77, loss : 0.037919178695629684ITERATION : 78, loss : 0.037919178695629684ITERATION : 79, loss : 0.037919178695629684ITERATION : 80, loss : 0.037919178695629684ITERATION : 81, loss : 0.037919178695629684ITERATION : 82, loss : 0.037919178695629684ITERATION : 83, loss : 0.037919178695629684ITERATION : 84, loss : 0.037919178695629684ITERATION : 85, loss : 0.037919178695629684ITERATION : 86, loss : 0.037919178695629684ITERATION : 87, loss : 0.037919178695629684ITERATION : 88, loss : 0.037919178695629684ITERATION : 89, loss : 0.037919178695629684ITERATION : 90, loss : 0.037919178695629684ITERATION : 91, loss : 0.037919178695629684ITERATION : 92, loss : 0.037919178695629684ITERATION : 93, loss : 0.037919178695629684ITERATION : 94, loss : 0.037919178695629684ITERATION : 95, loss : 0.037919178695629684ITERATION : 96, loss : 0.037919178695629684ITERATION : 97, loss : 0.037919178695629684ITERATION : 98, loss : 0.037919178695629684ITERATION : 99, loss : 0.037919178695629684ITERATION : 100, loss : 0.037919178695629684
ITERATION : 1, loss : 0.04365973571053393ITERATION : 2, loss : 0.06624690992796442ITERATION : 3, loss : 0.08210790564472814ITERATION : 4, loss : 0.09239942357238051ITERATION : 5, loss : 0.09930436607586347ITERATION : 6, loss : 0.1039615763645616ITERATION : 7, loss : 0.10711456093507374ITERATION : 8, loss : 0.10927033037875737ITERATION : 9, loss : 0.11075434098166455ITERATION : 10, loss : 0.11178048231196967ITERATION : 11, loss : 0.11249234727936562ITERATION : 12, loss : 0.1129873552722812ITERATION : 13, loss : 0.11333214672201393ITERATION : 14, loss : 0.11357259221343012ITERATION : 15, loss : 0.11374041047641852ITERATION : 16, loss : 0.11385760719982085ITERATION : 17, loss : 0.11393948562672834ITERATION : 18, loss : 0.11399670544265653ITERATION : 19, loss : 0.1140367008362948ITERATION : 20, loss : 0.11406466064430956ITERATION : 21, loss : 0.11408420861747963ITERATION : 22, loss : 0.11409787646910234ITERATION : 23, loss : 0.1141074334349921ITERATION : 24, loss : 0.11411411617528006ITERATION : 25, loss : 0.11411878925785084ITERATION : 26, loss : 0.11412205700202314ITERATION : 27, loss : 0.11412434215532098ITERATION : 28, loss : 0.11412594018706727ITERATION : 29, loss : 0.11412705776681593ITERATION : 30, loss : 0.11412783931362411ITERATION : 31, loss : 0.1141283858458204ITERATION : 32, loss : 0.11412876807123119ITERATION : 33, loss : 0.11412903536558118ITERATION : 34, loss : 0.11412922232681097ITERATION : 35, loss : 0.11412935306274954ITERATION : 36, loss : 0.11412944449735644ITERATION : 37, loss : 0.11412950844152926ITERATION : 38, loss : 0.11412955317403647ITERATION : 39, loss : 0.11412958447453406ITERATION : 40, loss : 0.11412960634561944ITERATION : 41, loss : 0.11412962167112066ITERATION : 42, loss : 0.11412963238251692ITERATION : 43, loss : 0.1141296398688749ITERATION : 44, loss : 0.1141296451025624ITERATION : 45, loss : 0.11412964877322113ITERATION : 46, loss : 0.11412965131629264ITERATION : 47, loss : 0.11412965309048713ITERATION : 48, loss : 0.11412965428299301ITERATION : 49, loss : 0.11412965518068383ITERATION : 50, loss : 0.11412965578168134ITERATION : 51, loss : 0.11412965618333108ITERATION : 52, loss : 0.11412965649870811ITERATION : 53, loss : 0.11412965670890282ITERATION : 54, loss : 0.11412965682493598ITERATION : 55, loss : 0.11412965693528637ITERATION : 56, loss : 0.1141296569636488ITERATION : 57, loss : 0.11412965696407468ITERATION : 58, loss : 0.11412965696407468ITERATION : 59, loss : 0.11412965696407468ITERATION : 60, loss : 0.11412965696407468ITERATION : 61, loss : 0.11412965696407468ITERATION : 62, loss : 0.11412965696407468ITERATION : 63, loss : 0.11412965696407468ITERATION : 64, loss : 0.11412965696407468ITERATION : 65, loss : 0.11412965696407468ITERATION : 66, loss : 0.11412965696407468ITERATION : 67, loss : 0.11412965696407468ITERATION : 68, loss : 0.11412965696407468ITERATION : 69, loss : 0.11412965696407468ITERATION : 70, loss : 0.11412965696407468ITERATION : 71, loss : 0.11412965696407468ITERATION : 72, loss : 0.11412965696407468ITERATION : 73, loss : 0.11412965696407468ITERATION : 74, loss : 0.11412965696407468ITERATION : 75, loss : 0.11412965696407468ITERATION : 76, loss : 0.11412965696407468ITERATION : 77, loss : 0.11412965696407468ITERATION : 78, loss : 0.11412965696407468ITERATION : 79, loss : 0.11412965696407468ITERATION : 80, loss : 0.11412965696407468ITERATION : 81, loss : 0.11412965696407468ITERATION : 82, loss : 0.11412965696407468ITERATION : 83, loss : 0.11412965696407468ITERATION : 84, loss : 0.11412965696407468ITERATION : 85, loss : 0.11412965696407468ITERATION : 86, loss : 0.11412965696407468ITERATION : 87, loss : 0.11412965696407468ITERATION : 88, loss : 0.11412965696407468ITERATION : 89, loss : 0.11412965696407468ITERATION : 90, loss : 0.11412965696407468ITERATION : 91, loss : 0.11412965696407468ITERATION : 92, loss : 0.11412965696407468ITERATION : 93, loss : 0.11412965696407468ITERATION : 94, loss : 0.11412965696407468ITERATION : 95, loss : 0.11412965696407468ITERATION : 96, loss : 0.11412965696407468ITERATION : 97, loss : 0.11412965696407468ITERATION : 98, loss : 0.11412965696407468ITERATION : 99, loss : 0.11412965696407468ITERATION : 100, loss : 0.11412965696407468
ITERATION : 1, loss : 0.03165654705830879ITERATION : 2, loss : 0.025490062144312417ITERATION : 3, loss : 0.025270319102897745ITERATION : 4, loss : 0.025859739274215154ITERATION : 5, loss : 0.02642594431555544ITERATION : 6, loss : 0.02689020993479091ITERATION : 7, loss : 0.027248404528615912ITERATION : 8, loss : 0.027516176870426397ITERATION : 9, loss : 0.027712582157408717ITERATION : 10, loss : 0.027854875512678505ITERATION : 11, loss : 0.027957114555236768ITERATION : 12, loss : 0.028030158124291542ITERATION : 13, loss : 0.02808213902617114ITERATION : 14, loss : 0.02811902959838805ITERATION : 15, loss : 0.02814516062579179ITERATION : 16, loss : 0.02816364535688519ITERATION : 17, loss : 0.028176709035713503ITERATION : 18, loss : 0.028185935352552243ITERATION : 19, loss : 0.028192448462765675ITERATION : 20, loss : 0.028197044768575437ITERATION : 21, loss : 0.028200287667143466ITERATION : 22, loss : 0.02820257531104802ITERATION : 23, loss : 0.028204188935217346ITERATION : 24, loss : 0.02820532702888126ITERATION : 25, loss : 0.028206129696603164ITERATION : 26, loss : 0.028206695794154874ITERATION : 27, loss : 0.028207095038380422ITERATION : 28, loss : 0.028207376576656444ITERATION : 29, loss : 0.028207575125044373ITERATION : 30, loss : 0.028207715172537947ITERATION : 31, loss : 0.028207813951017967ITERATION : 32, loss : 0.02820788357424877ITERATION : 33, loss : 0.02820793271415362ITERATION : 34, loss : 0.028207967343755597ITERATION : 35, loss : 0.028207991758168835ITERATION : 36, loss : 0.02820800898264546ITERATION : 37, loss : 0.028208021139039814ITERATION : 38, loss : 0.02820802973540045ITERATION : 39, loss : 0.02820803580379752ITERATION : 40, loss : 0.02820804008956566ITERATION : 41, loss : 0.028208043109661695ITERATION : 42, loss : 0.028208045259457186ITERATION : 43, loss : 0.028208046756041813ITERATION : 44, loss : 0.028208047829036575ITERATION : 45, loss : 0.02820804857321883ITERATION : 46, loss : 0.028208049121196958ITERATION : 47, loss : 0.02820804948877702ITERATION : 48, loss : 0.02820804975786474ITERATION : 49, loss : 0.028208049965941315ITERATION : 50, loss : 0.028208050055829093ITERATION : 51, loss : 0.028208050110878898ITERATION : 52, loss : 0.028208050115534702ITERATION : 53, loss : 0.028208050115534702ITERATION : 54, loss : 0.028208050115534702ITERATION : 55, loss : 0.028208050115534702ITERATION : 56, loss : 0.028208050115534702ITERATION : 57, loss : 0.028208050115534702ITERATION : 58, loss : 0.028208050115534702ITERATION : 59, loss : 0.028208050115534702ITERATION : 60, loss : 0.028208050115534702ITERATION : 61, loss : 0.028208050115534702ITERATION : 62, loss : 0.028208050115534702ITERATION : 63, loss : 0.028208050115534702ITERATION : 64, loss : 0.028208050115534702ITERATION : 65, loss : 0.028208050115534702ITERATION : 66, loss : 0.028208050115534702ITERATION : 67, loss : 0.028208050115534702ITERATION : 68, loss : 0.028208050115534702ITERATION : 69, loss : 0.028208050115534702ITERATION : 70, loss : 0.028208050115534702ITERATION : 71, loss : 0.028208050115534702ITERATION : 72, loss : 0.028208050115534702ITERATION : 73, loss : 0.028208050115534702ITERATION : 74, loss : 0.028208050115534702ITERATION : 75, loss : 0.028208050115534702ITERATION : 76, loss : 0.028208050115534702ITERATION : 77, loss : 0.028208050115534702ITERATION : 78, loss : 0.028208050115534702ITERATION : 79, loss : 0.028208050115534702ITERATION : 80, loss : 0.028208050115534702ITERATION : 81, loss : 0.028208050115534702ITERATION : 82, loss : 0.028208050115534702ITERATION : 83, loss : 0.028208050115534702ITERATION : 84, loss : 0.028208050115534702ITERATION : 85, loss : 0.028208050115534702ITERATION : 86, loss : 0.028208050115534702ITERATION : 87, loss : 0.028208050115534702ITERATION : 88, loss : 0.028208050115534702ITERATION : 89, loss : 0.028208050115534702ITERATION : 90, loss : 0.028208050115534702ITERATION : 91, loss : 0.028208050115534702ITERATION : 92, loss : 0.028208050115534702ITERATION : 93, loss : 0.028208050115534702ITERATION : 94, loss : 0.028208050115534702ITERATION : 95, loss : 0.028208050115534702ITERATION : 96, loss : 0.028208050115534702ITERATION : 97, loss : 0.028208050115534702ITERATION : 98, loss : 0.028208050115534702ITERATION : 99, loss : 0.028208050115534702ITERATION : 100, loss : 0.028208050115534702
ITERATION : 1, loss : 0.010774033817903489ITERATION : 2, loss : 0.010357426983080217ITERATION : 3, loss : 0.010798248641626488ITERATION : 4, loss : 0.011246925193667696ITERATION : 5, loss : 0.011625618874305571ITERATION : 6, loss : 0.011925257105766062ITERATION : 7, loss : 0.012152891083430567ITERATION : 8, loss : 0.012321062841879985ITERATION : 9, loss : 0.012442939574494757ITERATION : 10, loss : 0.01253010819847105ITERATION : 11, loss : 0.012591892770832743ITERATION : 12, loss : 0.012635416393819668ITERATION : 13, loss : 0.012665948034385423ITERATION : 14, loss : 0.01268730503830965ITERATION : 15, loss : 0.012702215685860626ITERATION : 16, loss : 0.012712612310491407ITERATION : 17, loss : 0.012719855231524213ITERATION : 18, loss : 0.012724898195062232ITERATION : 19, loss : 0.012728408086088352ITERATION : 20, loss : 0.012730850372548645ITERATION : 21, loss : 0.012732549498509414ITERATION : 22, loss : 0.012733731479468428ITERATION : 23, loss : 0.012734553676898326ITERATION : 24, loss : 0.012735125561915342ITERATION : 25, loss : 0.012735523357993848ITERATION : 26, loss : 0.012735800033718927ITERATION : 27, loss : 0.012735992488853877ITERATION : 28, loss : 0.012736126342861006ITERATION : 29, loss : 0.012736219447721069ITERATION : 30, loss : 0.012736284200669244ITERATION : 31, loss : 0.012736329244266249ITERATION : 32, loss : 0.012736360562742095ITERATION : 33, loss : 0.01273638236014185ITERATION : 34, loss : 0.012736397524632034ITERATION : 35, loss : 0.012736408064264088ITERATION : 36, loss : 0.012736415397095725ITERATION : 37, loss : 0.012736420495634086ITERATION : 38, loss : 0.012736424046903581ITERATION : 39, loss : 0.012736426514129847ITERATION : 40, loss : 0.012736428235270379ITERATION : 41, loss : 0.012736429414748725ITERATION : 42, loss : 0.012736430253901691ITERATION : 43, loss : 0.012736430816311721ITERATION : 44, loss : 0.012736431206892154ITERATION : 45, loss : 0.012736431465581373ITERATION : 46, loss : 0.012736431676508205ITERATION : 47, loss : 0.012736431793087111ITERATION : 48, loss : 0.012736431919055977ITERATION : 49, loss : 0.01273643195828009ITERATION : 50, loss : 0.012736432021970494ITERATION : 51, loss : 0.012736432040169372ITERATION : 52, loss : 0.01273643206701909ITERATION : 53, loss : 0.012736432066942643ITERATION : 54, loss : 0.012736432066942643ITERATION : 55, loss : 0.012736432066942643ITERATION : 56, loss : 0.012736432066942643ITERATION : 57, loss : 0.012736432066942643ITERATION : 58, loss : 0.012736432066942643ITERATION : 59, loss : 0.012736432066942643ITERATION : 60, loss : 0.012736432066942643ITERATION : 61, loss : 0.012736432066942643ITERATION : 62, loss : 0.012736432066942643ITERATION : 63, loss : 0.012736432066942643ITERATION : 64, loss : 0.012736432066942643ITERATION : 65, loss : 0.012736432066942643ITERATION : 66, loss : 0.012736432066942643ITERATION : 67, loss : 0.012736432066942643ITERATION : 68, loss : 0.012736432066942643ITERATION : 69, loss : 0.012736432066942643ITERATION : 70, loss : 0.012736432066942643ITERATION : 71, loss : 0.012736432066942643ITERATION : 72, loss : 0.012736432066942643ITERATION : 73, loss : 0.012736432066942643ITERATION : 74, loss : 0.012736432066942643ITERATION : 75, loss : 0.012736432066942643ITERATION : 76, loss : 0.012736432066942643ITERATION : 77, loss : 0.012736432066942643ITERATION : 78, loss : 0.012736432066942643ITERATION : 79, loss : 0.012736432066942643ITERATION : 80, loss : 0.012736432066942643ITERATION : 81, loss : 0.012736432066942643ITERATION : 82, loss : 0.012736432066942643ITERATION : 83, loss : 0.012736432066942643ITERATION : 84, loss : 0.012736432066942643ITERATION : 85, loss : 0.012736432066942643ITERATION : 86, loss : 0.012736432066942643ITERATION : 87, loss : 0.012736432066942643ITERATION : 88, loss : 0.012736432066942643ITERATION : 89, loss : 0.012736432066942643ITERATION : 90, loss : 0.012736432066942643ITERATION : 91, loss : 0.012736432066942643ITERATION : 92, loss : 0.012736432066942643ITERATION : 93, loss : 0.012736432066942643ITERATION : 94, loss : 0.012736432066942643ITERATION : 95, loss : 0.012736432066942643ITERATION : 96, loss : 0.012736432066942643ITERATION : 97, loss : 0.012736432066942643ITERATION : 98, loss : 0.012736432066942643ITERATION : 99, loss : 0.012736432066942643ITERATION : 100, loss : 0.012736432066942643
ITERATION : 1, loss : 0.07055976170392221ITERATION : 2, loss : 0.09498032435776722ITERATION : 3, loss : 0.11053600676620508ITERATION : 4, loss : 0.1203105021414142ITERATION : 5, loss : 0.1266995317850166ITERATION : 6, loss : 0.13102100898011648ITERATION : 7, loss : 0.13401762534992845ITERATION : 8, loss : 0.13613238048442214ITERATION : 9, loss : 0.13764344020845928ITERATION : 10, loss : 0.13873273026567545ITERATION : 11, loss : 0.13952297867368185ITERATION : 12, loss : 0.14009892148506148ITERATION : 13, loss : 0.14052008194842808ITERATION : 14, loss : 0.14082881124018484ITERATION : 15, loss : 0.14105552993723042ITERATION : 16, loss : 0.1412222431777091ITERATION : 17, loss : 0.14134495197695848ITERATION : 18, loss : 0.14143533637177466ITERATION : 19, loss : 0.1415019468689774ITERATION : 20, loss : 0.1415510561242246ITERATION : 21, loss : 0.14158727299249968ITERATION : 22, loss : 0.14161398791325494ITERATION : 23, loss : 0.14163369704586115ITERATION : 24, loss : 0.14164823934353457ITERATION : 25, loss : 0.14165897033442157ITERATION : 26, loss : 0.14166688945443312ITERATION : 27, loss : 0.14167273383848591ITERATION : 28, loss : 0.14167704717370885ITERATION : 29, loss : 0.14168023070519561ITERATION : 30, loss : 0.14168258036163126ITERATION : 31, loss : 0.1416843146422508ITERATION : 32, loss : 0.14168559471895636ITERATION : 33, loss : 0.14168653955888721ITERATION : 34, loss : 0.1416872369522834ITERATION : 35, loss : 0.14168775171344877ITERATION : 36, loss : 0.14168813169012628ITERATION : 37, loss : 0.14168841216038117ITERATION : 38, loss : 0.14168861917731185ITERATION : 39, loss : 0.14168877200104346ITERATION : 40, loss : 0.14168888479162733ITERATION : 41, loss : 0.14168896807888046ITERATION : 42, loss : 0.1416890295406693ITERATION : 43, loss : 0.14168907489795862ITERATION : 44, loss : 0.14168910838518936ITERATION : 45, loss : 0.14168913306582387ITERATION : 46, loss : 0.14168915136170768ITERATION : 47, loss : 0.1416891648292268ITERATION : 48, loss : 0.14168917478614348ITERATION : 49, loss : 0.14168918211588186ITERATION : 50, loss : 0.14168918754563997ITERATION : 51, loss : 0.14168919151006573ITERATION : 52, loss : 0.1416891944323225ITERATION : 53, loss : 0.14168919660807214ITERATION : 54, loss : 0.14168919822179415ITERATION : 55, loss : 0.1416891993917746ITERATION : 56, loss : 0.14168920026027723ITERATION : 57, loss : 0.1416892008868377ITERATION : 58, loss : 0.14168920136611868ITERATION : 59, loss : 0.14168920169935173ITERATION : 60, loss : 0.14168920196074658ITERATION : 61, loss : 0.14168920211894398ITERATION : 62, loss : 0.14168920225391382ITERATION : 63, loss : 0.14168920236777113ITERATION : 64, loss : 0.1416892024550053ITERATION : 65, loss : 0.14168920250981273ITERATION : 66, loss : 0.14168920256307485ITERATION : 67, loss : 0.1416892025653986ITERATION : 68, loss : 0.1416892025653986ITERATION : 69, loss : 0.1416892025653986ITERATION : 70, loss : 0.1416892025653986ITERATION : 71, loss : 0.1416892025653986ITERATION : 72, loss : 0.1416892025653986ITERATION : 73, loss : 0.1416892025653986ITERATION : 74, loss : 0.1416892025653986ITERATION : 75, loss : 0.1416892025653986ITERATION : 76, loss : 0.1416892025653986ITERATION : 77, loss : 0.1416892025653986ITERATION : 78, loss : 0.1416892025653986ITERATION : 79, loss : 0.1416892025653986ITERATION : 80, loss : 0.1416892025653986ITERATION : 81, loss : 0.1416892025653986ITERATION : 82, loss : 0.1416892025653986ITERATION : 83, loss : 0.1416892025653986ITERATION : 84, loss : 0.1416892025653986ITERATION : 85, loss : 0.1416892025653986ITERATION : 86, loss : 0.1416892025653986ITERATION : 87, loss : 0.1416892025653986ITERATION : 88, loss : 0.1416892025653986ITERATION : 89, loss : 0.1416892025653986ITERATION : 90, loss : 0.1416892025653986ITERATION : 91, loss : 0.1416892025653986ITERATION : 92, loss : 0.1416892025653986ITERATION : 93, loss : 0.1416892025653986ITERATION : 94, loss : 0.1416892025653986ITERATION : 95, loss : 0.1416892025653986ITERATION : 96, loss : 0.1416892025653986ITERATION : 97, loss : 0.1416892025653986ITERATION : 98, loss : 0.1416892025653986ITERATION : 99, loss : 0.1416892025653986ITERATION : 100, loss : 0.1416892025653986
ITERATION : 1, loss : 0.010863606411666596ITERATION : 2, loss : 0.011041856750670187ITERATION : 3, loss : 0.013180492798106692ITERATION : 4, loss : 0.0156311892739869ITERATION : 5, loss : 0.01783959894125886ITERATION : 6, loss : 0.019645574430906095ITERATION : 7, loss : 0.02105378121696915ITERATION : 8, loss : 0.022124278919046485ITERATION : 9, loss : 0.02292643112723446ITERATION : 10, loss : 0.023522383845936173ITERATION : 11, loss : 0.023962801164489813ITERATION : 12, loss : 0.024287174369330835ITERATION : 13, loss : 0.024525551521902937ITERATION : 14, loss : 0.02470047674837701ITERATION : 15, loss : 0.024828717830900824ITERATION : 16, loss : 0.024922677217202347ITERATION : 17, loss : 0.024991494387433758ITERATION : 18, loss : 0.025041887573732335ITERATION : 19, loss : 0.025078787053699057ITERATION : 20, loss : 0.025105806787026175ITERATION : 21, loss : 0.025125593869813095ITERATION : 22, loss : 0.025140086405190488ITERATION : 23, loss : 0.025150702938275666ITERATION : 24, loss : 0.025158481575105744ITERATION : 25, loss : 0.02516418210296104ITERATION : 26, loss : 0.025168360655198008ITERATION : 27, loss : 0.025171424287081932ITERATION : 28, loss : 0.025173670968482928ITERATION : 29, loss : 0.02517531898080713ITERATION : 30, loss : 0.02517652805187515ITERATION : 31, loss : 0.025177415365281633ITERATION : 32, loss : 0.025178066662817793ITERATION : 33, loss : 0.02517854480898399ITERATION : 34, loss : 0.025178895924835652ITERATION : 35, loss : 0.025179153787654927ITERATION : 36, loss : 0.025179343228050138ITERATION : 37, loss : 0.02517948242763753ITERATION : 38, loss : 0.025179584720116107ITERATION : 39, loss : 0.02517965989796678ITERATION : 40, loss : 0.02517971519075598ITERATION : 41, loss : 0.02517975580149564ITERATION : 42, loss : 0.02517978572481536ITERATION : 43, loss : 0.02517980772465415ITERATION : 44, loss : 0.025179823924616186ITERATION : 45, loss : 0.02517983581827185ITERATION : 46, loss : 0.025179844523378233ITERATION : 47, loss : 0.025179850951641596ITERATION : 48, loss : 0.0251798556597316ITERATION : 49, loss : 0.025179859133914184ITERATION : 50, loss : 0.025179861679443857ITERATION : 51, loss : 0.02517986356123215ITERATION : 52, loss : 0.0251798649257436ITERATION : 53, loss : 0.025179865952448196ITERATION : 54, loss : 0.025179866688208034ITERATION : 55, loss : 0.02517986724058436ITERATION : 56, loss : 0.025179867639175343ITERATION : 57, loss : 0.025179867953572556ITERATION : 58, loss : 0.02517986819563863ITERATION : 59, loss : 0.025179868347088838ITERATION : 60, loss : 0.025179868445533847ITERATION : 61, loss : 0.025179868523366802ITERATION : 62, loss : 0.025179868574522642ITERATION : 63, loss : 0.025179868620137942ITERATION : 64, loss : 0.025179868649353954ITERATION : 65, loss : 0.02517986865271736ITERATION : 66, loss : 0.025179868664509164ITERATION : 67, loss : 0.025179868667407745ITERATION : 68, loss : 0.025179868667407745ITERATION : 69, loss : 0.025179868667407745ITERATION : 70, loss : 0.025179868667407745ITERATION : 71, loss : 0.025179868667407745ITERATION : 72, loss : 0.025179868667407745ITERATION : 73, loss : 0.025179868667407745ITERATION : 74, loss : 0.025179868667407745ITERATION : 75, loss : 0.025179868667407745ITERATION : 76, loss : 0.025179868667407745ITERATION : 77, loss : 0.025179868667407745ITERATION : 78, loss : 0.025179868667407745ITERATION : 79, loss : 0.025179868667407745ITERATION : 80, loss : 0.025179868667407745ITERATION : 81, loss : 0.025179868667407745ITERATION : 82, loss : 0.025179868667407745ITERATION : 83, loss : 0.025179868667407745ITERATION : 84, loss : 0.025179868667407745ITERATION : 85, loss : 0.025179868667407745ITERATION : 86, loss : 0.025179868667407745ITERATION : 87, loss : 0.025179868667407745ITERATION : 88, loss : 0.025179868667407745ITERATION : 89, loss : 0.025179868667407745ITERATION : 90, loss : 0.025179868667407745ITERATION : 91, loss : 0.025179868667407745ITERATION : 92, loss : 0.025179868667407745ITERATION : 93, loss : 0.025179868667407745ITERATION : 94, loss : 0.025179868667407745ITERATION : 95, loss : 0.025179868667407745ITERATION : 96, loss : 0.025179868667407745ITERATION : 97, loss : 0.025179868667407745ITERATION : 98, loss : 0.025179868667407745ITERATION : 99, loss : 0.025179868667407745ITERATION : 100, loss : 0.025179868667407745
ITERATION : 1, loss : 0.03149093408488716ITERATION : 2, loss : 0.03738804042327684ITERATION : 3, loss : 0.04465576810795645ITERATION : 4, loss : 0.050822774282312216ITERATION : 5, loss : 0.05596647695872344ITERATION : 6, loss : 0.05969728422890871ITERATION : 7, loss : 0.062369399127995315ITERATION : 8, loss : 0.06427559680827834ITERATION : 9, loss : 0.06563443031037594ITERATION : 10, loss : 0.06660352131275879ITERATION : 11, loss : 0.06729520893633646ITERATION : 12, loss : 0.06778929866749506ITERATION : 13, loss : 0.0681424864828623ITERATION : 14, loss : 0.06839509778826341ITERATION : 15, loss : 0.06857585428392453ITERATION : 16, loss : 0.06870523977235507ITERATION : 17, loss : 0.06879787856702188ITERATION : 18, loss : 0.0688642207075056ITERATION : 19, loss : 0.06891173832473432ITERATION : 20, loss : 0.06894577706306727ITERATION : 21, loss : 0.06897016273108887ITERATION : 22, loss : 0.06898763426284034ITERATION : 23, loss : 0.06900015283640726ITERATION : 24, loss : 0.06900912300046709ITERATION : 25, loss : 0.06901555085786362ITERATION : 26, loss : 0.06902015712177306ITERATION : 27, loss : 0.06902345809442698ITERATION : 28, loss : 0.06902582373140861ITERATION : 29, loss : 0.06902751912085364ITERATION : 30, loss : 0.06902873420100203ITERATION : 31, loss : 0.06902960506331868ITERATION : 32, loss : 0.0690302292187049ITERATION : 33, loss : 0.06903067657594868ITERATION : 34, loss : 0.06903099721358043ITERATION : 35, loss : 0.06903122702941475ITERATION : 36, loss : 0.06903139174848184ITERATION : 37, loss : 0.0690315098182112ITERATION : 38, loss : 0.06903159445414754ITERATION : 39, loss : 0.06903165511500749ITERATION : 40, loss : 0.06903169860183424ITERATION : 41, loss : 0.06903172977016693ITERATION : 42, loss : 0.06903175212803009ITERATION : 43, loss : 0.06903176813826346ITERATION : 44, loss : 0.06903177962020728ITERATION : 45, loss : 0.0690317878311889ITERATION : 46, loss : 0.0690317937266431ITERATION : 47, loss : 0.06903179794920652ITERATION : 48, loss : 0.0690318009839673ITERATION : 49, loss : 0.06903180315363378ITERATION : 50, loss : 0.06903180471843318ITERATION : 51, loss : 0.06903180583755117ITERATION : 52, loss : 0.06903180664332541ITERATION : 53, loss : 0.06903180722107478ITERATION : 54, loss : 0.06903180764562573ITERATION : 55, loss : 0.06903180791174539ITERATION : 56, loss : 0.06903180812444386ITERATION : 57, loss : 0.06903180824654041ITERATION : 58, loss : 0.06903180836791284ITERATION : 59, loss : 0.06903180841057442ITERATION : 60, loss : 0.06903180848160614ITERATION : 61, loss : 0.06903180848305161ITERATION : 62, loss : 0.06903180852053413ITERATION : 63, loss : 0.06903180853336621ITERATION : 64, loss : 0.06903180853336621ITERATION : 65, loss : 0.06903180853336621ITERATION : 66, loss : 0.06903180853336621ITERATION : 67, loss : 0.06903180853336621ITERATION : 68, loss : 0.06903180853336621ITERATION : 69, loss : 0.06903180853336621ITERATION : 70, loss : 0.06903180853336621ITERATION : 71, loss : 0.06903180853336621ITERATION : 72, loss : 0.06903180853336621ITERATION : 73, loss : 0.06903180853336621ITERATION : 74, loss : 0.06903180853336621ITERATION : 75, loss : 0.06903180853336621ITERATION : 76, loss : 0.06903180853336621ITERATION : 77, loss : 0.06903180853336621ITERATION : 78, loss : 0.06903180853336621ITERATION : 79, loss : 0.06903180853336621ITERATION : 80, loss : 0.06903180853336621ITERATION : 81, loss : 0.06903180853336621ITERATION : 82, loss : 0.06903180853336621ITERATION : 83, loss : 0.06903180853336621ITERATION : 84, loss : 0.06903180853336621ITERATION : 85, loss : 0.06903180853336621ITERATION : 86, loss : 0.06903180853336621ITERATION : 87, loss : 0.06903180853336621ITERATION : 88, loss : 0.06903180853336621ITERATION : 89, loss : 0.06903180853336621ITERATION : 90, loss : 0.06903180853336621ITERATION : 91, loss : 0.06903180853336621ITERATION : 92, loss : 0.06903180853336621ITERATION : 93, loss : 0.06903180853336621ITERATION : 94, loss : 0.06903180853336621ITERATION : 95, loss : 0.06903180853336621ITERATION : 96, loss : 0.06903180853336621ITERATION : 97, loss : 0.06903180853336621ITERATION : 98, loss : 0.06903180853336621ITERATION : 99, loss : 0.06903180853336621ITERATION : 100, loss : 0.06903180853336621
gradient norm in None layer : 0.0031259283154867535
gradient norm in None layer : 8.127183783174828e-05
gradient norm in None layer : 5.8155918786389216e-05
gradient norm in None layer : 0.0011603077211430222
gradient norm in None layer : 8.022224644724399e-05
gradient norm in None layer : 5.5130852994420156e-05
gradient norm in None layer : 0.0007085056277389693
gradient norm in None layer : 2.3222677591701947e-05
gradient norm in None layer : 2.409689554654844e-05
gradient norm in None layer : 0.0004477879503290199
gradient norm in None layer : 1.9398584603887827e-05
gradient norm in None layer : 1.882236907071182e-05
gradient norm in None layer : 0.00014884239862277595
gradient norm in None layer : 4.824491949471732e-06
gradient norm in None layer : 3.7734189935278336e-06
gradient norm in None layer : 0.0001239286153120686
gradient norm in None layer : 4.493860746073743e-06
gradient norm in None layer : 3.6523928055641866e-06
gradient norm in None layer : 0.00014910630202139575
gradient norm in None layer : 1.7321140033155083e-06
gradient norm in None layer : 0.000368873125858498
gradient norm in None layer : 2.072507410321379e-05
gradient norm in None layer : 1.8709874488439063e-05
gradient norm in None layer : 0.00036331858417366496
gradient norm in None layer : 2.9222064430755415e-05
gradient norm in None layer : 2.9560474163323173e-05
gradient norm in None layer : 0.0006067018196916376
gradient norm in None layer : 4.957293018292476e-06
gradient norm in None layer : 0.0008807007574237764
gradient norm in None layer : 7.587560454676558e-05
gradient norm in None layer : 6.150519409481848e-05
gradient norm in None layer : 0.0009725986563878293
gradient norm in None layer : 0.00011693930313983669
gradient norm in None layer : 0.00014554358158507094
gradient norm in None layer : 8.579309392647853e-05
gradient norm in None layer : 2.1001359267002362e-05
Total gradient norm: 0.0037833630539714667
invariance loss : 4.618301484297534, avg_den : 0.4535675048828125, density loss : 0.35314032781963134, mse loss : 0.05794412953092996, solver time : 111.75662398338318 sec , total loss : 0.06291557134304712, running loss : 0.10227349207426525
Epoch 0/10 , batch 22/12500 
ITERATION : 1, loss : 0.01601628375805815ITERATION : 2, loss : 0.02059297386727684ITERATION : 3, loss : 0.02856318009513025ITERATION : 4, loss : 0.03568035922299753ITERATION : 5, loss : 0.04116345156320905ITERATION : 6, loss : 0.04513647189647554ITERATION : 7, loss : 0.047633001271145274ITERATION : 8, loss : 0.049305711591658204ITERATION : 9, loss : 0.05046806310684057ITERATION : 10, loss : 0.051274507347120336ITERATION : 11, loss : 0.051833724864097395ITERATION : 12, loss : 0.052221473605728416ITERATION : 13, loss : 0.0524903538519018ITERATION : 14, loss : 0.0526768328628205ITERATION : 15, loss : 0.05280618256728648ITERATION : 16, loss : 0.05289591716850765ITERATION : 17, loss : 0.052958176896670976ITERATION : 18, loss : 0.05300137866079354ITERATION : 19, loss : 0.053031359268360584ITERATION : 20, loss : 0.05305216672827171ITERATION : 21, loss : 0.05306660912231039ITERATION : 22, loss : 0.053076634420723794ITERATION : 23, loss : 0.05308359423204043ITERATION : 24, loss : 0.053088426367971994ITERATION : 25, loss : 0.053091781562579016ITERATION : 26, loss : 0.05309411143948598ITERATION : 27, loss : 0.05309572953989542ITERATION : 28, loss : 0.05309685343702578ITERATION : 29, loss : 0.053097634125395206ITERATION : 30, loss : 0.053098176466077104ITERATION : 31, loss : 0.053098553242743256ITERATION : 32, loss : 0.05309881504238064ITERATION : 33, loss : 0.053098996949534884ITERATION : 34, loss : 0.05309912336639875ITERATION : 35, loss : 0.05309921123159632ITERATION : 36, loss : 0.05309927229661382ITERATION : 37, loss : 0.05309931474445278ITERATION : 38, loss : 0.05309934426422547ITERATION : 39, loss : 0.05309936476206424ITERATION : 40, loss : 0.05309937902626541ITERATION : 41, loss : 0.053099388973756705ITERATION : 42, loss : 0.053099395874018775ITERATION : 43, loss : 0.053099400682760124ITERATION : 44, loss : 0.05309940401672726ITERATION : 45, loss : 0.05309940632817692ITERATION : 46, loss : 0.0530994079275283ITERATION : 47, loss : 0.053099409042190265ITERATION : 48, loss : 0.05309940981294978ITERATION : 49, loss : 0.053099410323006226ITERATION : 50, loss : 0.05309941068352673ITERATION : 51, loss : 0.053099410947909945ITERATION : 52, loss : 0.05309941117095591ITERATION : 53, loss : 0.05309941127023241ITERATION : 54, loss : 0.053099411354496015ITERATION : 55, loss : 0.05309941139177064ITERATION : 56, loss : 0.05309941140612352ITERATION : 57, loss : 0.053099411416897835ITERATION : 58, loss : 0.053099411416897835ITERATION : 59, loss : 0.053099411416897835ITERATION : 60, loss : 0.053099411416897835ITERATION : 61, loss : 0.053099411416897835ITERATION : 62, loss : 0.053099411416897835ITERATION : 63, loss : 0.053099411416897835ITERATION : 64, loss : 0.053099411416897835ITERATION : 65, loss : 0.053099411416897835ITERATION : 66, loss : 0.053099411416897835ITERATION : 67, loss : 0.053099411416897835ITERATION : 68, loss : 0.053099411416897835ITERATION : 69, loss : 0.053099411416897835ITERATION : 70, loss : 0.053099411416897835ITERATION : 71, loss : 0.053099411416897835ITERATION : 72, loss : 0.053099411416897835ITERATION : 73, loss : 0.053099411416897835ITERATION : 74, loss : 0.053099411416897835ITERATION : 75, loss : 0.053099411416897835ITERATION : 76, loss : 0.053099411416897835ITERATION : 77, loss : 0.053099411416897835ITERATION : 78, loss : 0.053099411416897835ITERATION : 79, loss : 0.053099411416897835ITERATION : 80, loss : 0.053099411416897835ITERATION : 81, loss : 0.053099411416897835ITERATION : 82, loss : 0.053099411416897835ITERATION : 83, loss : 0.053099411416897835ITERATION : 84, loss : 0.053099411416897835ITERATION : 85, loss : 0.053099411416897835ITERATION : 86, loss : 0.053099411416897835ITERATION : 87, loss : 0.053099411416897835ITERATION : 88, loss : 0.053099411416897835ITERATION : 89, loss : 0.053099411416897835ITERATION : 90, loss : 0.053099411416897835ITERATION : 91, loss : 0.053099411416897835ITERATION : 92, loss : 0.053099411416897835ITERATION : 93, loss : 0.053099411416897835ITERATION : 94, loss : 0.053099411416897835ITERATION : 95, loss : 0.053099411416897835ITERATION : 96, loss : 0.053099411416897835ITERATION : 97, loss : 0.053099411416897835ITERATION : 98, loss : 0.053099411416897835ITERATION : 99, loss : 0.053099411416897835ITERATION : 100, loss : 0.053099411416897835
ITERATION : 1, loss : 0.07470360398144112ITERATION : 2, loss : 0.08043868236885052ITERATION : 3, loss : 0.08490943473061273ITERATION : 4, loss : 0.08634675275469356ITERATION : 5, loss : 0.08761260066299248ITERATION : 6, loss : 0.08861657755024498ITERATION : 7, loss : 0.08936109020912206ITERATION : 8, loss : 0.08989212859676528ITERATION : 9, loss : 0.09026510042498721ITERATION : 10, loss : 0.09052475875911185ITERATION : 11, loss : 0.09070457705503918ITERATION : 12, loss : 0.09082869165713307ITERATION : 13, loss : 0.09091417192543302ITERATION : 14, loss : 0.09097295662584715ITERATION : 15, loss : 0.09101334083763955ITERATION : 16, loss : 0.09104106336899785ITERATION : 17, loss : 0.09106008371634385ITERATION : 18, loss : 0.09107312823324962ITERATION : 19, loss : 0.09108207172649045ITERATION : 20, loss : 0.09108820217024957ITERATION : 21, loss : 0.09109240365858347ITERATION : 22, loss : 0.09109528282211243ITERATION : 23, loss : 0.0910972556317658ITERATION : 24, loss : 0.09109860736924409ITERATION : 25, loss : 0.09109953347907476ITERATION : 26, loss : 0.09110016795328497ITERATION : 27, loss : 0.09110060266125529ITERATION : 28, loss : 0.09110090048239439ITERATION : 29, loss : 0.09110110453845877ITERATION : 30, loss : 0.09110124433953862ITERATION : 31, loss : 0.09110134014474179ITERATION : 32, loss : 0.09110140577132282ITERATION : 33, loss : 0.0911014507525961ITERATION : 34, loss : 0.0911014815717387ITERATION : 35, loss : 0.09110150271078712ITERATION : 36, loss : 0.09110151717097012ITERATION : 37, loss : 0.09110152709257187ITERATION : 38, loss : 0.0911015338709511ITERATION : 39, loss : 0.09110153853536142ITERATION : 40, loss : 0.09110154171932631ITERATION : 41, loss : 0.09110154388557322ITERATION : 42, loss : 0.0911015453811122ITERATION : 43, loss : 0.09110154635960914ITERATION : 44, loss : 0.09110154706831977ITERATION : 45, loss : 0.09110154753986056ITERATION : 46, loss : 0.09110154786530997ITERATION : 47, loss : 0.09110154808112401ITERATION : 48, loss : 0.09110154821490506ITERATION : 49, loss : 0.0911015483399459ITERATION : 50, loss : 0.09110154843900524ITERATION : 51, loss : 0.09110154846185078ITERATION : 52, loss : 0.09110154850417758ITERATION : 53, loss : 0.09110154850208957ITERATION : 54, loss : 0.09110154850208957ITERATION : 55, loss : 0.09110154850208957ITERATION : 56, loss : 0.09110154850208957ITERATION : 57, loss : 0.09110154850208957ITERATION : 58, loss : 0.09110154850208957ITERATION : 59, loss : 0.09110154850208957ITERATION : 60, loss : 0.09110154850208957ITERATION : 61, loss : 0.09110154850208957ITERATION : 62, loss : 0.09110154850208957ITERATION : 63, loss : 0.09110154850208957ITERATION : 64, loss : 0.09110154850208957ITERATION : 65, loss : 0.09110154850208957ITERATION : 66, loss : 0.09110154850208957ITERATION : 67, loss : 0.09110154850208957ITERATION : 68, loss : 0.09110154850208957ITERATION : 69, loss : 0.09110154850208957ITERATION : 70, loss : 0.09110154850208957ITERATION : 71, loss : 0.09110154850208957ITERATION : 72, loss : 0.09110154850208957ITERATION : 73, loss : 0.09110154850208957ITERATION : 74, loss : 0.09110154850208957ITERATION : 75, loss : 0.09110154850208957ITERATION : 76, loss : 0.09110154850208957ITERATION : 77, loss : 0.09110154850208957ITERATION : 78, loss : 0.09110154850208957ITERATION : 79, loss : 0.09110154850208957ITERATION : 80, loss : 0.09110154850208957ITERATION : 81, loss : 0.09110154850208957ITERATION : 82, loss : 0.09110154850208957ITERATION : 83, loss : 0.09110154850208957ITERATION : 84, loss : 0.09110154850208957ITERATION : 85, loss : 0.09110154850208957ITERATION : 86, loss : 0.09110154850208957ITERATION : 87, loss : 0.09110154850208957ITERATION : 88, loss : 0.09110154850208957ITERATION : 89, loss : 0.09110154850208957ITERATION : 90, loss : 0.09110154850208957ITERATION : 91, loss : 0.09110154850208957ITERATION : 92, loss : 0.09110154850208957ITERATION : 93, loss : 0.09110154850208957ITERATION : 94, loss : 0.09110154850208957ITERATION : 95, loss : 0.09110154850208957ITERATION : 96, loss : 0.09110154850208957ITERATION : 97, loss : 0.09110154850208957ITERATION : 98, loss : 0.09110154850208957ITERATION : 99, loss : 0.09110154850208957ITERATION : 100, loss : 0.09110154850208957
ITERATION : 1, loss : 0.07614444108488853ITERATION : 2, loss : 0.09410024867744274ITERATION : 3, loss : 0.0979335019134516ITERATION : 4, loss : 0.10243692815210337ITERATION : 5, loss : 0.1062984959082299ITERATION : 6, loss : 0.1092425977809956ITERATION : 7, loss : 0.11136111171710565ITERATION : 8, loss : 0.1128385288810689ITERATION : 9, loss : 0.11385078639902771ITERATION : 10, loss : 0.11453730335725176ITERATION : 11, loss : 0.1150001473530015ITERATION : 12, loss : 0.1153111135040357ITERATION : 13, loss : 0.1155196168310209ITERATION : 14, loss : 0.1156592554731766ITERATION : 15, loss : 0.11575271201226589ITERATION : 16, loss : 0.11581523763160204ITERATION : 17, loss : 0.11585706230895909ITERATION : 18, loss : 0.11588503692437588ITERATION : 19, loss : 0.1159037480870006ITERATION : 20, loss : 0.11591626372517849ITERATION : 21, loss : 0.11592463565985439ITERATION : 22, loss : 0.11593023612807596ITERATION : 23, loss : 0.11593398288313239ITERATION : 24, loss : 0.11593648982612152ITERATION : 25, loss : 0.11593816735516557ITERATION : 26, loss : 0.11593928991820529ITERATION : 27, loss : 0.11594004118239022ITERATION : 28, loss : 0.11594054401880646ITERATION : 29, loss : 0.11594088062953031ITERATION : 30, loss : 0.1159411059011686ITERATION : 31, loss : 0.11594125672390744ITERATION : 32, loss : 0.11594135773829425ITERATION : 33, loss : 0.11594142531919499ITERATION : 34, loss : 0.11594147080389869ITERATION : 35, loss : 0.11594150109757612ITERATION : 36, loss : 0.11594152143686848ITERATION : 37, loss : 0.11594153503066731ITERATION : 38, loss : 0.11594154417327475ITERATION : 39, loss : 0.11594155026148767ITERATION : 40, loss : 0.11594155427945045ITERATION : 41, loss : 0.11594155694343959ITERATION : 42, loss : 0.11594155886020609ITERATION : 43, loss : 0.11594155981974635ITERATION : 44, loss : 0.11594156061469758ITERATION : 45, loss : 0.11594156120336879ITERATION : 46, loss : 0.11594156150161075ITERATION : 47, loss : 0.11594156150145724ITERATION : 48, loss : 0.11594156179873312ITERATION : 49, loss : 0.11594156181311127ITERATION : 50, loss : 0.11594156181311127ITERATION : 51, loss : 0.11594156181311127ITERATION : 52, loss : 0.11594156181311127ITERATION : 53, loss : 0.11594156181311127ITERATION : 54, loss : 0.11594156181311127ITERATION : 55, loss : 0.11594156181311127ITERATION : 56, loss : 0.11594156181311127ITERATION : 57, loss : 0.11594156181311127ITERATION : 58, loss : 0.11594156181311127ITERATION : 59, loss : 0.11594156181311127ITERATION : 60, loss : 0.11594156181311127ITERATION : 61, loss : 0.11594156181311127ITERATION : 62, loss : 0.11594156181311127ITERATION : 63, loss : 0.11594156181311127ITERATION : 64, loss : 0.11594156181311127ITERATION : 65, loss : 0.11594156181311127ITERATION : 66, loss : 0.11594156181311127ITERATION : 67, loss : 0.11594156181311127ITERATION : 68, loss : 0.11594156181311127ITERATION : 69, loss : 0.11594156181311127ITERATION : 70, loss : 0.11594156181311127ITERATION : 71, loss : 0.11594156181311127ITERATION : 72, loss : 0.11594156181311127ITERATION : 73, loss : 0.11594156181311127ITERATION : 74, loss : 0.11594156181311127ITERATION : 75, loss : 0.11594156181311127ITERATION : 76, loss : 0.11594156181311127ITERATION : 77, loss : 0.11594156181311127ITERATION : 78, loss : 0.11594156181311127ITERATION : 79, loss : 0.11594156181311127ITERATION : 80, loss : 0.11594156181311127ITERATION : 81, loss : 0.11594156181311127ITERATION : 82, loss : 0.11594156181311127ITERATION : 83, loss : 0.11594156181311127ITERATION : 84, loss : 0.11594156181311127ITERATION : 85, loss : 0.11594156181311127ITERATION : 86, loss : 0.11594156181311127ITERATION : 87, loss : 0.11594156181311127ITERATION : 88, loss : 0.11594156181311127ITERATION : 89, loss : 0.11594156181311127ITERATION : 90, loss : 0.11594156181311127ITERATION : 91, loss : 0.11594156181311127ITERATION : 92, loss : 0.11594156181311127ITERATION : 93, loss : 0.11594156181311127ITERATION : 94, loss : 0.11594156181311127ITERATION : 95, loss : 0.11594156181311127ITERATION : 96, loss : 0.11594156181311127ITERATION : 97, loss : 0.11594156181311127ITERATION : 98, loss : 0.11594156181311127ITERATION : 99, loss : 0.11594156181311127ITERATION : 100, loss : 0.11594156181311127
ITERATION : 1, loss : 0.023079611980220207ITERATION : 2, loss : 0.017661182977587404ITERATION : 3, loss : 0.014798692193888172ITERATION : 4, loss : 0.013112323654671942ITERATION : 5, loss : 0.012087817832400627ITERATION : 6, loss : 0.011453318142498297ITERATION : 7, loss : 0.011052922478158731ITERATION : 8, loss : 0.010795667285799905ITERATION : 9, loss : 0.010627752915808147ITERATION : 10, loss : 0.010516736603477074ITERATION : 11, loss : 0.01044260612575267ITERATION : 12, loss : 0.010392737908365066ITERATION : 13, loss : 0.0103590098554248ITERATION : 14, loss : 0.010336110027720995ITERATION : 15, loss : 0.010320519906887829ITERATION : 16, loss : 0.010309886177443162ITERATION : 17, loss : 0.010302623762502749ITERATION : 18, loss : 0.01029765947955366ITERATION : 19, loss : 0.010294264202725115ITERATION : 20, loss : 0.010291941172979975ITERATION : 21, loss : 0.010290351395155077ITERATION : 22, loss : 0.010289263342360204ITERATION : 23, loss : 0.010288518624114703ITERATION : 24, loss : 0.010288008907568282ITERATION : 25, loss : 0.010287660048425604ITERATION : 26, loss : 0.010287421278041766ITERATION : 27, loss : 0.010287257877281887ITERATION : 28, loss : 0.010287146065716445ITERATION : 29, loss : 0.01028706960903506ITERATION : 30, loss : 0.010287017257698558ITERATION : 31, loss : 0.010286981500255378ITERATION : 32, loss : 0.010286957032250603ITERATION : 33, loss : 0.010286940291256418ITERATION : 34, loss : 0.010286928821802316ITERATION : 35, loss : 0.010286920978323314ITERATION : 36, loss : 0.010286915647082976ITERATION : 37, loss : 0.010286912017581612ITERATION : 38, loss : 0.010286909554630005ITERATION : 39, loss : 0.01028690788069285ITERATION : 40, loss : 0.010286906750458036ITERATION : 41, loss : 0.01028690599472863ITERATION : 42, loss : 0.010286905421899991ITERATION : 43, loss : 0.010286905084308304ITERATION : 44, loss : 0.010286904823488952ITERATION : 45, loss : 0.010286904661971994ITERATION : 46, loss : 0.010286904558536527ITERATION : 47, loss : 0.01028690447858672ITERATION : 48, loss : 0.010286904451513012ITERATION : 49, loss : 0.010286904406594907ITERATION : 50, loss : 0.010286904405313083ITERATION : 51, loss : 0.010286904405313083ITERATION : 52, loss : 0.010286904405313083ITERATION : 53, loss : 0.010286904405313083ITERATION : 54, loss : 0.010286904405313083ITERATION : 55, loss : 0.010286904405313083ITERATION : 56, loss : 0.010286904405313083ITERATION : 57, loss : 0.010286904405313083ITERATION : 58, loss : 0.010286904405313083ITERATION : 59, loss : 0.010286904405313083ITERATION : 60, loss : 0.010286904405313083ITERATION : 61, loss : 0.010286904405313083ITERATION : 62, loss : 0.010286904405313083ITERATION : 63, loss : 0.010286904405313083ITERATION : 64, loss : 0.010286904405313083ITERATION : 65, loss : 0.010286904405313083ITERATION : 66, loss : 0.010286904405313083ITERATION : 67, loss : 0.010286904405313083ITERATION : 68, loss : 0.010286904405313083ITERATION : 69, loss : 0.010286904405313083ITERATION : 70, loss : 0.010286904405313083ITERATION : 71, loss : 0.010286904405313083ITERATION : 72, loss : 0.010286904405313083ITERATION : 73, loss : 0.010286904405313083ITERATION : 74, loss : 0.010286904405313083ITERATION : 75, loss : 0.010286904405313083ITERATION : 76, loss : 0.010286904405313083ITERATION : 77, loss : 0.010286904405313083ITERATION : 78, loss : 0.010286904405313083ITERATION : 79, loss : 0.010286904405313083ITERATION : 80, loss : 0.010286904405313083ITERATION : 81, loss : 0.010286904405313083ITERATION : 82, loss : 0.010286904405313083ITERATION : 83, loss : 0.010286904405313083ITERATION : 84, loss : 0.010286904405313083ITERATION : 85, loss : 0.010286904405313083ITERATION : 86, loss : 0.010286904405313083ITERATION : 87, loss : 0.010286904405313083ITERATION : 88, loss : 0.010286904405313083ITERATION : 89, loss : 0.010286904405313083ITERATION : 90, loss : 0.010286904405313083ITERATION : 91, loss : 0.010286904405313083ITERATION : 92, loss : 0.010286904405313083ITERATION : 93, loss : 0.010286904405313083ITERATION : 94, loss : 0.010286904405313083ITERATION : 95, loss : 0.010286904405313083ITERATION : 96, loss : 0.010286904405313083ITERATION : 97, loss : 0.010286904405313083ITERATION : 98, loss : 0.010286904405313083ITERATION : 99, loss : 0.010286904405313083ITERATION : 100, loss : 0.010286904405313083
ITERATION : 1, loss : 0.047337690654081555ITERATION : 2, loss : 0.07044918971050634ITERATION : 3, loss : 0.08312593722703357ITERATION : 4, loss : 0.09164809894072891ITERATION : 5, loss : 0.09736780975998915ITERATION : 6, loss : 0.101111214452502ITERATION : 7, loss : 0.10352747956530191ITERATION : 8, loss : 0.10507457369049145ITERATION : 9, loss : 0.10606012228834541ITERATION : 10, loss : 0.10668579616574833ITERATION : 11, loss : 0.10708204053590258ITERATION : 12, loss : 0.10733254107262835ITERATION : 13, loss : 0.1074906953323688ITERATION : 14, loss : 0.10759044673696892ITERATION : 15, loss : 0.10765331381715888ITERATION : 16, loss : 0.10769291115768126ITERATION : 17, loss : 0.10771783984859555ITERATION : 18, loss : 0.10773352761239025ITERATION : 19, loss : 0.1077433967278564ITERATION : 20, loss : 0.10774960347270675ITERATION : 21, loss : 0.10775350591624576ITERATION : 22, loss : 0.10775595890707398ITERATION : 23, loss : 0.10775750041925909ITERATION : 24, loss : 0.10775846890322277ITERATION : 25, loss : 0.10775907719699858ITERATION : 26, loss : 0.10775945918894829ITERATION : 27, loss : 0.1077596989595713ITERATION : 28, loss : 0.10775984942214341ITERATION : 29, loss : 0.10775994381788936ITERATION : 30, loss : 0.10776000300750471ITERATION : 31, loss : 0.1077600400973435ITERATION : 32, loss : 0.10776006334370081ITERATION : 33, loss : 0.1077600778996321ITERATION : 34, loss : 0.1077600870078672ITERATION : 35, loss : 0.10776009268371844ITERATION : 36, loss : 0.10776009626904885ITERATION : 37, loss : 0.10776009856422815ITERATION : 38, loss : 0.10776009994209938ITERATION : 39, loss : 0.10776010074991366ITERATION : 40, loss : 0.10776010130364617ITERATION : 41, loss : 0.10776010162174847ITERATION : 42, loss : 0.10776010179788997ITERATION : 43, loss : 0.10776010191891013ITERATION : 44, loss : 0.10776010197824716ITERATION : 45, loss : 0.10776010198291895ITERATION : 46, loss : 0.10776010198291895ITERATION : 47, loss : 0.10776010198291895ITERATION : 48, loss : 0.10776010198291895ITERATION : 49, loss : 0.10776010198291895ITERATION : 50, loss : 0.10776010198291895ITERATION : 51, loss : 0.10776010198291895ITERATION : 52, loss : 0.10776010198291895ITERATION : 53, loss : 0.10776010198291895ITERATION : 54, loss : 0.10776010198291895ITERATION : 55, loss : 0.10776010198291895ITERATION : 56, loss : 0.10776010198291895ITERATION : 57, loss : 0.10776010198291895ITERATION : 58, loss : 0.10776010198291895ITERATION : 59, loss : 0.10776010198291895ITERATION : 60, loss : 0.10776010198291895ITERATION : 61, loss : 0.10776010198291895ITERATION : 62, loss : 0.10776010198291895ITERATION : 63, loss : 0.10776010198291895ITERATION : 64, loss : 0.10776010198291895ITERATION : 65, loss : 0.10776010198291895ITERATION : 66, loss : 0.10776010198291895ITERATION : 67, loss : 0.10776010198291895ITERATION : 68, loss : 0.10776010198291895ITERATION : 69, loss : 0.10776010198291895ITERATION : 70, loss : 0.10776010198291895ITERATION : 71, loss : 0.10776010198291895ITERATION : 72, loss : 0.10776010198291895ITERATION : 73, loss : 0.10776010198291895ITERATION : 74, loss : 0.10776010198291895ITERATION : 75, loss : 0.10776010198291895ITERATION : 76, loss : 0.10776010198291895ITERATION : 77, loss : 0.10776010198291895ITERATION : 78, loss : 0.10776010198291895ITERATION : 79, loss : 0.10776010198291895ITERATION : 80, loss : 0.10776010198291895ITERATION : 81, loss : 0.10776010198291895ITERATION : 82, loss : 0.10776010198291895ITERATION : 83, loss : 0.10776010198291895ITERATION : 84, loss : 0.10776010198291895ITERATION : 85, loss : 0.10776010198291895ITERATION : 86, loss : 0.10776010198291895ITERATION : 87, loss : 0.10776010198291895ITERATION : 88, loss : 0.10776010198291895ITERATION : 89, loss : 0.10776010198291895ITERATION : 90, loss : 0.10776010198291895ITERATION : 91, loss : 0.10776010198291895ITERATION : 92, loss : 0.10776010198291895ITERATION : 93, loss : 0.10776010198291895ITERATION : 94, loss : 0.10776010198291895ITERATION : 95, loss : 0.10776010198291895ITERATION : 96, loss : 0.10776010198291895ITERATION : 97, loss : 0.10776010198291895ITERATION : 98, loss : 0.10776010198291895ITERATION : 99, loss : 0.10776010198291895ITERATION : 100, loss : 0.10776010198291895
ITERATION : 1, loss : 0.03523908161684359ITERATION : 2, loss : 0.04605010362507368ITERATION : 3, loss : 0.05837343626854443ITERATION : 4, loss : 0.06659052789866758ITERATION : 5, loss : 0.07227105937454355ITERATION : 6, loss : 0.07629283854301325ITERATION : 7, loss : 0.07918475689185697ITERATION : 8, loss : 0.0812861099075618ITERATION : 9, loss : 0.0828241867257878ITERATION : 10, loss : 0.08395583814920463ITERATION : 11, loss : 0.08479159222611134ITERATION : 12, loss : 0.08541051343142439ITERATION : 13, loss : 0.08586978232562166ITERATION : 14, loss : 0.08621108817517666ITERATION : 15, loss : 0.08646500810725383ITERATION : 16, loss : 0.08665406937863844ITERATION : 17, loss : 0.08679492319183563ITERATION : 18, loss : 0.08689990822976572ITERATION : 19, loss : 0.0869781839882429ITERATION : 20, loss : 0.08703655978549499ITERATION : 21, loss : 0.08708010243729011ITERATION : 22, loss : 0.08711258515178183ITERATION : 23, loss : 0.08713681940381197ITERATION : 24, loss : 0.08715490098214ITERATION : 25, loss : 0.08716839248516318ITERATION : 26, loss : 0.08717845965197533ITERATION : 27, loss : 0.08718597174708657ITERATION : 28, loss : 0.08719157728675592ITERATION : 29, loss : 0.08719576019811917ITERATION : 30, loss : 0.08719888154074605ITERATION : 31, loss : 0.08720121072520486ITERATION : 32, loss : 0.08720294877602544ITERATION : 33, loss : 0.08720424574139955ITERATION : 34, loss : 0.08720521355966344ITERATION : 35, loss : 0.08720593574554063ITERATION : 36, loss : 0.08720647465781838ITERATION : 37, loss : 0.08720687678931531ITERATION : 38, loss : 0.08720717685851416ITERATION : 39, loss : 0.08720740074086678ITERATION : 40, loss : 0.08720756783504442ITERATION : 41, loss : 0.08720769253075758ITERATION : 42, loss : 0.08720778556697646ITERATION : 43, loss : 0.08720785507113187ITERATION : 44, loss : 0.08720790683127795ITERATION : 45, loss : 0.08720794548053153ITERATION : 46, loss : 0.08720797428308852ITERATION : 47, loss : 0.08720799582796214ITERATION : 48, loss : 0.08720801184341677ITERATION : 49, loss : 0.08720802382476374ITERATION : 50, loss : 0.08720803271769088ITERATION : 51, loss : 0.08720803941244547ITERATION : 52, loss : 0.08720804433934536ITERATION : 53, loss : 0.08720804804937385ITERATION : 54, loss : 0.08720805082231722ITERATION : 55, loss : 0.08720805288573279ITERATION : 56, loss : 0.087208054431348ITERATION : 57, loss : 0.08720805557294746ITERATION : 58, loss : 0.0872080564327293ITERATION : 59, loss : 0.08720805701554764ITERATION : 60, loss : 0.08720805749110495ITERATION : 61, loss : 0.0872080578617387ITERATION : 62, loss : 0.08720805813801077ITERATION : 63, loss : 0.08720805825897172ITERATION : 64, loss : 0.08720805839549507ITERATION : 65, loss : 0.08720805852746041ITERATION : 66, loss : 0.0872080586166969ITERATION : 67, loss : 0.08720805861813596ITERATION : 68, loss : 0.08720805861813596ITERATION : 69, loss : 0.08720805861813596ITERATION : 70, loss : 0.08720805861813596ITERATION : 71, loss : 0.08720805861813596ITERATION : 72, loss : 0.08720805861813596ITERATION : 73, loss : 0.08720805861813596ITERATION : 74, loss : 0.08720805861813596ITERATION : 75, loss : 0.08720805861813596ITERATION : 76, loss : 0.08720805861813596ITERATION : 77, loss : 0.08720805861813596ITERATION : 78, loss : 0.08720805861813596ITERATION : 79, loss : 0.08720805861813596ITERATION : 80, loss : 0.08720805861813596ITERATION : 81, loss : 0.08720805861813596ITERATION : 82, loss : 0.08720805861813596ITERATION : 83, loss : 0.08720805861813596ITERATION : 84, loss : 0.08720805861813596ITERATION : 85, loss : 0.08720805861813596ITERATION : 86, loss : 0.08720805861813596ITERATION : 87, loss : 0.08720805861813596ITERATION : 88, loss : 0.08720805861813596ITERATION : 89, loss : 0.08720805861813596ITERATION : 90, loss : 0.08720805861813596ITERATION : 91, loss : 0.08720805861813596ITERATION : 92, loss : 0.08720805861813596ITERATION : 93, loss : 0.08720805861813596ITERATION : 94, loss : 0.08720805861813596ITERATION : 95, loss : 0.08720805861813596ITERATION : 96, loss : 0.08720805861813596ITERATION : 97, loss : 0.08720805861813596ITERATION : 98, loss : 0.08720805861813596ITERATION : 99, loss : 0.08720805861813596ITERATION : 100, loss : 0.08720805861813596
ITERATION : 1, loss : 0.018368557239166496ITERATION : 2, loss : 0.027290130297215907ITERATION : 3, loss : 0.03461083303907469ITERATION : 4, loss : 0.04009197538809819ITERATION : 5, loss : 0.044125070411923074ITERATION : 6, loss : 0.047074074095767984ITERATION : 7, loss : 0.04929591868923569ITERATION : 8, loss : 0.05103176504768608ITERATION : 9, loss : 0.052284580134059136ITERATION : 10, loss : 0.05318832767903561ITERATION : 11, loss : 0.053847108403433305ITERATION : 12, loss : 0.05432326285546756ITERATION : 13, loss : 0.054666786400521514ITERATION : 14, loss : 0.05491462757743965ITERATION : 15, loss : 0.055093444835600275ITERATION : 16, loss : 0.05522246931753862ITERATION : 17, loss : 0.05531557288436646ITERATION : 18, loss : 0.05538276176192984ITERATION : 19, loss : 0.05543125349281451ITERATION : 20, loss : 0.05546625466195826ITERATION : 21, loss : 0.05549152106634729ITERATION : 22, loss : 0.055509762214274345ITERATION : 23, loss : 0.05552293306458425ITERATION : 24, loss : 0.055532444074997196ITERATION : 25, loss : 0.055539313057910294ITERATION : 26, loss : 0.055544274569485706ITERATION : 27, loss : 0.05554785875894115ITERATION : 28, loss : 0.05555044829988117ITERATION : 29, loss : 0.05555231940756828ITERATION : 30, loss : 0.05555367167267767ITERATION : 31, loss : 0.055554649018319954ITERATION : 32, loss : 0.05555535552145021ITERATION : 33, loss : 0.055555866326132214ITERATION : 34, loss : 0.05555623567320364ITERATION : 35, loss : 0.05555650278019647ITERATION : 36, loss : 0.05555669596757708ITERATION : 37, loss : 0.05555683572909191ITERATION : 38, loss : 0.055556936826295374ITERATION : 39, loss : 0.05555700997802788ITERATION : 40, loss : 0.055557062916072715ITERATION : 41, loss : 0.05555710122826305ITERATION : 42, loss : 0.05555712895891429ITERATION : 43, loss : 0.055557149052670056ITERATION : 44, loss : 0.055557163585150435ITERATION : 45, loss : 0.05555717410201022ITERATION : 46, loss : 0.05555718171979707ITERATION : 47, loss : 0.055557187214723414ITERATION : 48, loss : 0.055557191157453736ITERATION : 49, loss : 0.05555719404415763ITERATION : 50, loss : 0.05555719613325857ITERATION : 51, loss : 0.05555719765238654ITERATION : 52, loss : 0.05555719875637589ITERATION : 53, loss : 0.05555719953299625ITERATION : 54, loss : 0.055557200106535434ITERATION : 55, loss : 0.05555720051220187ITERATION : 56, loss : 0.055557200764606005ITERATION : 57, loss : 0.05555720100217445ITERATION : 58, loss : 0.05555720118679505ITERATION : 59, loss : 0.055557201302060835ITERATION : 60, loss : 0.05555720139076024ITERATION : 61, loss : 0.05555720142755247ITERATION : 62, loss : 0.05555720143079443ITERATION : 63, loss : 0.05555720143079443ITERATION : 64, loss : 0.05555720143079443ITERATION : 65, loss : 0.05555720143079443ITERATION : 66, loss : 0.05555720143079443ITERATION : 67, loss : 0.05555720143079443ITERATION : 68, loss : 0.05555720143079443ITERATION : 69, loss : 0.05555720143079443ITERATION : 70, loss : 0.05555720143079443ITERATION : 71, loss : 0.05555720143079443ITERATION : 72, loss : 0.05555720143079443ITERATION : 73, loss : 0.05555720143079443ITERATION : 74, loss : 0.05555720143079443ITERATION : 75, loss : 0.05555720143079443ITERATION : 76, loss : 0.05555720143079443ITERATION : 77, loss : 0.05555720143079443ITERATION : 78, loss : 0.05555720143079443ITERATION : 79, loss : 0.05555720143079443ITERATION : 80, loss : 0.05555720143079443ITERATION : 81, loss : 0.05555720143079443ITERATION : 82, loss : 0.05555720143079443ITERATION : 83, loss : 0.05555720143079443ITERATION : 84, loss : 0.05555720143079443ITERATION : 85, loss : 0.05555720143079443ITERATION : 86, loss : 0.05555720143079443ITERATION : 87, loss : 0.05555720143079443ITERATION : 88, loss : 0.05555720143079443ITERATION : 89, loss : 0.05555720143079443ITERATION : 90, loss : 0.05555720143079443ITERATION : 91, loss : 0.05555720143079443ITERATION : 92, loss : 0.05555720143079443ITERATION : 93, loss : 0.05555720143079443ITERATION : 94, loss : 0.05555720143079443ITERATION : 95, loss : 0.05555720143079443ITERATION : 96, loss : 0.05555720143079443ITERATION : 97, loss : 0.05555720143079443ITERATION : 98, loss : 0.05555720143079443ITERATION : 99, loss : 0.05555720143079443ITERATION : 100, loss : 0.05555720143079443
ITERATION : 1, loss : 0.07947080478770685ITERATION : 2, loss : 0.11153006490904334ITERATION : 3, loss : 0.12827812126026789ITERATION : 4, loss : 0.13956311094898233ITERATION : 5, loss : 0.14738100530658185ITERATION : 6, loss : 0.15288338917171398ITERATION : 7, loss : 0.1567890961256252ITERATION : 8, loss : 0.15957379405518454ITERATION : 9, loss : 0.16156375689336436ITERATION : 10, loss : 0.1629874027171189ITERATION : 11, loss : 0.1640064380662323ITERATION : 12, loss : 0.1647360244101673ITERATION : 13, loss : 0.16525842642896912ITERATION : 14, loss : 0.16563249676424247ITERATION : 15, loss : 0.16590036555509594ITERATION : 16, loss : 0.16609219803830905ITERATION : 17, loss : 0.1662295921790907ITERATION : 18, loss : 0.16632801083261248ITERATION : 19, loss : 0.16639852336299737ITERATION : 20, loss : 0.1664490535119901ITERATION : 21, loss : 0.16648527312225558ITERATION : 22, loss : 0.16651124223240982ITERATION : 23, loss : 0.16652986751955634ITERATION : 24, loss : 0.16654323005011587ITERATION : 25, loss : 0.16655282019405027ITERATION : 26, loss : 0.1665597055087642ITERATION : 27, loss : 0.16656465044782318ITERATION : 28, loss : 0.16656820325388189ITERATION : 29, loss : 0.16657075694358273ITERATION : 30, loss : 0.16657259318377057ITERATION : 31, loss : 0.16657391409336744ITERATION : 32, loss : 0.16657486473064662ITERATION : 33, loss : 0.16657554897591892ITERATION : 34, loss : 0.1665760418221055ITERATION : 35, loss : 0.16657639696684381ITERATION : 36, loss : 0.1665766529622414ITERATION : 37, loss : 0.1665768375825489ITERATION : 38, loss : 0.1665769708634503ITERATION : 39, loss : 0.1665770670314979ITERATION : 40, loss : 0.16657713647202688ITERATION : 41, loss : 0.1665771866573868ITERATION : 42, loss : 0.1665772229066983ITERATION : 43, loss : 0.16657724911037114ITERATION : 44, loss : 0.1665772680782638ITERATION : 45, loss : 0.16657728183567636ITERATION : 46, loss : 0.1665772917720503ITERATION : 47, loss : 0.16657729896158607ITERATION : 48, loss : 0.1665773041524367ITERATION : 49, loss : 0.1665773079148615ITERATION : 50, loss : 0.1665773106074875ITERATION : 51, loss : 0.16657731255580308ITERATION : 52, loss : 0.16657731403405288ITERATION : 53, loss : 0.16657731512999166ITERATION : 54, loss : 0.16657731586323618ITERATION : 55, loss : 0.16657731646253704ITERATION : 56, loss : 0.1665773167781798ITERATION : 57, loss : 0.16657731716224106ITERATION : 58, loss : 0.16657731735965878ITERATION : 59, loss : 0.16657731745358448ITERATION : 60, loss : 0.16657731757197672ITERATION : 61, loss : 0.16657731766847186ITERATION : 62, loss : 0.1665773176695074ITERATION : 63, loss : 0.1665773176695074ITERATION : 64, loss : 0.1665773176695074ITERATION : 65, loss : 0.1665773176695074ITERATION : 66, loss : 0.1665773176695074ITERATION : 67, loss : 0.1665773176695074ITERATION : 68, loss : 0.1665773176695074ITERATION : 69, loss : 0.1665773176695074ITERATION : 70, loss : 0.1665773176695074ITERATION : 71, loss : 0.1665773176695074ITERATION : 72, loss : 0.1665773176695074ITERATION : 73, loss : 0.1665773176695074ITERATION : 74, loss : 0.1665773176695074ITERATION : 75, loss : 0.1665773176695074ITERATION : 76, loss : 0.1665773176695074ITERATION : 77, loss : 0.1665773176695074ITERATION : 78, loss : 0.1665773176695074ITERATION : 79, loss : 0.1665773176695074ITERATION : 80, loss : 0.1665773176695074ITERATION : 81, loss : 0.1665773176695074ITERATION : 82, loss : 0.1665773176695074ITERATION : 83, loss : 0.1665773176695074ITERATION : 84, loss : 0.1665773176695074ITERATION : 85, loss : 0.1665773176695074ITERATION : 86, loss : 0.1665773176695074ITERATION : 87, loss : 0.1665773176695074ITERATION : 88, loss : 0.1665773176695074ITERATION : 89, loss : 0.1665773176695074ITERATION : 90, loss : 0.1665773176695074ITERATION : 91, loss : 0.1665773176695074ITERATION : 92, loss : 0.1665773176695074ITERATION : 93, loss : 0.1665773176695074ITERATION : 94, loss : 0.1665773176695074ITERATION : 95, loss : 0.1665773176695074ITERATION : 96, loss : 0.1665773176695074ITERATION : 97, loss : 0.1665773176695074ITERATION : 98, loss : 0.1665773176695074ITERATION : 99, loss : 0.1665773176695074ITERATION : 100, loss : 0.1665773176695074
gradient norm in None layer : 0.000950327067514053
gradient norm in None layer : 3.069453943609106e-05
gradient norm in None layer : 6.176250597795311e-05
gradient norm in None layer : 0.000598981054558218
gradient norm in None layer : 4.136563624745231e-05
gradient norm in None layer : 6.083392193277488e-05
gradient norm in None layer : 0.0003166165084029406
gradient norm in None layer : 1.4147267548414916e-05
gradient norm in None layer : 1.5601541788862558e-05
gradient norm in None layer : 0.00028825143334604454
gradient norm in None layer : 1.60041554101887e-05
gradient norm in None layer : 1.5376118717250284e-05
gradient norm in None layer : 8.963398197939334e-05
gradient norm in None layer : 2.789822660265304e-06
gradient norm in None layer : 2.5911161024327693e-06
gradient norm in None layer : 8.526745966253149e-05
gradient norm in None layer : 3.1547172074877405e-06
gradient norm in None layer : 2.91337548944038e-06
gradient norm in None layer : 0.00010872726406760995
gradient norm in None layer : 2.189979765143549e-06
gradient norm in None layer : 0.00026043785277235173
gradient norm in None layer : 1.5499985805525473e-05
gradient norm in None layer : 1.4297044432371086e-05
gradient norm in None layer : 0.0002468894933596857
gradient norm in None layer : 2.208968214086393e-05
gradient norm in None layer : 2.7175951025616134e-05
gradient norm in None layer : 0.0004236142104717319
gradient norm in None layer : 5.936316931507262e-06
gradient norm in None layer : 0.0005543445718079424
gradient norm in None layer : 4.1711920662192446e-05
gradient norm in None layer : 5.020434210304146e-05
gradient norm in None layer : 0.0005845470453601452
gradient norm in None layer : 6.81800651794616e-05
gradient norm in None layer : 9.592017999908606e-05
gradient norm in None layer : 4.345882585327013e-05
gradient norm in None layer : 8.091864469954848e-06
Total gradient norm: 0.0015692349021711467
invariance loss : 4.578127447542199, avg_den : 0.47327423095703125, density loss : 0.37210910857645174, mse loss : 0.08594151322984606, solver time : 109.70768713951111 sec , total loss : 0.0908917497859647, running loss : 0.10175614015206978
Epoch 0/10 , batch 23/12500 
ITERATION : 1, loss : 0.014635022823423391ITERATION : 2, loss : 0.012424745635468996ITERATION : 3, loss : 0.013581558786221106ITERATION : 4, loss : 0.015359214019481848ITERATION : 5, loss : 0.01702491837143741ITERATION : 6, loss : 0.01836233926573648ITERATION : 7, loss : 0.01935398527615147ITERATION : 8, loss : 0.020055278326551396ITERATION : 9, loss : 0.020536830834083135ITERATION : 10, loss : 0.020861385171353948ITERATION : 11, loss : 0.021077552806712682ITERATION : 12, loss : 0.021220458198564423ITERATION : 13, loss : 0.02131449218639479ITERATION : 14, loss : 0.021376193097288373ITERATION : 15, loss : 0.021416611890938225ITERATION : 16, loss : 0.021443066350934074ITERATION : 17, loss : 0.02146037462737846ITERATION : 18, loss : 0.021471698513007516ITERATION : 19, loss : 0.021479108502552254ITERATION : 20, loss : 0.021483959013660035ITERATION : 21, loss : 0.021487135285747232ITERATION : 22, loss : 0.02148921634500852ITERATION : 23, loss : 0.02149058068124387ITERATION : 24, loss : 0.021491475527339144ITERATION : 25, loss : 0.021492062925598753ITERATION : 26, loss : 0.021492448649018038ITERATION : 27, loss : 0.02149270206613175ITERATION : 28, loss : 0.021492868724355005ITERATION : 29, loss : 0.021492978439535165ITERATION : 30, loss : 0.021493050722918162ITERATION : 31, loss : 0.021493098357825906ITERATION : 32, loss : 0.02149312978280194ITERATION : 33, loss : 0.021493150495485554ITERATION : 34, loss : 0.021493164207277822ITERATION : 35, loss : 0.021493173251112933ITERATION : 36, loss : 0.021493179259046914ITERATION : 37, loss : 0.021493183219952788ITERATION : 38, loss : 0.021493185834946728ITERATION : 39, loss : 0.02149318754707895ITERATION : 40, loss : 0.021493188688726447ITERATION : 41, loss : 0.02149318942426123ITERATION : 42, loss : 0.02149318988519131ITERATION : 43, loss : 0.02149319021999205ITERATION : 44, loss : 0.02149319044586112ITERATION : 45, loss : 0.02149319057180829ITERATION : 46, loss : 0.021493190651107656ITERATION : 47, loss : 0.021493190679648166ITERATION : 48, loss : 0.021493190679648166ITERATION : 49, loss : 0.021493190679648166ITERATION : 50, loss : 0.021493190679648166ITERATION : 51, loss : 0.021493190679648166ITERATION : 52, loss : 0.021493190679648166ITERATION : 53, loss : 0.021493190679648166ITERATION : 54, loss : 0.021493190679648166ITERATION : 55, loss : 0.021493190679648166ITERATION : 56, loss : 0.021493190679648166ITERATION : 57, loss : 0.021493190679648166ITERATION : 58, loss : 0.021493190679648166ITERATION : 59, loss : 0.021493190679648166ITERATION : 60, loss : 0.021493190679648166ITERATION : 61, loss : 0.021493190679648166ITERATION : 62, loss : 0.021493190679648166ITERATION : 63, loss : 0.021493190679648166ITERATION : 64, loss : 0.021493190679648166ITERATION : 65, loss : 0.021493190679648166ITERATION : 66, loss : 0.021493190679648166ITERATION : 67, loss : 0.021493190679648166ITERATION : 68, loss : 0.021493190679648166ITERATION : 69, loss : 0.021493190679648166ITERATION : 70, loss : 0.021493190679648166ITERATION : 71, loss : 0.021493190679648166ITERATION : 72, loss : 0.021493190679648166ITERATION : 73, loss : 0.021493190679648166ITERATION : 74, loss : 0.021493190679648166ITERATION : 75, loss : 0.021493190679648166ITERATION : 76, loss : 0.021493190679648166ITERATION : 77, loss : 0.021493190679648166ITERATION : 78, loss : 0.021493190679648166ITERATION : 79, loss : 0.021493190679648166ITERATION : 80, loss : 0.021493190679648166ITERATION : 81, loss : 0.021493190679648166ITERATION : 82, loss : 0.021493190679648166ITERATION : 83, loss : 0.021493190679648166ITERATION : 84, loss : 0.021493190679648166ITERATION : 85, loss : 0.021493190679648166ITERATION : 86, loss : 0.021493190679648166ITERATION : 87, loss : 0.021493190679648166ITERATION : 88, loss : 0.021493190679648166ITERATION : 89, loss : 0.021493190679648166ITERATION : 90, loss : 0.021493190679648166ITERATION : 91, loss : 0.021493190679648166ITERATION : 92, loss : 0.021493190679648166ITERATION : 93, loss : 0.021493190679648166ITERATION : 94, loss : 0.021493190679648166ITERATION : 95, loss : 0.021493190679648166ITERATION : 96, loss : 0.021493190679648166ITERATION : 97, loss : 0.021493190679648166ITERATION : 98, loss : 0.021493190679648166ITERATION : 99, loss : 0.021493190679648166ITERATION : 100, loss : 0.021493190679648166
ITERATION : 1, loss : 0.019363539105178403ITERATION : 2, loss : 0.02098782282359595ITERATION : 3, loss : 0.024514824893843476ITERATION : 4, loss : 0.027366678577658572ITERATION : 5, loss : 0.029444288291691437ITERATION : 6, loss : 0.03089068664791484ITERATION : 7, loss : 0.031874363751355425ITERATION : 8, loss : 0.0325347190626809ITERATION : 9, loss : 0.03297468405635281ITERATION : 10, loss : 0.03326646432090399ITERATION : 11, loss : 0.03345939663739532ITERATION : 12, loss : 0.03358670879418959ITERATION : 13, loss : 0.033670593448424366ITERATION : 14, loss : 0.03372579792347633ITERATION : 15, loss : 0.033762090745197385ITERATION : 16, loss : 0.03378592829408188ITERATION : 17, loss : 0.03380157112564445ITERATION : 18, loss : 0.033811827261448844ITERATION : 19, loss : 0.033818545535209237ITERATION : 20, loss : 0.033822942406801705ITERATION : 21, loss : 0.0338258170993008ITERATION : 22, loss : 0.03382769473681528ITERATION : 23, loss : 0.03382891978317722ITERATION : 24, loss : 0.033829718109080996ITERATION : 25, loss : 0.033830237702226054ITERATION : 26, loss : 0.03383057542402164ITERATION : 27, loss : 0.03383079460613094ITERATION : 28, loss : 0.03383093663094981ITERATION : 29, loss : 0.03383102851379643ITERATION : 30, loss : 0.03383108782359021ITERATION : 31, loss : 0.03383112600454613ITERATION : 32, loss : 0.0338311505636004ITERATION : 33, loss : 0.033831166331179004ITERATION : 34, loss : 0.03383117641960515ITERATION : 35, loss : 0.03383118287208499ITERATION : 36, loss : 0.03383118695677104ITERATION : 37, loss : 0.033831189508136586ITERATION : 38, loss : 0.0338311911305804ITERATION : 39, loss : 0.033831192195991656ITERATION : 40, loss : 0.03383119283741636ITERATION : 41, loss : 0.03383119321153782ITERATION : 42, loss : 0.03383119342918435ITERATION : 43, loss : 0.03383119354815034ITERATION : 44, loss : 0.033831193628629935ITERATION : 45, loss : 0.03383119365630034ITERATION : 46, loss : 0.03383119366460657ITERATION : 47, loss : 0.033831193683807895ITERATION : 48, loss : 0.0338311936895616ITERATION : 49, loss : 0.03383119369032056ITERATION : 50, loss : 0.03383119369140851ITERATION : 51, loss : 0.03383119369137197ITERATION : 52, loss : 0.03383119369137197ITERATION : 53, loss : 0.03383119369137197ITERATION : 54, loss : 0.03383119369137197ITERATION : 55, loss : 0.03383119369137197ITERATION : 56, loss : 0.03383119369137197ITERATION : 57, loss : 0.03383119369137197ITERATION : 58, loss : 0.03383119369137197ITERATION : 59, loss : 0.03383119369137197ITERATION : 60, loss : 0.03383119369137197ITERATION : 61, loss : 0.03383119369137197ITERATION : 62, loss : 0.03383119369137197ITERATION : 63, loss : 0.03383119369137197ITERATION : 64, loss : 0.03383119369137197ITERATION : 65, loss : 0.03383119369137197ITERATION : 66, loss : 0.03383119369137197ITERATION : 67, loss : 0.03383119369137197ITERATION : 68, loss : 0.03383119369137197ITERATION : 69, loss : 0.03383119369137197ITERATION : 70, loss : 0.03383119369137197ITERATION : 71, loss : 0.03383119369137197ITERATION : 72, loss : 0.03383119369137197ITERATION : 73, loss : 0.03383119369137197ITERATION : 74, loss : 0.03383119369137197ITERATION : 75, loss : 0.03383119369137197ITERATION : 76, loss : 0.03383119369137197ITERATION : 77, loss : 0.03383119369137197ITERATION : 78, loss : 0.03383119369137197ITERATION : 79, loss : 0.03383119369137197ITERATION : 80, loss : 0.03383119369137197ITERATION : 81, loss : 0.03383119369137197ITERATION : 82, loss : 0.03383119369137197ITERATION : 83, loss : 0.03383119369137197ITERATION : 84, loss : 0.03383119369137197ITERATION : 85, loss : 0.03383119369137197ITERATION : 86, loss : 0.03383119369137197ITERATION : 87, loss : 0.03383119369137197ITERATION : 88, loss : 0.03383119369137197ITERATION : 89, loss : 0.03383119369137197ITERATION : 90, loss : 0.03383119369137197ITERATION : 91, loss : 0.03383119369137197ITERATION : 92, loss : 0.03383119369137197ITERATION : 93, loss : 0.03383119369137197ITERATION : 94, loss : 0.03383119369137197ITERATION : 95, loss : 0.03383119369137197ITERATION : 96, loss : 0.03383119369137197ITERATION : 97, loss : 0.03383119369137197ITERATION : 98, loss : 0.03383119369137197ITERATION : 99, loss : 0.03383119369137197ITERATION : 100, loss : 0.03383119369137197
ITERATION : 1, loss : 0.025529155544545545ITERATION : 2, loss : 0.02440467354920528ITERATION : 3, loss : 0.03246728696443118ITERATION : 4, loss : 0.04012134405523893ITERATION : 5, loss : 0.04583705966936862ITERATION : 6, loss : 0.04991130300919256ITERATION : 7, loss : 0.05276965288552864ITERATION : 8, loss : 0.05476525855546783ITERATION : 9, loss : 0.05615779377412495ITERATION : 10, loss : 0.05713062762594667ITERATION : 11, loss : 0.057811421518390936ITERATION : 12, loss : 0.058288703360859215ITERATION : 13, loss : 0.05862387882334873ITERATION : 14, loss : 0.05885962058183433ITERATION : 15, loss : 0.05902565307216557ITERATION : 16, loss : 0.05914273034803104ITERATION : 17, loss : 0.059225374881205405ITERATION : 18, loss : 0.059283768432885305ITERATION : 19, loss : 0.059325061771176836ITERATION : 20, loss : 0.05935428450604307ITERATION : 21, loss : 0.059374978764657346ITERATION : 22, loss : 0.0593896423987027ITERATION : 23, loss : 0.059400038446354185ITERATION : 24, loss : 0.059407412502739294ITERATION : 25, loss : 0.059412645247278795ITERATION : 26, loss : 0.05941635993287345ITERATION : 27, loss : 0.05941899791804474ITERATION : 28, loss : 0.05942087191345107ITERATION : 29, loss : 0.05942220350390555ITERATION : 30, loss : 0.059423149960250145ITERATION : 31, loss : 0.05942382282436837ITERATION : 32, loss : 0.059424301289708666ITERATION : 33, loss : 0.05942464156886288ITERATION : 34, loss : 0.05942488362461922ITERATION : 35, loss : 0.059425055841485996ITERATION : 36, loss : 0.05942517836255204ITERATION : 37, loss : 0.059425265557759364ITERATION : 38, loss : 0.05942532760924042ITERATION : 39, loss : 0.0594253717738674ITERATION : 40, loss : 0.05942540323939655ITERATION : 41, loss : 0.059425425592833814ITERATION : 42, loss : 0.059425441583545444ITERATION : 43, loss : 0.05942545290068065ITERATION : 44, loss : 0.059425460960460835ITERATION : 45, loss : 0.059425466688209694ITERATION : 46, loss : 0.059425470785319935ITERATION : 47, loss : 0.05942547367961851ITERATION : 48, loss : 0.05942547576723309ITERATION : 49, loss : 0.059425477208677815ITERATION : 50, loss : 0.0594254782642984ITERATION : 51, loss : 0.05942547898647563ITERATION : 52, loss : 0.05942547953097266ITERATION : 53, loss : 0.059425479921389926ITERATION : 54, loss : 0.05942548014095408ITERATION : 55, loss : 0.05942548032586938ITERATION : 56, loss : 0.05942548049153937ITERATION : 57, loss : 0.05942548059872979ITERATION : 58, loss : 0.05942548060005766ITERATION : 59, loss : 0.05942548066904716ITERATION : 60, loss : 0.05942548067125926ITERATION : 61, loss : 0.05942548067125926ITERATION : 62, loss : 0.05942548067125926ITERATION : 63, loss : 0.05942548067125926ITERATION : 64, loss : 0.05942548067125926ITERATION : 65, loss : 0.05942548067125926ITERATION : 66, loss : 0.05942548067125926ITERATION : 67, loss : 0.05942548067125926ITERATION : 68, loss : 0.05942548067125926ITERATION : 69, loss : 0.05942548067125926ITERATION : 70, loss : 0.05942548067125926ITERATION : 71, loss : 0.05942548067125926ITERATION : 72, loss : 0.05942548067125926ITERATION : 73, loss : 0.05942548067125926ITERATION : 74, loss : 0.05942548067125926ITERATION : 75, loss : 0.05942548067125926ITERATION : 76, loss : 0.05942548067125926ITERATION : 77, loss : 0.05942548067125926ITERATION : 78, loss : 0.05942548067125926ITERATION : 79, loss : 0.05942548067125926ITERATION : 80, loss : 0.05942548067125926ITERATION : 81, loss : 0.05942548067125926ITERATION : 82, loss : 0.05942548067125926ITERATION : 83, loss : 0.05942548067125926ITERATION : 84, loss : 0.05942548067125926ITERATION : 85, loss : 0.05942548067125926ITERATION : 86, loss : 0.05942548067125926ITERATION : 87, loss : 0.05942548067125926ITERATION : 88, loss : 0.05942548067125926ITERATION : 89, loss : 0.05942548067125926ITERATION : 90, loss : 0.05942548067125926ITERATION : 91, loss : 0.05942548067125926ITERATION : 92, loss : 0.05942548067125926ITERATION : 93, loss : 0.05942548067125926ITERATION : 94, loss : 0.05942548067125926ITERATION : 95, loss : 0.05942548067125926ITERATION : 96, loss : 0.05942548067125926ITERATION : 97, loss : 0.05942548067125926ITERATION : 98, loss : 0.05942548067125926ITERATION : 99, loss : 0.05942548067125926ITERATION : 100, loss : 0.05942548067125926
ITERATION : 1, loss : 0.014926490889868896ITERATION : 2, loss : 0.014828245580984343ITERATION : 3, loss : 0.01580976221414461ITERATION : 4, loss : 0.017931209330498278ITERATION : 5, loss : 0.020186802603547663ITERATION : 6, loss : 0.022169587778731265ITERATION : 7, loss : 0.02380125678758278ITERATION : 8, loss : 0.02510382308742667ITERATION : 9, loss : 0.02612621758311901ITERATION : 10, loss : 0.026920349159333196ITERATION : 11, loss : 0.02753294852673237ITERATION : 12, loss : 0.028003289317424403ITERATION : 13, loss : 0.028363210268876567ITERATION : 14, loss : 0.028637978476056467ITERATION : 15, loss : 0.028847376364792586ITERATION : 16, loss : 0.029006752253632164ITERATION : 17, loss : 0.02912794068739721ITERATION : 18, loss : 0.02922002626845269ITERATION : 19, loss : 0.02928996064295016ITERATION : 20, loss : 0.02934305093707282ITERATION : 21, loss : 0.029383341852561824ITERATION : 22, loss : 0.029413912090885265ITERATION : 23, loss : 0.029437102739364424ITERATION : 24, loss : 0.029454692841846424ITERATION : 25, loss : 0.029468033550992267ITERATION : 26, loss : 0.029478150597436224ITERATION : 27, loss : 0.029485822466165518ITERATION : 28, loss : 0.029491639843493324ITERATION : 29, loss : 0.02949605085183823ITERATION : 30, loss : 0.02949939536760608ITERATION : 31, loss : 0.029501931174624733ITERATION : 32, loss : 0.029503853805812965ITERATION : 33, loss : 0.029505311472747268ITERATION : 34, loss : 0.02950641661905105ITERATION : 35, loss : 0.029507254485287262ITERATION : 36, loss : 0.02950788972011174ITERATION : 37, loss : 0.02950837129811413ITERATION : 38, loss : 0.029508736407925135ITERATION : 39, loss : 0.029509013258534417ITERATION : 40, loss : 0.029509223121268184ITERATION : 41, loss : 0.029509382216897175ITERATION : 42, loss : 0.029509502856511147ITERATION : 43, loss : 0.029509594283910617ITERATION : 44, loss : 0.029509663632264826ITERATION : 45, loss : 0.029509716174451726ITERATION : 46, loss : 0.029509756038520633ITERATION : 47, loss : 0.029509786202010776ITERATION : 48, loss : 0.029509809120675225ITERATION : 49, loss : 0.02950982646492276ITERATION : 50, loss : 0.029509839631633044ITERATION : 51, loss : 0.029509849594194663ITERATION : 52, loss : 0.029509857168043215ITERATION : 53, loss : 0.02950986290229598ITERATION : 54, loss : 0.02950986724935829ITERATION : 55, loss : 0.029509870549707374ITERATION : 56, loss : 0.029509873035991358ITERATION : 57, loss : 0.029509874935751548ITERATION : 58, loss : 0.029509876334200647ITERATION : 59, loss : 0.029509877430228957ITERATION : 60, loss : 0.029509878241479312ITERATION : 61, loss : 0.02950987885117954ITERATION : 62, loss : 0.02950987932621682ITERATION : 63, loss : 0.0295098796638305ITERATION : 64, loss : 0.02950987989989534ITERATION : 65, loss : 0.02950988008847206ITERATION : 66, loss : 0.029509880181892753ITERATION : 67, loss : 0.0295098802636634ITERATION : 68, loss : 0.02950988026468626ITERATION : 69, loss : 0.02950988026468626ITERATION : 70, loss : 0.02950988026468626ITERATION : 71, loss : 0.02950988026468626ITERATION : 72, loss : 0.02950988026468626ITERATION : 73, loss : 0.02950988026468626ITERATION : 74, loss : 0.02950988026468626ITERATION : 75, loss : 0.02950988026468626ITERATION : 76, loss : 0.02950988026468626ITERATION : 77, loss : 0.02950988026468626ITERATION : 78, loss : 0.02950988026468626ITERATION : 79, loss : 0.02950988026468626ITERATION : 80, loss : 0.02950988026468626ITERATION : 81, loss : 0.02950988026468626ITERATION : 82, loss : 0.02950988026468626ITERATION : 83, loss : 0.02950988026468626ITERATION : 84, loss : 0.02950988026468626ITERATION : 85, loss : 0.02950988026468626ITERATION : 86, loss : 0.02950988026468626ITERATION : 87, loss : 0.02950988026468626ITERATION : 88, loss : 0.02950988026468626ITERATION : 89, loss : 0.02950988026468626ITERATION : 90, loss : 0.02950988026468626ITERATION : 91, loss : 0.02950988026468626ITERATION : 92, loss : 0.02950988026468626ITERATION : 93, loss : 0.02950988026468626ITERATION : 94, loss : 0.02950988026468626ITERATION : 95, loss : 0.02950988026468626ITERATION : 96, loss : 0.02950988026468626ITERATION : 97, loss : 0.02950988026468626ITERATION : 98, loss : 0.02950988026468626ITERATION : 99, loss : 0.02950988026468626ITERATION : 100, loss : 0.02950988026468626
ITERATION : 1, loss : 0.03139510393491736ITERATION : 2, loss : 0.028787994067806463ITERATION : 3, loss : 0.029907440064300534ITERATION : 4, loss : 0.031196203178746128ITERATION : 5, loss : 0.032236541842826476ITERATION : 6, loss : 0.033014405317293806ITERATION : 7, loss : 0.033575208452493774ITERATION : 8, loss : 0.033957894630015154ITERATION : 9, loss : 0.03421466505269657ITERATION : 10, loss : 0.03438472429249585ITERATION : 11, loss : 0.03449612582434328ITERATION : 12, loss : 0.034568375801978586ITERATION : 13, loss : 0.03461477858743776ITERATION : 14, loss : 0.03464428091916296ITERATION : 15, loss : 0.034662832082442356ITERATION : 16, loss : 0.034674350562482754ITERATION : 17, loss : 0.034681395175489155ITERATION : 18, loss : 0.03468562310980124ITERATION : 19, loss : 0.03468809872820151ITERATION : 20, loss : 0.034689499616523845ITERATION : 21, loss : 0.03469025292252561ITERATION : 22, loss : 0.034690624920684934ITERATION : 23, loss : 0.03469077935737437ITERATION : 24, loss : 0.03469081526009341ITERATION : 25, loss : 0.03469079126115589ITERATION : 26, loss : 0.03469074107360771ITERATION : 27, loss : 0.0346906832601118ITERATION : 28, loss : 0.034690627419874165ITERATION : 29, loss : 0.03469057790474701ITERATION : 30, loss : 0.03469053617674369ITERATION : 31, loss : 0.03469050219793411ITERATION : 32, loss : 0.03469047515917986ITERATION : 33, loss : 0.0346904540733031ITERATION : 34, loss : 0.0346904378441383ITERATION : 35, loss : 0.034690425492344246ITERATION : 36, loss : 0.034690416193435436ITERATION : 37, loss : 0.03469040922241478ITERATION : 38, loss : 0.034690404077590914ITERATION : 39, loss : 0.034690400258248225ITERATION : 40, loss : 0.03469039742942757ITERATION : 41, loss : 0.034690395378738595ITERATION : 42, loss : 0.03469039389775179ITERATION : 43, loss : 0.03469039282648637ITERATION : 44, loss : 0.03469039204338031ITERATION : 45, loss : 0.034690391477131696ITERATION : 46, loss : 0.03469039106800326ITERATION : 47, loss : 0.03469039078214924ITERATION : 48, loss : 0.03469039054395095ITERATION : 49, loss : 0.034690390399061584ITERATION : 50, loss : 0.03469039028417583ITERATION : 51, loss : 0.0346903901902586ITERATION : 52, loss : 0.03469039013565389ITERATION : 53, loss : 0.034690390104903164ITERATION : 54, loss : 0.03469039006927085ITERATION : 55, loss : 0.03469039005669625ITERATION : 56, loss : 0.03469039005642473ITERATION : 57, loss : 0.03469039005642473ITERATION : 58, loss : 0.03469039005642473ITERATION : 59, loss : 0.03469039005642473ITERATION : 60, loss : 0.03469039005642473ITERATION : 61, loss : 0.03469039005642473ITERATION : 62, loss : 0.03469039005642473ITERATION : 63, loss : 0.03469039005642473ITERATION : 64, loss : 0.03469039005642473ITERATION : 65, loss : 0.03469039005642473ITERATION : 66, loss : 0.03469039005642473ITERATION : 67, loss : 0.03469039005642473ITERATION : 68, loss : 0.03469039005642473ITERATION : 69, loss : 0.03469039005642473ITERATION : 70, loss : 0.03469039005642473ITERATION : 71, loss : 0.03469039005642473ITERATION : 72, loss : 0.03469039005642473ITERATION : 73, loss : 0.03469039005642473ITERATION : 74, loss : 0.03469039005642473ITERATION : 75, loss : 0.03469039005642473ITERATION : 76, loss : 0.03469039005642473ITERATION : 77, loss : 0.03469039005642473ITERATION : 78, loss : 0.03469039005642473ITERATION : 79, loss : 0.03469039005642473ITERATION : 80, loss : 0.03469039005642473ITERATION : 81, loss : 0.03469039005642473ITERATION : 82, loss : 0.03469039005642473ITERATION : 83, loss : 0.03469039005642473ITERATION : 84, loss : 0.03469039005642473ITERATION : 85, loss : 0.03469039005642473ITERATION : 86, loss : 0.03469039005642473ITERATION : 87, loss : 0.03469039005642473ITERATION : 88, loss : 0.03469039005642473ITERATION : 89, loss : 0.03469039005642473ITERATION : 90, loss : 0.03469039005642473ITERATION : 91, loss : 0.03469039005642473ITERATION : 92, loss : 0.03469039005642473ITERATION : 93, loss : 0.03469039005642473ITERATION : 94, loss : 0.03469039005642473ITERATION : 95, loss : 0.03469039005642473ITERATION : 96, loss : 0.03469039005642473ITERATION : 97, loss : 0.03469039005642473ITERATION : 98, loss : 0.03469039005642473ITERATION : 99, loss : 0.03469039005642473ITERATION : 100, loss : 0.03469039005642473
ITERATION : 1, loss : 0.02073326240804212ITERATION : 2, loss : 0.017734591853283535ITERATION : 3, loss : 0.016719606980184275ITERATION : 4, loss : 0.01631156478459904ITERATION : 5, loss : 0.016145004475773668ITERATION : 6, loss : 0.016093383849039772ITERATION : 7, loss : 0.0160961826745055ITERATION : 8, loss : 0.016121245519809348ITERATION : 9, loss : 0.016152113794365475ITERATION : 10, loss : 0.016181212094413963ITERATION : 11, loss : 0.016205710128115086ITERATION : 12, loss : 0.01622510227463248ITERATION : 13, loss : 0.016239882007837127ITERATION : 14, loss : 0.016250869033720992ITERATION : 15, loss : 0.016258897758474288ITERATION : 16, loss : 0.01626469320067771ITERATION : 17, loss : 0.0162688392188969ITERATION : 18, loss : 0.016271785587049406ITERATION : 19, loss : 0.01627386877796467ITERATION : 20, loss : 0.016275335847643554ITERATION : 21, loss : 0.016276365819751157ITERATION : 22, loss : 0.016277087159413197ITERATION : 23, loss : 0.016277591323607697ITERATION : 24, loss : 0.016277943057198995ITERATION : 25, loss : 0.016278188059022963ITERATION : 26, loss : 0.016278358495707246ITERATION : 27, loss : 0.016278477000167494ITERATION : 28, loss : 0.016278559287083237ITERATION : 29, loss : 0.01627861639265555ITERATION : 30, loss : 0.01627865599885138ITERATION : 31, loss : 0.01627868336563633ITERATION : 32, loss : 0.016278702318744082ITERATION : 33, loss : 0.016278715404787612ITERATION : 34, loss : 0.01627872451521897ITERATION : 35, loss : 0.01627873078456833ITERATION : 36, loss : 0.016278735180151074ITERATION : 37, loss : 0.0162787381953457ITERATION : 38, loss : 0.01627874034925058ITERATION : 39, loss : 0.01627874178189869ITERATION : 40, loss : 0.0162787426287031ITERATION : 41, loss : 0.016278743392632803ITERATION : 42, loss : 0.016278743863363258ITERATION : 43, loss : 0.016278744239790893ITERATION : 44, loss : 0.01627874438848644ITERATION : 45, loss : 0.01627874453776671ITERATION : 46, loss : 0.01627874453776671ITERATION : 47, loss : 0.01627874453776671ITERATION : 48, loss : 0.01627874453776671ITERATION : 49, loss : 0.01627874453776671ITERATION : 50, loss : 0.01627874453776671ITERATION : 51, loss : 0.01627874453776671ITERATION : 52, loss : 0.01627874453776671ITERATION : 53, loss : 0.01627874453776671ITERATION : 54, loss : 0.01627874453776671ITERATION : 55, loss : 0.01627874453776671ITERATION : 56, loss : 0.01627874453776671ITERATION : 57, loss : 0.01627874453776671ITERATION : 58, loss : 0.01627874453776671ITERATION : 59, loss : 0.01627874453776671ITERATION : 60, loss : 0.01627874453776671ITERATION : 61, loss : 0.01627874453776671ITERATION : 62, loss : 0.01627874453776671ITERATION : 63, loss : 0.01627874453776671ITERATION : 64, loss : 0.01627874453776671ITERATION : 65, loss : 0.01627874453776671ITERATION : 66, loss : 0.01627874453776671ITERATION : 67, loss : 0.01627874453776671ITERATION : 68, loss : 0.01627874453776671ITERATION : 69, loss : 0.01627874453776671ITERATION : 70, loss : 0.01627874453776671ITERATION : 71, loss : 0.01627874453776671ITERATION : 72, loss : 0.01627874453776671ITERATION : 73, loss : 0.01627874453776671ITERATION : 74, loss : 0.01627874453776671ITERATION : 75, loss : 0.01627874453776671ITERATION : 76, loss : 0.01627874453776671ITERATION : 77, loss : 0.01627874453776671ITERATION : 78, loss : 0.01627874453776671ITERATION : 79, loss : 0.01627874453776671ITERATION : 80, loss : 0.01627874453776671ITERATION : 81, loss : 0.01627874453776671ITERATION : 82, loss : 0.01627874453776671ITERATION : 83, loss : 0.01627874453776671ITERATION : 84, loss : 0.01627874453776671ITERATION : 85, loss : 0.01627874453776671ITERATION : 86, loss : 0.01627874453776671ITERATION : 87, loss : 0.01627874453776671ITERATION : 88, loss : 0.01627874453776671ITERATION : 89, loss : 0.01627874453776671ITERATION : 90, loss : 0.01627874453776671ITERATION : 91, loss : 0.01627874453776671ITERATION : 92, loss : 0.01627874453776671ITERATION : 93, loss : 0.01627874453776671ITERATION : 94, loss : 0.01627874453776671ITERATION : 95, loss : 0.01627874453776671ITERATION : 96, loss : 0.01627874453776671ITERATION : 97, loss : 0.01627874453776671ITERATION : 98, loss : 0.01627874453776671ITERATION : 99, loss : 0.01627874453776671ITERATION : 100, loss : 0.01627874453776671
ITERATION : 1, loss : 0.09319545063027443ITERATION : 2, loss : 0.10204207475943194ITERATION : 3, loss : 0.10944387015694598ITERATION : 4, loss : 0.11890312109182916ITERATION : 5, loss : 0.1247610209203398ITERATION : 6, loss : 0.1284300618870244ITERATION : 7, loss : 0.13074948109459603ITERATION : 8, loss : 0.13222520098296828ITERATION : 9, loss : 0.13316808270110667ITERATION : 10, loss : 0.13377211938202438ITERATION : 11, loss : 0.13415971326144294ITERATION : 12, loss : 0.13440866389678888ITERATION : 13, loss : 0.13456865428563297ITERATION : 14, loss : 0.13467150535250674ITERATION : 15, loss : 0.13473763417319798ITERATION : 16, loss : 0.1347801548459331ITERATION : 17, loss : 0.1348074957467539ITERATION : 18, loss : 0.1348250755924729ITERATION : 19, loss : 0.13483637874898682ITERATION : 20, loss : 0.13484364586005815ITERATION : 21, loss : 0.13484831780635126ITERATION : 22, loss : 0.1348513212015124ITERATION : 23, loss : 0.13485325182282468ITERATION : 24, loss : 0.13485449276948006ITERATION : 25, loss : 0.13485529033398772ITERATION : 26, loss : 0.1348558029594332ITERATION : 27, loss : 0.13485613237609584ITERATION : 28, loss : 0.13485634406534022ITERATION : 29, loss : 0.13485648011995977ITERATION : 30, loss : 0.13485656751906794ITERATION : 31, loss : 0.1348566236695033ITERATION : 32, loss : 0.13485665975020833ITERATION : 33, loss : 0.13485668294168146ITERATION : 34, loss : 0.13485669783564436ITERATION : 35, loss : 0.13485670739095584ITERATION : 36, loss : 0.13485671352533576ITERATION : 37, loss : 0.1348567174657013ITERATION : 38, loss : 0.1348567200079719ITERATION : 39, loss : 0.1348567216201889ITERATION : 40, loss : 0.1348567226438703ITERATION : 41, loss : 0.1348567233219381ITERATION : 42, loss : 0.13485672374455732ITERATION : 43, loss : 0.13485672402047338ITERATION : 44, loss : 0.1348567241979214ITERATION : 45, loss : 0.13485672429599171ITERATION : 46, loss : 0.13485672437460025ITERATION : 47, loss : 0.13485672440295834ITERATION : 48, loss : 0.13485672441670915ITERATION : 49, loss : 0.13485672443898725ITERATION : 50, loss : 0.13485672444824756ITERATION : 51, loss : 0.13485672444824756ITERATION : 52, loss : 0.13485672444824756ITERATION : 53, loss : 0.13485672444824756ITERATION : 54, loss : 0.13485672444824756ITERATION : 55, loss : 0.13485672444824756ITERATION : 56, loss : 0.13485672444824756ITERATION : 57, loss : 0.13485672444824756ITERATION : 58, loss : 0.13485672444824756ITERATION : 59, loss : 0.13485672444824756ITERATION : 60, loss : 0.13485672444824756ITERATION : 61, loss : 0.13485672444824756ITERATION : 62, loss : 0.13485672444824756ITERATION : 63, loss : 0.13485672444824756ITERATION : 64, loss : 0.13485672444824756ITERATION : 65, loss : 0.13485672444824756ITERATION : 66, loss : 0.13485672444824756ITERATION : 67, loss : 0.13485672444824756ITERATION : 68, loss : 0.13485672444824756ITERATION : 69, loss : 0.13485672444824756ITERATION : 70, loss : 0.13485672444824756ITERATION : 71, loss : 0.13485672444824756ITERATION : 72, loss : 0.13485672444824756ITERATION : 73, loss : 0.13485672444824756ITERATION : 74, loss : 0.13485672444824756ITERATION : 75, loss : 0.13485672444824756ITERATION : 76, loss : 0.13485672444824756ITERATION : 77, loss : 0.13485672444824756ITERATION : 78, loss : 0.13485672444824756ITERATION : 79, loss : 0.13485672444824756ITERATION : 80, loss : 0.13485672444824756ITERATION : 81, loss : 0.13485672444824756ITERATION : 82, loss : 0.13485672444824756ITERATION : 83, loss : 0.13485672444824756ITERATION : 84, loss : 0.13485672444824756ITERATION : 85, loss : 0.13485672444824756ITERATION : 86, loss : 0.13485672444824756ITERATION : 87, loss : 0.13485672444824756ITERATION : 88, loss : 0.13485672444824756ITERATION : 89, loss : 0.13485672444824756ITERATION : 90, loss : 0.13485672444824756ITERATION : 91, loss : 0.13485672444824756ITERATION : 92, loss : 0.13485672444824756ITERATION : 93, loss : 0.13485672444824756ITERATION : 94, loss : 0.13485672444824756ITERATION : 95, loss : 0.13485672444824756ITERATION : 96, loss : 0.13485672444824756ITERATION : 97, loss : 0.13485672444824756ITERATION : 98, loss : 0.13485672444824756ITERATION : 99, loss : 0.13485672444824756ITERATION : 100, loss : 0.13485672444824756
ITERATION : 1, loss : 0.03665563741390225ITERATION : 2, loss : 0.047871148929574885ITERATION : 3, loss : 0.05618108687420003ITERATION : 4, loss : 0.06224128549645237ITERATION : 5, loss : 0.06623064318210227ITERATION : 6, loss : 0.06833710670966191ITERATION : 7, loss : 0.06986312637873103ITERATION : 8, loss : 0.07097006534200614ITERATION : 9, loss : 0.07177378649223269ITERATION : 10, loss : 0.07235773105352657ITERATION : 11, loss : 0.07278216687414489ITERATION : 12, loss : 0.07309072850499383ITERATION : 13, loss : 0.0733150640063097ITERATION : 14, loss : 0.07347815651829692ITERATION : 15, loss : 0.07357933663596426ITERATION : 16, loss : 0.07365230855618814ITERATION : 17, loss : 0.07370532565357513ITERATION : 18, loss : 0.07374383738101511ITERATION : 19, loss : 0.07377180611585218ITERATION : 20, loss : 0.07379211344397835ITERATION : 21, loss : 0.07380685430785558ITERATION : 22, loss : 0.0738175518145375ITERATION : 23, loss : 0.07382531303567455ITERATION : 24, loss : 0.07383094247963509ITERATION : 25, loss : 0.07383502470455983ITERATION : 26, loss : 0.07383798422890876ITERATION : 27, loss : 0.07384012925823256ITERATION : 28, loss : 0.07384168367410951ITERATION : 29, loss : 0.07384280975054329ITERATION : 30, loss : 0.0738436253930358ITERATION : 31, loss : 0.0738442160191121ITERATION : 32, loss : 0.0738446435952228ITERATION : 33, loss : 0.07384495311027615ITERATION : 34, loss : 0.07384517709961991ITERATION : 35, loss : 0.0738453391499751ITERATION : 36, loss : 0.07384545637539382ITERATION : 37, loss : 0.07384554115857234ITERATION : 38, loss : 0.07384560246412748ITERATION : 39, loss : 0.07384564676218407ITERATION : 40, loss : 0.07384567880580116ITERATION : 41, loss : 0.07384570195280575ITERATION : 42, loss : 0.07384571870322787ITERATION : 43, loss : 0.07384573078838338ITERATION : 44, loss : 0.07384573950815237ITERATION : 45, loss : 0.07384574580445376ITERATION : 46, loss : 0.07384575031966568ITERATION : 47, loss : 0.07384575360403381ITERATION : 48, loss : 0.07384575593802133ITERATION : 49, loss : 0.0738457576500446ITERATION : 50, loss : 0.07384575892412702ITERATION : 51, loss : 0.07384575978415396ITERATION : 52, loss : 0.07384576040496021ITERATION : 53, loss : 0.07384576082416378ITERATION : 54, loss : 0.07384576117983475ITERATION : 55, loss : 0.07384576138964742ITERATION : 56, loss : 0.07384576156860757ITERATION : 57, loss : 0.07384576168263172ITERATION : 58, loss : 0.07384576174993834ITERATION : 59, loss : 0.07384576180901478ITERATION : 60, loss : 0.07384576183851195ITERATION : 61, loss : 0.07384576184095056ITERATION : 62, loss : 0.07384576184095056ITERATION : 63, loss : 0.07384576184095056ITERATION : 64, loss : 0.07384576184095056ITERATION : 65, loss : 0.07384576184095056ITERATION : 66, loss : 0.07384576184095056ITERATION : 67, loss : 0.07384576184095056ITERATION : 68, loss : 0.07384576184095056ITERATION : 69, loss : 0.07384576184095056ITERATION : 70, loss : 0.07384576184095056ITERATION : 71, loss : 0.07384576184095056ITERATION : 72, loss : 0.07384576184095056ITERATION : 73, loss : 0.07384576184095056ITERATION : 74, loss : 0.07384576184095056ITERATION : 75, loss : 0.07384576184095056ITERATION : 76, loss : 0.07384576184095056ITERATION : 77, loss : 0.07384576184095056ITERATION : 78, loss : 0.07384576184095056ITERATION : 79, loss : 0.07384576184095056ITERATION : 80, loss : 0.07384576184095056ITERATION : 81, loss : 0.07384576184095056ITERATION : 82, loss : 0.07384576184095056ITERATION : 83, loss : 0.07384576184095056ITERATION : 84, loss : 0.07384576184095056ITERATION : 85, loss : 0.07384576184095056ITERATION : 86, loss : 0.07384576184095056ITERATION : 87, loss : 0.07384576184095056ITERATION : 88, loss : 0.07384576184095056ITERATION : 89, loss : 0.07384576184095056ITERATION : 90, loss : 0.07384576184095056ITERATION : 91, loss : 0.07384576184095056ITERATION : 92, loss : 0.07384576184095056ITERATION : 93, loss : 0.07384576184095056ITERATION : 94, loss : 0.07384576184095056ITERATION : 95, loss : 0.07384576184095056ITERATION : 96, loss : 0.07384576184095056ITERATION : 97, loss : 0.07384576184095056ITERATION : 98, loss : 0.07384576184095056ITERATION : 99, loss : 0.07384576184095056ITERATION : 100, loss : 0.07384576184095056
gradient norm in None layer : 0.0014086763118516408
gradient norm in None layer : 4.0133324649105054e-05
gradient norm in None layer : 5.977206098894019e-05
gradient norm in None layer : 0.0006499349193407961
gradient norm in None layer : 4.875758811578363e-05
gradient norm in None layer : 6.331634406115174e-05
gradient norm in None layer : 0.000328253636708973
gradient norm in None layer : 1.2190510136053453e-05
gradient norm in None layer : 1.2781957979664595e-05
gradient norm in None layer : 0.0002717908849939087
gradient norm in None layer : 1.3298106571224653e-05
gradient norm in None layer : 1.2562194499965224e-05
gradient norm in None layer : 8.628847856592137e-05
gradient norm in None layer : 2.9061713358769244e-06
gradient norm in None layer : 2.558758126127196e-06
gradient norm in None layer : 8.47731956568507e-05
gradient norm in None layer : 3.1782546039637836e-06
gradient norm in None layer : 2.9949832532961518e-06
gradient norm in None layer : 0.00011919235778819268
gradient norm in None layer : 2.1540367196295267e-06
gradient norm in None layer : 0.00025404665848728376
gradient norm in None layer : 1.5836120473735343e-05
gradient norm in None layer : 1.3527020877681033e-05
gradient norm in None layer : 0.00026814646268835153
gradient norm in None layer : 2.8464226380518315e-05
gradient norm in None layer : 3.286017418473906e-05
gradient norm in None layer : 0.000462323169520631
gradient norm in None layer : 5.3818670043600435e-06
gradient norm in None layer : 0.0006344129772459926
gradient norm in None layer : 4.539738275978906e-05
gradient norm in None layer : 5.2869552431020035e-05
gradient norm in None layer : 0.0006701252096070979
gradient norm in None layer : 9.198580405610277e-05
gradient norm in None layer : 0.0001214142157661397
gradient norm in None layer : 6.176175133254407e-05
gradient norm in None layer : 1.3564876816805649e-05
Total gradient norm: 0.0019661235497233152
invariance loss : 4.463048985649607, avg_den : 0.4686431884765625, density loss : 0.3672229373283541, mse loss : 0.050491420773794406, solver time : 109.90980553627014 sec , total loss : 0.05532169269677237, running loss : 0.09973725113227425
Epoch 0/10 , batch 24/12500 
ITERATION : 1, loss : 0.08017352519545136ITERATION : 2, loss : 0.10912809738131268ITERATION : 3, loss : 0.12495897095673282ITERATION : 4, loss : 0.13496518248801412ITERATION : 5, loss : 0.14164788697192132ITERATION : 6, loss : 0.14624112444746884ITERATION : 7, loss : 0.14945306842385983ITERATION : 8, loss : 0.15172384084020735ITERATION : 9, loss : 0.15334089095216452ITERATION : 10, loss : 0.15449811693923227ITERATION : 11, loss : 0.15532914227077274ITERATION : 12, loss : 0.1559273922944092ITERATION : 13, loss : 0.1563588414703972ITERATION : 14, loss : 0.1566704053899541ITERATION : 15, loss : 0.1568956158820185ITERATION : 16, loss : 0.15705852467442472ITERATION : 17, loss : 0.15717643101857262ITERATION : 18, loss : 0.1572618013084885ITERATION : 19, loss : 0.1573236332346156ITERATION : 20, loss : 0.1573684270221253ITERATION : 21, loss : 0.15740088353969786ITERATION : 22, loss : 0.1574244041259994ITERATION : 23, loss : 0.15744145066194778ITERATION : 24, loss : 0.1574538061986753ITERATION : 25, loss : 0.15746276231380935ITERATION : 26, loss : 0.1574692544761358ITERATION : 27, loss : 0.15747396089862947ITERATION : 28, loss : 0.15747737271880657ITERATION : 29, loss : 0.15747984622208339ITERATION : 30, loss : 0.15748163947835778ITERATION : 31, loss : 0.15748293959572335ITERATION : 32, loss : 0.15748388220172507ITERATION : 33, loss : 0.15748456559778043ITERATION : 34, loss : 0.15748506107744373ITERATION : 35, loss : 0.1574854203106677ITERATION : 36, loss : 0.15748568079445643ITERATION : 37, loss : 0.15748586965155226ITERATION : 38, loss : 0.15748600656410863ITERATION : 39, loss : 0.15748610589196038ITERATION : 40, loss : 0.1574861779077281ITERATION : 41, loss : 0.15748623000142487ITERATION : 42, loss : 0.15748626788292644ITERATION : 43, loss : 0.1574862954350782ITERATION : 44, loss : 0.15748631516396783ITERATION : 45, loss : 0.15748632963304035ITERATION : 46, loss : 0.1574863399315285ITERATION : 47, loss : 0.15748634756388022ITERATION : 48, loss : 0.15748635300599617ITERATION : 49, loss : 0.15748635703468955ITERATION : 50, loss : 0.15748635977024883ITERATION : 51, loss : 0.15748636194293197ITERATION : 52, loss : 0.15748636348014072ITERATION : 53, loss : 0.1574863646296237ITERATION : 54, loss : 0.15748636543435937ITERATION : 55, loss : 0.15748636619558076ITERATION : 56, loss : 0.15748636649984857ITERATION : 57, loss : 0.15748636676560912ITERATION : 58, loss : 0.15748636689164877ITERATION : 59, loss : 0.15748636698185925ITERATION : 60, loss : 0.15748636704306476ITERATION : 61, loss : 0.15748636704484312ITERATION : 62, loss : 0.15748636704484312ITERATION : 63, loss : 0.15748636704484312ITERATION : 64, loss : 0.15748636704484312ITERATION : 65, loss : 0.15748636704484312ITERATION : 66, loss : 0.15748636704484312ITERATION : 67, loss : 0.15748636704484312ITERATION : 68, loss : 0.15748636704484312ITERATION : 69, loss : 0.15748636704484312ITERATION : 70, loss : 0.15748636704484312ITERATION : 71, loss : 0.15748636704484312ITERATION : 72, loss : 0.15748636704484312ITERATION : 73, loss : 0.15748636704484312ITERATION : 74, loss : 0.15748636704484312ITERATION : 75, loss : 0.15748636704484312ITERATION : 76, loss : 0.15748636704484312ITERATION : 77, loss : 0.15748636704484312ITERATION : 78, loss : 0.15748636704484312ITERATION : 79, loss : 0.15748636704484312ITERATION : 80, loss : 0.15748636704484312ITERATION : 81, loss : 0.15748636704484312ITERATION : 82, loss : 0.15748636704484312ITERATION : 83, loss : 0.15748636704484312ITERATION : 84, loss : 0.15748636704484312ITERATION : 85, loss : 0.15748636704484312ITERATION : 86, loss : 0.15748636704484312ITERATION : 87, loss : 0.15748636704484312ITERATION : 88, loss : 0.15748636704484312ITERATION : 89, loss : 0.15748636704484312ITERATION : 90, loss : 0.15748636704484312ITERATION : 91, loss : 0.15748636704484312ITERATION : 92, loss : 0.15748636704484312ITERATION : 93, loss : 0.15748636704484312ITERATION : 94, loss : 0.15748636704484312ITERATION : 95, loss : 0.15748636704484312ITERATION : 96, loss : 0.15748636704484312ITERATION : 97, loss : 0.15748636704484312ITERATION : 98, loss : 0.15748636704484312ITERATION : 99, loss : 0.15748636704484312ITERATION : 100, loss : 0.15748636704484312
ITERATION : 1, loss : 0.02470142060274803ITERATION : 2, loss : 0.019356680064096077ITERATION : 3, loss : 0.02229177152759432ITERATION : 4, loss : 0.027650306014614785ITERATION : 5, loss : 0.032433625080116504ITERATION : 6, loss : 0.03389485856212201ITERATION : 7, loss : 0.03377875432508774ITERATION : 8, loss : 0.0337084647938048ITERATION : 9, loss : 0.03366239454843529ITERATION : 10, loss : 0.033630914778510934ITERATION : 11, loss : 0.03360902866537234ITERATION : 12, loss : 0.03359373733343158ITERATION : 13, loss : 0.03358305730011078ITERATION : 14, loss : 0.03357561285669114ITERATION : 15, loss : 0.03357043438508803ITERATION : 16, loss : 0.03356683736831427ITERATION : 17, loss : 0.03356434083213658ITERATION : 18, loss : 0.03356260819244146ITERATION : 19, loss : 0.03356140519143045ITERATION : 20, loss : 0.033560569243748575ITERATION : 21, loss : 0.03355998772187958ITERATION : 22, loss : 0.033559582643997ITERATION : 23, loss : 0.03355930014417609ITERATION : 24, loss : 0.03355910277292248ITERATION : 25, loss : 0.03355896472479117ITERATION : 26, loss : 0.033558868047659195ITERATION : 27, loss : 0.03355880022824679ITERATION : 28, loss : 0.03355875259646827ITERATION : 29, loss : 0.033558719132146274ITERATION : 30, loss : 0.03355869557695309ITERATION : 31, loss : 0.03355867896480439ITERATION : 32, loss : 0.03355866726939932ITERATION : 33, loss : 0.0335586590015234ITERATION : 34, loss : 0.03355865316872023ITERATION : 35, loss : 0.03355864903952934ITERATION : 36, loss : 0.03355864611058763ITERATION : 37, loss : 0.03355864405406016ITERATION : 38, loss : 0.03355864259012063ITERATION : 39, loss : 0.03355864155832193ITERATION : 40, loss : 0.03355864083695674ITERATION : 41, loss : 0.03355864033649034ITERATION : 42, loss : 0.0335586399852329ITERATION : 43, loss : 0.033558639718268245ITERATION : 44, loss : 0.033558639539711554ITERATION : 45, loss : 0.03355863942935323ITERATION : 46, loss : 0.03355863936798568ITERATION : 47, loss : 0.03355863929095354ITERATION : 48, loss : 0.03355863922656644ITERATION : 49, loss : 0.03355863920401067ITERATION : 50, loss : 0.033558639190795755ITERATION : 51, loss : 0.03355863919053945ITERATION : 52, loss : 0.03355863919053945ITERATION : 53, loss : 0.03355863919053945ITERATION : 54, loss : 0.03355863919053945ITERATION : 55, loss : 0.03355863919053945ITERATION : 56, loss : 0.03355863919053945ITERATION : 57, loss : 0.03355863919053945ITERATION : 58, loss : 0.03355863919053945ITERATION : 59, loss : 0.03355863919053945ITERATION : 60, loss : 0.03355863919053945ITERATION : 61, loss : 0.03355863919053945ITERATION : 62, loss : 0.03355863919053945ITERATION : 63, loss : 0.03355863919053945ITERATION : 64, loss : 0.03355863919053945ITERATION : 65, loss : 0.03355863919053945ITERATION : 66, loss : 0.03355863919053945ITERATION : 67, loss : 0.03355863919053945ITERATION : 68, loss : 0.03355863919053945ITERATION : 69, loss : 0.03355863919053945ITERATION : 70, loss : 0.03355863919053945ITERATION : 71, loss : 0.03355863919053945ITERATION : 72, loss : 0.03355863919053945ITERATION : 73, loss : 0.03355863919053945ITERATION : 74, loss : 0.03355863919053945ITERATION : 75, loss : 0.03355863919053945ITERATION : 76, loss : 0.03355863919053945ITERATION : 77, loss : 0.03355863919053945ITERATION : 78, loss : 0.03355863919053945ITERATION : 79, loss : 0.03355863919053945ITERATION : 80, loss : 0.03355863919053945ITERATION : 81, loss : 0.03355863919053945ITERATION : 82, loss : 0.03355863919053945ITERATION : 83, loss : 0.03355863919053945ITERATION : 84, loss : 0.03355863919053945ITERATION : 85, loss : 0.03355863919053945ITERATION : 86, loss : 0.03355863919053945ITERATION : 87, loss : 0.03355863919053945ITERATION : 88, loss : 0.03355863919053945ITERATION : 89, loss : 0.03355863919053945ITERATION : 90, loss : 0.03355863919053945ITERATION : 91, loss : 0.03355863919053945ITERATION : 92, loss : 0.03355863919053945ITERATION : 93, loss : 0.03355863919053945ITERATION : 94, loss : 0.03355863919053945ITERATION : 95, loss : 0.03355863919053945ITERATION : 96, loss : 0.03355863919053945ITERATION : 97, loss : 0.03355863919053945ITERATION : 98, loss : 0.03355863919053945ITERATION : 99, loss : 0.03355863919053945ITERATION : 100, loss : 0.03355863919053945
ITERATION : 1, loss : 0.013051185114087782ITERATION : 2, loss : 0.018064341971010145ITERATION : 3, loss : 0.024181843932487554ITERATION : 4, loss : 0.029417870748048656ITERATION : 5, loss : 0.03358157091779399ITERATION : 6, loss : 0.03680871434294696ITERATION : 7, loss : 0.039283683810965205ITERATION : 8, loss : 0.04117282563088917ITERATION : 9, loss : 0.04261166308015426ITERATION : 10, loss : 0.043706482641718156ITERATION : 11, loss : 0.044539255879741994ITERATION : 12, loss : 0.04517269694659318ITERATION : 13, loss : 0.04565459628375087ITERATION : 14, loss : 0.046021301308426285ITERATION : 15, loss : 0.046300431919803015ITERATION : 16, loss : 0.04651297005851082ITERATION : 17, loss : 0.046674855216300874ITERATION : 18, loss : 0.046798198682226726ITERATION : 19, loss : 0.04689220570655549ITERATION : 20, loss : 0.046963875360559924ITERATION : 21, loss : 0.04701853089532122ITERATION : 22, loss : 0.04706022271868991ITERATION : 23, loss : 0.047092033951742635ITERATION : 24, loss : 0.04711631206810042ITERATION : 25, loss : 0.04713484523591019ITERATION : 26, loss : 0.047148995890115335ITERATION : 27, loss : 0.047159802574481734ITERATION : 28, loss : 0.047168057042145ITERATION : 29, loss : 0.047174363216660804ITERATION : 30, loss : 0.04717918173700096ITERATION : 31, loss : 0.047182864090357914ITERATION : 32, loss : 0.04718567855808756ITERATION : 33, loss : 0.04718782998227449ITERATION : 34, loss : 0.047189474776053425ITERATION : 35, loss : 0.047190732388300446ITERATION : 36, loss : 0.04719169407248589ITERATION : 37, loss : 0.047192429500027194ITERATION : 38, loss : 0.04719299199683359ITERATION : 39, loss : 0.04719342223729797ITERATION : 40, loss : 0.047193751362883035ITERATION : 41, loss : 0.0471940031197308ITERATION : 42, loss : 0.047194195761685674ITERATION : 43, loss : 0.047194343129290904ITERATION : 44, loss : 0.047194455937426355ITERATION : 45, loss : 0.04719454221336114ITERATION : 46, loss : 0.04719460827394014ITERATION : 47, loss : 0.047194658821332384ITERATION : 48, loss : 0.04719469745372164ITERATION : 49, loss : 0.04719472705080857ITERATION : 50, loss : 0.047194749659111995ITERATION : 51, loss : 0.04719476699678413ITERATION : 52, loss : 0.047194780234907835ITERATION : 53, loss : 0.047194790385717285ITERATION : 54, loss : 0.04719479813743574ITERATION : 55, loss : 0.04719480409139155ITERATION : 56, loss : 0.047194808614254215ITERATION : 57, loss : 0.04719481212196994ITERATION : 58, loss : 0.04719481476595389ITERATION : 59, loss : 0.04719481683715245ITERATION : 60, loss : 0.04719481839916901ITERATION : 61, loss : 0.04719481960818475ITERATION : 62, loss : 0.04719482047202527ITERATION : 63, loss : 0.04719482118846108ITERATION : 64, loss : 0.047194821723259096ITERATION : 65, loss : 0.04719482215123422ITERATION : 66, loss : 0.047194822432923986ITERATION : 67, loss : 0.047194822672560886ITERATION : 68, loss : 0.04719482284316247ITERATION : 69, loss : 0.04719482296268489ITERATION : 70, loss : 0.04719482307659067ITERATION : 71, loss : 0.047194823165013715ITERATION : 72, loss : 0.0471948231942076ITERATION : 73, loss : 0.047194823244012446ITERATION : 74, loss : 0.04719482324933511ITERATION : 75, loss : 0.04719482324933511ITERATION : 76, loss : 0.04719482324933511ITERATION : 77, loss : 0.04719482324933511ITERATION : 78, loss : 0.04719482324933511ITERATION : 79, loss : 0.04719482324933511ITERATION : 80, loss : 0.04719482324933511ITERATION : 81, loss : 0.04719482324933511ITERATION : 82, loss : 0.04719482324933511ITERATION : 83, loss : 0.04719482324933511ITERATION : 84, loss : 0.04719482324933511ITERATION : 85, loss : 0.04719482324933511ITERATION : 86, loss : 0.04719482324933511ITERATION : 87, loss : 0.04719482324933511ITERATION : 88, loss : 0.04719482324933511ITERATION : 89, loss : 0.04719482324933511ITERATION : 90, loss : 0.04719482324933511ITERATION : 91, loss : 0.04719482324933511ITERATION : 92, loss : 0.04719482324933511ITERATION : 93, loss : 0.04719482324933511ITERATION : 94, loss : 0.04719482324933511ITERATION : 95, loss : 0.04719482324933511ITERATION : 96, loss : 0.04719482324933511ITERATION : 97, loss : 0.04719482324933511ITERATION : 98, loss : 0.04719482324933511ITERATION : 99, loss : 0.04719482324933511ITERATION : 100, loss : 0.04719482324933511
ITERATION : 1, loss : 0.061282203279430784ITERATION : 2, loss : 0.06933019065975277ITERATION : 3, loss : 0.07974387758816766ITERATION : 4, loss : 0.08757751260803517ITERATION : 5, loss : 0.09310461229449078ITERATION : 6, loss : 0.09695191192389409ITERATION : 7, loss : 0.09962390724163254ITERATION : 8, loss : 0.10148094389642241ITERATION : 9, loss : 0.10277331581579255ITERATION : 10, loss : 0.10367388417834413ITERATION : 11, loss : 0.10430210883117368ITERATION : 12, loss : 0.10474072777779012ITERATION : 13, loss : 0.10504717607121916ITERATION : 14, loss : 0.10526139970582957ITERATION : 15, loss : 0.10541122226964265ITERATION : 16, loss : 0.10551604585855466ITERATION : 17, loss : 0.1055894114548837ITERATION : 18, loss : 0.1056407761395198ITERATION : 19, loss : 0.10567674814311324ITERATION : 20, loss : 0.10570194731668002ITERATION : 21, loss : 0.105719604625541ITERATION : 22, loss : 0.10573198041870945ITERATION : 23, loss : 0.10574065664973638ITERATION : 24, loss : 0.10574674065827698ITERATION : 25, loss : 0.10575100792944662ITERATION : 26, loss : 0.10575400164603263ITERATION : 27, loss : 0.10575610243923317ITERATION : 28, loss : 0.10575757675125105ITERATION : 29, loss : 0.10575861182008618ITERATION : 30, loss : 0.10575933855967126ITERATION : 31, loss : 0.10575984891241233ITERATION : 32, loss : 0.10576020739378517ITERATION : 33, loss : 0.10576045928389247ITERATION : 34, loss : 0.105760636218416ITERATION : 35, loss : 0.10576076059327295ITERATION : 36, loss : 0.10576084798874476ITERATION : 37, loss : 0.10576090943098355ITERATION : 38, loss : 0.10576095266302012ITERATION : 39, loss : 0.10576098305267773ITERATION : 40, loss : 0.10576100444403637ITERATION : 41, loss : 0.10576101949508271ITERATION : 42, loss : 0.10576103006372668ITERATION : 43, loss : 0.10576103747859969ITERATION : 44, loss : 0.10576104269018273ITERATION : 45, loss : 0.10576104635305357ITERATION : 46, loss : 0.10576104892544634ITERATION : 47, loss : 0.10576105073642111ITERATION : 48, loss : 0.10576105200367897ITERATION : 49, loss : 0.10576105288051986ITERATION : 50, loss : 0.10576105351909286ITERATION : 51, loss : 0.10576105395607384ITERATION : 52, loss : 0.10576105426926254ITERATION : 53, loss : 0.10576105448254844ITERATION : 54, loss : 0.10576105462814887ITERATION : 55, loss : 0.10576105470979122ITERATION : 56, loss : 0.1057610547870202ITERATION : 57, loss : 0.10576105483574562ITERATION : 58, loss : 0.10576105486177374ITERATION : 59, loss : 0.10576105486904357ITERATION : 60, loss : 0.10576105486955986ITERATION : 61, loss : 0.10576105486955986ITERATION : 62, loss : 0.10576105486955986ITERATION : 63, loss : 0.10576105486955986ITERATION : 64, loss : 0.10576105486955986ITERATION : 65, loss : 0.10576105486955986ITERATION : 66, loss : 0.10576105486955986ITERATION : 67, loss : 0.10576105486955986ITERATION : 68, loss : 0.10576105486955986ITERATION : 69, loss : 0.10576105486955986ITERATION : 70, loss : 0.10576105486955986ITERATION : 71, loss : 0.10576105486955986ITERATION : 72, loss : 0.10576105486955986ITERATION : 73, loss : 0.10576105486955986ITERATION : 74, loss : 0.10576105486955986ITERATION : 75, loss : 0.10576105486955986ITERATION : 76, loss : 0.10576105486955986ITERATION : 77, loss : 0.10576105486955986ITERATION : 78, loss : 0.10576105486955986ITERATION : 79, loss : 0.10576105486955986ITERATION : 80, loss : 0.10576105486955986ITERATION : 81, loss : 0.10576105486955986ITERATION : 82, loss : 0.10576105486955986ITERATION : 83, loss : 0.10576105486955986ITERATION : 84, loss : 0.10576105486955986ITERATION : 85, loss : 0.10576105486955986ITERATION : 86, loss : 0.10576105486955986ITERATION : 87, loss : 0.10576105486955986ITERATION : 88, loss : 0.10576105486955986ITERATION : 89, loss : 0.10576105486955986ITERATION : 90, loss : 0.10576105486955986ITERATION : 91, loss : 0.10576105486955986ITERATION : 92, loss : 0.10576105486955986ITERATION : 93, loss : 0.10576105486955986ITERATION : 94, loss : 0.10576105486955986ITERATION : 95, loss : 0.10576105486955986ITERATION : 96, loss : 0.10576105486955986ITERATION : 97, loss : 0.10576105486955986ITERATION : 98, loss : 0.10576105486955986ITERATION : 99, loss : 0.10576105486955986ITERATION : 100, loss : 0.10576105486955986
ITERATION : 1, loss : 0.0020303066765751943ITERATION : 2, loss : 0.001966665251433598ITERATION : 3, loss : 0.0024788194084932836ITERATION : 4, loss : 0.003056114701585771ITERATION : 5, loss : 0.003567044403097513ITERATION : 6, loss : 0.003976926218504825ITERATION : 7, loss : 0.004288700442490123ITERATION : 8, loss : 0.004518424568719735ITERATION : 9, loss : 0.004684351723002339ITERATION : 10, loss : 0.004802668205020382ITERATION : 11, loss : 0.004886324284369157ITERATION : 12, loss : 0.004945140006890209ITERATION : 13, loss : 0.004986333324270313ITERATION : 14, loss : 0.00501510899874239ITERATION : 15, loss : 0.005035174121224646ITERATION : 16, loss : 0.005049147879531312ITERATION : 17, loss : 0.0050588709998412255ITERATION : 18, loss : 0.0050656322092189785ITERATION : 19, loss : 0.005070331705653971ITERATION : 20, loss : 0.005073597105368565ITERATION : 21, loss : 0.005075865465074207ITERATION : 22, loss : 0.005077440876099313ITERATION : 23, loss : 0.005078534908732958ITERATION : 24, loss : 0.005079294550342133ITERATION : 25, loss : 0.005079821914818063ITERATION : 26, loss : 0.005080188018941529ITERATION : 27, loss : 0.005080442146230185ITERATION : 28, loss : 0.0050806185391724075ITERATION : 29, loss : 0.005080740957606362ITERATION : 30, loss : 0.005080825910603916ITERATION : 31, loss : 0.005080884864427175ITERATION : 32, loss : 0.0050809257702711256ITERATION : 33, loss : 0.005080954146835735ITERATION : 34, loss : 0.005080973868130306ITERATION : 35, loss : 0.005080987526942093ITERATION : 36, loss : 0.0050809969951643ITERATION : 37, loss : 0.005081003599720338ITERATION : 38, loss : 0.0050810081662591156ITERATION : 39, loss : 0.005081011323475355ITERATION : 40, loss : 0.0050810134968989965ITERATION : 41, loss : 0.005081015001761996ITERATION : 42, loss : 0.005081016031594918ITERATION : 43, loss : 0.005081016751680333ITERATION : 44, loss : 0.005081017245431446ITERATION : 45, loss : 0.00508101758825832ITERATION : 46, loss : 0.005081017816870962ITERATION : 47, loss : 0.005081017979793253ITERATION : 48, loss : 0.005081018091052412ITERATION : 49, loss : 0.005081018152035348ITERATION : 50, loss : 0.0050810182057297055ITERATION : 51, loss : 0.005081018223994309ITERATION : 52, loss : 0.005081018256631351ITERATION : 53, loss : 0.005081018257978556ITERATION : 54, loss : 0.005081018257978556ITERATION : 55, loss : 0.005081018257978556ITERATION : 56, loss : 0.005081018257978556ITERATION : 57, loss : 0.005081018257978556ITERATION : 58, loss : 0.005081018257978556ITERATION : 59, loss : 0.005081018257978556ITERATION : 60, loss : 0.005081018257978556ITERATION : 61, loss : 0.005081018257978556ITERATION : 62, loss : 0.005081018257978556ITERATION : 63, loss : 0.005081018257978556ITERATION : 64, loss : 0.005081018257978556ITERATION : 65, loss : 0.005081018257978556ITERATION : 66, loss : 0.005081018257978556ITERATION : 67, loss : 0.005081018257978556ITERATION : 68, loss : 0.005081018257978556ITERATION : 69, loss : 0.005081018257978556ITERATION : 70, loss : 0.005081018257978556ITERATION : 71, loss : 0.005081018257978556ITERATION : 72, loss : 0.005081018257978556ITERATION : 73, loss : 0.005081018257978556ITERATION : 74, loss : 0.005081018257978556ITERATION : 75, loss : 0.005081018257978556ITERATION : 76, loss : 0.005081018257978556ITERATION : 77, loss : 0.005081018257978556ITERATION : 78, loss : 0.005081018257978556ITERATION : 79, loss : 0.005081018257978556ITERATION : 80, loss : 0.005081018257978556ITERATION : 81, loss : 0.005081018257978556ITERATION : 82, loss : 0.005081018257978556ITERATION : 83, loss : 0.005081018257978556ITERATION : 84, loss : 0.005081018257978556ITERATION : 85, loss : 0.005081018257978556ITERATION : 86, loss : 0.005081018257978556ITERATION : 87, loss : 0.005081018257978556ITERATION : 88, loss : 0.005081018257978556ITERATION : 89, loss : 0.005081018257978556ITERATION : 90, loss : 0.005081018257978556ITERATION : 91, loss : 0.005081018257978556ITERATION : 92, loss : 0.005081018257978556ITERATION : 93, loss : 0.005081018257978556ITERATION : 94, loss : 0.005081018257978556ITERATION : 95, loss : 0.005081018257978556ITERATION : 96, loss : 0.005081018257978556ITERATION : 97, loss : 0.005081018257978556ITERATION : 98, loss : 0.005081018257978556ITERATION : 99, loss : 0.005081018257978556ITERATION : 100, loss : 0.005081018257978556
ITERATION : 1, loss : 0.031115674865057938ITERATION : 2, loss : 0.03155495462330648ITERATION : 3, loss : 0.0358612050667905ITERATION : 4, loss : 0.03974035022265814ITERATION : 5, loss : 0.0425645481358803ITERATION : 6, loss : 0.04457379735384432ITERATION : 7, loss : 0.04599384352107337ITERATION : 8, loss : 0.04699717466876282ITERATION : 9, loss : 0.047707618926497615ITERATION : 10, loss : 0.04821218209900933ITERATION : 11, loss : 0.04857163619858556ITERATION : 12, loss : 0.048828454592497676ITERATION : 13, loss : 0.04901241748443572ITERATION : 14, loss : 0.04914449117804457ITERATION : 15, loss : 0.04923949783581295ITERATION : 16, loss : 0.04930795689871465ITERATION : 17, loss : 0.04935735953590021ITERATION : 18, loss : 0.049393056567227527ITERATION : 19, loss : 0.04941887973080956ITERATION : 20, loss : 0.049437579181131446ITERATION : 21, loss : 0.04945113236354501ITERATION : 22, loss : 0.04946096361305332ITERATION : 23, loss : 0.049468100327988394ITERATION : 24, loss : 0.04947328454873329ITERATION : 25, loss : 0.04947705281351176ITERATION : 26, loss : 0.049479793333851455ITERATION : 27, loss : 0.049481787579338674ITERATION : 28, loss : 0.049483239448774745ITERATION : 29, loss : 0.04948429693384261ITERATION : 30, loss : 0.04948506754250173ITERATION : 31, loss : 0.04948562922824891ITERATION : 32, loss : 0.04948603884906678ITERATION : 33, loss : 0.049486337644719686ITERATION : 34, loss : 0.04948655564571637ITERATION : 35, loss : 0.04948671476304261ITERATION : 36, loss : 0.04948683095982428ITERATION : 37, loss : 0.04948691583644997ITERATION : 38, loss : 0.04948697785258069ITERATION : 39, loss : 0.04948702316255862ITERATION : 40, loss : 0.049487056280190025ITERATION : 41, loss : 0.049487080490779115ITERATION : 42, loss : 0.04948709817935673ITERATION : 43, loss : 0.049487111125341716ITERATION : 44, loss : 0.049487120556642ITERATION : 45, loss : 0.049487127490527576ITERATION : 46, loss : 0.04948713255991362ITERATION : 47, loss : 0.04948713631872124ITERATION : 48, loss : 0.04948713901763105ITERATION : 49, loss : 0.04948714097633403ITERATION : 50, loss : 0.04948714241771269ITERATION : 51, loss : 0.04948714347944933ITERATION : 52, loss : 0.04948714419871003ITERATION : 53, loss : 0.049487144779106526ITERATION : 54, loss : 0.049487145224905896ITERATION : 55, loss : 0.04948714550886391ITERATION : 56, loss : 0.049487145756874205ITERATION : 57, loss : 0.049487145860659ITERATION : 58, loss : 0.049487146016457546ITERATION : 59, loss : 0.04948714607798773ITERATION : 60, loss : 0.04948714615346576ITERATION : 61, loss : 0.04948714616043021ITERATION : 62, loss : 0.04948714616043021ITERATION : 63, loss : 0.04948714616043021ITERATION : 64, loss : 0.04948714616043021ITERATION : 65, loss : 0.04948714616043021ITERATION : 66, loss : 0.04948714616043021ITERATION : 67, loss : 0.04948714616043021ITERATION : 68, loss : 0.04948714616043021ITERATION : 69, loss : 0.04948714616043021ITERATION : 70, loss : 0.04948714616043021ITERATION : 71, loss : 0.04948714616043021ITERATION : 72, loss : 0.04948714616043021ITERATION : 73, loss : 0.04948714616043021ITERATION : 74, loss : 0.04948714616043021ITERATION : 75, loss : 0.04948714616043021ITERATION : 76, loss : 0.04948714616043021ITERATION : 77, loss : 0.04948714616043021ITERATION : 78, loss : 0.04948714616043021ITERATION : 79, loss : 0.04948714616043021ITERATION : 80, loss : 0.04948714616043021ITERATION : 81, loss : 0.04948714616043021ITERATION : 82, loss : 0.04948714616043021ITERATION : 83, loss : 0.04948714616043021ITERATION : 84, loss : 0.04948714616043021ITERATION : 85, loss : 0.04948714616043021ITERATION : 86, loss : 0.04948714616043021ITERATION : 87, loss : 0.04948714616043021ITERATION : 88, loss : 0.04948714616043021ITERATION : 89, loss : 0.04948714616043021ITERATION : 90, loss : 0.04948714616043021ITERATION : 91, loss : 0.04948714616043021ITERATION : 92, loss : 0.04948714616043021ITERATION : 93, loss : 0.04948714616043021ITERATION : 94, loss : 0.04948714616043021ITERATION : 95, loss : 0.04948714616043021ITERATION : 96, loss : 0.04948714616043021ITERATION : 97, loss : 0.04948714616043021ITERATION : 98, loss : 0.04948714616043021ITERATION : 99, loss : 0.04948714616043021ITERATION : 100, loss : 0.04948714616043021
ITERATION : 1, loss : 0.02547369261609121ITERATION : 2, loss : 0.02546321120878062ITERATION : 3, loss : 0.02624237457916763ITERATION : 4, loss : 0.02688413925180666ITERATION : 5, loss : 0.027377389224335705ITERATION : 6, loss : 0.02775465294711785ITERATION : 7, loss : 0.028041961701996364ITERATION : 8, loss : 0.028259417004791617ITERATION : 9, loss : 0.028422976346211777ITERATION : 10, loss : 0.028545337392816193ITERATION : 11, loss : 0.02863648803912435ITERATION : 12, loss : 0.028704171605994182ITERATION : 13, loss : 0.028754313331002472ITERATION : 14, loss : 0.028791399858128477ITERATION : 15, loss : 0.028818801409217776ITERATION : 16, loss : 0.02883903434104294ITERATION : 17, loss : 0.02885396938289619ITERATION : 18, loss : 0.028864992961930663ITERATION : 19, loss : 0.02887313033377011ITERATION : 20, loss : 0.02887913839685154ITERATION : 21, loss : 0.02888357588989458ITERATION : 22, loss : 0.028886854624798886ITERATION : 23, loss : 0.028889278221669167ITERATION : 24, loss : 0.028891070529285384ITERATION : 25, loss : 0.028892396626067973ITERATION : 26, loss : 0.02889337825792374ITERATION : 27, loss : 0.028894105230849013ITERATION : 28, loss : 0.028894643841363192ITERATION : 29, loss : 0.028895043130933516ITERATION : 30, loss : 0.028895339254133103ITERATION : 31, loss : 0.02889555894145155ITERATION : 32, loss : 0.028895722019756546ITERATION : 33, loss : 0.028895843086161905ITERATION : 34, loss : 0.02889593302167191ITERATION : 35, loss : 0.02889599984285388ITERATION : 36, loss : 0.02889604950248062ITERATION : 37, loss : 0.02889608645178664ITERATION : 38, loss : 0.02889611394734677ITERATION : 39, loss : 0.028896134390340743ITERATION : 40, loss : 0.02889614957736699ITERATION : 41, loss : 0.02889616087753506ITERATION : 42, loss : 0.02889616928723761ITERATION : 43, loss : 0.02889617556196029ITERATION : 44, loss : 0.028896180223234348ITERATION : 45, loss : 0.028896183699146656ITERATION : 46, loss : 0.028896186288031048ITERATION : 47, loss : 0.028896188206391674ITERATION : 48, loss : 0.028896189587536744ITERATION : 49, loss : 0.028896190637237834ITERATION : 50, loss : 0.028896191432282326ITERATION : 51, loss : 0.028896191985209718ITERATION : 52, loss : 0.028896192488502222ITERATION : 53, loss : 0.02889619279296069ITERATION : 54, loss : 0.028896192982178503ITERATION : 55, loss : 0.028896193209122026ITERATION : 56, loss : 0.02889619337153884ITERATION : 57, loss : 0.028896193450407063ITERATION : 58, loss : 0.028896193521232974ITERATION : 59, loss : 0.028896193585200073ITERATION : 60, loss : 0.02889619358865014ITERATION : 61, loss : 0.02889619358865014ITERATION : 62, loss : 0.02889619358865014ITERATION : 63, loss : 0.02889619358865014ITERATION : 64, loss : 0.02889619358865014ITERATION : 65, loss : 0.02889619358865014ITERATION : 66, loss : 0.02889619358865014ITERATION : 67, loss : 0.02889619358865014ITERATION : 68, loss : 0.02889619358865014ITERATION : 69, loss : 0.02889619358865014ITERATION : 70, loss : 0.02889619358865014ITERATION : 71, loss : 0.02889619358865014ITERATION : 72, loss : 0.02889619358865014ITERATION : 73, loss : 0.02889619358865014ITERATION : 74, loss : 0.02889619358865014ITERATION : 75, loss : 0.02889619358865014ITERATION : 76, loss : 0.02889619358865014ITERATION : 77, loss : 0.02889619358865014ITERATION : 78, loss : 0.02889619358865014ITERATION : 79, loss : 0.02889619358865014ITERATION : 80, loss : 0.02889619358865014ITERATION : 81, loss : 0.02889619358865014ITERATION : 82, loss : 0.02889619358865014ITERATION : 83, loss : 0.02889619358865014ITERATION : 84, loss : 0.02889619358865014ITERATION : 85, loss : 0.02889619358865014ITERATION : 86, loss : 0.02889619358865014ITERATION : 87, loss : 0.02889619358865014ITERATION : 88, loss : 0.02889619358865014ITERATION : 89, loss : 0.02889619358865014ITERATION : 90, loss : 0.02889619358865014ITERATION : 91, loss : 0.02889619358865014ITERATION : 92, loss : 0.02889619358865014ITERATION : 93, loss : 0.02889619358865014ITERATION : 94, loss : 0.02889619358865014ITERATION : 95, loss : 0.02889619358865014ITERATION : 96, loss : 0.02889619358865014ITERATION : 97, loss : 0.02889619358865014ITERATION : 98, loss : 0.02889619358865014ITERATION : 99, loss : 0.02889619358865014ITERATION : 100, loss : 0.02889619358865014
ITERATION : 1, loss : 0.03340434994542087ITERATION : 2, loss : 0.0407454522488143ITERATION : 3, loss : 0.04551153901112019ITERATION : 4, loss : 0.048582337597866725ITERATION : 5, loss : 0.05051352684894945ITERATION : 6, loss : 0.05169359465241212ITERATION : 7, loss : 0.052393790510220736ITERATION : 8, loss : 0.05279452680517577ITERATION : 9, loss : 0.05301203355091518ITERATION : 10, loss : 0.053119834260335355ITERATION : 11, loss : 0.05316383373128035ITERATION : 12, loss : 0.05317230865607834ITERATION : 13, loss : 0.05316237346298526ITERATION : 14, loss : 0.05314413180867071ITERATION : 15, loss : 0.05312333232030066ITERATION : 16, loss : 0.05310306018715971ITERATION : 17, loss : 0.05308480734082418ITERATION : 18, loss : 0.05306914259473885ITERATION : 19, loss : 0.05305612536535017ITERATION : 20, loss : 0.053045555923614096ITERATION : 21, loss : 0.05303712257656769ITERATION : 22, loss : 0.05303048485579378ITERATION : 23, loss : 0.05302531746745329ITERATION : 24, loss : 0.053021330802431314ITERATION : 25, loss : 0.05301827819105292ITERATION : 26, loss : 0.05301595564737845ITERATION : 27, loss : 0.05301419831813036ITERATION : 28, loss : 0.05301287493413731ITERATION : 29, loss : 0.053011882558745185ITERATION : 30, loss : 0.05301114118572986ITERATION : 31, loss : 0.05301058914418756ITERATION : 32, loss : 0.0530101792953069ITERATION : 33, loss : 0.0530098758465099ITERATION : 34, loss : 0.0530096516980219ITERATION : 35, loss : 0.05300948650651399ITERATION : 36, loss : 0.05300936502101688ITERATION : 37, loss : 0.05300927587707358ITERATION : 38, loss : 0.05300921053330532ITERATION : 39, loss : 0.05300916272731167ITERATION : 40, loss : 0.05300912781953857ITERATION : 41, loss : 0.053009102350870856ITERATION : 42, loss : 0.05300908380365106ITERATION : 43, loss : 0.05300907029973121ITERATION : 44, loss : 0.053009060473388096ITERATION : 45, loss : 0.05300905336321437ITERATION : 46, loss : 0.05300904821541117ITERATION : 47, loss : 0.05300904446823866ITERATION : 48, loss : 0.05300904175723882ITERATION : 49, loss : 0.05300903979071699ITERATION : 50, loss : 0.05300903837800793ITERATION : 51, loss : 0.05300903734703799ITERATION : 52, loss : 0.05300903659145744ITERATION : 53, loss : 0.05300903600850817ITERATION : 54, loss : 0.05300903566038909ITERATION : 55, loss : 0.05300903535913719ITERATION : 56, loss : 0.053009035192005505ITERATION : 57, loss : 0.05300903506949909ITERATION : 58, loss : 0.053009035013006925ITERATION : 59, loss : 0.053009034947216926ITERATION : 60, loss : 0.05300903491539881ITERATION : 61, loss : 0.05300903491539881ITERATION : 62, loss : 0.05300903491539881ITERATION : 63, loss : 0.05300903491539881ITERATION : 64, loss : 0.05300903491539881ITERATION : 65, loss : 0.05300903491539881ITERATION : 66, loss : 0.05300903491539881ITERATION : 67, loss : 0.05300903491539881ITERATION : 68, loss : 0.05300903491539881ITERATION : 69, loss : 0.05300903491539881ITERATION : 70, loss : 0.05300903491539881ITERATION : 71, loss : 0.05300903491539881ITERATION : 72, loss : 0.05300903491539881ITERATION : 73, loss : 0.05300903491539881ITERATION : 74, loss : 0.05300903491539881ITERATION : 75, loss : 0.05300903491539881ITERATION : 76, loss : 0.05300903491539881ITERATION : 77, loss : 0.05300903491539881ITERATION : 78, loss : 0.05300903491539881ITERATION : 79, loss : 0.05300903491539881ITERATION : 80, loss : 0.05300903491539881ITERATION : 81, loss : 0.05300903491539881ITERATION : 82, loss : 0.05300903491539881ITERATION : 83, loss : 0.05300903491539881ITERATION : 84, loss : 0.05300903491539881ITERATION : 85, loss : 0.05300903491539881ITERATION : 86, loss : 0.05300903491539881ITERATION : 87, loss : 0.05300903491539881ITERATION : 88, loss : 0.05300903491539881ITERATION : 89, loss : 0.05300903491539881ITERATION : 90, loss : 0.05300903491539881ITERATION : 91, loss : 0.05300903491539881ITERATION : 92, loss : 0.05300903491539881ITERATION : 93, loss : 0.05300903491539881ITERATION : 94, loss : 0.05300903491539881ITERATION : 95, loss : 0.05300903491539881ITERATION : 96, loss : 0.05300903491539881ITERATION : 97, loss : 0.05300903491539881ITERATION : 98, loss : 0.05300903491539881ITERATION : 99, loss : 0.05300903491539881ITERATION : 100, loss : 0.05300903491539881
gradient norm in None layer : 0.0013115896967521052
gradient norm in None layer : 4.420995764311686e-05
gradient norm in None layer : 3.125909752097849e-05
gradient norm in None layer : 0.0006450389496650313
gradient norm in None layer : 5.146967708059629e-05
gradient norm in None layer : 3.486784687622933e-05
gradient norm in None layer : 0.00037672004867046933
gradient norm in None layer : 1.5546304393256863e-05
gradient norm in None layer : 1.5831793192032107e-05
gradient norm in None layer : 0.0002735208803396932
gradient norm in None layer : 1.2932646870224243e-05
gradient norm in None layer : 9.919424016324745e-06
gradient norm in None layer : 7.979775570992191e-05
gradient norm in None layer : 2.624375723975496e-06
gradient norm in None layer : 2.263594488828302e-06
gradient norm in None layer : 7.370852001376678e-05
gradient norm in None layer : 2.5192686805718473e-06
gradient norm in None layer : 2.421090676808195e-06
gradient norm in None layer : 8.726340760069359e-05
gradient norm in None layer : 1.2151755094271268e-06
gradient norm in None layer : 0.00022563702383727208
gradient norm in None layer : 1.0894313374086045e-05
gradient norm in None layer : 1.0593722827561832e-05
gradient norm in None layer : 0.00020842951009828674
gradient norm in None layer : 1.7111985929220443e-05
gradient norm in None layer : 1.911788444658468e-05
gradient norm in None layer : 0.00036345598057033483
gradient norm in None layer : 2.55522698928136e-06
gradient norm in None layer : 0.0005541785608808073
gradient norm in None layer : 4.842660846963796e-05
gradient norm in None layer : 4.186349396761675e-05
gradient norm in None layer : 0.000606384411227903
gradient norm in None layer : 7.76136816752955e-05
gradient norm in None layer : 0.00010069218468794196
gradient norm in None layer : 5.386453981742628e-05
gradient norm in None layer : 1.4135579134006657e-05
Total gradient norm: 0.0018181305662204519
invariance loss : 4.404500780182495, avg_den : 0.46646881103515625, density loss : 0.3656341695467065, mse loss : 0.06005928465959191, solver time : 118.31286954879761 sec , total loss : 0.06482941960932112, running loss : 0.0982827581521512
Epoch 0/10 , batch 25/12500 
ITERATION : 1, loss : 0.036063537518302005ITERATION : 2, loss : 0.04465266240822232ITERATION : 3, loss : 0.051865245789115104ITERATION : 4, loss : 0.0573936557070897ITERATION : 5, loss : 0.06088320048578415ITERATION : 6, loss : 0.06284175088372454ITERATION : 7, loss : 0.06425004656672313ITERATION : 8, loss : 0.06517679154740864ITERATION : 9, loss : 0.06580043578900929ITERATION : 10, loss : 0.06623763191128258ITERATION : 11, loss : 0.06654231697442331ITERATION : 12, loss : 0.06675396319131893ITERATION : 13, loss : 0.06690073111246064ITERATION : 14, loss : 0.06700243044689064ITERATION : 15, loss : 0.06707288597554537ITERATION : 16, loss : 0.06712170260963567ITERATION : 17, loss : 0.06715553731195215ITERATION : 18, loss : 0.0671789983406588ITERATION : 19, loss : 0.06719527435045951ITERATION : 20, loss : 0.06720657159144142ITERATION : 21, loss : 0.06721441710836157ITERATION : 22, loss : 0.06721986836653536ITERATION : 23, loss : 0.06722365784590005ITERATION : 24, loss : 0.06722629335730239ITERATION : 25, loss : 0.06722812709787167ITERATION : 26, loss : 0.06722940352410435ITERATION : 27, loss : 0.06723029233730637ITERATION : 28, loss : 0.06723091148307099ITERATION : 29, loss : 0.06723134289585718ITERATION : 30, loss : 0.06723164356140413ITERATION : 31, loss : 0.06723185321749069ITERATION : 32, loss : 0.06723199941912547ITERATION : 33, loss : 0.06723210139994439ITERATION : 34, loss : 0.06723217256585838ITERATION : 35, loss : 0.06723222221956265ITERATION : 36, loss : 0.06723225690281275ITERATION : 37, loss : 0.06723228108995032ITERATION : 38, loss : 0.06723229798278386ITERATION : 39, loss : 0.06723230977529769ITERATION : 40, loss : 0.06723231801173467ITERATION : 41, loss : 0.06723232373694027ITERATION : 42, loss : 0.06723232773365725ITERATION : 43, loss : 0.06723233051880961ITERATION : 44, loss : 0.06723233248293584ITERATION : 45, loss : 0.0672323338394644ITERATION : 46, loss : 0.06723233480891833ITERATION : 47, loss : 0.06723233546965814ITERATION : 48, loss : 0.06723233593386932ITERATION : 49, loss : 0.0672323362199189ITERATION : 50, loss : 0.06723233646481352ITERATION : 51, loss : 0.06723233659548276ITERATION : 52, loss : 0.06723233672307283ITERATION : 53, loss : 0.06723233677768206ITERATION : 54, loss : 0.06723233684526149ITERATION : 55, loss : 0.06723233684697036ITERATION : 56, loss : 0.06723233684697036ITERATION : 57, loss : 0.06723233684697036ITERATION : 58, loss : 0.06723233684697036ITERATION : 59, loss : 0.06723233684697036ITERATION : 60, loss : 0.06723233684697036ITERATION : 61, loss : 0.06723233684697036ITERATION : 62, loss : 0.06723233684697036ITERATION : 63, loss : 0.06723233684697036ITERATION : 64, loss : 0.06723233684697036ITERATION : 65, loss : 0.06723233684697036ITERATION : 66, loss : 0.06723233684697036ITERATION : 67, loss : 0.06723233684697036ITERATION : 68, loss : 0.06723233684697036ITERATION : 69, loss : 0.06723233684697036ITERATION : 70, loss : 0.06723233684697036ITERATION : 71, loss : 0.06723233684697036ITERATION : 72, loss : 0.06723233684697036ITERATION : 73, loss : 0.06723233684697036ITERATION : 74, loss : 0.06723233684697036ITERATION : 75, loss : 0.06723233684697036ITERATION : 76, loss : 0.06723233684697036ITERATION : 77, loss : 0.06723233684697036ITERATION : 78, loss : 0.06723233684697036ITERATION : 79, loss : 0.06723233684697036ITERATION : 80, loss : 0.06723233684697036ITERATION : 81, loss : 0.06723233684697036ITERATION : 82, loss : 0.06723233684697036ITERATION : 83, loss : 0.06723233684697036ITERATION : 84, loss : 0.06723233684697036ITERATION : 85, loss : 0.06723233684697036ITERATION : 86, loss : 0.06723233684697036ITERATION : 87, loss : 0.06723233684697036ITERATION : 88, loss : 0.06723233684697036ITERATION : 89, loss : 0.06723233684697036ITERATION : 90, loss : 0.06723233684697036ITERATION : 91, loss : 0.06723233684697036ITERATION : 92, loss : 0.06723233684697036ITERATION : 93, loss : 0.06723233684697036ITERATION : 94, loss : 0.06723233684697036ITERATION : 95, loss : 0.06723233684697036ITERATION : 96, loss : 0.06723233684697036ITERATION : 97, loss : 0.06723233684697036ITERATION : 98, loss : 0.06723233684697036ITERATION : 99, loss : 0.06723233684697036ITERATION : 100, loss : 0.06723233684697036
ITERATION : 1, loss : 0.03234350778748769ITERATION : 2, loss : 0.04178819591620252ITERATION : 3, loss : 0.0504922699073057ITERATION : 4, loss : 0.05740640349276829ITERATION : 5, loss : 0.06263474117814015ITERATION : 6, loss : 0.06649810708622689ITERATION : 7, loss : 0.06931945441476114ITERATION : 8, loss : 0.0713666878277347ITERATION : 9, loss : 0.07284679996720549ITERATION : 10, loss : 0.07391461873682274ITERATION : 11, loss : 0.07468403591065075ITERATION : 12, loss : 0.07523805432459439ITERATION : 13, loss : 0.07563683575458294ITERATION : 14, loss : 0.07592384230907441ITERATION : 15, loss : 0.07613040822810935ITERATION : 16, loss : 0.07627909717773841ITERATION : 17, loss : 0.07638614556804717ITERATION : 18, loss : 0.0764632325740251ITERATION : 19, loss : 0.0765187582045635ITERATION : 20, loss : 0.07655876424325493ITERATION : 21, loss : 0.07658759654166206ITERATION : 22, loss : 0.07660838191473503ITERATION : 23, loss : 0.07662337058238725ITERATION : 24, loss : 0.07663418211451091ITERATION : 25, loss : 0.07664198281352161ITERATION : 26, loss : 0.07664761280202416ITERATION : 27, loss : 0.07665167715516624ITERATION : 28, loss : 0.07665461205014992ITERATION : 29, loss : 0.07665673187929613ITERATION : 30, loss : 0.0766582634237841ITERATION : 31, loss : 0.07665937016813112ITERATION : 32, loss : 0.07666017007556736ITERATION : 33, loss : 0.0766607484600451ITERATION : 34, loss : 0.07666116664243093ITERATION : 35, loss : 0.076661469146725ITERATION : 36, loss : 0.07666168800631716ITERATION : 37, loss : 0.07666184638979813ITERATION : 38, loss : 0.07666196101070084ITERATION : 39, loss : 0.07666204400341452ITERATION : 40, loss : 0.07666210407604412ITERATION : 41, loss : 0.07666214759937046ITERATION : 42, loss : 0.07666217908460879ITERATION : 43, loss : 0.07666220192123745ITERATION : 44, loss : 0.07666221851398347ITERATION : 45, loss : 0.07666223047698542ITERATION : 46, loss : 0.07666223915504074ITERATION : 47, loss : 0.07666224544058828ITERATION : 48, loss : 0.07666224998806268ITERATION : 49, loss : 0.0766622532905984ITERATION : 50, loss : 0.07666225563544059ITERATION : 51, loss : 0.07666225739593724ITERATION : 52, loss : 0.07666225867061903ITERATION : 53, loss : 0.0766622595935328ITERATION : 54, loss : 0.07666226015636318ITERATION : 55, loss : 0.07666226059618099ITERATION : 56, loss : 0.07666226098796756ITERATION : 57, loss : 0.07666226127446366ITERATION : 58, loss : 0.07666226146393215ITERATION : 59, loss : 0.07666226156087691ITERATION : 60, loss : 0.07666226168470942ITERATION : 61, loss : 0.07666226168470942ITERATION : 62, loss : 0.07666226168470942ITERATION : 63, loss : 0.07666226168470942ITERATION : 64, loss : 0.07666226168470942ITERATION : 65, loss : 0.07666226168470942ITERATION : 66, loss : 0.07666226168470942ITERATION : 67, loss : 0.07666226168470942ITERATION : 68, loss : 0.07666226168470942ITERATION : 69, loss : 0.07666226168470942ITERATION : 70, loss : 0.07666226168470942ITERATION : 71, loss : 0.07666226168470942ITERATION : 72, loss : 0.07666226168470942ITERATION : 73, loss : 0.07666226168470942ITERATION : 74, loss : 0.07666226168470942ITERATION : 75, loss : 0.07666226168470942ITERATION : 76, loss : 0.07666226168470942ITERATION : 77, loss : 0.07666226168470942ITERATION : 78, loss : 0.07666226168470942ITERATION : 79, loss : 0.07666226168470942ITERATION : 80, loss : 0.07666226168470942ITERATION : 81, loss : 0.07666226168470942ITERATION : 82, loss : 0.07666226168470942ITERATION : 83, loss : 0.07666226168470942ITERATION : 84, loss : 0.07666226168470942ITERATION : 85, loss : 0.07666226168470942ITERATION : 86, loss : 0.07666226168470942ITERATION : 87, loss : 0.07666226168470942ITERATION : 88, loss : 0.07666226168470942ITERATION : 89, loss : 0.07666226168470942ITERATION : 90, loss : 0.07666226168470942ITERATION : 91, loss : 0.07666226168470942ITERATION : 92, loss : 0.07666226168470942ITERATION : 93, loss : 0.07666226168470942ITERATION : 94, loss : 0.07666226168470942ITERATION : 95, loss : 0.07666226168470942ITERATION : 96, loss : 0.07666226168470942ITERATION : 97, loss : 0.07666226168470942ITERATION : 98, loss : 0.07666226168470942ITERATION : 99, loss : 0.07666226168470942ITERATION : 100, loss : 0.07666226168470942
ITERATION : 1, loss : 0.03758156935086002ITERATION : 2, loss : 0.05379327243140892ITERATION : 3, loss : 0.06350041401324928ITERATION : 4, loss : 0.07032055516829541ITERATION : 5, loss : 0.07515994735184886ITERATION : 6, loss : 0.07868215691357194ITERATION : 7, loss : 0.08127371416122474ITERATION : 8, loss : 0.08328300429528951ITERATION : 9, loss : 0.08499754872234755ITERATION : 10, loss : 0.08624854432877384ITERATION : 11, loss : 0.08716242086339009ITERATION : 12, loss : 0.0878307284366723ITERATION : 13, loss : 0.08831990696074603ITERATION : 14, loss : 0.08867826320858319ITERATION : 15, loss : 0.08894097640394147ITERATION : 16, loss : 0.08913370064299934ITERATION : 17, loss : 0.08927516638412153ITERATION : 18, loss : 0.08937906347806565ITERATION : 19, loss : 0.08945540701932124ITERATION : 20, loss : 0.08951152975896395ITERATION : 21, loss : 0.08955280486362156ITERATION : 22, loss : 0.08958317191457647ITERATION : 23, loss : 0.0896055216489965ITERATION : 24, loss : 0.08962197608589462ITERATION : 25, loss : 0.08963409388181026ITERATION : 26, loss : 0.08964302046470352ITERATION : 27, loss : 0.08964959783545896ITERATION : 28, loss : 0.08965444549779537ITERATION : 29, loss : 0.08965801902051343ITERATION : 30, loss : 0.089660653795339ITERATION : 31, loss : 0.08966259685456177ITERATION : 32, loss : 0.08966402994527424ITERATION : 33, loss : 0.08966508714062957ITERATION : 34, loss : 0.08966586714414672ITERATION : 35, loss : 0.08966644273649052ITERATION : 36, loss : 0.08966686752047587ITERATION : 37, loss : 0.08966718105709377ITERATION : 38, loss : 0.08966741248503461ITERATION : 39, loss : 0.08966758332947064ITERATION : 40, loss : 0.08966770945922643ITERATION : 41, loss : 0.08966780259788594ITERATION : 42, loss : 0.0896678713774515ITERATION : 43, loss : 0.08966792219313821ITERATION : 44, loss : 0.08966795966178726ITERATION : 45, loss : 0.08966798739937205ITERATION : 46, loss : 0.08966800777760339ITERATION : 47, loss : 0.0896680229199008ITERATION : 48, loss : 0.08966803413166505ITERATION : 49, loss : 0.08966804234554493ITERATION : 50, loss : 0.08966804837563587ITERATION : 51, loss : 0.08966805283333482ITERATION : 52, loss : 0.08966805616048563ITERATION : 53, loss : 0.08966805863331737ITERATION : 54, loss : 0.08966806045862374ITERATION : 55, loss : 0.0896680617591579ITERATION : 56, loss : 0.08966806277766062ITERATION : 57, loss : 0.08966806351410374ITERATION : 58, loss : 0.08966806408675775ITERATION : 59, loss : 0.08966806447576094ITERATION : 60, loss : 0.08966806477190088ITERATION : 61, loss : 0.0896680649741284ITERATION : 62, loss : 0.08966806508237511ITERATION : 63, loss : 0.08966806519779248ITERATION : 64, loss : 0.08966806525589875ITERATION : 65, loss : 0.0896680652696228ITERATION : 66, loss : 0.08966806527779377ITERATION : 67, loss : 0.08966806527779377ITERATION : 68, loss : 0.08966806527779377ITERATION : 69, loss : 0.08966806527779377ITERATION : 70, loss : 0.08966806527779377ITERATION : 71, loss : 0.08966806527779377ITERATION : 72, loss : 0.08966806527779377ITERATION : 73, loss : 0.08966806527779377ITERATION : 74, loss : 0.08966806527779377ITERATION : 75, loss : 0.08966806527779377ITERATION : 76, loss : 0.08966806527779377ITERATION : 77, loss : 0.08966806527779377ITERATION : 78, loss : 0.08966806527779377ITERATION : 79, loss : 0.08966806527779377ITERATION : 80, loss : 0.08966806527779377ITERATION : 81, loss : 0.08966806527779377ITERATION : 82, loss : 0.08966806527779377ITERATION : 83, loss : 0.08966806527779377ITERATION : 84, loss : 0.08966806527779377ITERATION : 85, loss : 0.08966806527779377ITERATION : 86, loss : 0.08966806527779377ITERATION : 87, loss : 0.08966806527779377ITERATION : 88, loss : 0.08966806527779377ITERATION : 89, loss : 0.08966806527779377ITERATION : 90, loss : 0.08966806527779377ITERATION : 91, loss : 0.08966806527779377ITERATION : 92, loss : 0.08966806527779377ITERATION : 93, loss : 0.08966806527779377ITERATION : 94, loss : 0.08966806527779377ITERATION : 95, loss : 0.08966806527779377ITERATION : 96, loss : 0.08966806527779377ITERATION : 97, loss : 0.08966806527779377ITERATION : 98, loss : 0.08966806527779377ITERATION : 99, loss : 0.08966806527779377ITERATION : 100, loss : 0.08966806527779377
ITERATION : 1, loss : 0.14521339300903505ITERATION : 2, loss : 0.1609133759640957ITERATION : 3, loss : 0.16958658266568385ITERATION : 4, loss : 0.175172519520477ITERATION : 5, loss : 0.17891130251542564ITERATION : 6, loss : 0.1814517149502754ITERATION : 7, loss : 0.183190449316951ITERATION : 8, loss : 0.18438503768626963ITERATION : 9, loss : 0.18520745859621657ITERATION : 10, loss : 0.1857742697915078ITERATION : 11, loss : 0.1861651167300542ITERATION : 12, loss : 0.1864346752868115ITERATION : 13, loss : 0.18662057932081655ITERATION : 14, loss : 0.18674877065042486ITERATION : 15, loss : 0.18683714526819978ITERATION : 16, loss : 0.1868980531509804ITERATION : 17, loss : 0.1869400175511041ITERATION : 18, loss : 0.1869689202678462ITERATION : 19, loss : 0.18698881955583643ITERATION : 20, loss : 0.18700251490941958ITERATION : 21, loss : 0.18701193678931377ITERATION : 22, loss : 0.18701841593373236ITERATION : 23, loss : 0.18702286957660114ITERATION : 24, loss : 0.18702592959196848ITERATION : 25, loss : 0.18702803108748742ITERATION : 26, loss : 0.18702947362621775ITERATION : 27, loss : 0.18703046332021525ITERATION : 28, loss : 0.18703114201415755ITERATION : 29, loss : 0.18703160713018044ITERATION : 30, loss : 0.18703192571842556ITERATION : 31, loss : 0.1870321438475861ITERATION : 32, loss : 0.18703229302411967ITERATION : 33, loss : 0.18703239500293714ITERATION : 34, loss : 0.1870324646774069ITERATION : 35, loss : 0.18703251218286857ITERATION : 36, loss : 0.18703254464248736ITERATION : 37, loss : 0.18703256679031202ITERATION : 38, loss : 0.18703258188268668ITERATION : 39, loss : 0.18703259218258234ITERATION : 40, loss : 0.1870325991847055ITERATION : 41, loss : 0.1870326039437504ITERATION : 42, loss : 0.18703260716165893ITERATION : 43, loss : 0.1870326093227128ITERATION : 44, loss : 0.18703261076559596ITERATION : 45, loss : 0.18703261173222668ITERATION : 46, loss : 0.18703261236919363ITERATION : 47, loss : 0.18703261278487185ITERATION : 48, loss : 0.18703261307024266ITERATION : 49, loss : 0.18703261326099171ITERATION : 50, loss : 0.187032613366878ITERATION : 51, loss : 0.18703261346285724ITERATION : 52, loss : 0.1870326135159901ITERATION : 53, loss : 0.18703261354165723ITERATION : 54, loss : 0.18703261356175155ITERATION : 55, loss : 0.18703261358335294ITERATION : 56, loss : 0.18703261358692527ITERATION : 57, loss : 0.18703261358705944ITERATION : 58, loss : 0.18703261358705944ITERATION : 59, loss : 0.18703261358705944ITERATION : 60, loss : 0.18703261358705944ITERATION : 61, loss : 0.18703261358705944ITERATION : 62, loss : 0.18703261358705944ITERATION : 63, loss : 0.18703261358705944ITERATION : 64, loss : 0.18703261358705944ITERATION : 65, loss : 0.18703261358705944ITERATION : 66, loss : 0.18703261358705944ITERATION : 67, loss : 0.18703261358705944ITERATION : 68, loss : 0.18703261358705944ITERATION : 69, loss : 0.18703261358705944ITERATION : 70, loss : 0.18703261358705944ITERATION : 71, loss : 0.18703261358705944ITERATION : 72, loss : 0.18703261358705944ITERATION : 73, loss : 0.18703261358705944ITERATION : 74, loss : 0.18703261358705944ITERATION : 75, loss : 0.18703261358705944ITERATION : 76, loss : 0.18703261358705944ITERATION : 77, loss : 0.18703261358705944ITERATION : 78, loss : 0.18703261358705944ITERATION : 79, loss : 0.18703261358705944ITERATION : 80, loss : 0.18703261358705944ITERATION : 81, loss : 0.18703261358705944ITERATION : 82, loss : 0.18703261358705944ITERATION : 83, loss : 0.18703261358705944ITERATION : 84, loss : 0.18703261358705944ITERATION : 85, loss : 0.18703261358705944ITERATION : 86, loss : 0.18703261358705944ITERATION : 87, loss : 0.18703261358705944ITERATION : 88, loss : 0.18703261358705944ITERATION : 89, loss : 0.18703261358705944ITERATION : 90, loss : 0.18703261358705944ITERATION : 91, loss : 0.18703261358705944ITERATION : 92, loss : 0.18703261358705944ITERATION : 93, loss : 0.18703261358705944ITERATION : 94, loss : 0.18703261358705944ITERATION : 95, loss : 0.18703261358705944ITERATION : 96, loss : 0.18703261358705944ITERATION : 97, loss : 0.18703261358705944ITERATION : 98, loss : 0.18703261358705944ITERATION : 99, loss : 0.18703261358705944ITERATION : 100, loss : 0.18703261358705944
ITERATION : 1, loss : 0.07959981119775031ITERATION : 2, loss : 0.08419010871475498ITERATION : 3, loss : 0.09144715080282786ITERATION : 4, loss : 0.098006674145317ITERATION : 5, loss : 0.10330373244061924ITERATION : 6, loss : 0.10739802830672202ITERATION : 7, loss : 0.11050410812938172ITERATION : 8, loss : 0.11284230444930526ITERATION : 9, loss : 0.11459750778939416ITERATION : 10, loss : 0.11591426043156436ITERATION : 11, loss : 0.11690237172188578ITERATION : 12, loss : 0.11764431115471079ITERATION : 13, loss : 0.11820176958225401ITERATION : 14, loss : 0.11862086386068303ITERATION : 15, loss : 0.11893609211781071ITERATION : 16, loss : 0.11917328987220806ITERATION : 17, loss : 0.11935182790192475ITERATION : 18, loss : 0.11948624505819809ITERATION : 19, loss : 0.11958746294481742ITERATION : 20, loss : 0.11966369167527567ITERATION : 21, loss : 0.11972110652757575ITERATION : 22, loss : 0.11976435410512372ITERATION : 23, loss : 0.11979693202699923ITERATION : 24, loss : 0.11982147347845325ITERATION : 25, loss : 0.11983996145910566ITERATION : 26, loss : 0.11985388937484961ITERATION : 27, loss : 0.11986438210495022ITERATION : 28, loss : 0.11987228692491365ITERATION : 29, loss : 0.11987824213797314ITERATION : 30, loss : 0.11988272856422938ITERATION : 31, loss : 0.11988610846457332ITERATION : 32, loss : 0.11988865473756664ITERATION : 33, loss : 0.11989057298088847ITERATION : 34, loss : 0.11989201808600007ITERATION : 35, loss : 0.11989310674528071ITERATION : 36, loss : 0.11989392687925005ITERATION : 37, loss : 0.1198945447208185ITERATION : 38, loss : 0.11989501015628572ITERATION : 39, loss : 0.11989536078332058ITERATION : 40, loss : 0.11989562491459933ITERATION : 41, loss : 0.11989582388359075ITERATION : 42, loss : 0.11989597375720534ITERATION : 43, loss : 0.11989608667009768ITERATION : 44, loss : 0.1198961717215628ITERATION : 45, loss : 0.11989623578238162ITERATION : 46, loss : 0.11989628406433607ITERATION : 47, loss : 0.11989632040568006ITERATION : 48, loss : 0.11989634778985922ITERATION : 49, loss : 0.11989636839614923ITERATION : 50, loss : 0.11989638396339557ITERATION : 51, loss : 0.11989639563440839ITERATION : 52, loss : 0.11989640448355857ITERATION : 53, loss : 0.11989641109069828ITERATION : 54, loss : 0.11989641621590429ITERATION : 55, loss : 0.11989641994690713ITERATION : 56, loss : 0.11989642277542303ITERATION : 57, loss : 0.11989642489278696ITERATION : 58, loss : 0.11989642645737106ITERATION : 59, loss : 0.11989642775237405ITERATION : 60, loss : 0.1198964286228208ITERATION : 61, loss : 0.11989642930878212ITERATION : 62, loss : 0.11989642977404193ITERATION : 63, loss : 0.1198964301713577ITERATION : 64, loss : 0.11989643043729979ITERATION : 65, loss : 0.11989643067016721ITERATION : 66, loss : 0.11989643077166161ITERATION : 67, loss : 0.1198964309111332ITERATION : 68, loss : 0.11989643103640817ITERATION : 69, loss : 0.11989643103821244ITERATION : 70, loss : 0.11989643103821244ITERATION : 71, loss : 0.11989643103821244ITERATION : 72, loss : 0.11989643103821244ITERATION : 73, loss : 0.11989643103821244ITERATION : 74, loss : 0.11989643103821244ITERATION : 75, loss : 0.11989643103821244ITERATION : 76, loss : 0.11989643103821244ITERATION : 77, loss : 0.11989643103821244ITERATION : 78, loss : 0.11989643103821244ITERATION : 79, loss : 0.11989643103821244ITERATION : 80, loss : 0.11989643103821244ITERATION : 81, loss : 0.11989643103821244ITERATION : 82, loss : 0.11989643103821244ITERATION : 83, loss : 0.11989643103821244ITERATION : 84, loss : 0.11989643103821244ITERATION : 85, loss : 0.11989643103821244ITERATION : 86, loss : 0.11989643103821244ITERATION : 87, loss : 0.11989643103821244ITERATION : 88, loss : 0.11989643103821244ITERATION : 89, loss : 0.11989643103821244ITERATION : 90, loss : 0.11989643103821244ITERATION : 91, loss : 0.11989643103821244ITERATION : 92, loss : 0.11989643103821244ITERATION : 93, loss : 0.11989643103821244ITERATION : 94, loss : 0.11989643103821244ITERATION : 95, loss : 0.11989643103821244ITERATION : 96, loss : 0.11989643103821244ITERATION : 97, loss : 0.11989643103821244ITERATION : 98, loss : 0.11989643103821244ITERATION : 99, loss : 0.11989643103821244ITERATION : 100, loss : 0.11989643103821244
ITERATION : 1, loss : 0.019246790254670226ITERATION : 2, loss : 0.028014808019661844ITERATION : 3, loss : 0.03435417082283556ITERATION : 4, loss : 0.03876752752394403ITERATION : 5, loss : 0.04178517707809192ITERATION : 6, loss : 0.04383617336329629ITERATION : 7, loss : 0.045228655116540235ITERATION : 8, loss : 0.046174775317307855ITERATION : 9, loss : 0.04681848958797492ITERATION : 10, loss : 0.04725709251271812ITERATION : 11, loss : 0.04755635068735876ITERATION : 12, loss : 0.04776078887735824ITERATION : 13, loss : 0.04790060851547909ITERATION : 14, loss : 0.04799633286009634ITERATION : 15, loss : 0.04806193136080091ITERATION : 16, loss : 0.048106925812471404ITERATION : 17, loss : 0.0481378146476274ITERATION : 18, loss : 0.04815903780546256ITERATION : 19, loss : 0.04817363178184439ITERATION : 20, loss : 0.048183675315500905ITERATION : 21, loss : 0.048190592623401106ITERATION : 22, loss : 0.04819536049738882ITERATION : 23, loss : 0.0481986493161665ITERATION : 24, loss : 0.04820091959419921ITERATION : 25, loss : 0.04820248792537791ITERATION : 26, loss : 0.04820357207073266ITERATION : 27, loss : 0.04820432205786496ITERATION : 28, loss : 0.04820484122151511ITERATION : 29, loss : 0.048205200822053845ITERATION : 30, loss : 0.048205450082521535ITERATION : 31, loss : 0.048205622938035896ITERATION : 32, loss : 0.04820574288782891ITERATION : 33, loss : 0.048205826217109ITERATION : 34, loss : 0.04820588410216965ITERATION : 35, loss : 0.04820592435145456ITERATION : 36, loss : 0.048205952343380966ITERATION : 37, loss : 0.04820597182758608ITERATION : 38, loss : 0.04820598538857134ITERATION : 39, loss : 0.04820599483670531ITERATION : 40, loss : 0.04820600143124138ITERATION : 41, loss : 0.04820600601589519ITERATION : 42, loss : 0.04820600921397154ITERATION : 43, loss : 0.04820601144321277ITERATION : 44, loss : 0.048206012988680255ITERATION : 45, loss : 0.04820601408750198ITERATION : 46, loss : 0.048206014822973056ITERATION : 47, loss : 0.04820601531977588ITERATION : 48, loss : 0.04820601567461929ITERATION : 49, loss : 0.048206015944123194ITERATION : 50, loss : 0.04820601613767247ITERATION : 51, loss : 0.0482060162693084ITERATION : 52, loss : 0.04820601635439725ITERATION : 53, loss : 0.04820601641431497ITERATION : 54, loss : 0.04820601644877973ITERATION : 55, loss : 0.04820601646110709ITERATION : 56, loss : 0.04820601646136605ITERATION : 57, loss : 0.04820601646136605ITERATION : 58, loss : 0.04820601646136605ITERATION : 59, loss : 0.04820601646136605ITERATION : 60, loss : 0.04820601646136605ITERATION : 61, loss : 0.04820601646136605ITERATION : 62, loss : 0.04820601646136605ITERATION : 63, loss : 0.04820601646136605ITERATION : 64, loss : 0.04820601646136605ITERATION : 65, loss : 0.04820601646136605ITERATION : 66, loss : 0.04820601646136605ITERATION : 67, loss : 0.04820601646136605ITERATION : 68, loss : 0.04820601646136605ITERATION : 69, loss : 0.04820601646136605ITERATION : 70, loss : 0.04820601646136605ITERATION : 71, loss : 0.04820601646136605ITERATION : 72, loss : 0.04820601646136605ITERATION : 73, loss : 0.04820601646136605ITERATION : 74, loss : 0.04820601646136605ITERATION : 75, loss : 0.04820601646136605ITERATION : 76, loss : 0.04820601646136605ITERATION : 77, loss : 0.04820601646136605ITERATION : 78, loss : 0.04820601646136605ITERATION : 79, loss : 0.04820601646136605ITERATION : 80, loss : 0.04820601646136605ITERATION : 81, loss : 0.04820601646136605ITERATION : 82, loss : 0.04820601646136605ITERATION : 83, loss : 0.04820601646136605ITERATION : 84, loss : 0.04820601646136605ITERATION : 85, loss : 0.04820601646136605ITERATION : 86, loss : 0.04820601646136605ITERATION : 87, loss : 0.04820601646136605ITERATION : 88, loss : 0.04820601646136605ITERATION : 89, loss : 0.04820601646136605ITERATION : 90, loss : 0.04820601646136605ITERATION : 91, loss : 0.04820601646136605ITERATION : 92, loss : 0.04820601646136605ITERATION : 93, loss : 0.04820601646136605ITERATION : 94, loss : 0.04820601646136605ITERATION : 95, loss : 0.04820601646136605ITERATION : 96, loss : 0.04820601646136605ITERATION : 97, loss : 0.04820601646136605ITERATION : 98, loss : 0.04820601646136605ITERATION : 99, loss : 0.04820601646136605ITERATION : 100, loss : 0.04820601646136605
ITERATION : 1, loss : 0.06750751215744898ITERATION : 2, loss : 0.07207467334492554ITERATION : 3, loss : 0.07733978218263257ITERATION : 4, loss : 0.0815398271809223ITERATION : 5, loss : 0.08461242591880609ITERATION : 6, loss : 0.08681402144246983ITERATION : 7, loss : 0.08838662537253042ITERATION : 8, loss : 0.08951195969637908ITERATION : 9, loss : 0.09031958809981186ITERATION : 10, loss : 0.09090087707877881ITERATION : 11, loss : 0.09132030826346023ITERATION : 12, loss : 0.09162357968596711ITERATION : 13, loss : 0.0918432304880968ITERATION : 14, loss : 0.09200253196805214ITERATION : 15, loss : 0.09211818925799797ITERATION : 16, loss : 0.09220223164581191ITERATION : 17, loss : 0.09226334298693646ITERATION : 18, loss : 0.09230780474840794ITERATION : 19, loss : 0.09234016740215752ITERATION : 20, loss : 0.09236373213002214ITERATION : 21, loss : 0.09238089612789306ITERATION : 22, loss : 0.09239340074102657ITERATION : 23, loss : 0.09240251287666917ITERATION : 24, loss : 0.09240915421995365ITERATION : 25, loss : 0.09241399558058903ITERATION : 26, loss : 0.09241752529417514ITERATION : 27, loss : 0.09242009915544239ITERATION : 28, loss : 0.0924219760402389ITERATION : 29, loss : 0.09242334493879771ITERATION : 30, loss : 0.09242434347566536ITERATION : 31, loss : 0.09242507190997079ITERATION : 32, loss : 0.09242560336365026ITERATION : 33, loss : 0.09242599111483389ITERATION : 34, loss : 0.0924262740435927ITERATION : 35, loss : 0.09242648051498281ITERATION : 36, loss : 0.0924266312300431ITERATION : 37, loss : 0.09242674121194135ITERATION : 38, loss : 0.09242682148538872ITERATION : 39, loss : 0.09242688008972695ITERATION : 40, loss : 0.09242692286854381ITERATION : 41, loss : 0.09242695409137422ITERATION : 42, loss : 0.09242697687047235ITERATION : 43, loss : 0.09242699352785688ITERATION : 44, loss : 0.09242700555715111ITERATION : 45, loss : 0.09242701440993668ITERATION : 46, loss : 0.0924270209741877ITERATION : 47, loss : 0.09242702562603014ITERATION : 48, loss : 0.09242702891564625ITERATION : 49, loss : 0.09242703148160811ITERATION : 50, loss : 0.09242703330718997ITERATION : 51, loss : 0.09242703456219893ITERATION : 52, loss : 0.09242703551304991ITERATION : 53, loss : 0.09242703628196713ITERATION : 54, loss : 0.0924270367990619ITERATION : 55, loss : 0.09242703724992167ITERATION : 56, loss : 0.09242703744102593ITERATION : 57, loss : 0.09242703765881599ITERATION : 58, loss : 0.09242703768140643ITERATION : 59, loss : 0.09242703768140643ITERATION : 60, loss : 0.09242703768140643ITERATION : 61, loss : 0.09242703768140643ITERATION : 62, loss : 0.09242703768140643ITERATION : 63, loss : 0.09242703768140643ITERATION : 64, loss : 0.09242703768140643ITERATION : 65, loss : 0.09242703768140643ITERATION : 66, loss : 0.09242703768140643ITERATION : 67, loss : 0.09242703768140643ITERATION : 68, loss : 0.09242703768140643ITERATION : 69, loss : 0.09242703768140643ITERATION : 70, loss : 0.09242703768140643ITERATION : 71, loss : 0.09242703768140643ITERATION : 72, loss : 0.09242703768140643ITERATION : 73, loss : 0.09242703768140643ITERATION : 74, loss : 0.09242703768140643ITERATION : 75, loss : 0.09242703768140643ITERATION : 76, loss : 0.09242703768140643ITERATION : 77, loss : 0.09242703768140643ITERATION : 78, loss : 0.09242703768140643ITERATION : 79, loss : 0.09242703768140643ITERATION : 80, loss : 0.09242703768140643ITERATION : 81, loss : 0.09242703768140643ITERATION : 82, loss : 0.09242703768140643ITERATION : 83, loss : 0.09242703768140643ITERATION : 84, loss : 0.09242703768140643ITERATION : 85, loss : 0.09242703768140643ITERATION : 86, loss : 0.09242703768140643ITERATION : 87, loss : 0.09242703768140643ITERATION : 88, loss : 0.09242703768140643ITERATION : 89, loss : 0.09242703768140643ITERATION : 90, loss : 0.09242703768140643ITERATION : 91, loss : 0.09242703768140643ITERATION : 92, loss : 0.09242703768140643ITERATION : 93, loss : 0.09242703768140643ITERATION : 94, loss : 0.09242703768140643ITERATION : 95, loss : 0.09242703768140643ITERATION : 96, loss : 0.09242703768140643ITERATION : 97, loss : 0.09242703768140643ITERATION : 98, loss : 0.09242703768140643ITERATION : 99, loss : 0.09242703768140643ITERATION : 100, loss : 0.09242703768140643
ITERATION : 1, loss : 0.008054156041130543ITERATION : 2, loss : 0.008386248877234678ITERATION : 3, loss : 0.00959654124969476ITERATION : 4, loss : 0.010756335536309943ITERATION : 5, loss : 0.0116904918130929ITERATION : 6, loss : 0.012394239542294851ITERATION : 7, loss : 0.012906335219677614ITERATION : 8, loss : 0.013271445293604588ITERATION : 9, loss : 0.013520898759626702ITERATION : 10, loss : 0.013679937085151918ITERATION : 11, loss : 0.013790279410876932ITERATION : 12, loss : 0.013866580865908884ITERATION : 13, loss : 0.013919227274472598ITERATION : 14, loss : 0.013955499863933574ITERATION : 15, loss : 0.013980467616569311ITERATION : 16, loss : 0.013997643467908362ITERATION : 17, loss : 0.014009454514905345ITERATION : 18, loss : 0.014017574456648151ITERATION : 19, loss : 0.014023156018948746ITERATION : 20, loss : 0.01402699238169479ITERATION : 21, loss : 0.014029629090502808ITERATION : 22, loss : 0.014031441234161777ITERATION : 23, loss : 0.014032686659081495ITERATION : 24, loss : 0.014033542595857271ITERATION : 25, loss : 0.014034130855500863ITERATION : 26, loss : 0.014034535152105787ITERATION : 27, loss : 0.01403481301313987ITERATION : 28, loss : 0.014035003991208798ITERATION : 29, loss : 0.014035135225833497ITERATION : 30, loss : 0.014035225424144517ITERATION : 31, loss : 0.01403528740004192ITERATION : 32, loss : 0.014035330004998216ITERATION : 33, loss : 0.014035359273036096ITERATION : 34, loss : 0.014035379396886349ITERATION : 35, loss : 0.014035393217455852ITERATION : 36, loss : 0.014035402722322473ITERATION : 37, loss : 0.014035409243320352ITERATION : 38, loss : 0.014035413730055851ITERATION : 39, loss : 0.014035416808802935ITERATION : 40, loss : 0.014035418929414336ITERATION : 41, loss : 0.014035420386939168ITERATION : 42, loss : 0.014035421390936982ITERATION : 43, loss : 0.014035422063676976ITERATION : 44, loss : 0.014035422529492437ITERATION : 45, loss : 0.014035422856433957ITERATION : 46, loss : 0.014035423083015999ITERATION : 47, loss : 0.014035423233385911ITERATION : 48, loss : 0.014035423336686134ITERATION : 49, loss : 0.014035423387287707ITERATION : 50, loss : 0.014035423443631461ITERATION : 51, loss : 0.014035423444280416ITERATION : 52, loss : 0.014035423444280416ITERATION : 53, loss : 0.014035423444280416ITERATION : 54, loss : 0.014035423444280416ITERATION : 55, loss : 0.014035423444280416ITERATION : 56, loss : 0.014035423444280416ITERATION : 57, loss : 0.014035423444280416ITERATION : 58, loss : 0.014035423444280416ITERATION : 59, loss : 0.014035423444280416ITERATION : 60, loss : 0.014035423444280416ITERATION : 61, loss : 0.014035423444280416ITERATION : 62, loss : 0.014035423444280416ITERATION : 63, loss : 0.014035423444280416ITERATION : 64, loss : 0.014035423444280416ITERATION : 65, loss : 0.014035423444280416ITERATION : 66, loss : 0.014035423444280416ITERATION : 67, loss : 0.014035423444280416ITERATION : 68, loss : 0.014035423444280416ITERATION : 69, loss : 0.014035423444280416ITERATION : 70, loss : 0.014035423444280416ITERATION : 71, loss : 0.014035423444280416ITERATION : 72, loss : 0.014035423444280416ITERATION : 73, loss : 0.014035423444280416ITERATION : 74, loss : 0.014035423444280416ITERATION : 75, loss : 0.014035423444280416ITERATION : 76, loss : 0.014035423444280416ITERATION : 77, loss : 0.014035423444280416ITERATION : 78, loss : 0.014035423444280416ITERATION : 79, loss : 0.014035423444280416ITERATION : 80, loss : 0.014035423444280416ITERATION : 81, loss : 0.014035423444280416ITERATION : 82, loss : 0.014035423444280416ITERATION : 83, loss : 0.014035423444280416ITERATION : 84, loss : 0.014035423444280416ITERATION : 85, loss : 0.014035423444280416ITERATION : 86, loss : 0.014035423444280416ITERATION : 87, loss : 0.014035423444280416ITERATION : 88, loss : 0.014035423444280416ITERATION : 89, loss : 0.014035423444280416ITERATION : 90, loss : 0.014035423444280416ITERATION : 91, loss : 0.014035423444280416ITERATION : 92, loss : 0.014035423444280416ITERATION : 93, loss : 0.014035423444280416ITERATION : 94, loss : 0.014035423444280416ITERATION : 95, loss : 0.014035423444280416ITERATION : 96, loss : 0.014035423444280416ITERATION : 97, loss : 0.014035423444280416ITERATION : 98, loss : 0.014035423444280416ITERATION : 99, loss : 0.014035423444280416ITERATION : 100, loss : 0.014035423444280416
gradient norm in None layer : 0.0010085061937017495
gradient norm in None layer : 3.543430382812366e-05
gradient norm in None layer : 6.840059318067306e-05
gradient norm in None layer : 0.000643595760663311
gradient norm in None layer : 4.326893219402578e-05
gradient norm in None layer : 6.179275256855631e-05
gradient norm in None layer : 0.0003055143802648508
gradient norm in None layer : 1.265320125701324e-05
gradient norm in None layer : 1.420219307266705e-05
gradient norm in None layer : 0.00027586855255407847
gradient norm in None layer : 1.5214691549696333e-05
gradient norm in None layer : 1.5219222477329634e-05
gradient norm in None layer : 8.066940648585163e-05
gradient norm in None layer : 2.407258652100364e-06
gradient norm in None layer : 2.3828700689101622e-06
gradient norm in None layer : 7.768300667833277e-05
gradient norm in None layer : 2.8689226400524632e-06
gradient norm in None layer : 2.6353149267561274e-06
gradient norm in None layer : 0.00010555872886958316
gradient norm in None layer : 2.3149679114161024e-06
gradient norm in None layer : 0.0002471387259052473
gradient norm in None layer : 1.4412158839304862e-05
gradient norm in None layer : 1.3041094920553886e-05
gradient norm in None layer : 0.00023787496119274332
gradient norm in None layer : 2.418516362318424e-05
gradient norm in None layer : 2.6806014214743102e-05
gradient norm in None layer : 0.0004150824292064862
gradient norm in None layer : 5.166088728417055e-06
gradient norm in None layer : 0.0005819949337284653
gradient norm in None layer : 4.1473092249346116e-05
gradient norm in None layer : 5.0366592057908666e-05
gradient norm in None layer : 0.0006039062594857056
gradient norm in None layer : 8.505718167189435e-05
gradient norm in None layer : 0.0001231208527033557
gradient norm in None layer : 5.9969748955943794e-05
gradient norm in None layer : 1.6033022736080824e-05
Total gradient norm: 0.0016317160580806018
invariance loss : 4.486166500527268, avg_den : 0.46030426025390625, density loss : 0.35971776970726954, mse loss : 0.0868950232527248, solver time : 113.67934131622314 sec , total loss : 0.09174090752295934, running loss : 0.09802108412698353
Epoch 0/10 , batch 26/12500 
ITERATION : 1, loss : 0.042546843982140835ITERATION : 2, loss : 0.05480890270765138ITERATION : 3, loss : 0.06425268456474001ITERATION : 4, loss : 0.07107774303669569ITERATION : 5, loss : 0.07610479666359556ITERATION : 6, loss : 0.07975950850702468ITERATION : 7, loss : 0.08240610033324437ITERATION : 8, loss : 0.0843207055617579ITERATION : 9, loss : 0.08570562975427164ITERATION : 10, loss : 0.08663967515942655ITERATION : 11, loss : 0.08727685186798909ITERATION : 12, loss : 0.08774017173455523ITERATION : 13, loss : 0.08807702696111079ITERATION : 14, loss : 0.08832192557585203ITERATION : 15, loss : 0.08849997916810161ITERATION : 16, loss : 0.08862945072932094ITERATION : 17, loss : 0.08872361633643185ITERATION : 18, loss : 0.08879212343874729ITERATION : 19, loss : 0.08884198099448311ITERATION : 20, loss : 0.08887828038581325ITERATION : 21, loss : 0.08890472025272016ITERATION : 22, loss : 0.08892398781920142ITERATION : 23, loss : 0.08893803567057064ITERATION : 24, loss : 0.08894828328395843ITERATION : 25, loss : 0.08895576266457592ITERATION : 26, loss : 0.08896122455961214ITERATION : 27, loss : 0.0889652154220668ITERATION : 28, loss : 0.08896813301970007ITERATION : 29, loss : 0.08897026717636049ITERATION : 30, loss : 0.08897182912401916ITERATION : 31, loss : 0.08897297295164502ITERATION : 32, loss : 0.08897381100265406ITERATION : 33, loss : 0.08897442539200998ITERATION : 34, loss : 0.08897487597501146ITERATION : 35, loss : 0.08897520664376526ITERATION : 36, loss : 0.08897544942984992ITERATION : 37, loss : 0.08897562776411828ITERATION : 38, loss : 0.08897575885832702ITERATION : 39, loss : 0.08897585532830131ITERATION : 40, loss : 0.08897592627264476ITERATION : 41, loss : 0.0889759784747797ITERATION : 42, loss : 0.08897601690480934ITERATION : 43, loss : 0.08897604520440569ITERATION : 44, loss : 0.08897606603523682ITERATION : 45, loss : 0.08897608146441567ITERATION : 46, loss : 0.0889760928322553ITERATION : 47, loss : 0.08897610115916448ITERATION : 48, loss : 0.08897610733398448ITERATION : 49, loss : 0.08897611185003432ITERATION : 50, loss : 0.08897611521342665ITERATION : 51, loss : 0.08897611766837724ITERATION : 52, loss : 0.08897611950667211ITERATION : 53, loss : 0.08897612083542415ITERATION : 54, loss : 0.08897612185207227ITERATION : 55, loss : 0.08897612257159866ITERATION : 56, loss : 0.08897612313296141ITERATION : 57, loss : 0.08897612349900362ITERATION : 58, loss : 0.08897612381271418ITERATION : 59, loss : 0.08897612399857921ITERATION : 60, loss : 0.08897612417928137ITERATION : 61, loss : 0.08897612427249653ITERATION : 62, loss : 0.088976124381128ITERATION : 63, loss : 0.08897612442331272ITERATION : 64, loss : 0.08897612448015457ITERATION : 65, loss : 0.08897612447983352ITERATION : 66, loss : 0.08897612447983352ITERATION : 67, loss : 0.08897612447983352ITERATION : 68, loss : 0.08897612447983352ITERATION : 69, loss : 0.08897612447983352ITERATION : 70, loss : 0.08897612447983352ITERATION : 71, loss : 0.08897612447983352ITERATION : 72, loss : 0.08897612447983352ITERATION : 73, loss : 0.08897612447983352ITERATION : 74, loss : 0.08897612447983352ITERATION : 75, loss : 0.08897612447983352ITERATION : 76, loss : 0.08897612447983352ITERATION : 77, loss : 0.08897612447983352ITERATION : 78, loss : 0.08897612447983352ITERATION : 79, loss : 0.08897612447983352ITERATION : 80, loss : 0.08897612447983352ITERATION : 81, loss : 0.08897612447983352ITERATION : 82, loss : 0.08897612447983352ITERATION : 83, loss : 0.08897612447983352ITERATION : 84, loss : 0.08897612447983352ITERATION : 85, loss : 0.08897612447983352ITERATION : 86, loss : 0.08897612447983352ITERATION : 87, loss : 0.08897612447983352ITERATION : 88, loss : 0.08897612447983352ITERATION : 89, loss : 0.08897612447983352ITERATION : 90, loss : 0.08897612447983352ITERATION : 91, loss : 0.08897612447983352ITERATION : 92, loss : 0.08897612447983352ITERATION : 93, loss : 0.08897612447983352ITERATION : 94, loss : 0.08897612447983352ITERATION : 95, loss : 0.08897612447983352ITERATION : 96, loss : 0.08897612447983352ITERATION : 97, loss : 0.08897612447983352ITERATION : 98, loss : 0.08897612447983352ITERATION : 99, loss : 0.08897612447983352ITERATION : 100, loss : 0.08897612447983352
ITERATION : 1, loss : 0.014073357963021066ITERATION : 2, loss : 0.007305576646492331ITERATION : 3, loss : 0.004789940783536747ITERATION : 4, loss : 0.004384428931067789ITERATION : 5, loss : 0.004392774737372772