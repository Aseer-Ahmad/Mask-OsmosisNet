CONFIG : 
{'EPOCHS': 10, 'RESUME_CHECKPOINT': None, 'SAVE_EVERY': 10, 'VAL_EVERY': 10, 'OUTPUT_DIR': 'unet-imgnet', 'EXP_NAME': 'test_3', 'TRAIN_FILENAME': 'iids_train.txt', 'TEST_FILENAME': 'iids_test.txt', 'ROOT_DIR': 'dataset\\imagenet\\images\\', 'IMG_SIZE': 128, 'INP_CHANNELS': 1, 'OUT_CHANNELS': 1, 'LR': 0.001, 'WEIGHT_DECAY': 0.0, 'MOMENTUM': 0.9, 'OPT': 'Adam', 'SCHEDL': 'lambdaLR', 'TRAIN_BATCH': 8, 'TEST_BATCH': 8, 'ALPHA': 0.001, 'MASK_DEN': 0.1, 'BIN_METH': 'QUANT', 'OFFSET': None, 'TAU': None, 'ITERATIONS': None}

train test dataset loaded
train size : 100000
test  size  : 1000
model loaded
model summary
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
??DoubleConv: 1-1                        --
|    ??Sequential: 2-1                   --
|    |    ??Conv2d: 3-1                  576
|    |    ??BatchNorm2d: 3-2             128
|    |    ??ReLU: 3-3                    --
|    |    ??Conv2d: 3-4                  36,864
|    |    ??BatchNorm2d: 3-5             128
|    |    ??ReLU: 3-6                    --
??Down: 1-2                              --
|    ??Sequential: 2-2                   --
|    |    ??MaxPool2d: 3-7               --
|    |    ??DoubleConv: 3-8              221,696
??Down: 1-3                              --
|    ??Sequential: 2-3                   --
|    |    ??MaxPool2d: 3-9               --
|    |    ??DoubleConv: 3-10             885,760
??Up: 1-4                                --
|    ??ConvTranspose2d: 2-4              131,200
|    ??DoubleConv: 2-5                   --
|    |    ??Sequential: 3-11             442,880
??Up: 1-5                                --
|    ??ConvTranspose2d: 2-6              32,832
|    ??DoubleConv: 2-7                   --
|    |    ??Sequential: 3-12             110,848
??OutConv: 1-6                           --
|    ??Conv2d: 2-8                       65
=================================================================
Total params: 1,862,977
Trainable params: 1,862,977
Non-trainable params: 0
=================================================================
device : cuda
trainer configurations set
train and test dataloaders created
total train batches  : 12500
total test  batches  : 125
optimizer : Adam, scheduler : lambdaLR loaded
optimizer : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
scheduler : <torch.optim.lr_scheduler.LambdaLR object at 0x000001D2134F5A60>
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization
initializing weights using Kaiming/He Initialization

beginning training ...
Epoch 0/10 , batch 1/12500 
ITERATION : 1, loss : 0.03912466813537928ITERATION : 2, loss : 0.03893767250910185ITERATION : 3, loss : 0.03988955915651217ITERATION : 4, loss : 0.04072163282060403ITERATION : 5, loss : 0.0413361500480648ITERATION : 6, loss : 0.04177277478096107ITERATION : 7, loss : 0.04207985243938685ITERATION : 8, loss : 0.04229535906346227ITERATION : 9, loss : 0.042446652189530164ITERATION : 10, loss : 0.04255298083940423ITERATION : 11, loss : 0.04262780409233333ITERATION : 12, loss : 0.04268052375084106ITERATION : 13, loss : 0.0427177130583135ITERATION : 14, loss : 0.042743974644886676ITERATION : 15, loss : 0.04276253655916765ITERATION : 16, loss : 0.042775666729883546ITERATION : 17, loss : 0.042784960869352524ITERATION : 18, loss : 0.04279154335107916ITERATION : 19, loss : 0.04279620752281455ITERATION : 20, loss : 0.04279951369160306ITERATION : 21, loss : 0.04280185799325139ITERATION : 22, loss : 0.042803520695689906ITERATION : 23, loss : 0.042804700201695155ITERATION : 24, loss : 0.042805537060354ITERATION : 25, loss : 0.04280613088284895ITERATION : 26, loss : 0.0428065523565313ITERATION : 27, loss : 0.042806851502534174ITERATION : 28, loss : 0.04280706385619337ITERATION : 29, loss : 0.04280721460538744ITERATION : 30, loss : 0.0428073216312441ITERATION : 31, loss : 0.04280739762341472ITERATION : 32, loss : 0.04280745158482207ITERATION : 33, loss : 0.042807489855547845ITERATION : 34, loss : 0.04280751704680517ITERATION : 35, loss : 0.04280753635036478ITERATION : 36, loss : 0.04280755005677747ITERATION : 37, loss : 0.042807559794689094ITERATION : 38, loss : 0.04280756670267926ITERATION : 39, loss : 0.042807571624685424ITERATION : 40, loss : 0.042807575107514324ITERATION : 41, loss : 0.04280757760676132ITERATION : 42, loss : 0.04280757933588276ITERATION : 43, loss : 0.04280758062824292ITERATION : 44, loss : 0.04280758153935231ITERATION : 45, loss : 0.04280758211697453ITERATION : 46, loss : 0.04280758253766648ITERATION : 47, loss : 0.04280758283655911ITERATION : 48, loss : 0.04280758308530353ITERATION : 49, loss : 0.04280758322762362ITERATION : 50, loss : 0.04280758337701123ITERATION : 51, loss : 0.04280758341938798ITERATION : 52, loss : 0.042807583492732906ITERATION : 53, loss : 0.042807583488426414ITERATION : 54, loss : 0.042807583488426414ITERATION : 55, loss : 0.042807583488426414ITERATION : 56, loss : 0.042807583488426414ITERATION : 57, loss : 0.042807583488426414ITERATION : 58, loss : 0.042807583488426414ITERATION : 59, loss : 0.042807583488426414ITERATION : 60, loss : 0.042807583488426414ITERATION : 61, loss : 0.042807583488426414ITERATION : 62, loss : 0.042807583488426414ITERATION : 63, loss : 0.042807583488426414ITERATION : 64, loss : 0.042807583488426414ITERATION : 65, loss : 0.042807583488426414ITERATION : 66, loss : 0.042807583488426414ITERATION : 67, loss : 0.042807583488426414ITERATION : 68, loss : 0.042807583488426414ITERATION : 69, loss : 0.042807583488426414ITERATION : 70, loss : 0.042807583488426414ITERATION : 71, loss : 0.042807583488426414ITERATION : 72, loss : 0.042807583488426414ITERATION : 73, loss : 0.042807583488426414ITERATION : 74, loss : 0.042807583488426414ITERATION : 75, loss : 0.042807583488426414ITERATION : 76, loss : 0.042807583488426414ITERATION : 77, loss : 0.042807583488426414ITERATION : 78, loss : 0.042807583488426414ITERATION : 79, loss : 0.042807583488426414ITERATION : 80, loss : 0.042807583488426414ITERATION : 81, loss : 0.042807583488426414ITERATION : 82, loss : 0.042807583488426414ITERATION : 83, loss : 0.042807583488426414ITERATION : 84, loss : 0.042807583488426414ITERATION : 85, loss : 0.042807583488426414ITERATION : 86, loss : 0.042807583488426414ITERATION : 87, loss : 0.042807583488426414ITERATION : 88, loss : 0.042807583488426414ITERATION : 89, loss : 0.042807583488426414ITERATION : 90, loss : 0.042807583488426414ITERATION : 91, loss : 0.042807583488426414ITERATION : 92, loss : 0.042807583488426414ITERATION : 93, loss : 0.042807583488426414ITERATION : 94, loss : 0.042807583488426414ITERATION : 95, loss : 0.042807583488426414ITERATION : 96, loss : 0.042807583488426414ITERATION : 97, loss : 0.042807583488426414ITERATION : 98, loss : 0.042807583488426414ITERATION : 99, loss : 0.042807583488426414ITERATION : 100, loss : 0.042807583488426414
ITERATION : 1, loss : 0.06759313746667756ITERATION : 2, loss : 0.06489077090627145ITERATION : 3, loss : 0.06458066985682052ITERATION : 4, loss : 0.06472933725125253ITERATION : 5, loss : 0.06495965519325278ITERATION : 6, loss : 0.06517443377439047ITERATION : 7, loss : 0.0653498055344847ITERATION : 8, loss : 0.06548474803669758ITERATION : 9, loss : 0.06558525069671066ITERATION : 10, loss : 0.06565863848501542ITERATION : 11, loss : 0.06571155572750348ITERATION : 12, loss : 0.06574939858424557ITERATION : 13, loss : 0.06577631291810405ITERATION : 14, loss : 0.06579538422876743ITERATION : 15, loss : 0.06580886436847223ITERATION : 16, loss : 0.06581837660898568ITERATION : 17, loss : 0.06582508130389177ITERATION : 18, loss : 0.06582980350693339ITERATION : 19, loss : 0.06583312774834395ITERATION : 20, loss : 0.06583546708375718ITERATION : 21, loss : 0.06583711298772227ITERATION : 22, loss : 0.0658382708285457ITERATION : 23, loss : 0.06583908524336986ITERATION : 24, loss : 0.06583965806564525ITERATION : 25, loss : 0.06584006097102071ITERATION : 26, loss : 0.06584034435705637ITERATION : 27, loss : 0.06584054368409598ITERATION : 28, loss : 0.06584068388047785ITERATION : 29, loss : 0.06584078250895814ITERATION : 30, loss : 0.06584085186494607ITERATION : 31, loss : 0.0658409006593929ITERATION : 32, loss : 0.06584093500457068ITERATION : 33, loss : 0.06584095914004748ITERATION : 34, loss : 0.06584097610036517ITERATION : 35, loss : 0.06584098807253425ITERATION : 36, loss : 0.06584099646117009ITERATION : 37, loss : 0.06584100234337656ITERATION : 38, loss : 0.06584100650447351ITERATION : 39, loss : 0.06584100940827287ITERATION : 40, loss : 0.06584101151420585ITERATION : 41, loss : 0.06584101292407646ITERATION : 42, loss : 0.06584101398182093ITERATION : 43, loss : 0.06584101468275173ITERATION : 44, loss : 0.06584101521112058ITERATION : 45, loss : 0.06584101552144003ITERATION : 46, loss : 0.06584101582430114ITERATION : 47, loss : 0.06584101595916908ITERATION : 48, loss : 0.06584101608696026ITERATION : 49, loss : 0.06584101616942638ITERATION : 50, loss : 0.06584101622924357ITERATION : 51, loss : 0.06584101623101125ITERATION : 52, loss : 0.06584101623101125ITERATION : 53, loss : 0.06584101623101125ITERATION : 54, loss : 0.06584101623101125ITERATION : 55, loss : 0.06584101623101125ITERATION : 56, loss : 0.06584101623101125ITERATION : 57, loss : 0.06584101623101125ITERATION : 58, loss : 0.06584101623101125ITERATION : 59, loss : 0.06584101623101125ITERATION : 60, loss : 0.06584101623101125ITERATION : 61, loss : 0.06584101623101125ITERATION : 62, loss : 0.06584101623101125ITERATION : 63, loss : 0.06584101623101125ITERATION : 64, loss : 0.06584101623101125ITERATION : 65, loss : 0.06584101623101125ITERATION : 66, loss : 0.06584101623101125ITERATION : 67, loss : 0.06584101623101125ITERATION : 68, loss : 0.06584101623101125ITERATION : 69, loss : 0.06584101623101125ITERATION : 70, loss : 0.06584101623101125ITERATION : 71, loss : 0.06584101623101125ITERATION : 72, loss : 0.06584101623101125ITERATION : 73, loss : 0.06584101623101125ITERATION : 74, loss : 0.06584101623101125ITERATION : 75, loss : 0.06584101623101125ITERATION : 76, loss : 0.06584101623101125ITERATION : 77, loss : 0.06584101623101125ITERATION : 78, loss : 0.06584101623101125ITERATION : 79, loss : 0.06584101623101125ITERATION : 80, loss : 0.06584101623101125ITERATION : 81, loss : 0.06584101623101125ITERATION : 82, loss : 0.06584101623101125ITERATION : 83, loss : 0.06584101623101125ITERATION : 84, loss : 0.06584101623101125ITERATION : 85, loss : 0.06584101623101125ITERATION : 86, loss : 0.06584101623101125ITERATION : 87, loss : 0.06584101623101125ITERATION : 88, loss : 0.06584101623101125ITERATION : 89, loss : 0.06584101623101125ITERATION : 90, loss : 0.06584101623101125ITERATION : 91, loss : 0.06584101623101125ITERATION : 92, loss : 0.06584101623101125ITERATION : 93, loss : 0.06584101623101125ITERATION : 94, loss : 0.06584101623101125ITERATION : 95, loss : 0.06584101623101125ITERATION : 96, loss : 0.06584101623101125ITERATION : 97, loss : 0.06584101623101125ITERATION : 98, loss : 0.06584101623101125ITERATION : 99, loss : 0.06584101623101125ITERATION : 100, loss : 0.06584101623101125
ITERATION : 1, loss : 0.023717968934390248ITERATION : 2, loss : 0.02471762552019373ITERATION : 3, loss : 0.025587786797878025ITERATION : 4, loss : 0.02624697242944668ITERATION : 5, loss : 0.02671982510676883ITERATION : 6, loss : 0.02705155060827559ITERATION : 7, loss : 0.027281805085333587ITERATION : 8, loss : 0.027440741894609898ITERATION : 9, loss : 0.027550128644478748ITERATION : 10, loss : 0.02762530273262028ITERATION : 11, loss : 0.02767693320234142ITERATION : 12, loss : 0.027712389316522246ITERATION : 13, loss : 0.027736741549067474ITERATION : 14, loss : 0.027753471963421185ITERATION : 15, loss : 0.027764969712049843ITERATION : 16, loss : 0.02777287373121398ITERATION : 17, loss : 0.027778308692362925ITERATION : 18, loss : 0.02778204663535058ITERATION : 19, loss : 0.02778461788186092ITERATION : 20, loss : 0.027786386659313135ITERATION : 21, loss : 0.027787603502059242ITERATION : 22, loss : 0.027788440623762917ITERATION : 23, loss : 0.027789016456638692ITERATION : 24, loss : 0.027789412535262327ITERATION : 25, loss : 0.027789684912711225ITERATION : 26, loss : 0.027789872228473837ITERATION : 27, loss : 0.027790001033746715ITERATION : 28, loss : 0.027790089533603093ITERATION : 29, loss : 0.027790150423536383ITERATION : 30, loss : 0.02779019220016192ITERATION : 31, loss : 0.027790220892551284ITERATION : 32, loss : 0.027790240629749493ITERATION : 33, loss : 0.027790254203790495ITERATION : 34, loss : 0.02779026352110853ITERATION : 35, loss : 0.02779026987696098ITERATION : 36, loss : 0.027790274213013843ITERATION : 37, loss : 0.02779027717766014ITERATION : 38, loss : 0.02779027919255483ITERATION : 39, loss : 0.027790280564195823ITERATION : 40, loss : 0.02779028149867667ITERATION : 41, loss : 0.027790282133119296ITERATION : 42, loss : 0.02779028256506035ITERATION : 43, loss : 0.02779028286043338ITERATION : 44, loss : 0.027790283052258086ITERATION : 45, loss : 0.027790283189276788ITERATION : 46, loss : 0.027790283280994768ITERATION : 47, loss : 0.02779028334255783ITERATION : 48, loss : 0.027790283369847516ITERATION : 49, loss : 0.027790283395337678ITERATION : 50, loss : 0.027790283418832225ITERATION : 51, loss : 0.027790283419714214ITERATION : 52, loss : 0.027790283419714214ITERATION : 53, loss : 0.027790283419714214ITERATION : 54, loss : 0.027790283419714214ITERATION : 55, loss : 0.027790283419714214ITERATION : 56, loss : 0.027790283419714214ITERATION : 57, loss : 0.027790283419714214ITERATION : 58, loss : 0.027790283419714214ITERATION : 59, loss : 0.027790283419714214ITERATION : 60, loss : 0.027790283419714214ITERATION : 61, loss : 0.027790283419714214ITERATION : 62, loss : 0.027790283419714214ITERATION : 63, loss : 0.027790283419714214ITERATION : 64, loss : 0.027790283419714214ITERATION : 65, loss : 0.027790283419714214ITERATION : 66, loss : 0.027790283419714214ITERATION : 67, loss : 0.027790283419714214ITERATION : 68, loss : 0.027790283419714214ITERATION : 69, loss : 0.027790283419714214ITERATION : 70, loss : 0.027790283419714214ITERATION : 71, loss : 0.027790283419714214ITERATION : 72, loss : 0.027790283419714214ITERATION : 73, loss : 0.027790283419714214ITERATION : 74, loss : 0.027790283419714214ITERATION : 75, loss : 0.027790283419714214ITERATION : 76, loss : 0.027790283419714214ITERATION : 77, loss : 0.027790283419714214ITERATION : 78, loss : 0.027790283419714214ITERATION : 79, loss : 0.027790283419714214ITERATION : 80, loss : 0.027790283419714214ITERATION : 81, loss : 0.027790283419714214ITERATION : 82, loss : 0.027790283419714214ITERATION : 83, loss : 0.027790283419714214ITERATION : 84, loss : 0.027790283419714214ITERATION : 85, loss : 0.027790283419714214ITERATION : 86, loss : 0.027790283419714214ITERATION : 87, loss : 0.027790283419714214ITERATION : 88, loss : 0.027790283419714214ITERATION : 89, loss : 0.027790283419714214ITERATION : 90, loss : 0.027790283419714214ITERATION : 91, loss : 0.027790283419714214ITERATION : 92, loss : 0.027790283419714214ITERATION : 93, loss : 0.027790283419714214ITERATION : 94, loss : 0.027790283419714214ITERATION : 95, loss : 0.027790283419714214ITERATION : 96, loss : 0.027790283419714214ITERATION : 97, loss : 0.027790283419714214ITERATION : 98, loss : 0.027790283419714214ITERATION : 99, loss : 0.027790283419714214ITERATION : 100, loss : 0.027790283419714214
ITERATION : 1, loss : 0.07224659211012552ITERATION : 2, loss : 0.07022149334352408ITERATION : 3, loss : 0.06998282545671862ITERATION : 4, loss : 0.07001146907212794ITERATION : 5, loss : 0.07009829405696794ITERATION : 6, loss : 0.07019258039607339ITERATION : 7, loss : 0.07027730500996003ITERATION : 8, loss : 0.07034722471496728ITERATION : 9, loss : 0.07040215297225282ITERATION : 10, loss : 0.07044396190798835ITERATION : 11, loss : 0.0704751116704149ITERATION : 12, loss : 0.07049797781583096ITERATION : 13, loss : 0.07051458676120453ITERATION : 14, loss : 0.07052655920874615ITERATION : 15, loss : 0.07053514199444579ITERATION : 16, loss : 0.07054126974774096ITERATION : 17, loss : 0.07054563142566225ITERATION : 18, loss : 0.0705487291074ITERATION : 19, loss : 0.07055092515446915ITERATION : 20, loss : 0.07055248040352177ITERATION : 21, loss : 0.0705535806981326ITERATION : 22, loss : 0.07055435835135666ITERATION : 23, loss : 0.07055490790269903ITERATION : 24, loss : 0.07055529601425871ITERATION : 25, loss : 0.07055556979987328ITERATION : 26, loss : 0.0705557631333527ITERATION : 27, loss : 0.07055589952970825ITERATION : 28, loss : 0.0705559955452353ITERATION : 29, loss : 0.07055606354547311ITERATION : 30, loss : 0.07055611114222655ITERATION : 31, loss : 0.0705561448312574ITERATION : 32, loss : 0.07055616862258873ITERATION : 33, loss : 0.07055618552963046ITERATION : 34, loss : 0.07055619745248365ITERATION : 35, loss : 0.07055620593085657ITERATION : 36, loss : 0.07055621186657433ITERATION : 37, loss : 0.070556216135787ITERATION : 38, loss : 0.07055621908870884ITERATION : 39, loss : 0.07055622126520446ITERATION : 40, loss : 0.0705562227062075ITERATION : 41, loss : 0.070556223800836ITERATION : 42, loss : 0.07055622447177666ITERATION : 43, loss : 0.07055622510585122ITERATION : 44, loss : 0.07055622534823888ITERATION : 45, loss : 0.07055622565164699ITERATION : 46, loss : 0.07055622575479292ITERATION : 47, loss : 0.07055622595851281ITERATION : 48, loss : 0.07055622595208473ITERATION : 49, loss : 0.07055622601867083ITERATION : 50, loss : 0.07055622605564534ITERATION : 51, loss : 0.07055622605564534ITERATION : 52, loss : 0.07055622605564534ITERATION : 53, loss : 0.07055622605564534ITERATION : 54, loss : 0.07055622605564534ITERATION : 55, loss : 0.07055622605564534ITERATION : 56, loss : 0.07055622605564534ITERATION : 57, loss : 0.07055622605564534ITERATION : 58, loss : 0.07055622605564534ITERATION : 59, loss : 0.07055622605564534ITERATION : 60, loss : 0.07055622605564534ITERATION : 61, loss : 0.07055622605564534ITERATION : 62, loss : 0.07055622605564534ITERATION : 63, loss : 0.07055622605564534ITERATION : 64, loss : 0.07055622605564534ITERATION : 65, loss : 0.07055622605564534ITERATION : 66, loss : 0.07055622605564534ITERATION : 67, loss : 0.07055622605564534ITERATION : 68, loss : 0.07055622605564534ITERATION : 69, loss : 0.07055622605564534ITERATION : 70, loss : 0.07055622605564534ITERATION : 71, loss : 0.07055622605564534ITERATION : 72, loss : 0.07055622605564534ITERATION : 73, loss : 0.07055622605564534ITERATION : 74, loss : 0.07055622605564534ITERATION : 75, loss : 0.07055622605564534ITERATION : 76, loss : 0.07055622605564534ITERATION : 77, loss : 0.07055622605564534ITERATION : 78, loss : 0.07055622605564534ITERATION : 79, loss : 0.07055622605564534ITERATION : 80, loss : 0.07055622605564534ITERATION : 81, loss : 0.07055622605564534ITERATION : 82, loss : 0.07055622605564534ITERATION : 83, loss : 0.07055622605564534ITERATION : 84, loss : 0.07055622605564534ITERATION : 85, loss : 0.07055622605564534ITERATION : 86, loss : 0.07055622605564534ITERATION : 87, loss : 0.07055622605564534ITERATION : 88, loss : 0.07055622605564534ITERATION : 89, loss : 0.07055622605564534ITERATION : 90, loss : 0.07055622605564534ITERATION : 91, loss : 0.07055622605564534ITERATION : 92, loss : 0.07055622605564534ITERATION : 93, loss : 0.07055622605564534ITERATION : 94, loss : 0.07055622605564534ITERATION : 95, loss : 0.07055622605564534ITERATION : 96, loss : 0.07055622605564534ITERATION : 97, loss : 0.07055622605564534ITERATION : 98, loss : 0.07055622605564534ITERATION : 99, loss : 0.07055622605564534ITERATION : 100, loss : 0.07055622605564534
ITERATION : 1, loss : 0.08800830570882616ITERATION : 2, loss : 0.08437462182314799ITERATION : 3, loss : 0.08243329185682309ITERATION : 4, loss : 0.08127853118837786ITERATION : 5, loss : 0.08054951022518086ITERATION : 6, loss : 0.08007166531852658ITERATION : 7, loss : 0.07975093382644141ITERATION : 8, loss : 0.07953248840689221ITERATION : 9, loss : 0.07938239685414097ITERATION : 10, loss : 0.07927873613052508ITERATION : 11, loss : 0.07920692790424418ITERATION : 12, loss : 0.07915709907054821ITERATION : 13, loss : 0.07912248945361698ITERATION : 14, loss : 0.07909843833438163ITERATION : 15, loss : 0.0790817198903582ITERATION : 16, loss : 0.07907009711456867ITERATION : 17, loss : 0.0790620165910613ITERATION : 18, loss : 0.07905639866956153ITERATION : 19, loss : 0.0790524930430702ITERATION : 20, loss : 0.07904977780900765ITERATION : 21, loss : 0.0790478900677487ITERATION : 22, loss : 0.07904657808844807ITERATION : 23, loss : 0.07904566603017198ITERATION : 24, loss : 0.07904503205406123ITERATION : 25, loss : 0.0790445913209069ITERATION : 26, loss : 0.07904428492171356ITERATION : 27, loss : 0.07904407207240591ITERATION : 28, loss : 0.07904392385318353ITERATION : 29, loss : 0.0790438211209782ITERATION : 30, loss : 0.07904374932885258ITERATION : 31, loss : 0.07904369980210953ITERATION : 32, loss : 0.07904366502488386ITERATION : 33, loss : 0.07904364115634013ITERATION : 34, loss : 0.07904362427294506ITERATION : 35, loss : 0.07904361238227225ITERATION : 36, loss : 0.07904360456262563ITERATION : 37, loss : 0.07904359885211623ITERATION : 38, loss : 0.07904359517871981ITERATION : 39, loss : 0.07904359262584945ITERATION : 40, loss : 0.0790435909309906ITERATION : 41, loss : 0.07904358951441261ITERATION : 42, loss : 0.07904358875638567ITERATION : 43, loss : 0.0790435880100681ITERATION : 44, loss : 0.0790435876816188ITERATION : 45, loss : 0.07904358760783937ITERATION : 46, loss : 0.07904358760783937ITERATION : 47, loss : 0.07904358760783937ITERATION : 48, loss : 0.07904358760783937ITERATION : 49, loss : 0.07904358760783937ITERATION : 50, loss : 0.07904358760783937ITERATION : 51, loss : 0.07904358760783937ITERATION : 52, loss : 0.07904358760783937ITERATION : 53, loss : 0.07904358760783937ITERATION : 54, loss : 0.07904358760783937ITERATION : 55, loss : 0.07904358760783937ITERATION : 56, loss : 0.07904358760783937ITERATION : 57, loss : 0.07904358760783937ITERATION : 58, loss : 0.07904358760783937ITERATION : 59, loss : 0.07904358760783937ITERATION : 60, loss : 0.07904358760783937ITERATION : 61, loss : 0.07904358760783937ITERATION : 62, loss : 0.07904358760783937ITERATION : 63, loss : 0.07904358760783937ITERATION : 64, loss : 0.07904358760783937ITERATION : 65, loss : 0.07904358760783937ITERATION : 66, loss : 0.07904358760783937ITERATION : 67, loss : 0.07904358760783937ITERATION : 68, loss : 0.07904358760783937ITERATION : 69, loss : 0.07904358760783937ITERATION : 70, loss : 0.07904358760783937ITERATION : 71, loss : 0.07904358760783937ITERATION : 72, loss : 0.07904358760783937ITERATION : 73, loss : 0.07904358760783937ITERATION : 74, loss : 0.07904358760783937ITERATION : 75, loss : 0.07904358760783937ITERATION : 76, loss : 0.07904358760783937ITERATION : 77, loss : 0.07904358760783937ITERATION : 78, loss : 0.07904358760783937ITERATION : 79, loss : 0.07904358760783937ITERATION : 80, loss : 0.07904358760783937ITERATION : 81, loss : 0.07904358760783937ITERATION : 82, loss : 0.07904358760783937ITERATION : 83, loss : 0.07904358760783937ITERATION : 84, loss : 0.07904358760783937ITERATION : 85, loss : 0.07904358760783937ITERATION : 86, loss : 0.07904358760783937ITERATION : 87, loss : 0.07904358760783937ITERATION : 88, loss : 0.07904358760783937ITERATION : 89, loss : 0.07904358760783937ITERATION : 90, loss : 0.07904358760783937ITERATION : 91, loss : 0.07904358760783937ITERATION : 92, loss : 0.07904358760783937ITERATION : 93, loss : 0.07904358760783937ITERATION : 94, loss : 0.07904358760783937ITERATION : 95, loss : 0.07904358760783937ITERATION : 96, loss : 0.07904358760783937ITERATION : 97, loss : 0.07904358760783937ITERATION : 98, loss : 0.07904358760783937ITERATION : 99, loss : 0.07904358760783937ITERATION : 100, loss : 0.07904358760783937
ITERATION : 1, loss : 0.040207864910827965ITERATION : 2, loss : 0.04244560291488242ITERATION : 3, loss : 0.04464148247391346ITERATION : 4, loss : 0.04663226824553504ITERATION : 5, loss : 0.048289783926444306ITERATION : 6, loss : 0.049596995303209324ITERATION : 7, loss : 0.05059403779765103ITERATION : 8, loss : 0.05133865861730892ITERATION : 9, loss : 0.051887223441890395ITERATION : 10, loss : 0.05228769610388828ITERATION : 11, loss : 0.05257825006989537ITERATION : 12, loss : 0.05278814875465542ITERATION : 13, loss : 0.05293931974872425ITERATION : 14, loss : 0.05304795596055151ITERATION : 15, loss : 0.053125900394364316ITERATION : 16, loss : 0.05318175750307917ITERATION : 17, loss : 0.053221750310763837ITERATION : 18, loss : 0.05325036466380219ITERATION : 19, loss : 0.05327082667541361ITERATION : 20, loss : 0.05328545268696374ITERATION : 21, loss : 0.05329590333769119ITERATION : 22, loss : 0.05330336833773568ITERATION : 23, loss : 0.05330869931292043ITERATION : 24, loss : 0.05331250521285285ITERATION : 25, loss : 0.053315221869846996ITERATION : 26, loss : 0.05331716067693813ITERATION : 27, loss : 0.05331854404782949ITERATION : 28, loss : 0.053319530946939156ITERATION : 29, loss : 0.053320234862240336ITERATION : 30, loss : 0.05332073683392997ITERATION : 31, loss : 0.05332109476038372ITERATION : 32, loss : 0.05332134992087751ITERATION : 33, loss : 0.053321531774081526ITERATION : 34, loss : 0.05332166140317046ITERATION : 35, loss : 0.05332175379267406ITERATION : 36, loss : 0.05332181960080978ITERATION : 37, loss : 0.05332186646052913ITERATION : 38, loss : 0.05332189986059796ITERATION : 39, loss : 0.05332192364126617ITERATION : 40, loss : 0.05332194061080683ITERATION : 41, loss : 0.05332195278342509ITERATION : 42, loss : 0.0533219613682819ITERATION : 43, loss : 0.05332196747123752ITERATION : 44, loss : 0.05332197182036817ITERATION : 45, loss : 0.0533219748978963ITERATION : 46, loss : 0.053321977120441184ITERATION : 47, loss : 0.0533219786965959ITERATION : 48, loss : 0.0533219797659689ITERATION : 49, loss : 0.05332198058722591ITERATION : 50, loss : 0.053321981246034864ITERATION : 51, loss : 0.05332198157903248ITERATION : 52, loss : 0.053321981841300554ITERATION : 53, loss : 0.053321981980544275ITERATION : 54, loss : 0.053321982021828356ITERATION : 55, loss : 0.053321982021828356ITERATION : 56, loss : 0.053321982021828356ITERATION : 57, loss : 0.053321982021828356ITERATION : 58, loss : 0.053321982021828356ITERATION : 59, loss : 0.053321982021828356ITERATION : 60, loss : 0.053321982021828356ITERATION : 61, loss : 0.053321982021828356ITERATION : 62, loss : 0.053321982021828356ITERATION : 63, loss : 0.053321982021828356ITERATION : 64, loss : 0.053321982021828356ITERATION : 65, loss : 0.053321982021828356ITERATION : 66, loss : 0.053321982021828356ITERATION : 67, loss : 0.053321982021828356ITERATION : 68, loss : 0.053321982021828356ITERATION : 69, loss : 0.053321982021828356ITERATION : 70, loss : 0.053321982021828356ITERATION : 71, loss : 0.053321982021828356ITERATION : 72, loss : 0.053321982021828356ITERATION : 73, loss : 0.053321982021828356ITERATION : 74, loss : 0.053321982021828356ITERATION : 75, loss : 0.053321982021828356ITERATION : 76, loss : 0.053321982021828356ITERATION : 77, loss : 0.053321982021828356ITERATION : 78, loss : 0.053321982021828356ITERATION : 79, loss : 0.053321982021828356ITERATION : 80, loss : 0.053321982021828356ITERATION : 81, loss : 0.053321982021828356ITERATION : 82, loss : 0.053321982021828356ITERATION : 83, loss : 0.053321982021828356ITERATION : 84, loss : 0.053321982021828356ITERATION : 85, loss : 0.053321982021828356ITERATION : 86, loss : 0.053321982021828356ITERATION : 87, loss : 0.053321982021828356ITERATION : 88, loss : 0.053321982021828356ITERATION : 89, loss : 0.053321982021828356ITERATION : 90, loss : 0.053321982021828356ITERATION : 91, loss : 0.053321982021828356ITERATION : 92, loss : 0.053321982021828356ITERATION : 93, loss : 0.053321982021828356ITERATION : 94, loss : 0.053321982021828356ITERATION : 95, loss : 0.053321982021828356ITERATION : 96, loss : 0.053321982021828356ITERATION : 97, loss : 0.053321982021828356ITERATION : 98, loss : 0.053321982021828356ITERATION : 99, loss : 0.053321982021828356ITERATION : 100, loss : 0.053321982021828356
ITERATION : 1, loss : 0.12202044423318165ITERATION : 2, loss : 0.1309638241967969ITERATION : 3, loss : 0.13754231875479223ITERATION : 4, loss : 0.1423813801783331ITERATION : 5, loss : 0.14586069528230092ITERATION : 6, loss : 0.14834747398602205ITERATION : 7, loss : 0.15012308992887738ITERATION : 8, loss : 0.15139167830994693ITERATION : 9, loss : 0.15229896403522267ITERATION : 10, loss : 0.15294853950721032ITERATION : 11, loss : 0.15341404464529804ITERATION : 12, loss : 0.15374790198659996ITERATION : 13, loss : 0.1539874945927752ITERATION : 14, loss : 0.15415952558697224ITERATION : 15, loss : 0.15428309619466307ITERATION : 16, loss : 0.15437188576686847ITERATION : 17, loss : 0.1544357003172252ITERATION : 18, loss : 0.1544815743031093ITERATION : 19, loss : 0.15451455689717414ITERATION : 20, loss : 0.154538273962105ITERATION : 21, loss : 0.15455533024915433ITERATION : 22, loss : 0.15456759757811583ITERATION : 23, loss : 0.15457642121576606ITERATION : 24, loss : 0.15458276834016232ITERATION : 25, loss : 0.15458733426066792ITERATION : 26, loss : 0.15459061902179838ITERATION : 27, loss : 0.15459298238806343ITERATION : 28, loss : 0.1545946826417ITERATION : 29, loss : 0.1545959060760829ITERATION : 30, loss : 0.15459678638013027ITERATION : 31, loss : 0.15459741984281342ITERATION : 32, loss : 0.15459787558060042ITERATION : 33, loss : 0.1545982035740785ITERATION : 34, loss : 0.15459843966555187ITERATION : 35, loss : 0.15459860956733626ITERATION : 36, loss : 0.15459873175999916ITERATION : 37, loss : 0.15459881965091188ITERATION : 38, loss : 0.15459888289301477ITERATION : 39, loss : 0.1545989283523383ITERATION : 40, loss : 0.15459896112074614ITERATION : 41, loss : 0.15459898465378258ITERATION : 42, loss : 0.15459900163855345ITERATION : 43, loss : 0.1545990138880492ITERATION : 44, loss : 0.15459902269250528ITERATION : 45, loss : 0.15459902901041314ITERATION : 46, loss : 0.1545990335471483ITERATION : 47, loss : 0.1545990368456023ITERATION : 48, loss : 0.15459903920473753ITERATION : 49, loss : 0.15459904074181038ITERATION : 50, loss : 0.15459904199200808ITERATION : 51, loss : 0.15459904269441446ITERATION : 52, loss : 0.15459904335838487ITERATION : 53, loss : 0.15459904375233732ITERATION : 54, loss : 0.15459904407877031ITERATION : 55, loss : 0.1545990441029785ITERATION : 56, loss : 0.1545990441029785ITERATION : 57, loss : 0.1545990441029785ITERATION : 58, loss : 0.1545990441029785ITERATION : 59, loss : 0.1545990441029785ITERATION : 60, loss : 0.1545990441029785ITERATION : 61, loss : 0.1545990441029785ITERATION : 62, loss : 0.1545990441029785ITERATION : 63, loss : 0.1545990441029785ITERATION : 64, loss : 0.1545990441029785ITERATION : 65, loss : 0.1545990441029785ITERATION : 66, loss : 0.1545990441029785ITERATION : 67, loss : 0.1545990441029785ITERATION : 68, loss : 0.1545990441029785ITERATION : 69, loss : 0.1545990441029785ITERATION : 70, loss : 0.1545990441029785ITERATION : 71, loss : 0.1545990441029785ITERATION : 72, loss : 0.1545990441029785ITERATION : 73, loss : 0.1545990441029785ITERATION : 74, loss : 0.1545990441029785ITERATION : 75, loss : 0.1545990441029785ITERATION : 76, loss : 0.1545990441029785ITERATION : 77, loss : 0.1545990441029785ITERATION : 78, loss : 0.1545990441029785ITERATION : 79, loss : 0.1545990441029785ITERATION : 80, loss : 0.1545990441029785ITERATION : 81, loss : 0.1545990441029785ITERATION : 82, loss : 0.1545990441029785ITERATION : 83, loss : 0.1545990441029785ITERATION : 84, loss : 0.1545990441029785ITERATION : 85, loss : 0.1545990441029785ITERATION : 86, loss : 0.1545990441029785ITERATION : 87, loss : 0.1545990441029785ITERATION : 88, loss : 0.1545990441029785ITERATION : 89, loss : 0.1545990441029785ITERATION : 90, loss : 0.1545990441029785ITERATION : 91, loss : 0.1545990441029785ITERATION : 92, loss : 0.1545990441029785ITERATION : 93, loss : 0.1545990441029785ITERATION : 94, loss : 0.1545990441029785ITERATION : 95, loss : 0.1545990441029785ITERATION : 96, loss : 0.1545990441029785ITERATION : 97, loss : 0.1545990441029785ITERATION : 98, loss : 0.1545990441029785ITERATION : 99, loss : 0.1545990441029785ITERATION : 100, loss : 0.1545990441029785
ITERATION : 1, loss : 0.06369955067441377ITERATION : 2, loss : 0.06536011542070011ITERATION : 3, loss : 0.06623362923284401ITERATION : 4, loss : 0.06685743663397718ITERATION : 5, loss : 0.06732829017766105ITERATION : 6, loss : 0.06767911646225588ITERATION : 7, loss : 0.06793548862183305ITERATION : 8, loss : 0.06811999477096785ITERATION : 9, loss : 0.06825136814821778ITERATION : 10, loss : 0.0683442387132207ITERATION : 11, loss : 0.06840957880221832ITERATION : 12, loss : 0.0684554063001185ITERATION : 13, loss : 0.06848748338051724ITERATION : 14, loss : 0.06850990679230051ITERATION : 15, loss : 0.06852556905255514ITERATION : 16, loss : 0.06853650338867102ITERATION : 17, loss : 0.06854413474790637ITERATION : 18, loss : 0.0685494600305774ITERATION : 19, loss : 0.0685531758096914ITERATION : 20, loss : 0.06855576848548556ITERATION : 21, loss : 0.06855757753407948ITERATION : 22, loss : 0.06855883983791039ITERATION : 23, loss : 0.06855972065576206ITERATION : 24, loss : 0.06856033530078194ITERATION : 25, loss : 0.06856076425558442ITERATION : 26, loss : 0.0685610636276651ITERATION : 27, loss : 0.06856127257443598ITERATION : 28, loss : 0.06856141840997469ITERATION : 29, loss : 0.06856152019635618ITERATION : 30, loss : 0.06856159124940868ITERATION : 31, loss : 0.06856164089369013ITERATION : 32, loss : 0.06856167550099221ITERATION : 33, loss : 0.06856169966412484ITERATION : 34, loss : 0.0685617165368443ITERATION : 35, loss : 0.06856172831198763ITERATION : 36, loss : 0.06856173652838198ITERATION : 37, loss : 0.06856174225647362ITERATION : 38, loss : 0.06856174625578823ITERATION : 39, loss : 0.06856174905260717ITERATION : 40, loss : 0.06856175101338029ITERATION : 41, loss : 0.06856175238275516ITERATION : 42, loss : 0.06856175333424978ITERATION : 43, loss : 0.06856175400436236ITERATION : 44, loss : 0.06856175445877707ITERATION : 45, loss : 0.06856175472976156ITERATION : 46, loss : 0.06856175497182093ITERATION : 47, loss : 0.06856175510934379ITERATION : 48, loss : 0.06856175521571997ITERATION : 49, loss : 0.06856175529424073ITERATION : 50, loss : 0.06856175529957673ITERATION : 51, loss : 0.06856175529957673ITERATION : 52, loss : 0.06856175529957673ITERATION : 53, loss : 0.06856175529957673ITERATION : 54, loss : 0.06856175529957673ITERATION : 55, loss : 0.06856175529957673ITERATION : 56, loss : 0.06856175529957673ITERATION : 57, loss : 0.06856175529957673ITERATION : 58, loss : 0.06856175529957673ITERATION : 59, loss : 0.06856175529957673ITERATION : 60, loss : 0.06856175529957673ITERATION : 61, loss : 0.06856175529957673ITERATION : 62, loss : 0.06856175529957673ITERATION : 63, loss : 0.06856175529957673ITERATION : 64, loss : 0.06856175529957673ITERATION : 65, loss : 0.06856175529957673ITERATION : 66, loss : 0.06856175529957673ITERATION : 67, loss : 0.06856175529957673ITERATION : 68, loss : 0.06856175529957673ITERATION : 69, loss : 0.06856175529957673ITERATION : 70, loss : 0.06856175529957673ITERATION : 71, loss : 0.06856175529957673ITERATION : 72, loss : 0.06856175529957673ITERATION : 73, loss : 0.06856175529957673ITERATION : 74, loss : 0.06856175529957673ITERATION : 75, loss : 0.06856175529957673ITERATION : 76, loss : 0.06856175529957673ITERATION : 77, loss : 0.06856175529957673ITERATION : 78, loss : 0.06856175529957673ITERATION : 79, loss : 0.06856175529957673ITERATION : 80, loss : 0.06856175529957673ITERATION : 81, loss : 0.06856175529957673ITERATION : 82, loss : 0.06856175529957673ITERATION : 83, loss : 0.06856175529957673ITERATION : 84, loss : 0.06856175529957673ITERATION : 85, loss : 0.06856175529957673ITERATION : 86, loss : 0.06856175529957673ITERATION : 87, loss : 0.06856175529957673ITERATION : 88, loss : 0.06856175529957673ITERATION : 89, loss : 0.06856175529957673ITERATION : 90, loss : 0.06856175529957673ITERATION : 91, loss : 0.06856175529957673ITERATION : 92, loss : 0.06856175529957673ITERATION : 93, loss : 0.06856175529957673ITERATION : 94, loss : 0.06856175529957673ITERATION : 95, loss : 0.06856175529957673ITERATION : 96, loss : 0.06856175529957673ITERATION : 97, loss : 0.06856175529957673ITERATION : 98, loss : 0.06856175529957673ITERATION : 99, loss : 0.06856175529957673ITERATION : 100, loss : 0.06856175529957673
gradient norm in None layer : 0.6170346175848519
gradient norm in None layer : 0.029704948098637853
gradient norm in None layer : 0.04033204671320969
gradient norm in None layer : 0.4954496627132203
gradient norm in None layer : 0.029875636899377653
gradient norm in None layer : 0.032822853465279224
gradient norm in None layer : 0.25329683772093076
gradient norm in None layer : 0.009118163739060393
gradient norm in None layer : 0.011296027709323789
gradient norm in None layer : 0.221458004183689
gradient norm in None layer : 0.008200437402550144
gradient norm in None layer : 0.009861405323795674
gradient norm in None layer : 0.07324101291629989
gradient norm in None layer : 0.0019205491284974856
gradient norm in None layer : 0.0021119957109561258
gradient norm in None layer : 0.0634431187080198
gradient norm in None layer : 0.002089045294412471
gradient norm in None layer : 0.002406083963306374
gradient norm in None layer : 0.07828982092340128
gradient norm in None layer : 0.0007276603693779786
gradient norm in None layer : 0.193396838872318
gradient norm in None layer : 0.006472389606609729
gradient norm in None layer : 0.008214139239238694
gradient norm in None layer : 0.16587285425612733
gradient norm in None layer : 0.010878668739189362
gradient norm in None layer : 0.016831658306775424
gradient norm in None layer : 0.2562130263224289
gradient norm in None layer : 0.0015566591860780917
gradient norm in None layer : 0.4757324400988373
gradient norm in None layer : 0.023611473335656677
gradient norm in None layer : 0.027438956400451252
gradient norm in None layer : 0.4247382620146792
gradient norm in None layer : 0.05278144192881033
gradient norm in None layer : 0.08049863823975544
gradient norm in None layer : 0.04605037337567071
gradient norm in None layer : 0.017054358477145763
Total gradient norm: 1.1448069923531652
invariance loss : 35.7423032200513, avg_den : 0.0463714599609375, density loss : 0.053628540039062506, mse loss : 0.07031518477837752, solver time : 95.37183094024658 sec , total loss : 0.10611111653846789, running loss : 0.10611111653846789
Epoch 0/10 , batch 2/12500 
ITERATION : 1, loss : 0.06913171845019704ITERATION : 2, loss : 0.06404472948845816ITERATION : 3, loss : 0.0636993999832044ITERATION : 4, loss : 0.064246013442651ITERATION : 5, loss : 0.06487612391959251ITERATION : 6, loss : 0.06539913430229607ITERATION : 7, loss : 0.06579193625229375ITERATION : 8, loss : 0.06607485543466235ITERATION : 9, loss : 0.06627463622668966ITERATION : 10, loss : 0.06641431260659598ITERATION : 11, loss : 0.06651146289232314ITERATION : 12, loss : 0.06657884877978376ITERATION : 13, loss : 0.06662551984124358ITERATION : 14, loss : 0.06665781734180673ITERATION : 15, loss : 0.0666801579253649ITERATION : 16, loss : 0.06669560713495942ITERATION : 17, loss : 0.06670628909522394ITERATION : 18, loss : 0.06671367421548098ITERATION : 19, loss : 0.06671877981022681ITERATION : 20, loss : 0.06672230935401305ITERATION : 21, loss : 0.0667247492803333ITERATION : 22, loss : 0.06672643595087586ITERATION : 23, loss : 0.06672760190820684ITERATION : 24, loss : 0.06672840792389916ITERATION : 25, loss : 0.06672896509386035ITERATION : 26, loss : 0.06672935022074884ITERATION : 27, loss : 0.06672961644616887ITERATION : 28, loss : 0.06672980055465971ITERATION : 29, loss : 0.06672992781965495ITERATION : 30, loss : 0.0667300157201421ITERATION : 31, loss : 0.06673007653379513ITERATION : 32, loss : 0.06673011857633046ITERATION : 33, loss : 0.06673014760581152ITERATION : 34, loss : 0.06673016772939214ITERATION : 35, loss : 0.06673018158320466ITERATION : 36, loss : 0.06673019121201054ITERATION : 37, loss : 0.06673019778278608ITERATION : 38, loss : 0.06673020237144944ITERATION : 39, loss : 0.06673020546549466ITERATION : 40, loss : 0.066730207685878ITERATION : 41, loss : 0.06673020911861942ITERATION : 42, loss : 0.06673021013581704ITERATION : 43, loss : 0.06673021080371849ITERATION : 44, loss : 0.06673021133267541ITERATION : 45, loss : 0.06673021169193662ITERATION : 46, loss : 0.06673021191804643ITERATION : 47, loss : 0.06673021208332433ITERATION : 48, loss : 0.06673021208515513ITERATION : 49, loss : 0.06673021208515513ITERATION : 50, loss : 0.06673021208515513ITERATION : 51, loss : 0.06673021208515513ITERATION : 52, loss : 0.06673021208515513ITERATION : 53, loss : 0.06673021208515513ITERATION : 54, loss : 0.06673021208515513ITERATION : 55, loss : 0.06673021208515513ITERATION : 56, loss : 0.06673021208515513ITERATION : 57, loss : 0.06673021208515513ITERATION : 58, loss : 0.06673021208515513ITERATION : 59, loss : 0.06673021208515513ITERATION : 60, loss : 0.06673021208515513ITERATION : 61, loss : 0.06673021208515513ITERATION : 62, loss : 0.06673021208515513ITERATION : 63, loss : 0.06673021208515513ITERATION : 64, loss : 0.06673021208515513ITERATION : 65, loss : 0.06673021208515513ITERATION : 66, loss : 0.06673021208515513ITERATION : 67, loss : 0.06673021208515513ITERATION : 68, loss : 0.06673021208515513ITERATION : 69, loss : 0.06673021208515513ITERATION : 70, loss : 0.06673021208515513ITERATION : 71, loss : 0.06673021208515513ITERATION : 72, loss : 0.06673021208515513ITERATION : 73, loss : 0.06673021208515513ITERATION : 74, loss : 0.06673021208515513ITERATION : 75, loss : 0.06673021208515513ITERATION : 76, loss : 0.06673021208515513ITERATION : 77, loss : 0.06673021208515513ITERATION : 78, loss : 0.06673021208515513ITERATION : 79, loss : 0.06673021208515513ITERATION : 80, loss : 0.06673021208515513ITERATION : 81, loss : 0.06673021208515513ITERATION : 82, loss : 0.06673021208515513ITERATION : 83, loss : 0.06673021208515513ITERATION : 84, loss : 0.06673021208515513ITERATION : 85, loss : 0.06673021208515513ITERATION : 86, loss : 0.06673021208515513ITERATION : 87, loss : 0.06673021208515513ITERATION : 88, loss : 0.06673021208515513ITERATION : 89, loss : 0.06673021208515513ITERATION : 90, loss : 0.06673021208515513ITERATION : 91, loss : 0.06673021208515513ITERATION : 92, loss : 0.06673021208515513ITERATION : 93, loss : 0.06673021208515513ITERATION : 94, loss : 0.06673021208515513ITERATION : 95, loss : 0.06673021208515513ITERATION : 96, loss : 0.06673021208515513ITERATION : 97, loss : 0.06673021208515513ITERATION : 98, loss : 0.06673021208515513ITERATION : 99, loss : 0.06673021208515513ITERATION : 100, loss : 0.06673021208515513
ITERATION : 1, loss : 0.051942704247876004ITERATION : 2, loss : 0.05433778981544324ITERATION : 3, loss : 0.058365733141791436ITERATION : 4, loss : 0.061871503925328405ITERATION : 5, loss : 0.06462048509486894ITERATION : 6, loss : 0.06669435627152755ITERATION : 7, loss : 0.06823137973833152ITERATION : 8, loss : 0.06936050131435682ITERATION : 9, loss : 0.07018618039270205ITERATION : 10, loss : 0.07078849187693452ITERATION : 11, loss : 0.07122727512850767ITERATION : 12, loss : 0.07154668629412171ITERATION : 13, loss : 0.07177909656299034ITERATION : 14, loss : 0.07194815532259329ITERATION : 15, loss : 0.07207110752641811ITERATION : 16, loss : 0.07216051481508397ITERATION : 17, loss : 0.07222552163863868ITERATION : 18, loss : 0.07227278261900609ITERATION : 19, loss : 0.07230713898955476ITERATION : 20, loss : 0.07233211238073646ITERATION : 21, loss : 0.07235026396249936ITERATION : 22, loss : 0.07236345626839209ITERATION : 23, loss : 0.0723730435879219ITERATION : 24, loss : 0.07238001059514014ITERATION : 25, loss : 0.07238507305898409ITERATION : 26, loss : 0.0723887515209342ITERATION : 27, loss : 0.0723914241298928ITERATION : 28, loss : 0.0723933657754472ITERATION : 29, loss : 0.07239477638340235ITERATION : 30, loss : 0.07239580108487348ITERATION : 31, loss : 0.07239654547713639ITERATION : 32, loss : 0.0723970862086286ITERATION : 33, loss : 0.07239747884431338ITERATION : 34, loss : 0.07239776409030885ITERATION : 35, loss : 0.07239797124713242ITERATION : 36, loss : 0.07239812170716846ITERATION : 37, loss : 0.07239823095739083ITERATION : 38, loss : 0.07239831030140854ITERATION : 39, loss : 0.0723983679067103ITERATION : 40, loss : 0.07239840973829065ITERATION : 41, loss : 0.07239844013412458ITERATION : 42, loss : 0.07239846223016408ITERATION : 43, loss : 0.07239847825022534ITERATION : 44, loss : 0.07239848986219176ITERATION : 45, loss : 0.07239849828736512ITERATION : 46, loss : 0.07239850438445633ITERATION : 47, loss : 0.07239850882506145ITERATION : 48, loss : 0.07239851201817138ITERATION : 49, loss : 0.07239851444044108ITERATION : 50, loss : 0.07239851611215466ITERATION : 51, loss : 0.07239851732364488ITERATION : 52, loss : 0.07239851821056519ITERATION : 53, loss : 0.07239851884174052ITERATION : 54, loss : 0.07239851930953249ITERATION : 55, loss : 0.07239851963775361ITERATION : 56, loss : 0.0723985198385656ITERATION : 57, loss : 0.07239852001359158ITERATION : 58, loss : 0.07239852014347542ITERATION : 59, loss : 0.07239852020143635ITERATION : 60, loss : 0.07239852024489736ITERATION : 61, loss : 0.07239852025833819ITERATION : 62, loss : 0.07239852025833819ITERATION : 63, loss : 0.07239852025833819ITERATION : 64, loss : 0.07239852025833819ITERATION : 65, loss : 0.07239852025833819ITERATION : 66, loss : 0.07239852025833819ITERATION : 67, loss : 0.07239852025833819ITERATION : 68, loss : 0.07239852025833819ITERATION : 69, loss : 0.07239852025833819ITERATION : 70, loss : 0.07239852025833819ITERATION : 71, loss : 0.07239852025833819ITERATION : 72, loss : 0.07239852025833819ITERATION : 73, loss : 0.07239852025833819ITERATION : 74, loss : 0.07239852025833819ITERATION : 75, loss : 0.07239852025833819ITERATION : 76, loss : 0.07239852025833819ITERATION : 77, loss : 0.07239852025833819ITERATION : 78, loss : 0.07239852025833819ITERATION : 79, loss : 0.07239852025833819ITERATION : 80, loss : 0.07239852025833819ITERATION : 81, loss : 0.07239852025833819ITERATION : 82, loss : 0.07239852025833819ITERATION : 83, loss : 0.07239852025833819ITERATION : 84, loss : 0.07239852025833819ITERATION : 85, loss : 0.07239852025833819ITERATION : 86, loss : 0.07239852025833819ITERATION : 87, loss : 0.07239852025833819ITERATION : 88, loss : 0.07239852025833819ITERATION : 89, loss : 0.07239852025833819ITERATION : 90, loss : 0.07239852025833819ITERATION : 91, loss : 0.07239852025833819ITERATION : 92, loss : 0.07239852025833819ITERATION : 93, loss : 0.07239852025833819ITERATION : 94, loss : 0.07239852025833819ITERATION : 95, loss : 0.07239852025833819ITERATION : 96, loss : 0.07239852025833819ITERATION : 97, loss : 0.07239852025833819ITERATION : 98, loss : 0.07239852025833819ITERATION : 99, loss : 0.07239852025833819ITERATION : 100, loss : 0.07239852025833819
ITERATION : 1, loss : 0.06056107891761116ITERATION : 2, loss : 0.05837953515544333ITERATION : 3, loss : 0.059633331761501825ITERATION : 4, loss : 0.06117194014372972ITERATION : 5, loss : 0.06252806568674164ITERATION : 6, loss : 0.06361858021124067ITERATION : 7, loss : 0.06446343888692752ITERATION : 8, loss : 0.06510632192530626ITERATION : 9, loss : 0.0655907843589663ITERATION : 10, loss : 0.06595377800981145ITERATION : 11, loss : 0.06622477626033685ITERATION : 12, loss : 0.06642660822555199ITERATION : 13, loss : 0.06657667837075866ITERATION : 14, loss : 0.06668813055579138ITERATION : 15, loss : 0.06677083275007713ITERATION : 16, loss : 0.0668321633512665ITERATION : 17, loss : 0.06687762451696593ITERATION : 18, loss : 0.06691131095332584ITERATION : 19, loss : 0.06693626595576559ITERATION : 20, loss : 0.06695474912810441ITERATION : 21, loss : 0.06696843688895723ITERATION : 22, loss : 0.0669785722316663ITERATION : 23, loss : 0.06698607642372931ITERATION : 24, loss : 0.06699163214330871ITERATION : 25, loss : 0.06699574509336051ITERATION : 26, loss : 0.06699878975086283ITERATION : 27, loss : 0.06700104354504495ITERATION : 28, loss : 0.06700271182024886ITERATION : 29, loss : 0.06700394668783179ITERATION : 30, loss : 0.0670048606999984ITERATION : 31, loss : 0.06700553721172442ITERATION : 32, loss : 0.06700603794436162ITERATION : 33, loss : 0.0670064085171675ITERATION : 34, loss : 0.06700668278093638ITERATION : 35, loss : 0.0670068857625387ITERATION : 36, loss : 0.0670070360068196ITERATION : 37, loss : 0.0670071472699563ITERATION : 38, loss : 0.06700722955283882ITERATION : 39, loss : 0.06700729047896932ITERATION : 40, loss : 0.06700733553519139ITERATION : 41, loss : 0.06700736893000882ITERATION : 42, loss : 0.06700739360096236ITERATION : 43, loss : 0.0670074119084558ITERATION : 44, loss : 0.06700742544735469ITERATION : 45, loss : 0.06700743546707953ITERATION : 46, loss : 0.06700744285321618ITERATION : 47, loss : 0.06700744834011987ITERATION : 48, loss : 0.06700745247017943ITERATION : 49, loss : 0.06700745547224046ITERATION : 50, loss : 0.06700745765317054ITERATION : 51, loss : 0.0670074593106431ITERATION : 52, loss : 0.06700746050052941ITERATION : 53, loss : 0.06700746138772068ITERATION : 54, loss : 0.06700746206137101ITERATION : 55, loss : 0.06700746249809111ITERATION : 56, loss : 0.06700746287783399ITERATION : 57, loss : 0.06700746323951459ITERATION : 58, loss : 0.06700746338948499ITERATION : 59, loss : 0.06700746353021834ITERATION : 60, loss : 0.06700746355212582ITERATION : 61, loss : 0.06700746363985448ITERATION : 62, loss : 0.06700746364045511ITERATION : 63, loss : 0.06700746364045511ITERATION : 64, loss : 0.06700746364045511ITERATION : 65, loss : 0.06700746364045511ITERATION : 66, loss : 0.06700746364045511ITERATION : 67, loss : 0.06700746364045511ITERATION : 68, loss : 0.06700746364045511ITERATION : 69, loss : 0.06700746364045511ITERATION : 70, loss : 0.06700746364045511ITERATION : 71, loss : 0.06700746364045511ITERATION : 72, loss : 0.06700746364045511ITERATION : 73, loss : 0.06700746364045511ITERATION : 74, loss : 0.06700746364045511ITERATION : 75, loss : 0.06700746364045511ITERATION : 76, loss : 0.06700746364045511ITERATION : 77, loss : 0.06700746364045511ITERATION : 78, loss : 0.06700746364045511ITERATION : 79, loss : 0.06700746364045511ITERATION : 80, loss : 0.06700746364045511ITERATION : 81, loss : 0.06700746364045511ITERATION : 82, loss : 0.06700746364045511ITERATION : 83, loss : 0.06700746364045511ITERATION : 84, loss : 0.06700746364045511ITERATION : 85, loss : 0.06700746364045511ITERATION : 86, loss : 0.06700746364045511ITERATION : 87, loss : 0.06700746364045511ITERATION : 88, loss : 0.06700746364045511ITERATION : 89, loss : 0.06700746364045511ITERATION : 90, loss : 0.06700746364045511ITERATION : 91, loss : 0.06700746364045511ITERATION : 92, loss : 0.06700746364045511ITERATION : 93, loss : 0.06700746364045511ITERATION : 94, loss : 0.06700746364045511ITERATION : 95, loss : 0.06700746364045511ITERATION : 96, loss : 0.06700746364045511ITERATION : 97, loss : 0.06700746364045511ITERATION : 98, loss : 0.06700746364045511ITERATION : 99, loss : 0.06700746364045511ITERATION : 100, loss : 0.06700746364045511
ITERATION : 1, loss : 0.05272969522055416ITERATION : 2, loss : 0.05225758368322028ITERATION : 3, loss : 0.054229437280426576ITERATION : 4, loss : 0.05607064573502009ITERATION : 5, loss : 0.05752940608039621ITERATION : 6, loss : 0.05863283750045799ITERATION : 7, loss : 0.05945130140682762ITERATION : 8, loss : 0.060052169372739404ITERATION : 9, loss : 0.06049057898302035ITERATION : 10, loss : 0.06080918480925768ITERATION : 11, loss : 0.06104010665164843ITERATION : 12, loss : 0.06120716755681289ITERATION : 13, loss : 0.06132787228683401ITERATION : 14, loss : 0.06141500442842806ITERATION : 15, loss : 0.06147786127592804ITERATION : 16, loss : 0.061523185249185784ITERATION : 17, loss : 0.06155585620492782ITERATION : 18, loss : 0.061579401046189745ITERATION : 19, loss : 0.06159636616642047ITERATION : 20, loss : 0.06160858893299458ITERATION : 21, loss : 0.06161739422609859ITERATION : 22, loss : 0.06162373723669949ITERATION : 23, loss : 0.06162830633629812ITERATION : 24, loss : 0.061631597538694974ITERATION : 25, loss : 0.061633968260845405ITERATION : 26, loss : 0.06163567586482622ITERATION : 27, loss : 0.06163690582624743ITERATION : 28, loss : 0.06163779173272851ITERATION : 29, loss : 0.06163842984291367ITERATION : 30, loss : 0.06163888947643072ITERATION : 31, loss : 0.06163922053606709ITERATION : 32, loss : 0.06163945899169081ITERATION : 33, loss : 0.06163963075640382ITERATION : 34, loss : 0.06163975448776971ITERATION : 35, loss : 0.061639843605467246ITERATION : 36, loss : 0.0616399077959697ITERATION : 37, loss : 0.06163995406933568ITERATION : 38, loss : 0.06163998737881337ITERATION : 39, loss : 0.06164001135570739ITERATION : 40, loss : 0.06164002862525814ITERATION : 41, loss : 0.061640041049972345ITERATION : 42, loss : 0.061640050002000804ITERATION : 43, loss : 0.061640056450473525ITERATION : 44, loss : 0.06164006107649724ITERATION : 45, loss : 0.061640064408662446ITERATION : 46, loss : 0.0616400668189791ITERATION : 47, loss : 0.06164006855841313ITERATION : 48, loss : 0.06164006977077164ITERATION : 49, loss : 0.061640070646986005ITERATION : 50, loss : 0.0616400713022321ITERATION : 51, loss : 0.06164007177965039ITERATION : 52, loss : 0.061640072114200174ITERATION : 53, loss : 0.0616400723640418ITERATION : 54, loss : 0.06164007247309721ITERATION : 55, loss : 0.061640072610487685ITERATION : 56, loss : 0.061640072666645646ITERATION : 57, loss : 0.061640072755432616ITERATION : 58, loss : 0.061640072755970256ITERATION : 59, loss : 0.061640072755970256ITERATION : 60, loss : 0.061640072755970256ITERATION : 61, loss : 0.061640072755970256ITERATION : 62, loss : 0.061640072755970256ITERATION : 63, loss : 0.061640072755970256ITERATION : 64, loss : 0.061640072755970256ITERATION : 65, loss : 0.061640072755970256ITERATION : 66, loss : 0.061640072755970256ITERATION : 67, loss : 0.061640072755970256ITERATION : 68, loss : 0.061640072755970256ITERATION : 69, loss : 0.061640072755970256ITERATION : 70, loss : 0.061640072755970256ITERATION : 71, loss : 0.061640072755970256ITERATION : 72, loss : 0.061640072755970256ITERATION : 73, loss : 0.061640072755970256ITERATION : 74, loss : 0.061640072755970256ITERATION : 75, loss : 0.061640072755970256ITERATION : 76, loss : 0.061640072755970256ITERATION : 77, loss : 0.061640072755970256ITERATION : 78, loss : 0.061640072755970256ITERATION : 79, loss : 0.061640072755970256ITERATION : 80, loss : 0.061640072755970256ITERATION : 81, loss : 0.061640072755970256ITERATION : 82, loss : 0.061640072755970256ITERATION : 83, loss : 0.061640072755970256ITERATION : 84, loss : 0.061640072755970256ITERATION : 85, loss : 0.061640072755970256ITERATION : 86, loss : 0.061640072755970256ITERATION : 87, loss : 0.061640072755970256ITERATION : 88, loss : 0.061640072755970256ITERATION : 89, loss : 0.061640072755970256ITERATION : 90, loss : 0.061640072755970256ITERATION : 91, loss : 0.061640072755970256ITERATION : 92, loss : 0.061640072755970256ITERATION : 93, loss : 0.061640072755970256ITERATION : 94, loss : 0.061640072755970256ITERATION : 95, loss : 0.061640072755970256ITERATION : 96, loss : 0.061640072755970256ITERATION : 97, loss : 0.061640072755970256ITERATION : 98, loss : 0.061640072755970256ITERATION : 99, loss : 0.061640072755970256ITERATION : 100, loss : 0.061640072755970256
ITERATION : 1, loss : 0.09616772145417096ITERATION : 2, loss : 0.10069223603658324ITERATION : 3, loss : 0.10479869368977765ITERATION : 4, loss : 0.10793047634077099ITERATION : 5, loss : 0.11018412904466146ITERATION : 6, loss : 0.11178065025610445ITERATION : 7, loss : 0.11290930258309034ITERATION : 8, loss : 0.11370789451353783ITERATION : 9, loss : 0.11427372932864933ITERATION : 10, loss : 0.11467516088401133ITERATION : 11, loss : 0.11496025551742892ITERATION : 12, loss : 0.11516289365500311ITERATION : 13, loss : 0.11530701176964253ITERATION : 14, loss : 0.11540955640308856ITERATION : 15, loss : 0.11548254345706228ITERATION : 16, loss : 0.11553450438894451ITERATION : 17, loss : 0.11557150198182733ITERATION : 18, loss : 0.11559784749145673ITERATION : 19, loss : 0.11561660874365992ITERATION : 20, loss : 0.11562996939861922ITERATION : 21, loss : 0.11563948394158656ITERATION : 22, loss : 0.11564625940823534ITERATION : 23, loss : 0.11565108415144155ITERATION : 24, loss : 0.11565451964196759ITERATION : 25, loss : 0.11565696579592862ITERATION : 26, loss : 0.11565870743828612ITERATION : 27, loss : 0.11565994740119692ITERATION : 28, loss : 0.11566083009405825ITERATION : 29, loss : 0.11566145846782473ITERATION : 30, loss : 0.11566190578423116ITERATION : 31, loss : 0.11566222415047615ITERATION : 32, loss : 0.11566245075933367ITERATION : 33, loss : 0.11566261193855897ITERATION : 34, loss : 0.11566272675122216ITERATION : 35, loss : 0.1156628084658978ITERATION : 36, loss : 0.11566286660179396ITERATION : 37, loss : 0.11566290796534581ITERATION : 38, loss : 0.11566293741114814ITERATION : 39, loss : 0.1156629583585471ITERATION : 40, loss : 0.11566297326539865ITERATION : 41, loss : 0.11566298387132945ITERATION : 42, loss : 0.11566299139746784ITERATION : 43, loss : 0.11566299673481131ITERATION : 44, loss : 0.11566300051788887ITERATION : 45, loss : 0.11566300327250768ITERATION : 46, loss : 0.11566300508360935ITERATION : 47, loss : 0.11566300648691533ITERATION : 48, loss : 0.11566300745775329ITERATION : 49, loss : 0.11566300814530474ITERATION : 50, loss : 0.11566300855524105ITERATION : 51, loss : 0.11566300883801665ITERATION : 52, loss : 0.11566300915957886ITERATION : 53, loss : 0.11566300935817062ITERATION : 54, loss : 0.11566300942817742ITERATION : 55, loss : 0.11566300949243323ITERATION : 56, loss : 0.11566300950019381ITERATION : 57, loss : 0.11566300950019381ITERATION : 58, loss : 0.11566300950019381ITERATION : 59, loss : 0.11566300950019381ITERATION : 60, loss : 0.11566300950019381ITERATION : 61, loss : 0.11566300950019381ITERATION : 62, loss : 0.11566300950019381ITERATION : 63, loss : 0.11566300950019381ITERATION : 64, loss : 0.11566300950019381ITERATION : 65, loss : 0.11566300950019381ITERATION : 66, loss : 0.11566300950019381ITERATION : 67, loss : 0.11566300950019381ITERATION : 68, loss : 0.11566300950019381ITERATION : 69, loss : 0.11566300950019381ITERATION : 70, loss : 0.11566300950019381ITERATION : 71, loss : 0.11566300950019381ITERATION : 72, loss : 0.11566300950019381ITERATION : 73, loss : 0.11566300950019381ITERATION : 74, loss : 0.11566300950019381ITERATION : 75, loss : 0.11566300950019381ITERATION : 76, loss : 0.11566300950019381ITERATION : 77, loss : 0.11566300950019381ITERATION : 78, loss : 0.11566300950019381ITERATION : 79, loss : 0.11566300950019381ITERATION : 80, loss : 0.11566300950019381ITERATION : 81, loss : 0.11566300950019381ITERATION : 82, loss : 0.11566300950019381ITERATION : 83, loss : 0.11566300950019381ITERATION : 84, loss : 0.11566300950019381ITERATION : 85, loss : 0.11566300950019381ITERATION : 86, loss : 0.11566300950019381ITERATION : 87, loss : 0.11566300950019381ITERATION : 88, loss : 0.11566300950019381ITERATION : 89, loss : 0.11566300950019381ITERATION : 90, loss : 0.11566300950019381ITERATION : 91, loss : 0.11566300950019381ITERATION : 92, loss : 0.11566300950019381ITERATION : 93, loss : 0.11566300950019381ITERATION : 94, loss : 0.11566300950019381ITERATION : 95, loss : 0.11566300950019381ITERATION : 96, loss : 0.11566300950019381ITERATION : 97, loss : 0.11566300950019381ITERATION : 98, loss : 0.11566300950019381ITERATION : 99, loss : 0.11566300950019381ITERATION : 100, loss : 0.11566300950019381
ITERATION : 1, loss : 0.027186585676327195ITERATION : 2, loss : 0.037789584611767735ITERATION : 3, loss : 0.04525543880356024ITERATION : 4, loss : 0.05020181388038021ITERATION : 5, loss : 0.05359324735089265ITERATION : 6, loss : 0.0559532492964071ITERATION : 7, loss : 0.057612514963986144ITERATION : 8, loss : 0.058787989378080226ITERATION : 9, loss : 0.05962547535599767ITERATION : 10, loss : 0.06022469682450886ITERATION : 11, loss : 0.06065479627172022ITERATION : 12, loss : 0.0609642270068599ITERATION : 13, loss : 0.061187225646053396ITERATION : 14, loss : 0.06134813650539348ITERATION : 15, loss : 0.06146435232301085ITERATION : 16, loss : 0.061548343482472435ITERATION : 17, loss : 0.061609074553786024ITERATION : 18, loss : 0.06165300237451509ITERATION : 19, loss : 0.0616847840842326ITERATION : 20, loss : 0.06170778223621622ITERATION : 21, loss : 0.06172442649961761ITERATION : 22, loss : 0.06173647340462443ITERATION : 23, loss : 0.061745193368660044ITERATION : 24, loss : 0.06175150545016711ITERATION : 25, loss : 0.06175607470622484ITERATION : 26, loss : 0.061759382407399296ITERATION : 27, loss : 0.061761776869290456ITERATION : 28, loss : 0.06176351024399072ITERATION : 29, loss : 0.06176476508800218ITERATION : 30, loss : 0.06176567345478353ITERATION : 31, loss : 0.0617663310718854ITERATION : 32, loss : 0.06176680713061142ITERATION : 33, loss : 0.06176715173598488ITERATION : 34, loss : 0.06176740121653034ITERATION : 35, loss : 0.06176758180429741ITERATION : 36, loss : 0.06176771256036463ITERATION : 37, loss : 0.06176780718912272ITERATION : 38, loss : 0.06176787567906815ITERATION : 39, loss : 0.06176792526125171ITERATION : 40, loss : 0.061767961177862325ITERATION : 41, loss : 0.06176798717412172ITERATION : 42, loss : 0.06176800600878786ITERATION : 43, loss : 0.06176801963154452ITERATION : 44, loss : 0.06176802948789109ITERATION : 45, loss : 0.06176803661497142ITERATION : 46, loss : 0.06176804176108374ITERATION : 47, loss : 0.06176804547470381ITERATION : 48, loss : 0.061768048172451843ITERATION : 49, loss : 0.06176805011319008ITERATION : 50, loss : 0.061768051528719066ITERATION : 51, loss : 0.061768052520504296ITERATION : 52, loss : 0.06176805323911436ITERATION : 53, loss : 0.06176805376002778ITERATION : 54, loss : 0.06176805415934308ITERATION : 55, loss : 0.061768054448614655ITERATION : 56, loss : 0.06176805466717055ITERATION : 57, loss : 0.06176805480072265ITERATION : 58, loss : 0.06176805491492918ITERATION : 59, loss : 0.06176805495582385ITERATION : 60, loss : 0.0617680550153206ITERATION : 61, loss : 0.061768055050489865ITERATION : 62, loss : 0.061768055051856015ITERATION : 63, loss : 0.061768055051856015ITERATION : 64, loss : 0.061768055051856015ITERATION : 65, loss : 0.061768055051856015ITERATION : 66, loss : 0.061768055051856015ITERATION : 67, loss : 0.061768055051856015ITERATION : 68, loss : 0.061768055051856015ITERATION : 69, loss : 0.061768055051856015ITERATION : 70, loss : 0.061768055051856015ITERATION : 71, loss : 0.061768055051856015ITERATION : 72, loss : 0.061768055051856015ITERATION : 73, loss : 0.061768055051856015ITERATION : 74, loss : 0.061768055051856015ITERATION : 75, loss : 0.061768055051856015ITERATION : 76, loss : 0.061768055051856015ITERATION : 77, loss : 0.061768055051856015ITERATION : 78, loss : 0.061768055051856015ITERATION : 79, loss : 0.061768055051856015ITERATION : 80, loss : 0.061768055051856015ITERATION : 81, loss : 0.061768055051856015ITERATION : 82, loss : 0.061768055051856015ITERATION : 83, loss : 0.061768055051856015ITERATION : 84, loss : 0.061768055051856015ITERATION : 85, loss : 0.061768055051856015ITERATION : 86, loss : 0.061768055051856015ITERATION : 87, loss : 0.061768055051856015ITERATION : 88, loss : 0.061768055051856015ITERATION : 89, loss : 0.061768055051856015ITERATION : 90, loss : 0.061768055051856015ITERATION : 91, loss : 0.061768055051856015ITERATION : 92, loss : 0.061768055051856015ITERATION : 93, loss : 0.061768055051856015ITERATION : 94, loss : 0.061768055051856015ITERATION : 95, loss : 0.061768055051856015ITERATION : 96, loss : 0.061768055051856015ITERATION : 97, loss : 0.061768055051856015ITERATION : 98, loss : 0.061768055051856015ITERATION : 99, loss : 0.061768055051856015ITERATION : 100, loss : 0.061768055051856015
ITERATION : 1, loss : 0.10497522342644117ITERATION : 2, loss : 0.11791520625611034ITERATION : 3, loss : 0.129022957516007ITERATION : 4, loss : 0.13724694354295006ITERATION : 5, loss : 0.14318760057668997ITERATION : 6, loss : 0.1474610562921783ITERATION : 7, loss : 0.15053984345828914ITERATION : 8, loss : 0.15275779074456536ITERATION : 9, loss : 0.15424629480165802ITERATION : 10, loss : 0.15532679091068965ITERATION : 11, loss : 0.1561132674314114ITERATION : 12, loss : 0.1566869963063873ITERATION : 13, loss : 0.15710626233420927ITERATION : 14, loss : 0.15741307769282492ITERATION : 15, loss : 0.15763785164436223ITERATION : 16, loss : 0.15780266841094176ITERATION : 17, loss : 0.1579236086915472ITERATION : 18, loss : 0.1580124058582714ITERATION : 19, loss : 0.15807763489268875ITERATION : 20, loss : 0.15812557125017354ITERATION : 21, loss : 0.15816081194678402ITERATION : 22, loss : 0.15818672736287467ITERATION : 23, loss : 0.15820579019374098ITERATION : 24, loss : 0.15821981583281047ITERATION : 25, loss : 0.1582301375163118ITERATION : 26, loss : 0.15823773481904935ITERATION : 27, loss : 0.15824332780096592ITERATION : 28, loss : 0.15824744578518402ITERATION : 29, loss : 0.1582504782603569ITERATION : 30, loss : 0.15825271166175167ITERATION : 31, loss : 0.15825435673564398ITERATION : 32, loss : 0.15825556863738988ITERATION : 33, loss : 0.15825646146245217ITERATION : 34, loss : 0.1582571193108997ITERATION : 35, loss : 0.15825760407472012ITERATION : 36, loss : 0.15825796131291256ITERATION : 37, loss : 0.15825822452342334ITERATION : 38, loss : 0.1582584185373566ITERATION : 39, loss : 0.1582585614956917ITERATION : 40, loss : 0.1582586669041148ITERATION : 41, loss : 0.15825874460034686ITERATION : 42, loss : 0.15825880185846763ITERATION : 43, loss : 0.15825884406205787ITERATION : 44, loss : 0.15825887512451703ITERATION : 45, loss : 0.1582588980615135ITERATION : 46, loss : 0.158258914956039ITERATION : 47, loss : 0.15825892749573542ITERATION : 48, loss : 0.15825893667777216ITERATION : 49, loss : 0.1582589434627413ITERATION : 50, loss : 0.15825894843484492ITERATION : 51, loss : 0.15825895210640167ITERATION : 52, loss : 0.1582589547986423ITERATION : 53, loss : 0.15825895680467356ITERATION : 54, loss : 0.1582589581960378ITERATION : 55, loss : 0.158258959267961ITERATION : 56, loss : 0.15825896005968798ITERATION : 57, loss : 0.15825896065583178ITERATION : 58, loss : 0.1582589611180619ITERATION : 59, loss : 0.1582589614066594ITERATION : 60, loss : 0.15825896160526404ITERATION : 61, loss : 0.15825896180065321ITERATION : 62, loss : 0.1582589619665233ITERATION : 63, loss : 0.15825896207421372ITERATION : 64, loss : 0.1582589620875691ITERATION : 65, loss : 0.1582589620895019ITERATION : 66, loss : 0.15825896209208895ITERATION : 67, loss : 0.15825896209208895ITERATION : 68, loss : 0.15825896209208895ITERATION : 69, loss : 0.15825896209208895ITERATION : 70, loss : 0.15825896209208895ITERATION : 71, loss : 0.15825896209208895ITERATION : 72, loss : 0.15825896209208895ITERATION : 73, loss : 0.15825896209208895ITERATION : 74, loss : 0.15825896209208895ITERATION : 75, loss : 0.15825896209208895ITERATION : 76, loss : 0.15825896209208895ITERATION : 77, loss : 0.15825896209208895ITERATION : 78, loss : 0.15825896209208895ITERATION : 79, loss : 0.15825896209208895ITERATION : 80, loss : 0.15825896209208895ITERATION : 81, loss : 0.15825896209208895ITERATION : 82, loss : 0.15825896209208895ITERATION : 83, loss : 0.15825896209208895ITERATION : 84, loss : 0.15825896209208895ITERATION : 85, loss : 0.15825896209208895ITERATION : 86, loss : 0.15825896209208895ITERATION : 87, loss : 0.15825896209208895ITERATION : 88, loss : 0.15825896209208895ITERATION : 89, loss : 0.15825896209208895ITERATION : 90, loss : 0.15825896209208895ITERATION : 91, loss : 0.15825896209208895ITERATION : 92, loss : 0.15825896209208895ITERATION : 93, loss : 0.15825896209208895ITERATION : 94, loss : 0.15825896209208895ITERATION : 95, loss : 0.15825896209208895ITERATION : 96, loss : 0.15825896209208895ITERATION : 97, loss : 0.15825896209208895ITERATION : 98, loss : 0.15825896209208895ITERATION : 99, loss : 0.15825896209208895ITERATION : 100, loss : 0.15825896209208895
ITERATION : 1, loss : 0.040003142689965215ITERATION : 2, loss : 0.03729598894192084ITERATION : 3, loss : 0.039338802216056346ITERATION : 4, loss : 0.04173649008818147ITERATION : 5, loss : 0.04374267475693849ITERATION : 6, loss : 0.045278844198783065ITERATION : 7, loss : 0.046413767444579666ITERATION : 8, loss : 0.04723781184433719ITERATION : 9, loss : 0.04783056212585117ITERATION : 10, loss : 0.048254651939602766ITERATION : 11, loss : 0.048557097315307635ITERATION : 12, loss : 0.04877236584663387ITERATION : 13, loss : 0.048925399362172974ITERATION : 14, loss : 0.049034109556095905ITERATION : 15, loss : 0.04911130018121331ITERATION : 16, loss : 0.04916609687036409ITERATION : 17, loss : 0.04920499211206346ITERATION : 18, loss : 0.049232599751454525ITERATION : 19, loss : 0.04925219626370819ITERATION : 20, loss : 0.049266107317471705ITERATION : 21, loss : 0.049275983414254834ITERATION : 22, loss : 0.04928299570944068ITERATION : 23, loss : 0.049287975280101766ITERATION : 24, loss : 0.04929151176694521ITERATION : 25, loss : 0.04929402373679789ITERATION : 26, loss : 0.04929580816663742ITERATION : 27, loss : 0.04929707600757098ITERATION : 28, loss : 0.049297976892498256ITERATION : 29, loss : 0.04929861707904563ITERATION : 30, loss : 0.04929907212044309ITERATION : 31, loss : 0.049299395566900904ITERATION : 32, loss : 0.0492996254855008ITERATION : 33, loss : 0.049299788960729785ITERATION : 34, loss : 0.04929990520733573ITERATION : 35, loss : 0.04929998788477986ITERATION : 36, loss : 0.04930004665302826ITERATION : 37, loss : 0.04930008844191622ITERATION : 38, loss : 0.04930011817088124ITERATION : 39, loss : 0.04930013930475836ITERATION : 40, loss : 0.0493001543490158ITERATION : 41, loss : 0.04930016501951593ITERATION : 42, loss : 0.04930017263171652ITERATION : 43, loss : 0.04930017803092136ITERATION : 44, loss : 0.049300181845519646ITERATION : 45, loss : 0.049300184579043324ITERATION : 46, loss : 0.049300186541749974ITERATION : 47, loss : 0.04930018789615905ITERATION : 48, loss : 0.04930018886511732ITERATION : 49, loss : 0.04930018956780957ITERATION : 50, loss : 0.04930019008275388ITERATION : 51, loss : 0.0493001904473066ITERATION : 52, loss : 0.04930019072241462ITERATION : 53, loss : 0.04930019087048486ITERATION : 54, loss : 0.04930019099157379ITERATION : 55, loss : 0.04930019108637176ITERATION : 56, loss : 0.04930019108874428ITERATION : 57, loss : 0.04930019108874428ITERATION : 58, loss : 0.04930019108874428ITERATION : 59, loss : 0.04930019108874428ITERATION : 60, loss : 0.04930019108874428ITERATION : 61, loss : 0.04930019108874428ITERATION : 62, loss : 0.04930019108874428ITERATION : 63, loss : 0.04930019108874428ITERATION : 64, loss : 0.04930019108874428ITERATION : 65, loss : 0.04930019108874428ITERATION : 66, loss : 0.04930019108874428ITERATION : 67, loss : 0.04930019108874428ITERATION : 68, loss : 0.04930019108874428ITERATION : 69, loss : 0.04930019108874428ITERATION : 70, loss : 0.04930019108874428ITERATION : 71, loss : 0.04930019108874428ITERATION : 72, loss : 0.04930019108874428ITERATION : 73, loss : 0.04930019108874428ITERATION : 74, loss : 0.04930019108874428ITERATION : 75, loss : 0.04930019108874428ITERATION : 76, loss : 0.04930019108874428ITERATION : 77, loss : 0.04930019108874428ITERATION : 78, loss : 0.04930019108874428ITERATION : 79, loss : 0.04930019108874428ITERATION : 80, loss : 0.04930019108874428ITERATION : 81, loss : 0.04930019108874428ITERATION : 82, loss : 0.04930019108874428ITERATION : 83, loss : 0.04930019108874428ITERATION : 84, loss : 0.04930019108874428ITERATION : 85, loss : 0.04930019108874428ITERATION : 86, loss : 0.04930019108874428ITERATION : 87, loss : 0.04930019108874428ITERATION : 88, loss : 0.04930019108874428ITERATION : 89, loss : 0.04930019108874428ITERATION : 90, loss : 0.04930019108874428ITERATION : 91, loss : 0.04930019108874428ITERATION : 92, loss : 0.04930019108874428ITERATION : 93, loss : 0.04930019108874428ITERATION : 94, loss : 0.04930019108874428ITERATION : 95, loss : 0.04930019108874428ITERATION : 96, loss : 0.04930019108874428ITERATION : 97, loss : 0.04930019108874428ITERATION : 98, loss : 0.04930019108874428ITERATION : 99, loss : 0.04930019108874428ITERATION : 100, loss : 0.04930019108874428
gradient norm in None layer : 0.011001959424871779
gradient norm in None layer : 0.000586270630956206
gradient norm in None layer : 0.0006204090447023546
gradient norm in None layer : 0.011027508339557008
gradient norm in None layer : 0.0004970967133089828
gradient norm in None layer : 0.0005367234574671213
gradient norm in None layer : 0.004868619054994562
gradient norm in None layer : 0.0001617191978553778
gradient norm in None layer : 0.00017006331867944235
gradient norm in None layer : 0.003722625826825523
gradient norm in None layer : 0.0001478421042820046
gradient norm in None layer : 0.00014281411227552908
gradient norm in None layer : 0.0010599405963786042
gradient norm in None layer : 2.9840835863460382e-05
gradient norm in None layer : 2.8728817344963985e-05
gradient norm in None layer : 0.001021109107289319
gradient norm in None layer : 3.5908543354045075e-05
gradient norm in None layer : 3.741495725944573e-05
gradient norm in None layer : 0.0013848621785249838
gradient norm in None layer : 1.758035763267709e-05
gradient norm in None layer : 0.003625798050192044
gradient norm in None layer : 0.00015744079531495635
gradient norm in None layer : 0.00014896775085919683
gradient norm in None layer : 0.003728564362337623
gradient norm in None layer : 0.0003607564973368073
gradient norm in None layer : 0.0005113562750590192
gradient norm in None layer : 0.010086087079514904
gradient norm in None layer : 4.761147081969468e-05
gradient norm in None layer : 0.010858113212395866
gradient norm in None layer : 0.0008062766822107904
gradient norm in None layer : 0.0005838575715391888
gradient norm in None layer : 0.013305847753318082
gradient norm in None layer : 0.0021221382870106552
gradient norm in None layer : 0.0030810934841305586
gradient norm in None layer : 0.001808167687944018
gradient norm in None layer : 0.0005981889062394809
Total gradient norm: 0.02698959033654403
invariance loss : 7.06245337027943, avg_den : 0.24861907958984375, density loss : 0.14861907958984377, mse loss : 0.08159581080910022, solver time : 106.60665822029114 sec , total loss : 0.08880688325896949, running loss : 0.09745899989871869
Epoch 0/10 , batch 3/12500 
ITERATION : 1, loss : 0.05170550235341217ITERATION : 2, loss : 0.04454459978959938ITERATION : 3, loss : 0.04471729255845622ITERATION : 4, loss : 0.04616489635758385ITERATION : 5, loss : 0.04755300760251607ITERATION : 6, loss : 0.048614452230042064ITERATION : 7, loss : 0.049362121277431385ITERATION : 8, loss : 0.0498687554352469ITERATION : 9, loss : 0.050204948447791144ITERATION : 10, loss : 0.050425276457510916ITERATION : 11, loss : 0.05056850437867391ITERATION : 12, loss : 0.050661073951722736ITERATION : 13, loss : 0.050720629358170274ITERATION : 14, loss : 0.050758791988237ITERATION : 15, loss : 0.050783153067226514ITERATION : 16, loss : 0.05079864291374583ITERATION : 17, loss : 0.050808450036430905ITERATION : 18, loss : 0.05081462928950247ITERATION : 19, loss : 0.05081850084019157ITERATION : 20, loss : 0.05082091025300312ITERATION : 21, loss : 0.05082239749608973ITERATION : 22, loss : 0.050823306186185066ITERATION : 23, loss : 0.05082385418909123ITERATION : 24, loss : 0.05082417909602659ITERATION : 25, loss : 0.05082436728042056ITERATION : 26, loss : 0.05082447272177852ITERATION : 27, loss : 0.05082452885725686ITERATION : 28, loss : 0.05082455626584466ITERATION : 29, loss : 0.05082456738344507ITERATION : 30, loss : 0.050824569692233854ITERATION : 31, loss : 0.05082456758335197ITERATION : 32, loss : 0.05082456356729318ITERATION : 33, loss : 0.05082455899596939ITERATION : 34, loss : 0.0508245546275459ITERATION : 35, loss : 0.05082455072233545ITERATION : 36, loss : 0.05082454738712527ITERATION : 37, loss : 0.050824544649367955ITERATION : 38, loss : 0.050824542494687944ITERATION : 39, loss : 0.05082454077210367ITERATION : 40, loss : 0.05082453943556257ITERATION : 41, loss : 0.05082453838587871ITERATION : 42, loss : 0.05082453755936795ITERATION : 43, loss : 0.050824536929600116ITERATION : 44, loss : 0.05082453643934854ITERATION : 45, loss : 0.05082453607104992ITERATION : 46, loss : 0.05082453580349853ITERATION : 47, loss : 0.050824535592972495ITERATION : 48, loss : 0.05082453544094558ITERATION : 49, loss : 0.050824535327100365ITERATION : 50, loss : 0.05082453523676632ITERATION : 51, loss : 0.05082453517703624ITERATION : 52, loss : 0.05082453512954578ITERATION : 53, loss : 0.05082453509543345ITERATION : 54, loss : 0.050824535066560615ITERATION : 55, loss : 0.0508245350494937ITERATION : 56, loss : 0.05082453503431463ITERATION : 57, loss : 0.050824535027356454ITERATION : 58, loss : 0.05082453502300629ITERATION : 59, loss : 0.05082453501536633ITERATION : 60, loss : 0.05082453501120463ITERATION : 61, loss : 0.05082453500909404ITERATION : 62, loss : 0.05082453500916386ITERATION : 63, loss : 0.05082453500916386ITERATION : 64, loss : 0.05082453500916386ITERATION : 65, loss : 0.05082453500916386ITERATION : 66, loss : 0.05082453500916386ITERATION : 67, loss : 0.05082453500916386ITERATION : 68, loss : 0.05082453500916386ITERATION : 69, loss : 0.05082453500916386ITERATION : 70, loss : 0.05082453500916386ITERATION : 71, loss : 0.05082453500916386ITERATION : 72, loss : 0.05082453500916386ITERATION : 73, loss : 0.05082453500916386ITERATION : 74, loss : 0.05082453500916386ITERATION : 75, loss : 0.05082453500916386ITERATION : 76, loss : 0.05082453500916386ITERATION : 77, loss : 0.05082453500916386ITERATION : 78, loss : 0.05082453500916386ITERATION : 79, loss : 0.05082453500916386ITERATION : 80, loss : 0.05082453500916386ITERATION : 81, loss : 0.05082453500916386ITERATION : 82, loss : 0.05082453500916386ITERATION : 83, loss : 0.05082453500916386ITERATION : 84, loss : 0.05082453500916386ITERATION : 85, loss : 0.05082453500916386ITERATION : 86, loss : 0.05082453500916386ITERATION : 87, loss : 0.05082453500916386ITERATION : 88, loss : 0.05082453500916386ITERATION : 89, loss : 0.05082453500916386ITERATION : 90, loss : 0.05082453500916386ITERATION : 91, loss : 0.05082453500916386ITERATION : 92, loss : 0.05082453500916386ITERATION : 93, loss : 0.05082453500916386ITERATION : 94, loss : 0.05082453500916386ITERATION : 95, loss : 0.05082453500916386ITERATION : 96, loss : 0.05082453500916386ITERATION : 97, loss : 0.05082453500916386ITERATION : 98, loss : 0.05082453500916386ITERATION : 99, loss : 0.05082453500916386ITERATION : 100, loss : 0.05082453500916386
ITERATION : 1, loss : 0.030706032128353093ITERATION : 2, loss : 0.028630926449988603ITERATION : 3, loss : 0.027767232379841043ITERATION : 4, loss : 0.028201167546033072ITERATION : 5, loss : 0.028696275373958798ITERATION : 6, loss : 0.029117390854319202ITERATION : 7, loss : 0.02944203886713188ITERATION : 8, loss : 0.029681891212236647ITERATION : 9, loss : 0.029855589493505422ITERATION : 10, loss : 0.029980186465999058ITERATION : 11, loss : 0.030069168391021667ITERATION : 12, loss : 0.030132596092918237ITERATION : 13, loss : 0.030177779755263607ITERATION : 14, loss : 0.030209965644106534ITERATION : 15, loss : 0.03023289775431602ITERATION : 16, loss : 0.030249241950737367ITERATION : 17, loss : 0.030260894778383146ITERATION : 18, loss : 0.03026920542678283ITERATION : 19, loss : 0.03027513425241338ITERATION : 20, loss : 0.030279365045189823ITERATION : 21, loss : 0.030282384811181746ITERATION : 22, loss : 0.03028454069259556ITERATION : 23, loss : 0.030286080072068ITERATION : 24, loss : 0.03028717947630697ITERATION : 25, loss : 0.030287964804050906ITERATION : 26, loss : 0.03028852578913398ITERATION : 27, loss : 0.03028892660393408ITERATION : 28, loss : 0.030289213003732374ITERATION : 29, loss : 0.030289417725445827ITERATION : 30, loss : 0.03028956400804166ITERATION : 31, loss : 0.030289668575880592ITERATION : 32, loss : 0.030289743352931126ITERATION : 33, loss : 0.030289796775204877ITERATION : 34, loss : 0.030289834991916138ITERATION : 35, loss : 0.030289862345049982ITERATION : 36, loss : 0.030289881877065113ITERATION : 37, loss : 0.030289895836918976ITERATION : 38, loss : 0.03028990576039681ITERATION : 39, loss : 0.030289912890359948ITERATION : 40, loss : 0.03028991797735662ITERATION : 41, loss : 0.030289921622783586ITERATION : 42, loss : 0.03028992420466979ITERATION : 43, loss : 0.030289926050405352ITERATION : 44, loss : 0.030289927374856796ITERATION : 45, loss : 0.03028992832709191ITERATION : 46, loss : 0.03028992895638876ITERATION : 47, loss : 0.0302899294238326ITERATION : 48, loss : 0.030289929801286062ITERATION : 49, loss : 0.030289929998818958ITERATION : 50, loss : 0.03028993022390659ITERATION : 51, loss : 0.030289930318234248ITERATION : 52, loss : 0.030289930372399954ITERATION : 53, loss : 0.030289930418715034ITERATION : 54, loss : 0.030289930419813423ITERATION : 55, loss : 0.030289930419813423ITERATION : 56, loss : 0.030289930419813423ITERATION : 57, loss : 0.030289930419813423ITERATION : 58, loss : 0.030289930419813423ITERATION : 59, loss : 0.030289930419813423ITERATION : 60, loss : 0.030289930419813423ITERATION : 61, loss : 0.030289930419813423ITERATION : 62, loss : 0.030289930419813423ITERATION : 63, loss : 0.030289930419813423ITERATION : 64, loss : 0.030289930419813423ITERATION : 65, loss : 0.030289930419813423ITERATION : 66, loss : 0.030289930419813423ITERATION : 67, loss : 0.030289930419813423ITERATION : 68, loss : 0.030289930419813423ITERATION : 69, loss : 0.030289930419813423ITERATION : 70, loss : 0.030289930419813423ITERATION : 71, loss : 0.030289930419813423ITERATION : 72, loss : 0.030289930419813423ITERATION : 73, loss : 0.030289930419813423ITERATION : 74, loss : 0.030289930419813423ITERATION : 75, loss : 0.030289930419813423ITERATION : 76, loss : 0.030289930419813423ITERATION : 77, loss : 0.030289930419813423ITERATION : 78, loss : 0.030289930419813423ITERATION : 79, loss : 0.030289930419813423ITERATION : 80, loss : 0.030289930419813423ITERATION : 81, loss : 0.030289930419813423ITERATION : 82, loss : 0.030289930419813423ITERATION : 83, loss : 0.030289930419813423ITERATION : 84, loss : 0.030289930419813423ITERATION : 85, loss : 0.030289930419813423ITERATION : 86, loss : 0.030289930419813423ITERATION : 87, loss : 0.030289930419813423ITERATION : 88, loss : 0.030289930419813423ITERATION : 89, loss : 0.030289930419813423ITERATION : 90, loss : 0.030289930419813423ITERATION : 91, loss : 0.030289930419813423ITERATION : 92, loss : 0.030289930419813423ITERATION : 93, loss : 0.030289930419813423ITERATION : 94, loss : 0.030289930419813423ITERATION : 95, loss : 0.030289930419813423ITERATION : 96, loss : 0.030289930419813423ITERATION : 97, loss : 0.030289930419813423ITERATION : 98, loss : 0.030289930419813423ITERATION : 99, loss : 0.030289930419813423ITERATION : 100, loss : 0.030289930419813423
ITERATION : 1, loss : 0.007772192300677236ITERATION : 2, loss : 0.0070621567787867105ITERATION : 3, loss : 0.00734415927729035ITERATION : 4, loss : 0.007823089028093859ITERATION : 5, loss : 0.008235748414311576ITERATION : 6, loss : 0.008547253981790535ITERATION : 7, loss : 0.008771139353023203ITERATION : 8, loss : 0.008944759539074403ITERATION : 9, loss : 0.009089618405271994ITERATION : 10, loss : 0.009189173419480081ITERATION : 11, loss : 0.00925733520324058ITERATION : 12, loss : 0.009303874333612502ITERATION : 13, loss : 0.009335579227784375ITERATION : 14, loss : 0.00935713615509824ITERATION : 15, loss : 0.009371766342976182ITERATION : 16, loss : 0.009381677674606993ITERATION : 17, loss : 0.00938837988321212ITERATION : 18, loss : 0.009392903120780626ITERATION : 19, loss : 0.009395949722628345ITERATION : 20, loss : 0.009397997254490519ITERATION : 21, loss : 0.009399370095144605ITERATION : 22, loss : 0.009400288080958875ITERATION : 23, loss : 0.009400900176562213ITERATION : 24, loss : 0.009401307042109546ITERATION : 25, loss : 0.00940157638583986ITERATION : 26, loss : 0.009401754094424356ITERATION : 27, loss : 0.009401870697936344ITERATION : 28, loss : 0.009401946843557035ITERATION : 29, loss : 0.009401996205578297ITERATION : 30, loss : 0.009402028002736207ITERATION : 31, loss : 0.009402048277401109ITERATION : 32, loss : 0.009402061220634023ITERATION : 33, loss : 0.009402069217570053ITERATION : 34, loss : 0.00940207420806144ITERATION : 35, loss : 0.009402077154054153ITERATION : 36, loss : 0.009402078945267563ITERATION : 37, loss : 0.009402079848459885ITERATION : 38, loss : 0.009402080334044854ITERATION : 39, loss : 0.00940208037586625ITERATION : 40, loss : 0.009402080365617158ITERATION : 41, loss : 0.009402080184487197ITERATION : 42, loss : 0.009402080095775743ITERATION : 43, loss : 0.00940207989313663ITERATION : 44, loss : 0.009402079786554385ITERATION : 45, loss : 0.00940207964009352ITERATION : 46, loss : 0.009402079557963091ITERATION : 47, loss : 0.009402079446579975ITERATION : 48, loss : 0.00940207944638673ITERATION : 49, loss : 0.009402079341442784ITERATION : 50, loss : 0.009402079382570157ITERATION : 51, loss : 0.009402079264192335ITERATION : 52, loss : 0.009402079325295446ITERATION : 53, loss : 0.00940207923528497ITERATION : 54, loss : 0.009402079306737556ITERATION : 55, loss : 0.009402079239330303ITERATION : 56, loss : 0.009402079265273987ITERATION : 57, loss : 0.009402079265273987ITERATION : 58, loss : 0.009402079265273987ITERATION : 59, loss : 0.009402079265273987ITERATION : 60, loss : 0.009402079265273987ITERATION : 61, loss : 0.009402079265273987ITERATION : 62, loss : 0.009402079265273987ITERATION : 63, loss : 0.009402079265273987ITERATION : 64, loss : 0.009402079265273987ITERATION : 65, loss : 0.009402079265273987ITERATION : 66, loss : 0.009402079265273987ITERATION : 67, loss : 0.009402079265273987ITERATION : 68, loss : 0.009402079265273987ITERATION : 69, loss : 0.009402079265273987ITERATION : 70, loss : 0.009402079265273987ITERATION : 71, loss : 0.009402079265273987ITERATION : 72, loss : 0.009402079265273987ITERATION : 73, loss : 0.009402079265273987ITERATION : 74, loss : 0.009402079265273987ITERATION : 75, loss : 0.009402079265273987ITERATION : 76, loss : 0.009402079265273987ITERATION : 77, loss : 0.009402079265273987ITERATION : 78, loss : 0.009402079265273987ITERATION : 79, loss : 0.009402079265273987ITERATION : 80, loss : 0.009402079265273987ITERATION : 81, loss : 0.009402079265273987ITERATION : 82, loss : 0.009402079265273987ITERATION : 83, loss : 0.009402079265273987ITERATION : 84, loss : 0.009402079265273987ITERATION : 85, loss : 0.009402079265273987ITERATION : 86, loss : 0.009402079265273987ITERATION : 87, loss : 0.009402079265273987ITERATION : 88, loss : 0.009402079265273987ITERATION : 89, loss : 0.009402079265273987ITERATION : 90, loss : 0.009402079265273987ITERATION : 91, loss : 0.009402079265273987ITERATION : 92, loss : 0.009402079265273987ITERATION : 93, loss : 0.009402079265273987ITERATION : 94, loss : 0.009402079265273987ITERATION : 95, loss : 0.009402079265273987ITERATION : 96, loss : 0.009402079265273987ITERATION : 97, loss : 0.009402079265273987ITERATION : 98, loss : 0.009402079265273987ITERATION : 99, loss : 0.009402079265273987ITERATION : 100, loss : 0.009402079265273987
ITERATION : 1, loss : 0.020016019681164218ITERATION : 2, loss : 0.02431809054379092ITERATION : 3, loss : 0.02539069627684429ITERATION : 4, loss : 0.025394640068767245ITERATION : 5, loss : 0.025199114613660506ITERATION : 6, loss : 0.025010598189337603ITERATION : 7, loss : 0.02486405658694851ITERATION : 8, loss : 0.02475626177049266ITERATION : 9, loss : 0.024678014422174786ITERATION : 10, loss : 0.02462119677687596ITERATION : 11, loss : 0.024579755134220702ITERATION : 12, loss : 0.02454937526894446ITERATION : 13, loss : 0.024527006491064002ITERATION : 14, loss : 0.024510480450967716ITERATION : 15, loss : 0.024498241601201783ITERATION : 16, loss : 0.024489163260306295ITERATION : 17, loss : 0.024482422635377702ITERATION : 18, loss : 0.024477415187002676ITERATION : 19, loss : 0.024473694678722176ITERATION : 20, loss : 0.024470930365830486ITERATION : 21, loss : 0.02446887707742709ITERATION : 22, loss : 0.024467352307639835ITERATION : 23, loss : 0.024466220457244288ITERATION : 24, loss : 0.0244653806147389ITERATION : 25, loss : 0.024464757730182047ITERATION : 26, loss : 0.024464295992207643ITERATION : 27, loss : 0.024463953844985228ITERATION : 28, loss : 0.02446370047068727ITERATION : 29, loss : 0.024463512924337086ITERATION : 30, loss : 0.024463374083673658ITERATION : 31, loss : 0.0244632713812592ITERATION : 32, loss : 0.02446319543378588ITERATION : 33, loss : 0.024463139299677437ITERATION : 34, loss : 0.024463097826001964ITERATION : 35, loss : 0.024463067192374188ITERATION : 36, loss : 0.02446304456963886ITERATION : 37, loss : 0.02446302788406772ITERATION : 38, loss : 0.024463015567239867ITERATION : 39, loss : 0.024463006494806427ITERATION : 40, loss : 0.02446299978106576ITERATION : 41, loss : 0.024462994809704305ITERATION : 42, loss : 0.024462991185292875ITERATION : 43, loss : 0.02446298844906677ITERATION : 44, loss : 0.02446298649440349ITERATION : 45, loss : 0.024462984981954ITERATION : 46, loss : 0.02446298396390014ITERATION : 47, loss : 0.024462983118285004ITERATION : 48, loss : 0.024462982578707555ITERATION : 49, loss : 0.024462982090667624ITERATION : 50, loss : 0.024462981821096484ITERATION : 51, loss : 0.024462981524154385ITERATION : 52, loss : 0.024462981421474253ITERATION : 53, loss : 0.024462981261573653ITERATION : 54, loss : 0.024462981162260016ITERATION : 55, loss : 0.02446298111995689ITERATION : 56, loss : 0.024462981009336ITERATION : 57, loss : 0.024462981035491202ITERATION : 58, loss : 0.024462981034115754ITERATION : 59, loss : 0.024462981034115754ITERATION : 60, loss : 0.024462981034115754ITERATION : 61, loss : 0.024462981034115754ITERATION : 62, loss : 0.024462981034115754ITERATION : 63, loss : 0.024462981034115754ITERATION : 64, loss : 0.024462981034115754ITERATION : 65, loss : 0.024462981034115754ITERATION : 66, loss : 0.024462981034115754ITERATION : 67, loss : 0.024462981034115754ITERATION : 68, loss : 0.024462981034115754ITERATION : 69, loss : 0.024462981034115754ITERATION : 70, loss : 0.024462981034115754ITERATION : 71, loss : 0.024462981034115754ITERATION : 72, loss : 0.024462981034115754ITERATION : 73, loss : 0.024462981034115754ITERATION : 74, loss : 0.024462981034115754ITERATION : 75, loss : 0.024462981034115754ITERATION : 76, loss : 0.024462981034115754ITERATION : 77, loss : 0.024462981034115754ITERATION : 78, loss : 0.024462981034115754ITERATION : 79, loss : 0.024462981034115754ITERATION : 80, loss : 0.024462981034115754ITERATION : 81, loss : 0.024462981034115754ITERATION : 82, loss : 0.024462981034115754ITERATION : 83, loss : 0.024462981034115754ITERATION : 84, loss : 0.024462981034115754ITERATION : 85, loss : 0.024462981034115754ITERATION : 86, loss : 0.024462981034115754ITERATION : 87, loss : 0.024462981034115754ITERATION : 88, loss : 0.024462981034115754ITERATION : 89, loss : 0.024462981034115754ITERATION : 90, loss : 0.024462981034115754ITERATION : 91, loss : 0.024462981034115754ITERATION : 92, loss : 0.024462981034115754ITERATION : 93, loss : 0.024462981034115754ITERATION : 94, loss : 0.024462981034115754ITERATION : 95, loss : 0.024462981034115754ITERATION : 96, loss : 0.024462981034115754ITERATION : 97, loss : 0.024462981034115754ITERATION : 98, loss : 0.024462981034115754ITERATION : 99, loss : 0.024462981034115754ITERATION : 100, loss : 0.024462981034115754
ITERATION : 1, loss : 0.03754524763097227ITERATION : 2, loss : 0.037535747352843214ITERATION : 3, loss : 0.035583930826013244ITERATION : 4, loss : 0.03452261424688732ITERATION : 5, loss : 0.033890862917475686ITERATION : 6, loss : 0.03350009190213227ITERATION : 7, loss : 0.03325096331044103ITERATION : 8, loss : 0.03308794667883093ITERATION : 9, loss : 0.032978999643629554ITERATION : 10, loss : 0.03290501357844148ITERATION : 11, loss : 0.0328541914919793ITERATION : 12, loss : 0.032819008419453136ITERATION : 13, loss : 0.03279452864814231ITERATION : 14, loss : 0.032777442744675374ITERATION : 15, loss : 0.03276549616852594ITERATION : 16, loss : 0.03275713509212537ITERATION : 17, loss : 0.03275128117272868ITERATION : 18, loss : 0.032747182365066255ITERATION : 19, loss : 0.03274431287522311ITERATION : 20, loss : 0.032742304517968425ITERATION : 21, loss : 0.032740899157395774ITERATION : 22, loss : 0.032739916174571684ITERATION : 23, loss : 0.032739228793681144ITERATION : 24, loss : 0.032738748196751995ITERATION : 25, loss : 0.032738412355825676ITERATION : 26, loss : 0.03273817771146847ITERATION : 27, loss : 0.03273801375252815ITERATION : 28, loss : 0.03273789926105445ITERATION : 29, loss : 0.03273781932359208ITERATION : 30, loss : 0.0327377635116388ITERATION : 31, loss : 0.03273772454007388ITERATION : 32, loss : 0.03273769735559416ITERATION : 33, loss : 0.032737678376028874ITERATION : 34, loss : 0.032737665138309405ITERATION : 35, loss : 0.03273765591184633ITERATION : 36, loss : 0.032737649484628636ITERATION : 37, loss : 0.03273764498416833ITERATION : 38, loss : 0.032737641838517ITERATION : 39, loss : 0.03273763964807765ITERATION : 40, loss : 0.03273763811526808ITERATION : 41, loss : 0.03273763706140307ITERATION : 42, loss : 0.03273763632685631ITERATION : 43, loss : 0.03273763583387691ITERATION : 44, loss : 0.032737635473619076ITERATION : 45, loss : 0.03273763521713071ITERATION : 46, loss : 0.032737635050758405ITERATION : 47, loss : 0.0327376349469406ITERATION : 48, loss : 0.032737634879166415ITERATION : 49, loss : 0.03273763482001867ITERATION : 50, loss : 0.0327376347855998ITERATION : 51, loss : 0.03273763473608107ITERATION : 52, loss : 0.03273763473173109ITERATION : 53, loss : 0.032737634715048654ITERATION : 54, loss : 0.03273763471509971ITERATION : 55, loss : 0.03273763471509971ITERATION : 56, loss : 0.03273763471509971ITERATION : 57, loss : 0.03273763471509971ITERATION : 58, loss : 0.03273763471509971ITERATION : 59, loss : 0.03273763471509971ITERATION : 60, loss : 0.03273763471509971ITERATION : 61, loss : 0.03273763471509971ITERATION : 62, loss : 0.03273763471509971ITERATION : 63, loss : 0.03273763471509971ITERATION : 64, loss : 0.03273763471509971ITERATION : 65, loss : 0.03273763471509971ITERATION : 66, loss : 0.03273763471509971ITERATION : 67, loss : 0.03273763471509971ITERATION : 68, loss : 0.03273763471509971ITERATION : 69, loss : 0.03273763471509971ITERATION : 70, loss : 0.03273763471509971ITERATION : 71, loss : 0.03273763471509971ITERATION : 72, loss : 0.03273763471509971ITERATION : 73, loss : 0.03273763471509971ITERATION : 74, loss : 0.03273763471509971ITERATION : 75, loss : 0.03273763471509971ITERATION : 76, loss : 0.03273763471509971ITERATION : 77, loss : 0.03273763471509971ITERATION : 78, loss : 0.03273763471509971ITERATION : 79, loss : 0.03273763471509971ITERATION : 80, loss : 0.03273763471509971ITERATION : 81, loss : 0.03273763471509971ITERATION : 82, loss : 0.03273763471509971ITERATION : 83, loss : 0.03273763471509971ITERATION : 84, loss : 0.03273763471509971ITERATION : 85, loss : 0.03273763471509971ITERATION : 86, loss : 0.03273763471509971ITERATION : 87, loss : 0.03273763471509971ITERATION : 88, loss : 0.03273763471509971ITERATION : 89, loss : 0.03273763471509971ITERATION : 90, loss : 0.03273763471509971ITERATION : 91, loss : 0.03273763471509971ITERATION : 92, loss : 0.03273763471509971ITERATION : 93, loss : 0.03273763471509971ITERATION : 94, loss : 0.03273763471509971ITERATION : 95, loss : 0.03273763471509971ITERATION : 96, loss : 0.03273763471509971ITERATION : 97, loss : 0.03273763471509971ITERATION : 98, loss : 0.03273763471509971ITERATION : 99, loss : 0.03273763471509971ITERATION : 100, loss : 0.03273763471509971
ITERATION : 1, loss : 0.08334014532377937ITERATION : 2, loss : 0.07799031828070746ITERATION : 3, loss : 0.07759841748658608ITERATION : 4, loss : 0.07795170711611267ITERATION : 5, loss : 0.0783861960694673ITERATION : 6, loss : 0.0787607045570995ITERATION : 7, loss : 0.07905439786966602ITERATION : 8, loss : 0.07927592595431263ITERATION : 9, loss : 0.0794397478147425ITERATION : 10, loss : 0.0795595288177805ITERATION : 11, loss : 0.07964649935468264ITERATION : 12, loss : 0.07970936411267897ITERATION : 13, loss : 0.07975466956305872ITERATION : 14, loss : 0.07978725509778663ITERATION : 15, loss : 0.07981065966851816ITERATION : 16, loss : 0.07982745419573009ITERATION : 17, loss : 0.07983949767005984ITERATION : 18, loss : 0.07984813019919303ITERATION : 19, loss : 0.07985431584972738ITERATION : 20, loss : 0.07985874720400526ITERATION : 21, loss : 0.07986192135880846ITERATION : 22, loss : 0.0798641947591187ITERATION : 23, loss : 0.07986582289548617ITERATION : 24, loss : 0.07986698887936604ITERATION : 25, loss : 0.07986782385738134ITERATION : 26, loss : 0.07986842180008956ITERATION : 27, loss : 0.07986885001936811ITERATION : 28, loss : 0.07986915667209418ITERATION : 29, loss : 0.07986937627921907ITERATION : 30, loss : 0.07986953354502566ITERATION : 31, loss : 0.07986964613222274ITERATION : 32, loss : 0.07986972681959459ITERATION : 33, loss : 0.0798697846118168ITERATION : 34, loss : 0.0798698260099873ITERATION : 35, loss : 0.07986985562702319ITERATION : 36, loss : 0.07986987684310361ITERATION : 37, loss : 0.07986989196013691ITERATION : 38, loss : 0.0798699027982122ITERATION : 39, loss : 0.07986991059994711ITERATION : 40, loss : 0.07986991619559851ITERATION : 41, loss : 0.07986992024141368ITERATION : 42, loss : 0.07986992312598831ITERATION : 43, loss : 0.07986992522148655ITERATION : 44, loss : 0.0798699267000908ITERATION : 45, loss : 0.07986992777977578ITERATION : 46, loss : 0.07986992854493057ITERATION : 47, loss : 0.07986992910920555ITERATION : 48, loss : 0.07986992950171927ITERATION : 49, loss : 0.07986992979826216ITERATION : 50, loss : 0.07986993000068304ITERATION : 51, loss : 0.07986993014239077ITERATION : 52, loss : 0.07986993023919528ITERATION : 53, loss : 0.07986993031591001ITERATION : 54, loss : 0.07986993033524338ITERATION : 55, loss : 0.07986993035740497ITERATION : 56, loss : 0.07986993035625048ITERATION : 57, loss : 0.07986993035625048ITERATION : 58, loss : 0.07986993035625048ITERATION : 59, loss : 0.07986993035625048ITERATION : 60, loss : 0.07986993035625048ITERATION : 61, loss : 0.07986993035625048ITERATION : 62, loss : 0.07986993035625048ITERATION : 63, loss : 0.07986993035625048ITERATION : 64, loss : 0.07986993035625048ITERATION : 65, loss : 0.07986993035625048ITERATION : 66, loss : 0.07986993035625048ITERATION : 67, loss : 0.07986993035625048ITERATION : 68, loss : 0.07986993035625048ITERATION : 69, loss : 0.07986993035625048ITERATION : 70, loss : 0.07986993035625048ITERATION : 71, loss : 0.07986993035625048ITERATION : 72, loss : 0.07986993035625048ITERATION : 73, loss : 0.07986993035625048ITERATION : 74, loss : 0.07986993035625048ITERATION : 75, loss : 0.07986993035625048ITERATION : 76, loss : 0.07986993035625048ITERATION : 77, loss : 0.07986993035625048ITERATION : 78, loss : 0.07986993035625048ITERATION : 79, loss : 0.07986993035625048ITERATION : 80, loss : 0.07986993035625048ITERATION : 81, loss : 0.07986993035625048ITERATION : 82, loss : 0.07986993035625048ITERATION : 83, loss : 0.07986993035625048ITERATION : 84, loss : 0.07986993035625048ITERATION : 85, loss : 0.07986993035625048ITERATION : 86, loss : 0.07986993035625048ITERATION : 87, loss : 0.07986993035625048ITERATION : 88, loss : 0.07986993035625048ITERATION : 89, loss : 0.07986993035625048ITERATION : 90, loss : 0.07986993035625048ITERATION : 91, loss : 0.07986993035625048ITERATION : 92, loss : 0.07986993035625048ITERATION : 93, loss : 0.07986993035625048ITERATION : 94, loss : 0.07986993035625048ITERATION : 95, loss : 0.07986993035625048ITERATION : 96, loss : 0.07986993035625048ITERATION : 97, loss : 0.07986993035625048ITERATION : 98, loss : 0.07986993035625048ITERATION : 99, loss : 0.07986993035625048ITERATION : 100, loss : 0.07986993035625048
ITERATION : 1, loss : 0.026820254587231352ITERATION : 2, loss : 0.030846391985978907ITERATION : 3, loss : 0.03395166436695889ITERATION : 4, loss : 0.03489713568007059ITERATION : 5, loss : 0.03569219123461696ITERATION : 6, loss : 0.036311183372038466ITERATION : 7, loss : 0.036775749883864894ITERATION : 8, loss : 0.0371173415051878ITERATION : 9, loss : 0.037365419209966055ITERATION : 10, loss : 0.03754417468816285ITERATION : 11, loss : 0.03767231872353395ITERATION : 12, loss : 0.037763864508492805ITERATION : 13, loss : 0.037829110785245895ITERATION : 14, loss : 0.03787553707142009ITERATION : 15, loss : 0.037908534414429276ITERATION : 16, loss : 0.03793196844911011ITERATION : 17, loss : 0.037948601354510746ITERATION : 18, loss : 0.03796040229006822ITERATION : 19, loss : 0.03796877260497267ITERATION : 20, loss : 0.03797470834514185ITERATION : 21, loss : 0.03797891704757989ITERATION : 22, loss : 0.037981900878541835ITERATION : 23, loss : 0.03798401617693665ITERATION : 24, loss : 0.03798551567741039ITERATION : 25, loss : 0.03798657861227194ITERATION : 26, loss : 0.03798733202859491ITERATION : 27, loss : 0.037987866102736446ITERATION : 28, loss : 0.03798824463759515ITERATION : 29, loss : 0.037988512935119886ITERATION : 30, loss : 0.03798870312577718ITERATION : 31, loss : 0.03798883795338676ITERATION : 32, loss : 0.037988933518871476ITERATION : 33, loss : 0.037989001255320155ITERATION : 34, loss : 0.037989049275346964ITERATION : 35, loss : 0.0379890833094861ITERATION : 36, loss : 0.03798910743750363ITERATION : 37, loss : 0.037989124532385905ITERATION : 38, loss : 0.03798913664072992ITERATION : 39, loss : 0.037989145232443886ITERATION : 40, loss : 0.03798915131464831ITERATION : 41, loss : 0.037989155608354315ITERATION : 42, loss : 0.037989158677197044ITERATION : 43, loss : 0.037989160824414565ITERATION : 44, loss : 0.037989162370301935ITERATION : 45, loss : 0.03798916340068995ITERATION : 46, loss : 0.03798916419311215ITERATION : 47, loss : 0.03798916474212049ITERATION : 48, loss : 0.0379891651522839ITERATION : 49, loss : 0.037989165453815324ITERATION : 50, loss : 0.037989165647164154ITERATION : 51, loss : 0.03798916572595135ITERATION : 52, loss : 0.037989165831315286ITERATION : 53, loss : 0.037989165888471316ITERATION : 54, loss : 0.03798916590928067ITERATION : 55, loss : 0.037989165910507036ITERATION : 56, loss : 0.037989165910507036ITERATION : 57, loss : 0.037989165910507036ITERATION : 58, loss : 0.037989165910507036ITERATION : 59, loss : 0.037989165910507036ITERATION : 60, loss : 0.037989165910507036ITERATION : 61, loss : 0.037989165910507036ITERATION : 62, loss : 0.037989165910507036ITERATION : 63, loss : 0.037989165910507036ITERATION : 64, loss : 0.037989165910507036ITERATION : 65, loss : 0.037989165910507036ITERATION : 66, loss : 0.037989165910507036ITERATION : 67, loss : 0.037989165910507036ITERATION : 68, loss : 0.037989165910507036ITERATION : 69, loss : 0.037989165910507036ITERATION : 70, loss : 0.037989165910507036ITERATION : 71, loss : 0.037989165910507036ITERATION : 72, loss : 0.037989165910507036ITERATION : 73, loss : 0.037989165910507036ITERATION : 74, loss : 0.037989165910507036ITERATION : 75, loss : 0.037989165910507036ITERATION : 76, loss : 0.037989165910507036ITERATION : 77, loss : 0.037989165910507036ITERATION : 78, loss : 0.037989165910507036ITERATION : 79, loss : 0.037989165910507036ITERATION : 80, loss : 0.037989165910507036ITERATION : 81, loss : 0.037989165910507036ITERATION : 82, loss : 0.037989165910507036ITERATION : 83, loss : 0.037989165910507036ITERATION : 84, loss : 0.037989165910507036ITERATION : 85, loss : 0.037989165910507036ITERATION : 86, loss : 0.037989165910507036ITERATION : 87, loss : 0.037989165910507036ITERATION : 88, loss : 0.037989165910507036ITERATION : 89, loss : 0.037989165910507036ITERATION : 90, loss : 0.037989165910507036ITERATION : 91, loss : 0.037989165910507036ITERATION : 92, loss : 0.037989165910507036ITERATION : 93, loss : 0.037989165910507036ITERATION : 94, loss : 0.037989165910507036ITERATION : 95, loss : 0.037989165910507036ITERATION : 96, loss : 0.037989165910507036ITERATION : 97, loss : 0.037989165910507036ITERATION : 98, loss : 0.037989165910507036ITERATION : 99, loss : 0.037989165910507036ITERATION : 100, loss : 0.037989165910507036
ITERATION : 1, loss : 0.04750773912830919ITERATION : 2, loss : 0.05508483592241533ITERATION : 3, loss : 0.06123703097957715ITERATION : 4, loss : 0.0657329577404227ITERATION : 5, loss : 0.06895919694358717ITERATION : 6, loss : 0.07124797821489867ITERATION : 7, loss : 0.07286003026438931ITERATION : 8, loss : 0.07399060654003464ITERATION : 9, loss : 0.07478161759163661ITERATION : 10, loss : 0.07533436169362764ITERATION : 11, loss : 0.07572039275698174ITERATION : 12, loss : 0.07598995224901246ITERATION : 13, loss : 0.07617819954185107ITERATION : 14, loss : 0.07630969471799245ITERATION : 15, loss : 0.07640157872974207ITERATION : 16, loss : 0.07646580968919049ITERATION : 17, loss : 0.07651072978228542ITERATION : 18, loss : 0.07654215946389331ITERATION : 19, loss : 0.07656416082201642ITERATION : 20, loss : 0.07657956992301229ITERATION : 21, loss : 0.07659036732631071ITERATION : 22, loss : 0.07659793710210121ITERATION : 23, loss : 0.07660324679691229ITERATION : 24, loss : 0.07660697308900213ITERATION : 25, loss : 0.07660958950382125ITERATION : 26, loss : 0.07661142756000183ITERATION : 27, loss : 0.0766127193886286ITERATION : 28, loss : 0.07661362781740536ITERATION : 29, loss : 0.07661426699986085ITERATION : 30, loss : 0.07661471685264995ITERATION : 31, loss : 0.07661503356143973ITERATION : 32, loss : 0.07661525672006478ITERATION : 33, loss : 0.0766154140422224ITERATION : 34, loss : 0.07661552501793742ITERATION : 35, loss : 0.07661560331301756ITERATION : 36, loss : 0.07661565850599482ITERATION : 37, loss : 0.07661569751060886ITERATION : 38, loss : 0.0766157250637621ITERATION : 39, loss : 0.07661574453354887ITERATION : 40, loss : 0.07661575830441368ITERATION : 41, loss : 0.07661576810262935ITERATION : 42, loss : 0.0766157750578846ITERATION : 43, loss : 0.07661577990428656ITERATION : 44, loss : 0.07661578332636661ITERATION : 45, loss : 0.07661578571586668ITERATION : 46, loss : 0.07661578744907091ITERATION : 47, loss : 0.07661578873366957ITERATION : 48, loss : 0.07661578952802306ITERATION : 49, loss : 0.07661579010973707ITERATION : 50, loss : 0.0766157905381901ITERATION : 51, loss : 0.07661579081579088ITERATION : 52, loss : 0.07661579101853874ITERATION : 53, loss : 0.07661579117811819ITERATION : 54, loss : 0.07661579124866363ITERATION : 55, loss : 0.07661579133033417ITERATION : 56, loss : 0.07661579133299329ITERATION : 57, loss : 0.07661579133299329ITERATION : 58, loss : 0.07661579133299329ITERATION : 59, loss : 0.07661579133299329ITERATION : 60, loss : 0.07661579133299329ITERATION : 61, loss : 0.07661579133299329ITERATION : 62, loss : 0.07661579133299329ITERATION : 63, loss : 0.07661579133299329ITERATION : 64, loss : 0.07661579133299329ITERATION : 65, loss : 0.07661579133299329ITERATION : 66, loss : 0.07661579133299329ITERATION : 67, loss : 0.07661579133299329ITERATION : 68, loss : 0.07661579133299329ITERATION : 69, loss : 0.07661579133299329ITERATION : 70, loss : 0.07661579133299329ITERATION : 71, loss : 0.07661579133299329ITERATION : 72, loss : 0.07661579133299329ITERATION : 73, loss : 0.07661579133299329ITERATION : 74, loss : 0.07661579133299329ITERATION : 75, loss : 0.07661579133299329ITERATION : 76, loss : 0.07661579133299329ITERATION : 77, loss : 0.07661579133299329ITERATION : 78, loss : 0.07661579133299329ITERATION : 79, loss : 0.07661579133299329ITERATION : 80, loss : 0.07661579133299329ITERATION : 81, loss : 0.07661579133299329ITERATION : 82, loss : 0.07661579133299329ITERATION : 83, loss : 0.07661579133299329ITERATION : 84, loss : 0.07661579133299329ITERATION : 85, loss : 0.07661579133299329ITERATION : 86, loss : 0.07661579133299329ITERATION : 87, loss : 0.07661579133299329ITERATION : 88, loss : 0.07661579133299329ITERATION : 89, loss : 0.07661579133299329ITERATION : 90, loss : 0.07661579133299329ITERATION : 91, loss : 0.07661579133299329ITERATION : 92, loss : 0.07661579133299329ITERATION : 93, loss : 0.07661579133299329ITERATION : 94, loss : 0.07661579133299329ITERATION : 95, loss : 0.07661579133299329ITERATION : 96, loss : 0.07661579133299329ITERATION : 97, loss : 0.07661579133299329ITERATION : 98, loss : 0.07661579133299329ITERATION : 99, loss : 0.07661579133299329ITERATION : 100, loss : 0.07661579133299329
gradient norm in None layer : 0.01317954044926683
gradient norm in None layer : 0.0006127571826909681
gradient norm in None layer : 0.0005411169377842664
gradient norm in None layer : 0.0094763120121845
gradient norm in None layer : 0.0004903224007028369
gradient norm in None layer : 0.0004156447176470314
gradient norm in None layer : 0.0037397633411850916
gradient norm in None layer : 0.00012702062615868438
gradient norm in None layer : 0.00010297744480738712
gradient norm in None layer : 0.0032240706711884155
gradient norm in None layer : 0.00014368827959819084
gradient norm in None layer : 0.00010030876795501303
gradient norm in None layer : 0.0013671289948352255
gradient norm in None layer : 3.992830206029321e-05
gradient norm in None layer : 2.2632681149921053e-05
gradient norm in None layer : 0.001271034987641527
gradient norm in None layer : 4.8559573075293844e-05
gradient norm in None layer : 2.918887239287653e-05
gradient norm in None layer : 0.0017819275242149343
gradient norm in None layer : 9.168012135546443e-06
gradient norm in None layer : 0.0030296952834806
gradient norm in None layer : 0.00015245952551026462
gradient norm in None layer : 0.00010782371469213136
gradient norm in None layer : 0.0034725733670679464
gradient norm in None layer : 0.000277035387610953
gradient norm in None layer : 0.0002803181456112417
gradient norm in None layer : 0.007731254206628696
gradient norm in None layer : 2.5462205645687776e-05
gradient norm in None layer : 0.008981181311604167
gradient norm in None layer : 0.0005529279622231211
gradient norm in None layer : 0.00046051671259712015
gradient norm in None layer : 0.009601014164140314
gradient norm in None layer : 0.0015209001835650372
gradient norm in None layer : 0.0019725851771866462
gradient norm in None layer : 0.0013732677676113545
gradient norm in None layer : 0.0003850613918633071
Total gradient norm: 0.02363225751955065
invariance loss : 6.868791482422756, avg_den : 0.32205963134765625, density loss : 0.23215179443359377, mse loss : 0.04277400600540219, solver time : 106.17982363700867 sec , total loss : 0.04987494928225854, running loss : 0.08159764969323197
Epoch 0/10 , batch 4/12500 
ITERATION : 1, loss : 0.02203810609401922ITERATION : 2, loss : 0.025877131670875846ITERATION : 3, loss : 0.027585699932906ITERATION : 4, loss : 0.0272412301385063ITERATION : 5, loss : 0.027021323352721345ITERATION : 6, loss : 0.026887088945216936ITERATION : 7, loss : 0.02673129599878091ITERATION : 8, loss : 0.026633750800901343ITERATION : 9, loss : 0.026571267442576738ITERATION : 10, loss : 0.026530351597615895ITERATION : 11, loss : 0.02650302426675ITERATION : 12, loss : 0.026484463857975836ITERATION : 13, loss : 0.026471684277851022ITERATION : 14, loss : 0.026462789500929354ITERATION : 15, loss : 0.026456546892517457ITERATION : 16, loss : 0.02645213796237191ITERATION : 17, loss : 0.026449009455386165ITERATION : 18, loss : 0.02644678181173517ITERATION : 19, loss : 0.02644519158429502ITERATION : 20, loss : 0.026444054292772592ITERATION : 21, loss : 0.026443239840346928ITERATION : 22, loss : 0.02644265602579091ITERATION : 23, loss : 0.026442237251194784ITERATION : 24, loss : 0.026441936701742916ITERATION : 25, loss : 0.026441720936871384ITERATION : 26, loss : 0.02644156600117891ITERATION : 27, loss : 0.02644145471884199ITERATION : 28, loss : 0.0264413747886913ITERATION : 29, loss : 0.02644131737699479ITERATION : 30, loss : 0.02644127613000528ITERATION : 31, loss : 0.02644124649598232ITERATION : 32, loss : 0.026441225205937866ITERATION : 33, loss : 0.026441209917594053ITERATION : 34, loss : 0.026441198925133212ITERATION : 35, loss : 0.02644119102918908ITERATION : 36, loss : 0.026441185365250277ITERATION : 37, loss : 0.026441181295838872ITERATION : 38, loss : 0.026441178366869298ITERATION : 39, loss : 0.026441176269104194ITERATION : 40, loss : 0.026441174755068977ITERATION : 41, loss : 0.026441173667996448ITERATION : 42, loss : 0.026441172885046584ITERATION : 43, loss : 0.026441172329576292ITERATION : 44, loss : 0.02644117191692256ITERATION : 45, loss : 0.026441171637091688ITERATION : 46, loss : 0.026441171421544788ITERATION : 47, loss : 0.026441171284825937ITERATION : 48, loss : 0.026441171168638065ITERATION : 49, loss : 0.02644117109671293ITERATION : 50, loss : 0.026441171055397072ITERATION : 51, loss : 0.02644117100943852ITERATION : 52, loss : 0.026441170982878716ITERATION : 53, loss : 0.026441170961092095ITERATION : 54, loss : 0.026441170949445855ITERATION : 55, loss : 0.026441170928285154ITERATION : 56, loss : 0.0264411709310579ITERATION : 57, loss : 0.026441170930955514ITERATION : 58, loss : 0.026441170930955514ITERATION : 59, loss : 0.026441170930955514ITERATION : 60, loss : 0.026441170930955514ITERATION : 61, loss : 0.026441170930955514ITERATION : 62, loss : 0.026441170930955514ITERATION : 63, loss : 0.026441170930955514ITERATION : 64, loss : 0.026441170930955514ITERATION : 65, loss : 0.026441170930955514ITERATION : 66, loss : 0.026441170930955514ITERATION : 67, loss : 0.026441170930955514ITERATION : 68, loss : 0.026441170930955514ITERATION : 69, loss : 0.026441170930955514ITERATION : 70, loss : 0.026441170930955514ITERATION : 71, loss : 0.026441170930955514ITERATION : 72, loss : 0.026441170930955514ITERATION : 73, loss : 0.026441170930955514ITERATION : 74, loss : 0.026441170930955514ITERATION : 75, loss : 0.026441170930955514ITERATION : 76, loss : 0.026441170930955514ITERATION : 77, loss : 0.026441170930955514ITERATION : 78, loss : 0.026441170930955514ITERATION : 79, loss : 0.026441170930955514ITERATION : 80, loss : 0.026441170930955514ITERATION : 81, loss : 0.026441170930955514ITERATION : 82, loss : 0.026441170930955514ITERATION : 83, loss : 0.026441170930955514ITERATION : 84, loss : 0.026441170930955514ITERATION : 85, loss : 0.026441170930955514ITERATION : 86, loss : 0.026441170930955514ITERATION : 87, loss : 0.026441170930955514ITERATION : 88, loss : 0.026441170930955514ITERATION : 89, loss : 0.026441170930955514ITERATION : 90, loss : 0.026441170930955514ITERATION : 91, loss : 0.026441170930955514ITERATION : 92, loss : 0.026441170930955514ITERATION : 93, loss : 0.026441170930955514ITERATION : 94, loss : 0.026441170930955514ITERATION : 95, loss : 0.026441170930955514ITERATION : 96, loss : 0.026441170930955514ITERATION : 97, loss : 0.026441170930955514ITERATION : 98, loss : 0.026441170930955514ITERATION : 99, loss : 0.026441170930955514ITERATION : 100, loss : 0.026441170930955514
ITERATION : 1, loss : 0.025571917694176455ITERATION : 2, loss : 0.025467731763382396ITERATION : 3, loss : 0.028581001238908183ITERATION : 4, loss : 0.031495590618339966ITERATION : 5, loss : 0.03383014412899258ITERATION : 6, loss : 0.03561986282680189ITERATION : 7, loss : 0.03696688340694776ITERATION : 8, loss : 0.03797114077489836ITERATION : 9, loss : 0.03871575799208591ITERATION : 10, loss : 0.03926599128084111ITERATION : 11, loss : 0.03967169777696265ITERATION : 12, loss : 0.0399704101393602ITERATION : 13, loss : 0.04019013572162793ITERATION : 14, loss : 0.040351658748368484ITERATION : 15, loss : 0.040470347703944295ITERATION : 16, loss : 0.04055753919153499ITERATION : 17, loss : 0.04062158232931575ITERATION : 18, loss : 0.040668619093388675ITERATION : 19, loss : 0.040703164669039345ITERATION : 20, loss : 0.0407285364972018ITERATION : 21, loss : 0.04074717151847072ITERATION : 22, loss : 0.040760859287556406ITERATION : 23, loss : 0.04077091387736683ITERATION : 24, loss : 0.040778300234183246ITERATION : 25, loss : 0.04078372687538616ITERATION : 26, loss : 0.04078771404202176ITERATION : 27, loss : 0.040790643856050496ITERATION : 28, loss : 0.04079279691114608ITERATION : 29, loss : 0.04079437922096805ITERATION : 30, loss : 0.04079554218111972ITERATION : 31, loss : 0.04079639702127719ITERATION : 32, loss : 0.04079702543536538ITERATION : 33, loss : 0.04079748743053479ITERATION : 34, loss : 0.04079782710698761ITERATION : 35, loss : 0.04079807685974228ITERATION : 36, loss : 0.0407982605123142ITERATION : 37, loss : 0.040798395566113164ITERATION : 38, loss : 0.04079849488690047ITERATION : 39, loss : 0.040798567937543086ITERATION : 40, loss : 0.04079862165903359ITERATION : 41, loss : 0.040798661182578376ITERATION : 42, loss : 0.040798690245383136ITERATION : 43, loss : 0.04079871163357147ITERATION : 44, loss : 0.04079872736379562ITERATION : 45, loss : 0.04079873893950071ITERATION : 46, loss : 0.04079874747958203ITERATION : 47, loss : 0.04079875372278236ITERATION : 48, loss : 0.040798758321025155ITERATION : 49, loss : 0.04079876171917544ITERATION : 50, loss : 0.04079876419155121ITERATION : 51, loss : 0.04079876603125431ITERATION : 52, loss : 0.04079876735303183ITERATION : 53, loss : 0.0407987683467002ITERATION : 54, loss : 0.04079876902397944ITERATION : 55, loss : 0.04079876956045847ITERATION : 56, loss : 0.04079876995366976ITERATION : 57, loss : 0.04079877022133586ITERATION : 58, loss : 0.04079877046106888ITERATION : 59, loss : 0.04079877063986073ITERATION : 60, loss : 0.040798770756568715ITERATION : 61, loss : 0.040798770851762624ITERATION : 62, loss : 0.04079877088023721ITERATION : 63, loss : 0.04079877092388021ITERATION : 64, loss : 0.04079877092388021ITERATION : 65, loss : 0.04079877092388021ITERATION : 66, loss : 0.04079877092388021ITERATION : 67, loss : 0.04079877092388021ITERATION : 68, loss : 0.04079877092388021ITERATION : 69, loss : 0.04079877092388021ITERATION : 70, loss : 0.04079877092388021ITERATION : 71, loss : 0.04079877092388021ITERATION : 72, loss : 0.04079877092388021ITERATION : 73, loss : 0.04079877092388021ITERATION : 74, loss : 0.04079877092388021ITERATION : 75, loss : 0.04079877092388021ITERATION : 76, loss : 0.04079877092388021ITERATION : 77, loss : 0.04079877092388021ITERATION : 78, loss : 0.04079877092388021ITERATION : 79, loss : 0.04079877092388021ITERATION : 80, loss : 0.04079877092388021ITERATION : 81, loss : 0.04079877092388021ITERATION : 82, loss : 0.04079877092388021ITERATION : 83, loss : 0.04079877092388021ITERATION : 84, loss : 0.04079877092388021ITERATION : 85, loss : 0.04079877092388021ITERATION : 86, loss : 0.04079877092388021ITERATION : 87, loss : 0.04079877092388021ITERATION : 88, loss : 0.04079877092388021ITERATION : 89, loss : 0.04079877092388021ITERATION : 90, loss : 0.04079877092388021ITERATION : 91, loss : 0.04079877092388021ITERATION : 92, loss : 0.04079877092388021ITERATION : 93, loss : 0.04079877092388021ITERATION : 94, loss : 0.04079877092388021ITERATION : 95, loss : 0.04079877092388021ITERATION : 96, loss : 0.04079877092388021ITERATION : 97, loss : 0.04079877092388021ITERATION : 98, loss : 0.04079877092388021ITERATION : 99, loss : 0.04079877092388021ITERATION : 100, loss : 0.04079877092388021
ITERATION : 1, loss : 0.027803458860178907ITERATION : 2, loss : 0.028180034006125725ITERATION : 3, loss : 0.02801663009369837ITERATION : 4, loss : 0.02794647969934524ITERATION : 5, loss : 0.027870471499968266ITERATION : 6, loss : 0.02779058147129752ITERATION : 7, loss : 0.02771618699453278ITERATION : 8, loss : 0.02765212299315943ITERATION : 9, loss : 0.027599470986677005ITERATION : 10, loss : 0.027557415661306622ITERATION : 11, loss : 0.02752443183945743ITERATION : 12, loss : 0.027498877472454237ITERATION : 13, loss : 0.02747924862189771ITERATION : 14, loss : 0.02746426546776355ITERATION : 15, loss : 0.02745288220931473ITERATION : 16, loss : 0.027444265218065297ITERATION : 17, loss : 0.02743776090084118ITERATION : 18, loss : 0.027432862386125592ITERATION : 19, loss : 0.027429180011268872ITERATION : 20, loss : 0.027426415946853588ITERATION : 21, loss : 0.02742434381281546ITERATION : 22, loss : 0.02742279194490737ITERATION : 23, loss : 0.0274216307428928ITERATION : 24, loss : 0.027420762594942926ITERATION : 25, loss : 0.027420113910173977ITERATION : 26, loss : 0.027419629498437945ITERATION : 27, loss : 0.0274192679501817ITERATION : 28, loss : 0.02741899820774515ITERATION : 29, loss : 0.027418797089275214ITERATION : 30, loss : 0.027418647159037822ITERATION : 31, loss : 0.02741853542355587ITERATION : 32, loss : 0.027418452165125343ITERATION : 33, loss : 0.027418390191336878ITERATION : 34, loss : 0.027418344036689586ITERATION : 35, loss : 0.02741830968135711ITERATION : 36, loss : 0.02741828412883865ITERATION : 37, loss : 0.027418265117133217ITERATION : 38, loss : 0.027418250977975837ITERATION : 39, loss : 0.027418240458191294ITERATION : 40, loss : 0.02741823263812458ITERATION : 41, loss : 0.02741822682624179ITERATION : 42, loss : 0.02741822251039028ITERATION : 43, loss : 0.027418219295645706ITERATION : 44, loss : 0.02741821692328987ITERATION : 45, loss : 0.027418215132557697ITERATION : 46, loss : 0.02741821382344816ITERATION : 47, loss : 0.02741821284756299ITERATION : 48, loss : 0.027418212081799693ITERATION : 49, loss : 0.02741821156809362ITERATION : 50, loss : 0.02741821119535133ITERATION : 51, loss : 0.027418210948665634ITERATION : 52, loss : 0.027418210683054833ITERATION : 53, loss : 0.027418210551451255ITERATION : 54, loss : 0.027418210404964095ITERATION : 55, loss : 0.027418210346724016ITERATION : 56, loss : 0.027418210270453502ITERATION : 57, loss : 0.0274182102531377ITERATION : 58, loss : 0.027418210214331995ITERATION : 59, loss : 0.027418210206575467ITERATION : 60, loss : 0.027418210203455303ITERATION : 61, loss : 0.027418210199009054ITERATION : 62, loss : 0.027418210197551057ITERATION : 63, loss : 0.027418210197551057ITERATION : 64, loss : 0.027418210197551057ITERATION : 65, loss : 0.027418210197551057ITERATION : 66, loss : 0.027418210197551057ITERATION : 67, loss : 0.027418210197551057ITERATION : 68, loss : 0.027418210197551057ITERATION : 69, loss : 0.027418210197551057ITERATION : 70, loss : 0.027418210197551057ITERATION : 71, loss : 0.027418210197551057ITERATION : 72, loss : 0.027418210197551057ITERATION : 73, loss : 0.027418210197551057ITERATION : 74, loss : 0.027418210197551057ITERATION : 75, loss : 0.027418210197551057ITERATION : 76, loss : 0.027418210197551057ITERATION : 77, loss : 0.027418210197551057ITERATION : 78, loss : 0.027418210197551057ITERATION : 79, loss : 0.027418210197551057ITERATION : 80, loss : 0.027418210197551057ITERATION : 81, loss : 0.027418210197551057ITERATION : 82, loss : 0.027418210197551057ITERATION : 83, loss : 0.027418210197551057ITERATION : 84, loss : 0.027418210197551057ITERATION : 85, loss : 0.027418210197551057ITERATION : 86, loss : 0.027418210197551057ITERATION : 87, loss : 0.027418210197551057ITERATION : 88, loss : 0.027418210197551057ITERATION : 89, loss : 0.027418210197551057ITERATION : 90, loss : 0.027418210197551057ITERATION : 91, loss : 0.027418210197551057ITERATION : 92, loss : 0.027418210197551057ITERATION : 93, loss : 0.027418210197551057ITERATION : 94, loss : 0.027418210197551057ITERATION : 95, loss : 0.027418210197551057ITERATION : 96, loss : 0.027418210197551057ITERATION : 97, loss : 0.027418210197551057ITERATION : 98, loss : 0.027418210197551057ITERATION : 99, loss : 0.027418210197551057ITERATION : 100, loss : 0.027418210197551057
ITERATION : 1, loss : 0.04942651908931766ITERATION : 2, loss : 0.04572713326650396ITERATION : 3, loss : 0.044796245337826585ITERATION : 4, loss : 0.04464378615468595ITERATION : 5, loss : 0.04471635519950435ITERATION : 6, loss : 0.04483698222433091ITERATION : 7, loss : 0.04495022537261729ITERATION : 8, loss : 0.045042028474835336ITERATION : 9, loss : 0.0451120109860773ITERATION : 10, loss : 0.0451637157853967ITERATION : 11, loss : 0.04520123967838606ITERATION : 12, loss : 0.04522817248910349ITERATION : 13, loss : 0.04524736425573249ITERATION : 14, loss : 0.04526097200651736ITERATION : 15, loss : 0.04527058687353246ITERATION : 16, loss : 0.04527736342970614ITERATION : 17, loss : 0.04528213063494517ITERATION : 18, loss : 0.04528547985624578ITERATION : 19, loss : 0.045287830315633174ITERATION : 20, loss : 0.04528947858778205ITERATION : 21, loss : 0.04529063376907085ITERATION : 22, loss : 0.04529144306107676ITERATION : 23, loss : 0.045292009877652195ITERATION : 24, loss : 0.045292406631130405ITERATION : 25, loss : 0.045292684399324934ITERATION : 26, loss : 0.045292878769175156ITERATION : 27, loss : 0.04529301483587578ITERATION : 28, loss : 0.04529311002922449ITERATION : 29, loss : 0.045293176658361994ITERATION : 30, loss : 0.04529322326954315ITERATION : 31, loss : 0.04529325588059855ITERATION : 32, loss : 0.04529327860702691ITERATION : 33, loss : 0.045293294545023045ITERATION : 34, loss : 0.04529330559776109ITERATION : 35, loss : 0.04529331340145408ITERATION : 36, loss : 0.04529331876568185ITERATION : 37, loss : 0.045293322594816415ITERATION : 38, loss : 0.045293325175640324ITERATION : 39, loss : 0.045293327056228204ITERATION : 40, loss : 0.04529332829116926ITERATION : 41, loss : 0.04529332924423063ITERATION : 42, loss : 0.04529332981969286ITERATION : 43, loss : 0.04529333031778929ITERATION : 44, loss : 0.04529333058617283ITERATION : 45, loss : 0.04529333082619907ITERATION : 46, loss : 0.04529333092018842ITERATION : 47, loss : 0.04529333104720235ITERATION : 48, loss : 0.04529333111489998ITERATION : 49, loss : 0.0452933311817233ITERATION : 50, loss : 0.0452933311879358ITERATION : 51, loss : 0.045293331213348526ITERATION : 52, loss : 0.045293331213348526ITERATION : 53, loss : 0.045293331213348526ITERATION : 54, loss : 0.045293331213348526ITERATION : 55, loss : 0.045293331213348526ITERATION : 56, loss : 0.045293331213348526ITERATION : 57, loss : 0.045293331213348526ITERATION : 58, loss : 0.045293331213348526ITERATION : 59, loss : 0.045293331213348526ITERATION : 60, loss : 0.045293331213348526ITERATION : 61, loss : 0.045293331213348526ITERATION : 62, loss : 0.045293331213348526ITERATION : 63, loss : 0.045293331213348526ITERATION : 64, loss : 0.045293331213348526ITERATION : 65, loss : 0.045293331213348526ITERATION : 66, loss : 0.045293331213348526ITERATION : 67, loss : 0.045293331213348526ITERATION : 68, loss : 0.045293331213348526ITERATION : 69, loss : 0.045293331213348526ITERATION : 70, loss : 0.045293331213348526ITERATION : 71, loss : 0.045293331213348526ITERATION : 72, loss : 0.045293331213348526ITERATION : 73, loss : 0.045293331213348526ITERATION : 74, loss : 0.045293331213348526ITERATION : 75, loss : 0.045293331213348526ITERATION : 76, loss : 0.045293331213348526ITERATION : 77, loss : 0.045293331213348526ITERATION : 78, loss : 0.045293331213348526ITERATION : 79, loss : 0.045293331213348526ITERATION : 80, loss : 0.045293331213348526ITERATION : 81, loss : 0.045293331213348526ITERATION : 82, loss : 0.045293331213348526ITERATION : 83, loss : 0.045293331213348526ITERATION : 84, loss : 0.045293331213348526ITERATION : 85, loss : 0.045293331213348526ITERATION : 86, loss : 0.045293331213348526ITERATION : 87, loss : 0.045293331213348526ITERATION : 88, loss : 0.045293331213348526ITERATION : 89, loss : 0.045293331213348526ITERATION : 90, loss : 0.045293331213348526ITERATION : 91, loss : 0.045293331213348526ITERATION : 92, loss : 0.045293331213348526ITERATION : 93, loss : 0.045293331213348526ITERATION : 94, loss : 0.045293331213348526ITERATION : 95, loss : 0.045293331213348526ITERATION : 96, loss : 0.045293331213348526ITERATION : 97, loss : 0.045293331213348526ITERATION : 98, loss : 0.045293331213348526ITERATION : 99, loss : 0.045293331213348526ITERATION : 100, loss : 0.045293331213348526
ITERATION : 1, loss : 0.072697500583641ITERATION : 2, loss : 0.07675452106308521ITERATION : 3, loss : 0.08012945661112358ITERATION : 4, loss : 0.0829545893234186ITERATION : 5, loss : 0.08536859127104676ITERATION : 6, loss : 0.0873682093832121ITERATION : 7, loss : 0.0889609597215721ITERATION : 8, loss : 0.09018824958771027ITERATION : 9, loss : 0.09111049297594415ITERATION : 10, loss : 0.09179099430105177ITERATION : 11, loss : 0.09228662679253391ITERATION : 12, loss : 0.09264429207653219ITERATION : 13, loss : 0.09290070839890376ITERATION : 14, loss : 0.09308368226695049ITERATION : 15, loss : 0.09321381513711237ITERATION : 16, loss : 0.09330614586333545ITERATION : 17, loss : 0.09337154244210868ITERATION : 18, loss : 0.09341780314626277ITERATION : 19, loss : 0.0934504974330371ITERATION : 20, loss : 0.09347358727906903ITERATION : 21, loss : 0.09348988550896173ITERATION : 22, loss : 0.09350138492271204ITERATION : 23, loss : 0.09350949593169894ITERATION : 24, loss : 0.09351521529967176ITERATION : 25, loss : 0.0935192473255322ITERATION : 26, loss : 0.09352208929160956ITERATION : 27, loss : 0.09352409207800845ITERATION : 28, loss : 0.09352550300792825ITERATION : 29, loss : 0.09352649716634191ITERATION : 30, loss : 0.09352719744429028ITERATION : 31, loss : 0.09352769067315726ITERATION : 32, loss : 0.09352803797257349ITERATION : 33, loss : 0.09352828255932912ITERATION : 34, loss : 0.09352845480166883ITERATION : 35, loss : 0.09352857606364806ITERATION : 36, loss : 0.09352866135610328ITERATION : 37, loss : 0.09352872142102782ITERATION : 38, loss : 0.09352876365868652ITERATION : 39, loss : 0.09352879339005442ITERATION : 40, loss : 0.09352881427785745ITERATION : 41, loss : 0.0935288289640411ITERATION : 42, loss : 0.09352883926633912ITERATION : 43, loss : 0.09352884652894125ITERATION : 44, loss : 0.09352885160949789ITERATION : 45, loss : 0.09352885521149949ITERATION : 46, loss : 0.09352885773161909ITERATION : 47, loss : 0.09352885952485046ITERATION : 48, loss : 0.09352886065576825ITERATION : 49, loss : 0.09352886144025312ITERATION : 50, loss : 0.0935288621171923ITERATION : 51, loss : 0.09352886260717767ITERATION : 52, loss : 0.09352886286428852ITERATION : 53, loss : 0.09352886298074516ITERATION : 54, loss : 0.09352886303514622ITERATION : 55, loss : 0.09352886320152276ITERATION : 56, loss : 0.09352886320859644ITERATION : 57, loss : 0.09352886320859644ITERATION : 58, loss : 0.09352886320859644ITERATION : 59, loss : 0.09352886320859644ITERATION : 60, loss : 0.09352886320859644ITERATION : 61, loss : 0.09352886320859644ITERATION : 62, loss : 0.09352886320859644ITERATION : 63, loss : 0.09352886320859644ITERATION : 64, loss : 0.09352886320859644ITERATION : 65, loss : 0.09352886320859644ITERATION : 66, loss : 0.09352886320859644ITERATION : 67, loss : 0.09352886320859644ITERATION : 68, loss : 0.09352886320859644ITERATION : 69, loss : 0.09352886320859644ITERATION : 70, loss : 0.09352886320859644ITERATION : 71, loss : 0.09352886320859644ITERATION : 72, loss : 0.09352886320859644ITERATION : 73, loss : 0.09352886320859644ITERATION : 74, loss : 0.09352886320859644ITERATION : 75, loss : 0.09352886320859644ITERATION : 76, loss : 0.09352886320859644ITERATION : 77, loss : 0.09352886320859644ITERATION : 78, loss : 0.09352886320859644ITERATION : 79, loss : 0.09352886320859644ITERATION : 80, loss : 0.09352886320859644ITERATION : 81, loss : 0.09352886320859644ITERATION : 82, loss : 0.09352886320859644ITERATION : 83, loss : 0.09352886320859644ITERATION : 84, loss : 0.09352886320859644ITERATION : 85, loss : 0.09352886320859644ITERATION : 86, loss : 0.09352886320859644ITERATION : 87, loss : 0.09352886320859644ITERATION : 88, loss : 0.09352886320859644ITERATION : 89, loss : 0.09352886320859644ITERATION : 90, loss : 0.09352886320859644ITERATION : 91, loss : 0.09352886320859644ITERATION : 92, loss : 0.09352886320859644ITERATION : 93, loss : 0.09352886320859644ITERATION : 94, loss : 0.09352886320859644ITERATION : 95, loss : 0.09352886320859644ITERATION : 96, loss : 0.09352886320859644ITERATION : 97, loss : 0.09352886320859644ITERATION : 98, loss : 0.09352886320859644ITERATION : 99, loss : 0.09352886320859644ITERATION : 100, loss : 0.09352886320859644
ITERATION : 1, loss : 0.08874265458617037ITERATION : 2, loss : 0.11705994785954116ITERATION : 3, loss : 0.13681310097121815ITERATION : 4, loss : 0.15029232159805722ITERATION : 5, loss : 0.15916575903988187ITERATION : 6, loss : 0.16558117044857ITERATION : 7, loss : 0.17029694327905873ITERATION : 8, loss : 0.173805141121201ITERATION : 9, loss : 0.17643789438194127ITERATION : 10, loss : 0.17842640180711314ITERATION : 11, loss : 0.17993547658411296ITERATION : 12, loss : 0.1810847762464901ITERATION : 13, loss : 0.1819623978745611ITERATION : 14, loss : 0.18263389832383117ITERATION : 15, loss : 0.18314846063018403ITERATION : 16, loss : 0.18354321218155512ITERATION : 17, loss : 0.18384631162880302ITERATION : 18, loss : 0.1840791918811289ITERATION : 19, loss : 0.1842582112203728ITERATION : 20, loss : 0.1843958798418151ITERATION : 21, loss : 0.1845017809964742ITERATION : 22, loss : 0.18458326404357006ITERATION : 23, loss : 0.18464597046416711ITERATION : 24, loss : 0.1846942339580777ITERATION : 25, loss : 0.1847313852624124ITERATION : 26, loss : 0.18475998543305447ITERATION : 27, loss : 0.1847820041951499ITERATION : 28, loss : 0.18479895692275214ITERATION : 29, loss : 0.18481200996054606ITERATION : 30, loss : 0.18482206068785617ITERATION : 31, loss : 0.18482979993486062ITERATION : 32, loss : 0.18483575946491662ITERATION : 33, loss : 0.1848403486102014ITERATION : 34, loss : 0.18484388255323647ITERATION : 35, loss : 0.1848466039601573ITERATION : 36, loss : 0.1848486996946964ITERATION : 37, loss : 0.18485031361125634ITERATION : 38, loss : 0.1848515564792891ITERATION : 39, loss : 0.18485251362984356ITERATION : 40, loss : 0.18485325072821146ITERATION : 41, loss : 0.18485381840527465ITERATION : 42, loss : 0.18485425560816185ITERATION : 43, loss : 0.18485459225100379ITERATION : 44, loss : 0.18485485158690892ITERATION : 45, loss : 0.18485505132417193ITERATION : 46, loss : 0.18485520515383583ITERATION : 47, loss : 0.18485532359782209ITERATION : 48, loss : 0.18485541481121842ITERATION : 49, loss : 0.18485548502057697ITERATION : 50, loss : 0.18485553906356833ITERATION : 51, loss : 0.18485558074502334ITERATION : 52, loss : 0.18485561274005022ITERATION : 53, loss : 0.18485563746198047ITERATION : 54, loss : 0.18485565643375115ITERATION : 55, loss : 0.18485567108305817ITERATION : 56, loss : 0.18485568234460156ITERATION : 57, loss : 0.1848556910320402ITERATION : 58, loss : 0.1848556977888314ITERATION : 59, loss : 0.18485570288357142ITERATION : 60, loss : 0.18485570681905056ITERATION : 61, loss : 0.18485570984028057ITERATION : 62, loss : 0.18485571214176566ITERATION : 63, loss : 0.18485571391791622ITERATION : 64, loss : 0.18485571534408327ITERATION : 65, loss : 0.1848557164366214ITERATION : 66, loss : 0.18485571720320115ITERATION : 67, loss : 0.18485571779420917ITERATION : 68, loss : 0.18485571825939948ITERATION : 69, loss : 0.1848557186886521ITERATION : 70, loss : 0.18485571892891806ITERATION : 71, loss : 0.18485571907803988ITERATION : 72, loss : 0.18485571928258407ITERATION : 73, loss : 0.1848557194337379ITERATION : 74, loss : 0.18485571952000712ITERATION : 75, loss : 0.18485571952020075ITERATION : 76, loss : 0.18485571952020075ITERATION : 77, loss : 0.18485571952020075ITERATION : 78, loss : 0.18485571952020075ITERATION : 79, loss : 0.18485571952020075ITERATION : 80, loss : 0.18485571952020075ITERATION : 81, loss : 0.18485571952020075ITERATION : 82, loss : 0.18485571952020075ITERATION : 83, loss : 0.18485571952020075ITERATION : 84, loss : 0.18485571952020075ITERATION : 85, loss : 0.18485571952020075ITERATION : 86, loss : 0.18485571952020075ITERATION : 87, loss : 0.18485571952020075ITERATION : 88, loss : 0.18485571952020075ITERATION : 89, loss : 0.18485571952020075ITERATION : 90, loss : 0.18485571952020075ITERATION : 91, loss : 0.18485571952020075ITERATION : 92, loss : 0.18485571952020075ITERATION : 93, loss : 0.18485571952020075ITERATION : 94, loss : 0.18485571952020075ITERATION : 95, loss : 0.18485571952020075ITERATION : 96, loss : 0.18485571952020075ITERATION : 97, loss : 0.18485571952020075ITERATION : 98, loss : 0.18485571952020075ITERATION : 99, loss : 0.18485571952020075ITERATION : 100, loss : 0.18485571952020075
ITERATION : 1, loss : 0.2296110033425843ITERATION : 2, loss : 0.24714246505078039ITERATION : 3, loss : 0.25658429313110753ITERATION : 4, loss : 0.26252401086292976ITERATION : 5, loss : 0.26643670835669325ITERATION : 6, loss : 0.26906770544306274ITERATION : 7, loss : 0.27085834108495743ITERATION : 8, loss : 0.27208676845280694ITERATION : 9, loss : 0.27293412241469067ITERATION : 10, loss : 0.2735208429317261ITERATION : 11, loss : 0.2739281730214204ITERATION : 12, loss : 0.2742114792852412ITERATION : 13, loss : 0.2744087684423747ITERATION : 14, loss : 0.27454626873169163ITERATION : 15, loss : 0.27464214724670283ITERATION : 16, loss : 0.27470902075818243ITERATION : 17, loss : 0.27475566831407583ITERATION : 18, loss : 0.27478820598693393ITERATION : 19, loss : 0.2748108985988754ITERATION : 20, loss : 0.27482672150805715ITERATION : 21, loss : 0.27483775140444683ITERATION : 22, loss : 0.27484543783106025ITERATION : 23, loss : 0.27485079222347236ITERATION : 24, loss : 0.27485452057549814ITERATION : 25, loss : 0.27485711564628224ITERATION : 26, loss : 0.2748589210844023ITERATION : 27, loss : 0.27486017649846406ITERATION : 28, loss : 0.2748610491295861ITERATION : 29, loss : 0.2748616553514616ITERATION : 30, loss : 0.27486207624613496ITERATION : 31, loss : 0.27486236842989153ITERATION : 32, loss : 0.27486257066910863ITERATION : 33, loss : 0.2748627109856941ITERATION : 34, loss : 0.27486280828724863ITERATION : 35, loss : 0.27486287573980184ITERATION : 36, loss : 0.2748629224263028ITERATION : 37, loss : 0.27486295464618654ITERATION : 38, loss : 0.27486297680243155ITERATION : 39, loss : 0.27486299221913446ITERATION : 40, loss : 0.2748630028594706ITERATION : 41, loss : 0.27486301015541836ITERATION : 42, loss : 0.27486301513431555ITERATION : 43, loss : 0.2748630185074249ITERATION : 44, loss : 0.2748630208313833ITERATION : 45, loss : 0.2748630223735537ITERATION : 46, loss : 0.27486302349333075ITERATION : 47, loss : 0.2748630242002073ITERATION : 48, loss : 0.27486302468199386ITERATION : 49, loss : 0.2748630249468926ITERATION : 50, loss : 0.2748630251716229ITERATION : 51, loss : 0.27486302532050305ITERATION : 52, loss : 0.2748630254259047ITERATION : 53, loss : 0.274863025471717ITERATION : 54, loss : 0.2748630255230811ITERATION : 55, loss : 0.2748630255202345ITERATION : 56, loss : 0.2748630255390702ITERATION : 57, loss : 0.27486302553807673ITERATION : 58, loss : 0.27486302553807673ITERATION : 59, loss : 0.27486302553807673ITERATION : 60, loss : 0.27486302553807673ITERATION : 61, loss : 0.27486302553807673ITERATION : 62, loss : 0.27486302553807673ITERATION : 63, loss : 0.27486302553807673ITERATION : 64, loss : 0.27486302553807673ITERATION : 65, loss : 0.27486302553807673ITERATION : 66, loss : 0.27486302553807673ITERATION : 67, loss : 0.27486302553807673ITERATION : 68, loss : 0.27486302553807673ITERATION : 69, loss : 0.27486302553807673ITERATION : 70, loss : 0.27486302553807673ITERATION : 71, loss : 0.27486302553807673ITERATION : 72, loss : 0.27486302553807673ITERATION : 73, loss : 0.27486302553807673ITERATION : 74, loss : 0.27486302553807673ITERATION : 75, loss : 0.27486302553807673ITERATION : 76, loss : 0.27486302553807673ITERATION : 77, loss : 0.27486302553807673ITERATION : 78, loss : 0.27486302553807673ITERATION : 79, loss : 0.27486302553807673ITERATION : 80, loss : 0.27486302553807673ITERATION : 81, loss : 0.27486302553807673ITERATION : 82, loss : 0.27486302553807673ITERATION : 83, loss : 0.27486302553807673ITERATION : 84, loss : 0.27486302553807673ITERATION : 85, loss : 0.27486302553807673ITERATION : 86, loss : 0.27486302553807673ITERATION : 87, loss : 0.27486302553807673ITERATION : 88, loss : 0.27486302553807673ITERATION : 89, loss : 0.27486302553807673ITERATION : 90, loss : 0.27486302553807673ITERATION : 91, loss : 0.27486302553807673ITERATION : 92, loss : 0.27486302553807673ITERATION : 93, loss : 0.27486302553807673ITERATION : 94, loss : 0.27486302553807673ITERATION : 95, loss : 0.27486302553807673ITERATION : 96, loss : 0.27486302553807673ITERATION : 97, loss : 0.27486302553807673ITERATION : 98, loss : 0.27486302553807673ITERATION : 99, loss : 0.27486302553807673ITERATION : 100, loss : 0.27486302553807673
ITERATION : 1, loss : 0.023544734723064387ITERATION : 2, loss : 0.02463146729736924ITERATION : 3, loss : 0.02497350773885267ITERATION : 4, loss : 0.024384514254701374ITERATION : 5, loss : 0.024454349356469663ITERATION : 6, loss : 0.024630212469995423ITERATION : 7, loss : 0.024821511318103093ITERATION : 8, loss : 0.024994904466560417ITERATION : 9, loss : 0.025140222758140297ITERATION : 10, loss : 0.025257070161570798ITERATION : 11, loss : 0.025348757197591585ITERATION : 12, loss : 0.025419599026166907ITERATION : 13, loss : 0.02547377595669371ITERATION : 14, loss : 0.02551491545751844ITERATION : 15, loss : 0.02554599731960899ITERATION : 16, loss : 0.025569393060271332ITERATION : 17, loss : 0.025586953935859857ITERATION : 18, loss : 0.02560010644004242ITERATION : 19, loss : 0.02560994012574075ITERATION : 20, loss : 0.02561728208959968ITERATION : 21, loss : 0.02562275719116011ITERATION : 22, loss : 0.02562683601644215ITERATION : 23, loss : 0.02562987198678928ITERATION : 24, loss : 0.02563213007735713ITERATION : 25, loss : 0.025633808400249508ITERATION : 26, loss : 0.02563505503115577ITERATION : 27, loss : 0.025635980473035103ITERATION : 28, loss : 0.025636667082703984ITERATION : 29, loss : 0.02563717633986804ITERATION : 30, loss : 0.025637553845534313ITERATION : 31, loss : 0.025637833558811562ITERATION : 32, loss : 0.02563804072386463ITERATION : 33, loss : 0.025638194104149335ITERATION : 34, loss : 0.025638307631917965ITERATION : 35, loss : 0.02563839160263264ITERATION : 36, loss : 0.025638453732016068ITERATION : 37, loss : 0.02563849965444714ITERATION : 38, loss : 0.02563853360366925ITERATION : 39, loss : 0.025638558718101265ITERATION : 40, loss : 0.025638577254927006ITERATION : 41, loss : 0.025638590928068163ITERATION : 42, loss : 0.025638601018958564ITERATION : 43, loss : 0.025638608474561432ITERATION : 44, loss : 0.02563861398204871ITERATION : 45, loss : 0.025638618056714253ITERATION : 46, loss : 0.025638621046692015ITERATION : 47, loss : 0.025638623286500604ITERATION : 48, loss : 0.025638624921099484ITERATION : 49, loss : 0.02563862612242927ITERATION : 50, loss : 0.025638627013760563ITERATION : 51, loss : 0.025638627693532978ITERATION : 52, loss : 0.02563862817171833ITERATION : 53, loss : 0.02563862850490284ITERATION : 54, loss : 0.025638628752526493ITERATION : 55, loss : 0.02563862896428596ITERATION : 56, loss : 0.02563862909344866ITERATION : 57, loss : 0.025638629182819585ITERATION : 58, loss : 0.02563862925046012ITERATION : 59, loss : 0.025638629316577708ITERATION : 60, loss : 0.025638629318707137ITERATION : 61, loss : 0.025638629318707137ITERATION : 62, loss : 0.025638629318707137ITERATION : 63, loss : 0.025638629318707137ITERATION : 64, loss : 0.025638629318707137ITERATION : 65, loss : 0.025638629318707137ITERATION : 66, loss : 0.025638629318707137ITERATION : 67, loss : 0.025638629318707137ITERATION : 68, loss : 0.025638629318707137ITERATION : 69, loss : 0.025638629318707137ITERATION : 70, loss : 0.025638629318707137ITERATION : 71, loss : 0.025638629318707137ITERATION : 72, loss : 0.025638629318707137ITERATION : 73, loss : 0.025638629318707137ITERATION : 74, loss : 0.025638629318707137ITERATION : 75, loss : 0.025638629318707137ITERATION : 76, loss : 0.025638629318707137ITERATION : 77, loss : 0.025638629318707137ITERATION : 78, loss : 0.025638629318707137ITERATION : 79, loss : 0.025638629318707137ITERATION : 80, loss : 0.025638629318707137ITERATION : 81, loss : 0.025638629318707137ITERATION : 82, loss : 0.025638629318707137ITERATION : 83, loss : 0.025638629318707137ITERATION : 84, loss : 0.025638629318707137ITERATION : 85, loss : 0.025638629318707137ITERATION : 86, loss : 0.025638629318707137ITERATION : 87, loss : 0.025638629318707137ITERATION : 88, loss : 0.025638629318707137ITERATION : 89, loss : 0.025638629318707137ITERATION : 90, loss : 0.025638629318707137ITERATION : 91, loss : 0.025638629318707137ITERATION : 92, loss : 0.025638629318707137ITERATION : 93, loss : 0.025638629318707137ITERATION : 94, loss : 0.025638629318707137ITERATION : 95, loss : 0.025638629318707137ITERATION : 96, loss : 0.025638629318707137ITERATION : 97, loss : 0.025638629318707137ITERATION : 98, loss : 0.025638629318707137ITERATION : 99, loss : 0.025638629318707137ITERATION : 100, loss : 0.025638629318707137
gradient norm in None layer : 0.02377434247431557
gradient norm in None layer : 0.0012995230716897795
gradient norm in None layer : 0.0008154637497991824
gradient norm in None layer : 0.019148082925076422
gradient norm in None layer : 0.0009412703912890549
gradient norm in None layer : 0.0009249295686302434
gradient norm in None layer : 0.008326735083287367
gradient norm in None layer : 0.00026800137140007484
gradient norm in None layer : 0.0002400968197445605
gradient norm in None layer : 0.007324422859880663
gradient norm in None layer : 0.00028123763838655964
gradient norm in None layer : 0.00022689331575400333
gradient norm in None layer : 0.0021988391464011
gradient norm in None layer : 6.67933911718563e-05
gradient norm in None layer : 5.3729689459629904e-05
gradient norm in None layer : 0.002039203258038609
gradient norm in None layer : 7.208072825224467e-05
gradient norm in None layer : 6.158742854166695e-05
gradient norm in None layer : 0.0029233161959470355
gradient norm in None layer : 2.204714493352074e-05
gradient norm in None layer : 0.006533456290518157
gradient norm in None layer : 0.000304667463228948
gradient norm in None layer : 0.0002535691371153928
gradient norm in None layer : 0.006966319451590383
gradient norm in None layer : 0.0005384777654066133
gradient norm in None layer : 0.0005032712318049893
gradient norm in None layer : 0.014224731434316299
gradient norm in None layer : 5.3673618695950475e-05
gradient norm in None layer : 0.017888074876279695
gradient norm in None layer : 0.0012360584271124918
gradient norm in None layer : 0.0010670282797288566
gradient norm in None layer : 0.017320015030998438
gradient norm in None layer : 0.002523570108428448
gradient norm in None layer : 0.0031849909195531155
gradient norm in None layer : 0.002303051004311991
gradient norm in None layer : 0.0005737012047657367
Total gradient norm: 0.04489767711097081
invariance loss : 9.46132355978411, avg_den : 0.3057861328125, density loss : 0.228594970703125, mse loss : 0.08985471510641455, solver time : 116.67611312866211 sec , total loss : 0.09954463363690179, running loss : 0.08608439567914943
Epoch 0/10 , batch 5/12500 
ITERATION : 1, loss : 0.02833360581888303ITERATION : 2, loss : 0.028405268910400808ITERATION : 3, loss : 0.02832369331973582ITERATION : 4, loss : 0.02827823453988061ITERATION : 5, loss : 0.02828961670432307ITERATION : 6, loss : 0.028328732344290588ITERATION : 7, loss : 0.02837380178983369ITERATION : 8, loss : 0.02841458639571231ITERATION : 9, loss : 0.02844777761085051ITERATION : 10, loss : 0.02847329427364345ITERATION : 11, loss : 0.028492260202864394ITERATION : 12, loss : 0.028506061657090097ITERATION : 13, loss : 0.02851596728358707ITERATION : 14, loss : 0.02852301157210464ITERATION : 15, loss : 0.028527989860237857ITERATION : 16, loss : 0.02853149299411886ITERATION : 17, loss : 0.028533950773465523ITERATION : 18, loss : 0.028535671562191358ITERATION : 19, loss : 0.0285368746149009ITERATION : 20, loss : 0.028537714848541906ITERATION : 21, loss : 0.028538301243569698ITERATION : 22, loss : 0.028538710284272933ITERATION : 23, loss : 0.02853899549496809ITERATION : 24, loss : 0.02853919432146695ITERATION : 25, loss : 0.028539332882194204ITERATION : 26, loss : 0.028539429435504592ITERATION : 27, loss : 0.02853949669778352ITERATION : 28, loss : 0.028539543583038212ITERATION : 29, loss : 0.02853957622956499ITERATION : 30, loss : 0.028539598980508125ITERATION : 31, loss : 0.02853961483415399ITERATION : 32, loss : 0.028539625865668515ITERATION : 33, loss : 0.028539633548325498ITERATION : 34, loss : 0.028539638891623125ITERATION : 35, loss : 0.028539642605648466ITERATION : 36, loss : 0.0285396451943449ITERATION : 37, loss : 0.028539646989756515ITERATION : 38, loss : 0.028539648244799157ITERATION : 39, loss : 0.028539649108823867ITERATION : 40, loss : 0.02853964971450147ITERATION : 41, loss : 0.028539650131748612ITERATION : 42, loss : 0.028539650431320167ITERATION : 43, loss : 0.028539650627355792ITERATION : 44, loss : 0.028539650774215317ITERATION : 45, loss : 0.028539650865269866ITERATION : 46, loss : 0.028539650942751703ITERATION : 47, loss : 0.028539650978461877ITERATION : 48, loss : 0.028539651016618307ITERATION : 49, loss : 0.028539651027049685ITERATION : 50, loss : 0.028539651052873902ITERATION : 51, loss : 0.02853965105241564ITERATION : 52, loss : 0.02853965105365993ITERATION : 53, loss : 0.028539651054024843ITERATION : 54, loss : 0.028539651054024843ITERATION : 55, loss : 0.028539651054024843ITERATION : 56, loss : 0.028539651054024843ITERATION : 57, loss : 0.028539651054024843ITERATION : 58, loss : 0.028539651054024843ITERATION : 59, loss : 0.028539651054024843ITERATION : 60, loss : 0.028539651054024843ITERATION : 61, loss : 0.028539651054024843ITERATION : 62, loss : 0.028539651054024843ITERATION : 63, loss : 0.028539651054024843ITERATION : 64, loss : 0.028539651054024843ITERATION : 65, loss : 0.028539651054024843ITERATION : 66, loss : 0.028539651054024843ITERATION : 67, loss : 0.028539651054024843ITERATION : 68, loss : 0.028539651054024843ITERATION : 69, loss : 0.028539651054024843ITERATION : 70, loss : 0.028539651054024843ITERATION : 71, loss : 0.028539651054024843ITERATION : 72, loss : 0.028539651054024843ITERATION : 73, loss : 0.028539651054024843ITERATION : 74, loss : 0.028539651054024843ITERATION : 75, loss : 0.028539651054024843ITERATION : 76, loss : 0.028539651054024843ITERATION : 77, loss : 0.028539651054024843ITERATION : 78, loss : 0.028539651054024843ITERATION : 79, loss : 0.028539651054024843ITERATION : 80, loss : 0.028539651054024843ITERATION : 81, loss : 0.028539651054024843ITERATION : 82, loss : 0.028539651054024843ITERATION : 83, loss : 0.028539651054024843ITERATION : 84, loss : 0.028539651054024843ITERATION : 85, loss : 0.028539651054024843ITERATION : 86, loss : 0.028539651054024843ITERATION : 87, loss : 0.028539651054024843ITERATION : 88, loss : 0.028539651054024843ITERATION : 89, loss : 0.028539651054024843ITERATION : 90, loss : 0.028539651054024843ITERATION : 91, loss : 0.028539651054024843ITERATION : 92, loss : 0.028539651054024843ITERATION : 93, loss : 0.028539651054024843ITERATION : 94, loss : 0.028539651054024843ITERATION : 95, loss : 0.028539651054024843ITERATION : 96, loss : 0.028539651054024843ITERATION : 97, loss : 0.028539651054024843ITERATION : 98, loss : 0.028539651054024843ITERATION : 99, loss : 0.028539651054024843ITERATION : 100, loss : 0.028539651054024843
ITERATION : 1, loss : 0.029069730469996273ITERATION : 2, loss : 0.028080072962091456ITERATION : 3, loss : 0.028376587609980564ITERATION : 4, loss : 0.02880536992444215ITERATION : 5, loss : 0.029185751437233388ITERATION : 6, loss : 0.028999902072364626ITERATION : 7, loss : 0.028827137754202686ITERATION : 8, loss : 0.028726798184668712ITERATION : 9, loss : 0.028667833128466987ITERATION : 10, loss : 0.028632974101870965ITERATION : 11, loss : 0.028612367147546603ITERATION : 12, loss : 0.028600270399857703ITERATION : 13, loss : 0.028593280910618114ITERATION : 14, loss : 0.028589355418148932ITERATION : 15, loss : 0.028587256449689322ITERATION : 16, loss : 0.028586231186168ITERATION : 17, loss : 0.028585822026726396ITERATION : 18, loss : 0.028585752454409127ITERATION : 19, loss : 0.028585857317019115ITERATION : 20, loss : 0.028586040183662387ITERATION : 21, loss : 0.028586246501505562ITERATION : 22, loss : 0.02858644714262186ITERATION : 23, loss : 0.028586627965789865ITERATION : 24, loss : 0.028586783758054318ITERATION : 25, loss : 0.02858691369534172ITERATION : 26, loss : 0.028587019781114865ITERATION : 27, loss : 0.028587104972175332ITERATION : 28, loss : 0.02858717245065786ITERATION : 29, loss : 0.028587225373018558ITERATION : 30, loss : 0.02858726654996353ITERATION : 31, loss : 0.028587298340838896ITERATION : 32, loss : 0.028587322738617933ITERATION : 33, loss : 0.02858734136230969ITERATION : 34, loss : 0.02858735548854685ITERATION : 35, loss : 0.028587366210406705ITERATION : 36, loss : 0.028587374316570824ITERATION : 37, loss : 0.02858738037379856ITERATION : 38, loss : 0.028587384922401017ITERATION : 39, loss : 0.02858738836946795ITERATION : 40, loss : 0.028587390867321566ITERATION : 41, loss : 0.028587392739903048ITERATION : 42, loss : 0.02858739416620294ITERATION : 43, loss : 0.028587395227087826ITERATION : 44, loss : 0.02858739604000876ITERATION : 45, loss : 0.028587396610924867ITERATION : 46, loss : 0.028587397053203012ITERATION : 47, loss : 0.028587397374873733ITERATION : 48, loss : 0.028587397637926032ITERATION : 49, loss : 0.028587397791803897ITERATION : 50, loss : 0.02858739794501648ITERATION : 51, loss : 0.028587398020270487ITERATION : 52, loss : 0.02858739810616525ITERATION : 53, loss : 0.028587398144929072ITERATION : 54, loss : 0.028587398182630726ITERATION : 55, loss : 0.02858739819107598ITERATION : 56, loss : 0.0285873981946179ITERATION : 57, loss : 0.0285873981946179ITERATION : 58, loss : 0.0285873981946179ITERATION : 59, loss : 0.0285873981946179ITERATION : 60, loss : 0.0285873981946179ITERATION : 61, loss : 0.0285873981946179ITERATION : 62, loss : 0.0285873981946179ITERATION : 63, loss : 0.0285873981946179ITERATION : 64, loss : 0.0285873981946179ITERATION : 65, loss : 0.0285873981946179ITERATION : 66, loss : 0.0285873981946179ITERATION : 67, loss : 0.0285873981946179ITERATION : 68, loss : 0.0285873981946179ITERATION : 69, loss : 0.0285873981946179ITERATION : 70, loss : 0.0285873981946179ITERATION : 71, loss : 0.0285873981946179ITERATION : 72, loss : 0.0285873981946179ITERATION : 73, loss : 0.0285873981946179ITERATION : 74, loss : 0.0285873981946179ITERATION : 75, loss : 0.0285873981946179ITERATION : 76, loss : 0.0285873981946179ITERATION : 77, loss : 0.0285873981946179ITERATION : 78, loss : 0.0285873981946179ITERATION : 79, loss : 0.0285873981946179ITERATION : 80, loss : 0.0285873981946179ITERATION : 81, loss : 0.0285873981946179ITERATION : 82, loss : 0.0285873981946179ITERATION : 83, loss : 0.0285873981946179ITERATION : 84, loss : 0.0285873981946179ITERATION : 85, loss : 0.0285873981946179ITERATION : 86, loss : 0.0285873981946179ITERATION : 87, loss : 0.0285873981946179ITERATION : 88, loss : 0.0285873981946179ITERATION : 89, loss : 0.0285873981946179ITERATION : 90, loss : 0.0285873981946179ITERATION : 91, loss : 0.0285873981946179ITERATION : 92, loss : 0.0285873981946179ITERATION : 93, loss : 0.0285873981946179ITERATION : 94, loss : 0.0285873981946179ITERATION : 95, loss : 0.0285873981946179ITERATION : 96, loss : 0.0285873981946179ITERATION : 97, loss : 0.0285873981946179ITERATION : 98, loss : 0.0285873981946179ITERATION : 99, loss : 0.0285873981946179ITERATION : 100, loss : 0.0285873981946179
ITERATION : 1, loss : 0.060818615662463237ITERATION : 2, loss : 0.04522544132939406ITERATION : 3, loss : 0.03921188543568313ITERATION : 4, loss : 0.036495440426081494ITERATION : 5, loss : 0.03504734479630189ITERATION : 6, loss : 0.034167543952272034ITERATION : 7, loss : 0.03358187308859551ITERATION : 8, loss : 0.033169768479327476ITERATION : 9, loss : 0.03287105856105919ITERATION : 10, loss : 0.03264722229405445ITERATION : 11, loss : 0.032481296244019894ITERATION : 12, loss : 0.03235896880610325ITERATION : 13, loss : 0.0322689025504105ITERATION : 14, loss : 0.032202751246447635ITERATION : 15, loss : 0.032154305254489327ITERATION : 16, loss : 0.0321189312846922ITERATION : 17, loss : 0.03209317591189878ITERATION : 18, loss : 0.032074473532641944ITERATION : 19, loss : 0.03206092567278632ITERATION : 20, loss : 0.03205113304029863ITERATION : 21, loss : 0.03204406850521803ITERATION : 22, loss : 0.03203898079930427ITERATION : 23, loss : 0.03203532230497267ITERATION : 24, loss : 0.03203269499338228ITERATION : 25, loss : 0.032030810504240656ITERATION : 26, loss : 0.03202946018966047ITERATION : 27, loss : 0.03202849350128609ITERATION : 28, loss : 0.03202780211646578ITERATION : 29, loss : 0.03202730777813148ITERATION : 30, loss : 0.0320269547452481ITERATION : 31, loss : 0.03202670260816217ITERATION : 32, loss : 0.03202652275711382ITERATION : 33, loss : 0.03202639444929244ITERATION : 34, loss : 0.03202630290797672ITERATION : 35, loss : 0.03202623774177988ITERATION : 36, loss : 0.03202619126233695ITERATION : 37, loss : 0.03202615815691863ITERATION : 38, loss : 0.03202613452704995ITERATION : 39, loss : 0.03202611767770598ITERATION : 40, loss : 0.032026105648362044ITERATION : 41, loss : 0.03202609712681723ITERATION : 42, loss : 0.032026091078783396ITERATION : 43, loss : 0.032026086749431554ITERATION : 44, loss : 0.03202608363659789ITERATION : 45, loss : 0.03202608149554434ITERATION : 46, loss : 0.03202607993945129ITERATION : 47, loss : 0.03202607888857891ITERATION : 48, loss : 0.032026078163566715ITERATION : 49, loss : 0.03202607766544406ITERATION : 50, loss : 0.032026077246600816ITERATION : 51, loss : 0.03202607702508232ITERATION : 52, loss : 0.032026076777124525ITERATION : 53, loss : 0.032026076734132075ITERATION : 54, loss : 0.032026076555271565ITERATION : 55, loss : 0.03202607652072128ITERATION : 56, loss : 0.03202607640923223ITERATION : 57, loss : 0.03202607640923223ITERATION : 58, loss : 0.03202607640923223ITERATION : 59, loss : 0.03202607640923223ITERATION : 60, loss : 0.03202607640923223ITERATION : 61, loss : 0.03202607640923223ITERATION : 62, loss : 0.03202607640923223ITERATION : 63, loss : 0.03202607640923223ITERATION : 64, loss : 0.03202607640923223ITERATION : 65, loss : 0.03202607640923223ITERATION : 66, loss : 0.03202607640923223ITERATION : 67, loss : 0.03202607640923223ITERATION : 68, loss : 0.03202607640923223ITERATION : 69, loss : 0.03202607640923223ITERATION : 70, loss : 0.03202607640923223ITERATION : 71, loss : 0.03202607640923223ITERATION : 72, loss : 0.03202607640923223ITERATION : 73, loss : 0.03202607640923223ITERATION : 74, loss : 0.03202607640923223ITERATION : 75, loss : 0.03202607640923223ITERATION : 76, loss : 0.03202607640923223ITERATION : 77, loss : 0.03202607640923223ITERATION : 78, loss : 0.03202607640923223ITERATION : 79, loss : 0.03202607640923223ITERATION : 80, loss : 0.03202607640923223ITERATION : 81, loss : 0.03202607640923223ITERATION : 82, loss : 0.03202607640923223ITERATION : 83, loss : 0.03202607640923223ITERATION : 84, loss : 0.03202607640923223ITERATION : 85, loss : 0.03202607640923223ITERATION : 86, loss : 0.03202607640923223ITERATION : 87, loss : 0.03202607640923223ITERATION : 88, loss : 0.03202607640923223ITERATION : 89, loss : 0.03202607640923223ITERATION : 90, loss : 0.03202607640923223ITERATION : 91, loss : 0.03202607640923223ITERATION : 92, loss : 0.03202607640923223ITERATION : 93, loss : 0.03202607640923223ITERATION : 94, loss : 0.03202607640923223ITERATION : 95, loss : 0.03202607640923223ITERATION : 96, loss : 0.03202607640923223ITERATION : 97, loss : 0.03202607640923223ITERATION : 98, loss : 0.03202607640923223ITERATION : 99, loss : 0.03202607640923223ITERATION : 100, loss : 0.03202607640923223
ITERATION : 1, loss : 0.028844439293322713ITERATION : 2, loss : 0.020578410236376757ITERATION : 3, loss : 0.016220221141318037ITERATION : 4, loss : 0.014782302993578232ITERATION : 5, loss : 0.0145024959529778ITERATION : 6, loss : 0.014690564039144847ITERATION : 7, loss : 0.014956551728774067ITERATION : 8, loss : 0.015207658613595755ITERATION : 9, loss : 0.015415580809420282ITERATION : 10, loss : 0.015577538267043202ITERATION : 11, loss : 0.015699590962033213ITERATION : 12, loss : 0.015789808692927374ITERATION : 13, loss : 0.01585570968422079ITERATION : 14, loss : 0.015903492197231042ITERATION : 15, loss : 0.01593797631723811ITERATION : 16, loss : 0.015962791397168087ITERATION : 17, loss : 0.015980618341103845ITERATION : 18, loss : 0.015993413910081997ITERATION : 19, loss : 0.01600259530125787ITERATION : 20, loss : 0.01600918410626142ITERATION : 21, loss : 0.0160139143680024ITERATION : 22, loss : 0.016017312488498733ITERATION : 23, loss : 0.016019755575994377ITERATION : 24, loss : 0.01602151375936784ITERATION : 25, loss : 0.016022780316073123ITERATION : 26, loss : 0.016023693720018457ITERATION : 27, loss : 0.01602435326748891ITERATION : 28, loss : 0.016024830097694148ITERATION : 29, loss : 0.016025175293095616ITERATION : 30, loss : 0.01602542551513243ITERATION : 31, loss : 0.016025607141384044ITERATION : 32, loss : 0.016025739172496282ITERATION : 33, loss : 0.016025835251075704ITERATION : 34, loss : 0.016025905309630623ITERATION : 35, loss : 0.016025956453075148ITERATION : 36, loss : 0.01602599382131755ITERATION : 37, loss : 0.016026021184623963ITERATION : 38, loss : 0.01602604126399754ITERATION : 39, loss : 0.016026056003258637ITERATION : 40, loss : 0.016026066863946427ITERATION : 41, loss : 0.016026074847360492ITERATION : 42, loss : 0.016026080736189486ITERATION : 43, loss : 0.01602608509198438ITERATION : 44, loss : 0.01602608830600696ITERATION : 45, loss : 0.01602609070160389ITERATION : 46, loss : 0.016026092469405315ITERATION : 47, loss : 0.016026093786200818ITERATION : 48, loss : 0.01602609474678687ITERATION : 49, loss : 0.01602609547186451ITERATION : 50, loss : 0.016026095994620366ITERATION : 51, loss : 0.016026096395845634ITERATION : 52, loss : 0.016026096674805573ITERATION : 53, loss : 0.01602609688642378ITERATION : 54, loss : 0.016026097055387885ITERATION : 55, loss : 0.016026097165071387ITERATION : 56, loss : 0.016026097253873356ITERATION : 57, loss : 0.016026097325969064ITERATION : 58, loss : 0.016026097372897903ITERATION : 59, loss : 0.016026097404956953ITERATION : 60, loss : 0.016026097435641845ITERATION : 61, loss : 0.016026097473017423ITERATION : 62, loss : 0.016026097477398072ITERATION : 63, loss : 0.016026097477646124ITERATION : 64, loss : 0.016026097477646124ITERATION : 65, loss : 0.016026097477646124ITERATION : 66, loss : 0.016026097477646124ITERATION : 67, loss : 0.016026097477646124ITERATION : 68, loss : 0.016026097477646124ITERATION : 69, loss : 0.016026097477646124ITERATION : 70, loss : 0.016026097477646124ITERATION : 71, loss : 0.016026097477646124ITERATION : 72, loss : 0.016026097477646124ITERATION : 73, loss : 0.016026097477646124ITERATION : 74, loss : 0.016026097477646124ITERATION : 75, loss : 0.016026097477646124ITERATION : 76, loss : 0.016026097477646124ITERATION : 77, loss : 0.016026097477646124ITERATION : 78, loss : 0.016026097477646124ITERATION : 79, loss : 0.016026097477646124ITERATION : 80, loss : 0.016026097477646124ITERATION : 81, loss : 0.016026097477646124ITERATION : 82, loss : 0.016026097477646124ITERATION : 83, loss : 0.016026097477646124ITERATION : 84, loss : 0.016026097477646124ITERATION : 85, loss : 0.016026097477646124ITERATION : 86, loss : 0.016026097477646124ITERATION : 87, loss : 0.016026097477646124ITERATION : 88, loss : 0.016026097477646124ITERATION : 89, loss : 0.016026097477646124ITERATION : 90, loss : 0.016026097477646124ITERATION : 91, loss : 0.016026097477646124ITERATION : 92, loss : 0.016026097477646124ITERATION : 93, loss : 0.016026097477646124ITERATION : 94, loss : 0.016026097477646124ITERATION : 95, loss : 0.016026097477646124ITERATION : 96, loss : 0.016026097477646124ITERATION : 97, loss : 0.016026097477646124ITERATION : 98, loss : 0.016026097477646124ITERATION : 99, loss : 0.016026097477646124ITERATION : 100, loss : 0.016026097477646124
ITERATION : 1, loss : 0.018989173506465964ITERATION : 2, loss : 0.02265838127590083ITERATION : 3, loss : 0.02728051244682304ITERATION : 4, loss : 0.030591286451692255ITERATION : 5, loss : 0.03271418545963905ITERATION : 6, loss : 0.034101759658425525ITERATION : 7, loss : 0.034632383126978444ITERATION : 8, loss : 0.03455883597338267ITERATION : 9, loss : 0.03448554331268913ITERATION : 10, loss : 0.03442074551128274ITERATION : 11, loss : 0.03436741065154442ITERATION : 12, loss : 0.03432555011711303ITERATION : 13, loss : 0.03429380386001366ITERATION : 14, loss : 0.034270358696794ITERATION : 15, loss : 0.03425342071959485ITERATION : 16, loss : 0.0342414188270233ITERATION : 17, loss : 0.03423307002512711ITERATION : 18, loss : 0.03422737012799164ITERATION : 19, loss : 0.03422355696749627ITERATION : 20, loss : 0.0342210660149219ITERATION : 21, loss : 0.03421948699830413ITERATION : 22, loss : 0.03421852598998914ITERATION : 23, loss : 0.034217976043704246ITERATION : 24, loss : 0.034217693028638946ITERATION : 25, loss : 0.034217578498522944ITERATION : 26, loss : 0.034217566419870275ITERATION : 27, loss : 0.03421761242119418ITERATION : 28, loss : 0.03421768817328805ITERATION : 29, loss : 0.034217775655963754ITERATION : 30, loss : 0.03421786428495185ITERATION : 31, loss : 0.03421794799875976ITERATION : 32, loss : 0.034218023696471464ITERATION : 33, loss : 0.03421809015794663ITERATION : 34, loss : 0.03421814694767021ITERATION : 35, loss : 0.03421819489207786ITERATION : 36, loss : 0.03421823499875717ITERATION : 37, loss : 0.034218267755048465ITERATION : 38, loss : 0.03421829475999068ITERATION : 39, loss : 0.03421831652332271ITERATION : 40, loss : 0.03421833413630022ITERATION : 41, loss : 0.034218348289845976ITERATION : 42, loss : 0.03421835962692049ITERATION : 43, loss : 0.03421836868605775ITERATION : 44, loss : 0.03421837589230379ITERATION : 45, loss : 0.03421838149389009ITERATION : 46, loss : 0.03421838596634028ITERATION : 47, loss : 0.03421838952921823ITERATION : 48, loss : 0.03421839233941632ITERATION : 49, loss : 0.03421839458414988ITERATION : 50, loss : 0.03421839631603804ITERATION : 51, loss : 0.03421839769951244ITERATION : 52, loss : 0.0342183987619261ITERATION : 53, loss : 0.03421839959655128ITERATION : 54, loss : 0.0342184002579564ITERATION : 55, loss : 0.03421840076643359ITERATION : 56, loss : 0.03421840115841854ITERATION : 57, loss : 0.03421840147718089ITERATION : 58, loss : 0.03421840166920292ITERATION : 59, loss : 0.034218401865115676ITERATION : 60, loss : 0.034218401990596684ITERATION : 61, loss : 0.03421840210042008ITERATION : 62, loss : 0.03421840221344559ITERATION : 63, loss : 0.03421840226131379ITERATION : 64, loss : 0.03421840233640759ITERATION : 65, loss : 0.03421840236317933ITERATION : 66, loss : 0.03421840237235008ITERATION : 67, loss : 0.03421840240445967ITERATION : 68, loss : 0.03421840239940525ITERATION : 69, loss : 0.03421840239940525ITERATION : 70, loss : 0.03421840239940525ITERATION : 71, loss : 0.03421840239940525ITERATION : 72, loss : 0.03421840239940525ITERATION : 73, loss : 0.03421840239940525ITERATION : 74, loss : 0.03421840239940525ITERATION : 75, loss : 0.03421840239940525ITERATION : 76, loss : 0.03421840239940525ITERATION : 77, loss : 0.03421840239940525ITERATION : 78, loss : 0.03421840239940525ITERATION : 79, loss : 0.03421840239940525ITERATION : 80, loss : 0.03421840239940525ITERATION : 81, loss : 0.03421840239940525ITERATION : 82, loss : 0.03421840239940525ITERATION : 83, loss : 0.03421840239940525ITERATION : 84, loss : 0.03421840239940525ITERATION : 85, loss : 0.03421840239940525ITERATION : 86, loss : 0.03421840239940525ITERATION : 87, loss : 0.03421840239940525ITERATION : 88, loss : 0.03421840239940525ITERATION : 89, loss : 0.03421840239940525ITERATION : 90, loss : 0.03421840239940525ITERATION : 91, loss : 0.03421840239940525ITERATION : 92, loss : 0.03421840239940525ITERATION : 93, loss : 0.03421840239940525ITERATION : 94, loss : 0.03421840239940525ITERATION : 95, loss : 0.03421840239940525ITERATION : 96, loss : 0.03421840239940525ITERATION : 97, loss : 0.03421840239940525ITERATION : 98, loss : 0.03421840239940525ITERATION : 99, loss : 0.03421840239940525ITERATION : 100, loss : 0.03421840239940525
ITERATION : 1, loss : 0.042038874916951456ITERATION : 2, loss : 0.03936999629509207ITERATION : 3, loss : 0.038950857758368526ITERATION : 4, loss : 0.038868392919948876ITERATION : 5, loss : 0.038851388624572034ITERATION : 6, loss : 0.03884968113701485ITERATION : 7, loss : 0.03885170254338966ITERATION : 8, loss : 0.038854344694203354ITERATION : 9, loss : 0.03885673259935159ITERATION : 10, loss : 0.03885866438066767ITERATION : 11, loss : 0.03886015130344825ITERATION : 12, loss : 0.038861265495423084ITERATION : 13, loss : 0.038862087014528986ITERATION : 14, loss : 0.038862686476785ITERATION : 15, loss : 0.03886312087438687ITERATION : 16, loss : 0.0388634341514262ITERATION : 17, loss : 0.038863659305384775ITERATION : 18, loss : 0.03886382073792129ITERATION : 19, loss : 0.03886393627927852ITERATION : 20, loss : 0.038864018874255946ITERATION : 21, loss : 0.03886407786545624ITERATION : 22, loss : 0.0388641199768667ITERATION : 23, loss : 0.038864150028280246ITERATION : 24, loss : 0.03886417146942742ITERATION : 25, loss : 0.03886418676337728ITERATION : 26, loss : 0.03886419767399409ITERATION : 27, loss : 0.03886420545959164ITERATION : 28, loss : 0.038864211008553ITERATION : 29, loss : 0.038864214973280076ITERATION : 30, loss : 0.03886421780553707ITERATION : 31, loss : 0.038864219817936654ITERATION : 32, loss : 0.03886422125847854ITERATION : 33, loss : 0.038864222288699825ITERATION : 34, loss : 0.03886422302354494ITERATION : 35, loss : 0.03886422354889972ITERATION : 36, loss : 0.038864223928968766ITERATION : 37, loss : 0.03886422420013566ITERATION : 38, loss : 0.038864224397352376ITERATION : 39, loss : 0.038864224543933845ITERATION : 40, loss : 0.03886422463965471ITERATION : 41, loss : 0.038864224707468864ITERATION : 42, loss : 0.03886422475212355ITERATION : 43, loss : 0.03886422478885534ITERATION : 44, loss : 0.03886422481332792ITERATION : 45, loss : 0.03886422483279038ITERATION : 46, loss : 0.038864224856096544ITERATION : 47, loss : 0.03886422486019094ITERATION : 48, loss : 0.03886422486681411ITERATION : 49, loss : 0.03886422486771697ITERATION : 50, loss : 0.03886422486771697ITERATION : 51, loss : 0.03886422486771697ITERATION : 52, loss : 0.03886422486771697ITERATION : 53, loss : 0.03886422486771697ITERATION : 54, loss : 0.03886422486771697ITERATION : 55, loss : 0.03886422486771697ITERATION : 56, loss : 0.03886422486771697ITERATION : 57, loss : 0.03886422486771697ITERATION : 58, loss : 0.03886422486771697ITERATION : 59, loss : 0.03886422486771697ITERATION : 60, loss : 0.03886422486771697ITERATION : 61, loss : 0.03886422486771697ITERATION : 62, loss : 0.03886422486771697ITERATION : 63, loss : 0.03886422486771697ITERATION : 64, loss : 0.03886422486771697ITERATION : 65, loss : 0.03886422486771697ITERATION : 66, loss : 0.03886422486771697ITERATION : 67, loss : 0.03886422486771697ITERATION : 68, loss : 0.03886422486771697ITERATION : 69, loss : 0.03886422486771697ITERATION : 70, loss : 0.03886422486771697ITERATION : 71, loss : 0.03886422486771697ITERATION : 72, loss : 0.03886422486771697ITERATION : 73, loss : 0.03886422486771697ITERATION : 74, loss : 0.03886422486771697ITERATION : 75, loss : 0.03886422486771697ITERATION : 76, loss : 0.03886422486771697ITERATION : 77, loss : 0.03886422486771697ITERATION : 78, loss : 0.03886422486771697ITERATION : 79, loss : 0.03886422486771697ITERATION : 80, loss : 0.03886422486771697ITERATION : 81, loss : 0.03886422486771697ITERATION : 82, loss : 0.03886422486771697ITERATION : 83, loss : 0.03886422486771697ITERATION : 84, loss : 0.03886422486771697ITERATION : 85, loss : 0.03886422486771697ITERATION : 86, loss : 0.03886422486771697ITERATION : 87, loss : 0.03886422486771697ITERATION : 88, loss : 0.03886422486771697ITERATION : 89, loss : 0.03886422486771697ITERATION : 90, loss : 0.03886422486771697ITERATION : 91, loss : 0.03886422486771697ITERATION : 92, loss : 0.03886422486771697ITERATION : 93, loss : 0.03886422486771697ITERATION : 94, loss : 0.03886422486771697ITERATION : 95, loss : 0.03886422486771697ITERATION : 96, loss : 0.03886422486771697ITERATION : 97, loss : 0.03886422486771697ITERATION : 98, loss : 0.03886422486771697ITERATION : 99, loss : 0.03886422486771697ITERATION : 100, loss : 0.03886422486771697
ITERATION : 1, loss : 0.041673653260109404ITERATION : 2, loss : 0.04081058783367008ITERATION : 3, loss : 0.040374430147612494ITERATION : 4, loss : 0.04023726459459165ITERATION : 5, loss : 0.04027009409171982ITERATION : 6, loss : 0.040364435282992175ITERATION : 7, loss : 0.040464002292783095ITERATION : 8, loss : 0.04054714045497136ITERATION : 9, loss : 0.040609133523619244ITERATION : 10, loss : 0.040652090736568426ITERATION : 11, loss : 0.040680126644693ITERATION : 12, loss : 0.040697342507755016ITERATION : 13, loss : 0.040707129724709086ITERATION : 14, loss : 0.040712047443527515ITERATION : 15, loss : 0.040713920466322766ITERATION : 16, loss : 0.04071399519123079ITERATION : 17, loss : 0.040713091775349125ITERATION : 18, loss : 0.04071172997456082ITERATION : 19, loss : 0.040710224856973405ITERATION : 20, loss : 0.04070875637941173ITERATION : 21, loss : 0.040707417849255176ITERATION : 22, loss : 0.040706249220299126ITERATION : 23, loss : 0.04070525869181682ITERATION : 24, loss : 0.04070443713803552ITERATION : 25, loss : 0.04070376686264224ITERATION : 26, loss : 0.04070322723585002ITERATION : 27, loss : 0.04070279736762673ITERATION : 28, loss : 0.040702457797050284ITERATION : 29, loss : 0.04070219143253033ITERATION : 30, loss : 0.04070198375731699ITERATION : 31, loss : 0.04070182284453132ITERATION : 32, loss : 0.04070169855607066ITERATION : 33, loss : 0.04070160283511613ITERATION : 34, loss : 0.04070152968412773ITERATION : 35, loss : 0.04070147385458179ITERATION : 36, loss : 0.04070143123362232ITERATION : 37, loss : 0.040701398953990185ITERATION : 38, loss : 0.04070137439495461ITERATION : 39, loss : 0.04070135578370427ITERATION : 40, loss : 0.04070134180899985ITERATION : 41, loss : 0.040701331284846445ITERATION : 42, loss : 0.040701323316435827ITERATION : 43, loss : 0.04070131720843561ITERATION : 44, loss : 0.04070131273360667ITERATION : 45, loss : 0.04070130933900075ITERATION : 46, loss : 0.04070130682329504ITERATION : 47, loss : 0.04070130490618264ITERATION : 48, loss : 0.04070130349899626ITERATION : 49, loss : 0.04070130247915351ITERATION : 50, loss : 0.040701301668978784ITERATION : 51, loss : 0.04070130104975469ITERATION : 52, loss : 0.040701300604257376ITERATION : 53, loss : 0.040701300292315966ITERATION : 54, loss : 0.04070130006868497ITERATION : 55, loss : 0.04070129976840372ITERATION : 56, loss : 0.0407012996670099ITERATION : 57, loss : 0.040701299613126865ITERATION : 58, loss : 0.04070129951170702ITERATION : 59, loss : 0.040701299511345226ITERATION : 60, loss : 0.04070129946095065ITERATION : 61, loss : 0.04070129946095065ITERATION : 62, loss : 0.04070129946095065ITERATION : 63, loss : 0.04070129946095065ITERATION : 64, loss : 0.04070129946095065ITERATION : 65, loss : 0.04070129946095065ITERATION : 66, loss : 0.04070129946095065ITERATION : 67, loss : 0.04070129946095065ITERATION : 68, loss : 0.04070129946095065ITERATION : 69, loss : 0.04070129946095065ITERATION : 70, loss : 0.04070129946095065ITERATION : 71, loss : 0.04070129946095065ITERATION : 72, loss : 0.04070129946095065ITERATION : 73, loss : 0.04070129946095065ITERATION : 74, loss : 0.04070129946095065ITERATION : 75, loss : 0.04070129946095065ITERATION : 76, loss : 0.04070129946095065ITERATION : 77, loss : 0.04070129946095065ITERATION : 78, loss : 0.04070129946095065ITERATION : 79, loss : 0.04070129946095065ITERATION : 80, loss : 0.04070129946095065ITERATION : 81, loss : 0.04070129946095065ITERATION : 82, loss : 0.04070129946095065ITERATION : 83, loss : 0.04070129946095065ITERATION : 84, loss : 0.04070129946095065ITERATION : 85, loss : 0.04070129946095065ITERATION : 86, loss : 0.04070129946095065ITERATION : 87, loss : 0.04070129946095065ITERATION : 88, loss : 0.04070129946095065ITERATION : 89, loss : 0.04070129946095065ITERATION : 90, loss : 0.04070129946095065ITERATION : 91, loss : 0.04070129946095065ITERATION : 92, loss : 0.04070129946095065ITERATION : 93, loss : 0.04070129946095065ITERATION : 94, loss : 0.04070129946095065ITERATION : 95, loss : 0.04070129946095065ITERATION : 96, loss : 0.04070129946095065ITERATION : 97, loss : 0.04070129946095065ITERATION : 98, loss : 0.04070129946095065ITERATION : 99, loss : 0.04070129946095065ITERATION : 100, loss : 0.04070129946095065
ITERATION : 1, loss : 0.0712692850085432ITERATION : 2, loss : 0.05946380834471509ITERATION : 3, loss : 0.05799953841931896ITERATION : 4, loss : 0.057953837858108766ITERATION : 5, loss : 0.05815653215868134ITERATION : 6, loss : 0.0583617194284739ITERATION : 7, loss : 0.058522251238505275ITERATION : 8, loss : 0.05863684507918738ITERATION : 9, loss : 0.05871457769810534ITERATION : 10, loss : 0.05876544282380535ITERATION : 11, loss : 0.05879776461484733ITERATION : 12, loss : 0.058817758770843144ITERATION : 13, loss : 0.05882979194962481ITERATION : 14, loss : 0.05883681054990674ITERATION : 15, loss : 0.058840744179771906ITERATION : 16, loss : 0.058842826641336714ITERATION : 17, loss : 0.0588438298909186ITERATION : 18, loss : 0.058844226944404505ITERATION : 19, loss : 0.05884430118935235ITERATION : 20, loss : 0.0588442177974722ITERATION : 21, loss : 0.058844069780774874ITERATION : 22, loss : 0.05884390706402477ITERATION : 23, loss : 0.05884375412128611ITERATION : 24, loss : 0.058843621392473046ITERATION : 25, loss : 0.05884351171671765ITERATION : 26, loss : 0.05884342385949601ITERATION : 27, loss : 0.05884335514208469ITERATION : 28, loss : 0.05884330233062097ITERATION : 29, loss : 0.05884326229759106ITERATION : 30, loss : 0.05884323229299383ITERATION : 31, loss : 0.05884320998651551ITERATION : 32, loss : 0.05884319347486211ITERATION : 33, loss : 0.058843181466922176ITERATION : 34, loss : 0.05884317270885022ITERATION : 35, loss : 0.05884316625206062ITERATION : 36, loss : 0.05884316163355934ITERATION : 37, loss : 0.05884315837475867ITERATION : 38, loss : 0.058843156002489225ITERATION : 39, loss : 0.058843154325509385ITERATION : 40, loss : 0.058843153112132736ITERATION : 41, loss : 0.058843152205068715ITERATION : 42, loss : 0.058843151589680315ITERATION : 43, loss : 0.05884315114147446ITERATION : 44, loss : 0.058843150826338154ITERATION : 45, loss : 0.058843150591143825ITERATION : 46, loss : 0.058843150447453724ITERATION : 47, loss : 0.05884315033709262ITERATION : 48, loss : 0.05884315026706346ITERATION : 49, loss : 0.058843150222017616ITERATION : 50, loss : 0.05884315019149583ITERATION : 51, loss : 0.0588431501530804ITERATION : 52, loss : 0.05884315010949512ITERATION : 53, loss : 0.05884315010949512ITERATION : 54, loss : 0.05884315010949512ITERATION : 55, loss : 0.05884315010949512ITERATION : 56, loss : 0.05884315010949512ITERATION : 57, loss : 0.05884315010949512ITERATION : 58, loss : 0.05884315010949512ITERATION : 59, loss : 0.05884315010949512ITERATION : 60, loss : 0.05884315010949512ITERATION : 61, loss : 0.05884315010949512ITERATION : 62, loss : 0.05884315010949512ITERATION : 63, loss : 0.05884315010949512ITERATION : 64, loss : 0.05884315010949512ITERATION : 65, loss : 0.05884315010949512ITERATION : 66, loss : 0.05884315010949512ITERATION : 67, loss : 0.05884315010949512ITERATION : 68, loss : 0.05884315010949512ITERATION : 69, loss : 0.05884315010949512ITERATION : 70, loss : 0.05884315010949512ITERATION : 71, loss : 0.05884315010949512ITERATION : 72, loss : 0.05884315010949512ITERATION : 73, loss : 0.05884315010949512ITERATION : 74, loss : 0.05884315010949512ITERATION : 75, loss : 0.05884315010949512ITERATION : 76, loss : 0.05884315010949512ITERATION : 77, loss : 0.05884315010949512ITERATION : 78, loss : 0.05884315010949512ITERATION : 79, loss : 0.05884315010949512ITERATION : 80, loss : 0.05884315010949512ITERATION : 81, loss : 0.05884315010949512ITERATION : 82, loss : 0.05884315010949512ITERATION : 83, loss : 0.05884315010949512ITERATION : 84, loss : 0.05884315010949512ITERATION : 85, loss : 0.05884315010949512ITERATION : 86, loss : 0.05884315010949512ITERATION : 87, loss : 0.05884315010949512ITERATION : 88, loss : 0.05884315010949512ITERATION : 89, loss : 0.05884315010949512ITERATION : 90, loss : 0.05884315010949512ITERATION : 91, loss : 0.05884315010949512ITERATION : 92, loss : 0.05884315010949512ITERATION : 93, loss : 0.05884315010949512ITERATION : 94, loss : 0.05884315010949512ITERATION : 95, loss : 0.05884315010949512ITERATION : 96, loss : 0.05884315010949512ITERATION : 97, loss : 0.05884315010949512ITERATION : 98, loss : 0.05884315010949512ITERATION : 99, loss : 0.05884315010949512ITERATION : 100, loss : 0.05884315010949512
gradient norm in None layer : 0.004493793805191985
gradient norm in None layer : 0.00020266966113293192
gradient norm in None layer : 0.00020575620456854746
gradient norm in None layer : 0.003502022346384224
gradient norm in None layer : 0.00017445105301558812
gradient norm in None layer : 0.00018930128850449222
gradient norm in None layer : 0.0011657154758122998
gradient norm in None layer : 5.019282985106513e-05
gradient norm in None layer : 4.310463321096294e-05
gradient norm in None layer : 0.0010207124971880277
gradient norm in None layer : 4.282407047891803e-05
gradient norm in None layer : 4.3883740024999904e-05
gradient norm in None layer : 0.0002643845297327132
gradient norm in None layer : 7.4734386532311636e-06
gradient norm in None layer : 6.612667750675438e-06
gradient norm in None layer : 0.00025698888729012943
gradient norm in None layer : 1.1557325251418874e-05
gradient norm in None layer : 1.105241270710847e-05
gradient norm in None layer : 0.0004137881644751173
gradient norm in None layer : 5.072969900763655e-06
gradient norm in None layer : 0.0010033276926919266
gradient norm in None layer : 5.162371423845798e-05
gradient norm in None layer : 4.940286219628715e-05
gradient norm in None layer : 0.0010078349819651867
gradient norm in None layer : 9.311073171123823e-05
gradient norm in None layer : 0.00011925291172542826
gradient norm in None layer : 0.0020948422555727994
gradient norm in None layer : 1.3941120628226982e-05
gradient norm in None layer : 0.002925589304804772
gradient norm in None layer : 0.0002399331038730452
gradient norm in None layer : 0.00023112058785434164
gradient norm in None layer : 0.00308978852883629
gradient norm in None layer : 0.0004241495077550823
gradient norm in None layer : 0.0006054072494368517
gradient norm in None layer : 0.000402221559137203
gradient norm in None layer : 0.00010990960946441578
Total gradient norm: 0.007791054879253773
invariance loss : 5.132024929329577, avg_den : 0.35223388671875, density loss : 0.25223388671875, mse loss : 0.034725787496636136, solver time : 124.87530183792114 sec , total loss : 0.040110046312684465, running loss : 0.07688952580585644
Epoch 0/10 , batch 6/12500 
ITERATION : 1, loss : 0.03704854241912873ITERATION : 2, loss : 0.031069925604467457ITERATION : 3, loss : 0.02966562526696873ITERATION : 4, loss : 0.029662802712238208ITERATION : 5, loss : 0.030060660574761377ITERATION : 6, loss : 0.030508582749347ITERATION : 7, loss : 0.030895781599230326ITERATION : 8, loss : 0.031199244023069426ITERATION : 9, loss : 0.031425908539249835ITERATION : 10, loss : 0.031590696814212824ITERATION : 11, loss : 0.03170854801998772ITERATION : 12, loss : 0.031791950447383324ITERATION : 13, loss : 0.031848671611039096ITERATION : 14, loss : 0.03187341470348083ITERATION : 15, loss : 0.03189090573328444ITERATION : 16, loss : 0.03190319464944443ITERATION : 17, loss : 0.031911792560769044ITERATION : 18, loss : 0.03191779078814504ITERATION : 19, loss : 0.03192196710596974ITERATION : 20, loss : 0.0319248709590434ITERATION : 21, loss : 0.031926888207433654ITERATION : 22, loss : 0.031928288698710235ITERATION : 23, loss : 0.03192926062693604ITERATION : 24, loss : 0.031929934990492244ITERATION : 25, loss : 0.03193040283643609ITERATION : 26, loss : 0.03193072740840874ITERATION : 27, loss : 0.031930952592312975ITERATION : 28, loss : 0.03193110884164401ITERATION : 29, loss : 0.03193121727628786ITERATION : 30, loss : 0.03193129252955922ITERATION : 31, loss : 0.03193134475814733ITERATION : 32, loss : 0.031931381018175255ITERATION : 33, loss : 0.03193140621528965ITERATION : 34, loss : 0.031931423708227644ITERATION : 35, loss : 0.031931435877297916ITERATION : 36, loss : 0.03193144431501994ITERATION : 37, loss : 0.03193145019000518ITERATION : 38, loss : 0.03193145427380649ITERATION : 39, loss : 0.031931457124239615ITERATION : 40, loss : 0.03193145910376441ITERATION : 41, loss : 0.03193146049291379ITERATION : 42, loss : 0.03193146145993865ITERATION : 43, loss : 0.03193146212995751ITERATION : 44, loss : 0.031931462585811324ITERATION : 45, loss : 0.031931462907234046ITERATION : 46, loss : 0.031931463121978296ITERATION : 47, loss : 0.0319314632679574ITERATION : 48, loss : 0.03193146337144852ITERATION : 49, loss : 0.03193146344267793ITERATION : 50, loss : 0.03193146349067687ITERATION : 51, loss : 0.03193146352385171ITERATION : 52, loss : 0.03193146354767271ITERATION : 53, loss : 0.031931463569142185ITERATION : 54, loss : 0.03193146357711055ITERATION : 55, loss : 0.03193146359230951ITERATION : 56, loss : 0.03193146359151691ITERATION : 57, loss : 0.03193146359151691ITERATION : 58, loss : 0.03193146359151691ITERATION : 59, loss : 0.03193146359151691ITERATION : 60, loss : 0.03193146359151691ITERATION : 61, loss : 0.03193146359151691ITERATION : 62, loss : 0.03193146359151691ITERATION : 63, loss : 0.03193146359151691ITERATION : 64, loss : 0.03193146359151691ITERATION : 65, loss : 0.03193146359151691ITERATION : 66, loss : 0.03193146359151691ITERATION : 67, loss : 0.03193146359151691ITERATION : 68, loss : 0.03193146359151691ITERATION : 69, loss : 0.03193146359151691ITERATION : 70, loss : 0.03193146359151691ITERATION : 71, loss : 0.03193146359151691ITERATION : 72, loss : 0.03193146359151691ITERATION : 73, loss : 0.03193146359151691ITERATION : 74, loss : 0.03193146359151691ITERATION : 75, loss : 0.03193146359151691ITERATION : 76, loss : 0.03193146359151691ITERATION : 77, loss : 0.03193146359151691ITERATION : 78, loss : 0.03193146359151691ITERATION : 79, loss : 0.03193146359151691ITERATION : 80, loss : 0.03193146359151691ITERATION : 81, loss : 0.03193146359151691ITERATION : 82, loss : 0.03193146359151691ITERATION : 83, loss : 0.03193146359151691ITERATION : 84, loss : 0.03193146359151691ITERATION : 85, loss : 0.03193146359151691ITERATION : 86, loss : 0.03193146359151691ITERATION : 87, loss : 0.03193146359151691ITERATION : 88, loss : 0.03193146359151691ITERATION : 89, loss : 0.03193146359151691ITERATION : 90, loss : 0.03193146359151691ITERATION : 91, loss : 0.03193146359151691ITERATION : 92, loss : 0.03193146359151691ITERATION : 93, loss : 0.03193146359151691ITERATION : 94, loss : 0.03193146359151691ITERATION : 95, loss : 0.03193146359151691ITERATION : 96, loss : 0.03193146359151691ITERATION : 97, loss : 0.03193146359151691ITERATION : 98, loss : 0.03193146359151691ITERATION : 99, loss : 0.03193146359151691ITERATION : 100, loss : 0.03193146359151691
ITERATION : 1, loss : 0.06122391020276266ITERATION : 2, loss : 0.061158890390538546ITERATION : 3, loss : 0.06110118440765117ITERATION : 4, loss : 0.06099778195403263ITERATION : 5, loss : 0.06090600475254562ITERATION : 6, loss : 0.06083767438791141ITERATION : 7, loss : 0.06078949579870115ITERATION : 8, loss : 0.06075598732461282ITERATION : 9, loss : 0.06073263927328398ITERATION : 10, loss : 0.0607162469221083ITERATION : 11, loss : 0.06070463326424131ITERATION : 12, loss : 0.060696333730696585ITERATION : 13, loss : 0.06069035780156265ITERATION : 14, loss : 0.06068602873001505ITERATION : 15, loss : 0.060682877584929ITERATION : 16, loss : 0.06068057550624461ITERATION : 17, loss : 0.06067888902529034ITERATION : 18, loss : 0.060677651259986184ITERATION : 19, loss : 0.060676741490310715ITERATION : 20, loss : 0.06067607214474239ITERATION : 21, loss : 0.060675579375976396ITERATION : 22, loss : 0.060675216436012216ITERATION : 23, loss : 0.0606749491096002ITERATION : 24, loss : 0.060674752132927605ITERATION : 25, loss : 0.06067460701403029ITERATION : 26, loss : 0.06067450008750297ITERATION : 27, loss : 0.060674421297538514ITERATION : 28, loss : 0.06067436325153081ITERATION : 29, loss : 0.06067432050994205ITERATION : 30, loss : 0.06067428907813243ITERATION : 31, loss : 0.06067426592088876ITERATION : 32, loss : 0.06067424886419337ITERATION : 33, loss : 0.060674236317163024ITERATION : 34, loss : 0.06067422706768974ITERATION : 35, loss : 0.060674220259832295ITERATION : 36, loss : 0.06067421526352917ITERATION : 37, loss : 0.060674211594542296ITERATION : 38, loss : 0.06067420890932576ITERATION : 39, loss : 0.06067420691928381ITERATION : 40, loss : 0.060674205450716744ITERATION : 41, loss : 0.06067420434751549ITERATION : 42, loss : 0.06067420359086413ITERATION : 43, loss : 0.060674202988829695ITERATION : 44, loss : 0.06067420255419236ITERATION : 45, loss : 0.06067420223195079ITERATION : 46, loss : 0.060674201994758786ITERATION : 47, loss : 0.060674201822600436ITERATION : 48, loss : 0.0606742017020955ITERATION : 49, loss : 0.06067420160712049ITERATION : 50, loss : 0.0606742015881115ITERATION : 51, loss : 0.060674201542039216ITERATION : 52, loss : 0.060674201541873175ITERATION : 53, loss : 0.060674201541873175ITERATION : 54, loss : 0.060674201541873175ITERATION : 55, loss : 0.060674201541873175ITERATION : 56, loss : 0.060674201541873175ITERATION : 57, loss : 0.060674201541873175ITERATION : 58, loss : 0.060674201541873175ITERATION : 59, loss : 0.060674201541873175ITERATION : 60, loss : 0.060674201541873175ITERATION : 61, loss : 0.060674201541873175ITERATION : 62, loss : 0.060674201541873175ITERATION : 63, loss : 0.060674201541873175ITERATION : 64, loss : 0.060674201541873175ITERATION : 65, loss : 0.060674201541873175ITERATION : 66, loss : 0.060674201541873175ITERATION : 67, loss : 0.060674201541873175ITERATION : 68, loss : 0.060674201541873175ITERATION : 69, loss : 0.060674201541873175ITERATION : 70, loss : 0.060674201541873175ITERATION : 71, loss : 0.060674201541873175ITERATION : 72, loss : 0.060674201541873175ITERATION : 73, loss : 0.060674201541873175ITERATION : 74, loss : 0.060674201541873175ITERATION : 75, loss : 0.060674201541873175ITERATION : 76, loss : 0.060674201541873175ITERATION : 77, loss : 0.060674201541873175ITERATION : 78, loss : 0.060674201541873175ITERATION : 79, loss : 0.060674201541873175ITERATION : 80, loss : 0.060674201541873175ITERATION : 81, loss : 0.060674201541873175ITERATION : 82, loss : 0.060674201541873175ITERATION : 83, loss : 0.060674201541873175ITERATION : 84, loss : 0.060674201541873175ITERATION : 85, loss : 0.060674201541873175ITERATION : 86, loss : 0.060674201541873175ITERATION : 87, loss : 0.060674201541873175ITERATION : 88, loss : 0.060674201541873175ITERATION : 89, loss : 0.060674201541873175ITERATION : 90, loss : 0.060674201541873175ITERATION : 91, loss : 0.060674201541873175ITERATION : 92, loss : 0.060674201541873175ITERATION : 93, loss : 0.060674201541873175ITERATION : 94, loss : 0.060674201541873175ITERATION : 95, loss : 0.060674201541873175ITERATION : 96, loss : 0.060674201541873175ITERATION : 97, loss : 0.060674201541873175ITERATION : 98, loss : 0.060674201541873175ITERATION : 99, loss : 0.060674201541873175ITERATION : 100, loss : 0.060674201541873175
ITERATION : 1, loss : 0.0347224680544795ITERATION : 2, loss : 0.0417422008832139ITERATION : 3, loss : 0.04973215062343343ITERATION : 4, loss : 0.048648356231851674ITERATION : 5, loss : 0.048040694491459555ITERATION : 6, loss : 0.04766387793647632ITERATION : 7, loss : 0.047415236337501394ITERATION : 8, loss : 0.0472447720876917ITERATION : 9, loss : 0.04712507755923297ITERATION : 10, loss : 0.04703974068323417ITERATION : 11, loss : 0.04697829424679836ITERATION : 12, loss : 0.04693376111409707ITERATION : 13, loss : 0.046901347002443444ITERATION : 14, loss : 0.046877687266928855ITERATION : 15, loss : 0.04686038653122246ITERATION : 16, loss : 0.04684772165848598ITERATION : 17, loss : 0.046838444245983574ITERATION : 18, loss : 0.04683164624563018ITERATION : 19, loss : 0.046826664448791515ITERATION : 20, loss : 0.046823013936580095ITERATION : 21, loss : 0.04682033939180237ITERATION : 22, loss : 0.04681838048410154ITERATION : 23, loss : 0.046816945917565106ITERATION : 24, loss : 0.04681589572818573ITERATION : 25, loss : 0.04681512718188491ITERATION : 26, loss : 0.04681456493705086ITERATION : 27, loss : 0.04681415379089991ITERATION : 28, loss : 0.04681385319686529ITERATION : 29, loss : 0.04681363347154295ITERATION : 30, loss : 0.04681347292810884ITERATION : 31, loss : 0.04681335559478706ITERATION : 32, loss : 0.04681326996947382ITERATION : 33, loss : 0.04681320746387636ITERATION : 34, loss : 0.04681316181426542ITERATION : 35, loss : 0.04681312861204384ITERATION : 36, loss : 0.04681310430507708ITERATION : 37, loss : 0.046813086564258796ITERATION : 38, loss : 0.04681307371921943ITERATION : 39, loss : 0.046813064270510865ITERATION : 40, loss : 0.04681305740919481ITERATION : 41, loss : 0.04681305232289543ITERATION : 42, loss : 0.04681304872556303ITERATION : 43, loss : 0.04681304609548765ITERATION : 44, loss : 0.04681304415244092ITERATION : 45, loss : 0.04681304266253251ITERATION : 46, loss : 0.04681304173799231ITERATION : 47, loss : 0.04681304093489822ITERATION : 48, loss : 0.04681304039790494ITERATION : 49, loss : 0.0468130400441304ITERATION : 50, loss : 0.04681303976886368ITERATION : 51, loss : 0.046813039476276644ITERATION : 52, loss : 0.04681303941441386ITERATION : 53, loss : 0.04681303930768302ITERATION : 54, loss : 0.04681303930302123ITERATION : 55, loss : 0.04681303930302123ITERATION : 56, loss : 0.04681303930302123ITERATION : 57, loss : 0.04681303930302123ITERATION : 58, loss : 0.04681303930302123ITERATION : 59, loss : 0.04681303930302123ITERATION : 60, loss : 0.04681303930302123ITERATION : 61, loss : 0.04681303930302123ITERATION : 62, loss : 0.04681303930302123ITERATION : 63, loss : 0.04681303930302123ITERATION : 64, loss : 0.04681303930302123ITERATION : 65, loss : 0.04681303930302123ITERATION : 66, loss : 0.04681303930302123ITERATION : 67, loss : 0.04681303930302123ITERATION : 68, loss : 0.04681303930302123ITERATION : 69, loss : 0.04681303930302123ITERATION : 70, loss : 0.04681303930302123ITERATION : 71, loss : 0.04681303930302123ITERATION : 72, loss : 0.04681303930302123ITERATION : 73, loss : 0.04681303930302123ITERATION : 74, loss : 0.04681303930302123ITERATION : 75, loss : 0.04681303930302123ITERATION : 76, loss : 0.04681303930302123ITERATION : 77, loss : 0.04681303930302123ITERATION : 78, loss : 0.04681303930302123ITERATION : 79, loss : 0.04681303930302123ITERATION : 80, loss : 0.04681303930302123ITERATION : 81, loss : 0.04681303930302123ITERATION : 82, loss : 0.04681303930302123ITERATION : 83, loss : 0.04681303930302123ITERATION : 84, loss : 0.04681303930302123ITERATION : 85, loss : 0.04681303930302123ITERATION : 86, loss : 0.04681303930302123ITERATION : 87, loss : 0.04681303930302123ITERATION : 88, loss : 0.04681303930302123ITERATION : 89, loss : 0.04681303930302123ITERATION : 90, loss : 0.04681303930302123ITERATION : 91, loss : 0.04681303930302123ITERATION : 92, loss : 0.04681303930302123ITERATION : 93, loss : 0.04681303930302123ITERATION : 94, loss : 0.04681303930302123ITERATION : 95, loss : 0.04681303930302123ITERATION : 96, loss : 0.04681303930302123ITERATION : 97, loss : 0.04681303930302123ITERATION : 98, loss : 0.04681303930302123ITERATION : 99, loss : 0.04681303930302123ITERATION : 100, loss : 0.04681303930302123
ITERATION : 1, loss : 0.02920431739361044ITERATION : 2, loss : 0.019583885472067835ITERATION : 3, loss : 0.01706361337490722ITERATION : 4, loss : 0.01979760894504439ITERATION : 5, loss : 0.02315268048093435ITERATION : 6, loss : 0.02376321631401791ITERATION : 7, loss : 0.024327250513651703ITERATION : 8, loss : 0.02479411523348586ITERATION : 9, loss : 0.025158932863032845ITERATION : 10, loss : 0.025434405325910465ITERATION : 11, loss : 0.025637928825038635ITERATION : 12, loss : 0.02578614795597749ITERATION : 13, loss : 0.025893049867683062ITERATION : 14, loss : 0.025969645092448115ITERATION : 15, loss : 0.02602427855148504ITERATION : 16, loss : 0.026063128153609446ITERATION : 17, loss : 0.026090697040644213ITERATION : 18, loss : 0.02611023456827723ITERATION : 19, loss : 0.026124068737170682ITERATION : 20, loss : 0.026133859977011446ITERATION : 21, loss : 0.02614078806171864ITERATION : 22, loss : 0.026145690077907152ITERATION : 23, loss : 0.026149159035281054ITERATION : 24, loss : 0.02615161427254885ITERATION : 25, loss : 0.026153352595242717ITERATION : 26, loss : 0.026154583807930707ITERATION : 27, loss : 0.026155455867714755ITERATION : 28, loss : 0.026156073919728875ITERATION : 29, loss : 0.02615651206919305ITERATION : 30, loss : 0.026156822844146763ITERATION : 31, loss : 0.026157043373460035ITERATION : 32, loss : 0.026157199916316272ITERATION : 33, loss : 0.026157311077424368ITERATION : 34, loss : 0.026157390097064838ITERATION : 35, loss : 0.02615744626368437ITERATION : 36, loss : 0.026157486189062832ITERATION : 37, loss : 0.026157514588334912ITERATION : 38, loss : 0.026157534766457248ITERATION : 39, loss : 0.026157549165336882ITERATION : 40, loss : 0.026157559451417133ITERATION : 41, loss : 0.026157566740999432ITERATION : 42, loss : 0.02615757196480548ITERATION : 43, loss : 0.02615757564351779ITERATION : 44, loss : 0.026157578462738265ITERATION : 45, loss : 0.02615758025458346ITERATION : 46, loss : 0.026157581604078044ITERATION : 47, loss : 0.026157582460694877ITERATION : 48, loss : 0.02615758319632788ITERATION : 49, loss : 0.026157583577305566ITERATION : 50, loss : 0.02615758397080468ITERATION : 51, loss : 0.026157584185630873ITERATION : 52, loss : 0.02615758437878339ITERATION : 53, loss : 0.02615758441789019ITERATION : 54, loss : 0.026157584440657182ITERATION : 55, loss : 0.02615758444801974ITERATION : 56, loss : 0.02615758444801974ITERATION : 57, loss : 0.02615758444801974ITERATION : 58, loss : 0.02615758444801974ITERATION : 59, loss : 0.02615758444801974ITERATION : 60, loss : 0.02615758444801974ITERATION : 61, loss : 0.02615758444801974ITERATION : 62, loss : 0.02615758444801974ITERATION : 63, loss : 0.02615758444801974ITERATION : 64, loss : 0.02615758444801974ITERATION : 65, loss : 0.02615758444801974ITERATION : 66, loss : 0.02615758444801974ITERATION : 67, loss : 0.02615758444801974ITERATION : 68, loss : 0.02615758444801974ITERATION : 69, loss : 0.02615758444801974ITERATION : 70, loss : 0.02615758444801974ITERATION : 71, loss : 0.02615758444801974ITERATION : 72, loss : 0.02615758444801974ITERATION : 73, loss : 0.02615758444801974ITERATION : 74, loss : 0.02615758444801974ITERATION : 75, loss : 0.02615758444801974ITERATION : 76, loss : 0.02615758444801974ITERATION : 77, loss : 0.02615758444801974ITERATION : 78, loss : 0.02615758444801974ITERATION : 79, loss : 0.02615758444801974ITERATION : 80, loss : 0.02615758444801974ITERATION : 81, loss : 0.02615758444801974ITERATION : 82, loss : 0.02615758444801974ITERATION : 83, loss : 0.02615758444801974ITERATION : 84, loss : 0.02615758444801974ITERATION : 85, loss : 0.02615758444801974ITERATION : 86, loss : 0.02615758444801974ITERATION : 87, loss : 0.02615758444801974ITERATION : 88, loss : 0.02615758444801974ITERATION : 89, loss : 0.02615758444801974ITERATION : 90, loss : 0.02615758444801974ITERATION : 91, loss : 0.02615758444801974ITERATION : 92, loss : 0.02615758444801974ITERATION : 93, loss : 0.02615758444801974ITERATION : 94, loss : 0.02615758444801974ITERATION : 95, loss : 0.02615758444801974ITERATION : 96, loss : 0.02615758444801974ITERATION : 97, loss : 0.02615758444801974ITERATION : 98, loss : 0.02615758444801974ITERATION : 99, loss : 0.02615758444801974ITERATION : 100, loss : 0.02615758444801974
ITERATION : 1, loss : 0.02659487889884521ITERATION : 2, loss : 0.025821872343593438ITERATION : 3, loss : 0.02910582538047979ITERATION : 4, loss : 0.03248194733165691ITERATION : 5, loss : 0.035138454139081385ITERATION : 6, loss : 0.03715114420107954ITERATION : 7, loss : 0.03864011536167674ITERATION : 8, loss : 0.03972805511076711ITERATION : 9, loss : 0.0405172936852834ITERATION : 10, loss : 0.04108730038721074ITERATION : 11, loss : 0.041497779671974454ITERATION : 12, loss : 0.041792798226800806ITERATION : 13, loss : 0.042004543842779074ITERATION : 14, loss : 0.042156374467778826ITERATION : 15, loss : 0.042265168257962574ITERATION : 16, loss : 0.04234308525511554ITERATION : 17, loss : 0.04239886870789711ITERATION : 18, loss : 0.0424387957947294ITERATION : 19, loss : 0.04246736848349841ITERATION : 20, loss : 0.04248781319040524ITERATION : 21, loss : 0.042502440901652096ITERATION : 22, loss : 0.042512906086890706ITERATION : 23, loss : 0.04252039309511484ITERATION : 24, loss : 0.0425257494632626ITERATION : 25, loss : 0.04252958156145677ITERATION : 26, loss : 0.042532323309166085ITERATION : 27, loss : 0.04253428480274338ITERATION : 28, loss : 0.042535688214692065ITERATION : 29, loss : 0.04253669239958948ITERATION : 30, loss : 0.042537410913427026ITERATION : 31, loss : 0.04253792507603928ITERATION : 32, loss : 0.04253829301676033ITERATION : 33, loss : 0.04253855634912727ITERATION : 34, loss : 0.04253874482867061ITERATION : 35, loss : 0.04253887973691904ITERATION : 36, loss : 0.042538976300374974ITERATION : 37, loss : 0.042539045412821164ITERATION : 38, loss : 0.0425390948594183ITERATION : 39, loss : 0.042539130240105257ITERATION : 40, loss : 0.04253915562341225ITERATION : 41, loss : 0.042539173770302856ITERATION : 42, loss : 0.04253918677660826ITERATION : 43, loss : 0.04253919600430877ITERATION : 44, loss : 0.04253920272011202ITERATION : 45, loss : 0.042539207488562436ITERATION : 46, loss : 0.0425392109426838ITERATION : 47, loss : 0.04253921342796508ITERATION : 48, loss : 0.042539215160121296ITERATION : 49, loss : 0.04253921647325235ITERATION : 50, loss : 0.04253921734114966ITERATION : 51, loss : 0.04253921799352085ITERATION : 52, loss : 0.04253921843033447ITERATION : 53, loss : 0.042539218737981874ITERATION : 54, loss : 0.042539218977431265ITERATION : 55, loss : 0.04253921916011369ITERATION : 56, loss : 0.042539219234292304ITERATION : 57, loss : 0.042539219337684904ITERATION : 58, loss : 0.042539219359448203ITERATION : 59, loss : 0.04253921942101155ITERATION : 60, loss : 0.04253921942101155ITERATION : 61, loss : 0.04253921942101155ITERATION : 62, loss : 0.04253921942101155ITERATION : 63, loss : 0.04253921942101155ITERATION : 64, loss : 0.04253921942101155ITERATION : 65, loss : 0.04253921942101155ITERATION : 66, loss : 0.04253921942101155ITERATION : 67, loss : 0.04253921942101155ITERATION : 68, loss : 0.04253921942101155ITERATION : 69, loss : 0.04253921942101155ITERATION : 70, loss : 0.04253921942101155ITERATION : 71, loss : 0.04253921942101155ITERATION : 72, loss : 0.04253921942101155ITERATION : 73, loss : 0.04253921942101155ITERATION : 74, loss : 0.04253921942101155ITERATION : 75, loss : 0.04253921942101155ITERATION : 76, loss : 0.04253921942101155ITERATION : 77, loss : 0.04253921942101155ITERATION : 78, loss : 0.04253921942101155ITERATION : 79, loss : 0.04253921942101155ITERATION : 80, loss : 0.04253921942101155ITERATION : 81, loss : 0.04253921942101155ITERATION : 82, loss : 0.04253921942101155ITERATION : 83, loss : 0.04253921942101155ITERATION : 84, loss : 0.04253921942101155ITERATION : 85, loss : 0.04253921942101155ITERATION : 86, loss : 0.04253921942101155ITERATION : 87, loss : 0.04253921942101155ITERATION : 88, loss : 0.04253921942101155ITERATION : 89, loss : 0.04253921942101155ITERATION : 90, loss : 0.04253921942101155ITERATION : 91, loss : 0.04253921942101155ITERATION : 92, loss : 0.04253921942101155ITERATION : 93, loss : 0.04253921942101155ITERATION : 94, loss : 0.04253921942101155ITERATION : 95, loss : 0.04253921942101155ITERATION : 96, loss : 0.04253921942101155ITERATION : 97, loss : 0.04253921942101155ITERATION : 98, loss : 0.04253921942101155ITERATION : 99, loss : 0.04253921942101155ITERATION : 100, loss : 0.04253921942101155
ITERATION : 1, loss : 0.017498715972840726ITERATION : 2, loss : 0.020689135835627693ITERATION : 3, loss : 0.02596788512635158ITERATION : 4, loss : 0.028241371683183265ITERATION : 5, loss : 0.02722297308945313ITERATION : 6, loss : 0.02675335510477898ITERATION : 7, loss : 0.02652154468927574ITERATION : 8, loss : 0.02639996206254932ITERATION : 9, loss : 0.026332819752400635ITERATION : 10, loss : 0.02629419650929235ITERATION : 11, loss : 0.026271298212602674ITERATION : 12, loss : 0.02625743525467083ITERATION : 13, loss : 0.026248926550807208ITERATION : 14, loss : 0.026243660317869937ITERATION : 15, loss : 0.026240386205232204ITERATION : 16, loss : 0.02623834697834509ITERATION : 17, loss : 0.02623707725339089ITERATION : 18, loss : 0.026236288083751086ITERATION : 19, loss : 0.026235799153913834ITERATION : 20, loss : 0.02623549751323438ITERATION : 21, loss : 0.02623531247255994ITERATION : 22, loss : 0.026235199754220628ITERATION : 23, loss : 0.02623513172505823ITERATION : 24, loss : 0.026235091067309743ITERATION : 25, loss : 0.026235067138625126ITERATION : 26, loss : 0.026235053267176185ITERATION : 27, loss : 0.02623504547193714ITERATION : 28, loss : 0.026235041193610347ITERATION : 29, loss : 0.026235038976664446ITERATION : 30, loss : 0.026235037945473133ITERATION : 31, loss : 0.026235037560392974ITERATION : 32, loss : 0.026235037483327277ITERATION : 33, loss : 0.026235037581280502ITERATION : 34, loss : 0.026235037726680566ITERATION : 35, loss : 0.026235037879077376ITERATION : 36, loss : 0.026235038032354118ITERATION : 37, loss : 0.026235038160263686ITERATION : 38, loss : 0.026235038254594674ITERATION : 39, loss : 0.026235038341928314ITERATION : 40, loss : 0.026235038394555408ITERATION : 41, loss : 0.02623503845964427ITERATION : 42, loss : 0.026235038483946937ITERATION : 43, loss : 0.026235038516126113ITERATION : 44, loss : 0.026235038541568913ITERATION : 45, loss : 0.026235038559332118ITERATION : 46, loss : 0.02623503856467024ITERATION : 47, loss : 0.02623503858807975ITERATION : 48, loss : 0.026235038580870628ITERATION : 49, loss : 0.026235038580870628ITERATION : 50, loss : 0.026235038580870628ITERATION : 51, loss : 0.026235038580870628ITERATION : 52, loss : 0.026235038580870628ITERATION : 53, loss : 0.026235038580870628ITERATION : 54, loss : 0.026235038580870628ITERATION : 55, loss : 0.026235038580870628ITERATION : 56, loss : 0.026235038580870628ITERATION : 57, loss : 0.026235038580870628ITERATION : 58, loss : 0.026235038580870628ITERATION : 59, loss : 0.026235038580870628ITERATION : 60, loss : 0.026235038580870628ITERATION : 61, loss : 0.026235038580870628ITERATION : 62, loss : 0.026235038580870628ITERATION : 63, loss : 0.026235038580870628ITERATION : 64, loss : 0.026235038580870628ITERATION : 65, loss : 0.026235038580870628ITERATION : 66, loss : 0.026235038580870628ITERATION : 67, loss : 0.026235038580870628ITERATION : 68, loss : 0.026235038580870628ITERATION : 69, loss : 0.026235038580870628ITERATION : 70, loss : 0.026235038580870628ITERATION : 71, loss : 0.026235038580870628ITERATION : 72, loss : 0.026235038580870628ITERATION : 73, loss : 0.026235038580870628ITERATION : 74, loss : 0.026235038580870628ITERATION : 75, loss : 0.026235038580870628ITERATION : 76, loss : 0.026235038580870628ITERATION : 77, loss : 0.026235038580870628ITERATION : 78, loss : 0.026235038580870628ITERATION : 79, loss : 0.026235038580870628ITERATION : 80, loss : 0.026235038580870628ITERATION : 81, loss : 0.026235038580870628ITERATION : 82, loss : 0.026235038580870628ITERATION : 83, loss : 0.026235038580870628ITERATION : 84, loss : 0.026235038580870628ITERATION : 85, loss : 0.026235038580870628ITERATION : 86, loss : 0.026235038580870628ITERATION : 87, loss : 0.026235038580870628ITERATION : 88, loss : 0.026235038580870628ITERATION : 89, loss : 0.026235038580870628ITERATION : 90, loss : 0.026235038580870628ITERATION : 91, loss : 0.026235038580870628ITERATION : 92, loss : 0.026235038580870628ITERATION : 93, loss : 0.026235038580870628ITERATION : 94, loss : 0.026235038580870628ITERATION : 95, loss : 0.026235038580870628ITERATION : 96, loss : 0.026235038580870628ITERATION : 97, loss : 0.026235038580870628ITERATION : 98, loss : 0.026235038580870628ITERATION : 99, loss : 0.026235038580870628ITERATION : 100, loss : 0.026235038580870628
ITERATION : 1, loss : 0.0614061641668718ITERATION : 2, loss : 0.04888445870611212ITERATION : 3, loss : 0.04485787083261284ITERATION : 4, loss : 0.04315027526903486ITERATION : 5, loss : 0.04237702249376091ITERATION : 6, loss : 0.04201626098016107ITERATION : 7, loss : 0.0418468306040018ITERATION : 8, loss : 0.04176851809173768ITERATION : 9, loss : 0.04173396657957709ITERATION : 10, loss : 0.04172022854053274ITERATION : 11, loss : 0.04171607248330231ITERATION : 12, loss : 0.041716029808376555ITERATION : 13, loss : 0.04171749828149506ITERATION : 14, loss : 0.041719298761125914ITERATION : 15, loss : 0.04172094708428996ITERATION : 16, loss : 0.04172228675682359ITERATION : 17, loss : 0.041723306222322114ITERATION : 18, loss : 0.0417240489279068ITERATION : 19, loss : 0.04172457304444578ITERATION : 20, loss : 0.04172493325040747ITERATION : 21, loss : 0.04172517492214914ITERATION : 22, loss : 0.041725333324935464ITERATION : 23, loss : 0.04172543447195765ITERATION : 24, loss : 0.04172549705643167ITERATION : 25, loss : 0.04172553416654514ITERATION : 26, loss : 0.04172555487824283ITERATION : 27, loss : 0.041725565277320596ITERATION : 28, loss : 0.04172556940598417ITERATION : 29, loss : 0.04172556992274ITERATION : 30, loss : 0.041725568480835154ITERATION : 31, loss : 0.04172556608572407ITERATION : 32, loss : 0.041725563388688956ITERATION : 33, loss : 0.04172556074339842ITERATION : 34, loss : 0.04172555833153826ITERATION : 35, loss : 0.04172555623023177ITERATION : 36, loss : 0.04172555444394849ITERATION : 37, loss : 0.04172555296943583ITERATION : 38, loss : 0.04172555179353255ITERATION : 39, loss : 0.04172555082839843ITERATION : 40, loss : 0.04172555003637382ITERATION : 41, loss : 0.041725549376707714ITERATION : 42, loss : 0.04172554887302475ITERATION : 43, loss : 0.041725548447563855ITERATION : 44, loss : 0.04172554813656507ITERATION : 45, loss : 0.04172554788041667ITERATION : 46, loss : 0.04172554769020075ITERATION : 47, loss : 0.04172554752518146ITERATION : 48, loss : 0.04172554741378167ITERATION : 49, loss : 0.04172554732340395ITERATION : 50, loss : 0.04172554726551756ITERATION : 51, loss : 0.04172554720385962ITERATION : 52, loss : 0.041725547180765245ITERATION : 53, loss : 0.041725547149993415ITERATION : 54, loss : 0.04172554715075437ITERATION : 55, loss : 0.041725547150157194ITERATION : 56, loss : 0.041725547150157194ITERATION : 57, loss : 0.041725547150157194ITERATION : 58, loss : 0.041725547150157194ITERATION : 59, loss : 0.041725547150157194ITERATION : 60, loss : 0.041725547150157194ITERATION : 61, loss : 0.041725547150157194ITERATION : 62, loss : 0.041725547150157194ITERATION : 63, loss : 0.041725547150157194ITERATION : 64, loss : 0.041725547150157194ITERATION : 65, loss : 0.041725547150157194ITERATION : 66, loss : 0.041725547150157194ITERATION : 67, loss : 0.041725547150157194ITERATION : 68, loss : 0.041725547150157194ITERATION : 69, loss : 0.041725547150157194ITERATION : 70, loss : 0.041725547150157194ITERATION : 71, loss : 0.041725547150157194ITERATION : 72, loss : 0.041725547150157194ITERATION : 73, loss : 0.041725547150157194ITERATION : 74, loss : 0.041725547150157194ITERATION : 75, loss : 0.041725547150157194ITERATION : 76, loss : 0.041725547150157194ITERATION : 77, loss : 0.041725547150157194ITERATION : 78, loss : 0.041725547150157194ITERATION : 79, loss : 0.041725547150157194ITERATION : 80, loss : 0.041725547150157194ITERATION : 81, loss : 0.041725547150157194ITERATION : 82, loss : 0.041725547150157194ITERATION : 83, loss : 0.041725547150157194ITERATION : 84, loss : 0.041725547150157194ITERATION : 85, loss : 0.041725547150157194ITERATION : 86, loss : 0.041725547150157194ITERATION : 87, loss : 0.041725547150157194ITERATION : 88, loss : 0.041725547150157194ITERATION : 89, loss : 0.041725547150157194ITERATION : 90, loss : 0.041725547150157194ITERATION : 91, loss : 0.041725547150157194ITERATION : 92, loss : 0.041725547150157194ITERATION : 93, loss : 0.041725547150157194ITERATION : 94, loss : 0.041725547150157194ITERATION : 95, loss : 0.041725547150157194ITERATION : 96, loss : 0.041725547150157194ITERATION : 97, loss : 0.041725547150157194ITERATION : 98, loss : 0.041725547150157194ITERATION : 99, loss : 0.041725547150157194ITERATION : 100, loss : 0.041725547150157194
ITERATION : 1, loss : 0.05788439059490911ITERATION : 2, loss : 0.06408971341177916ITERATION : 3, loss : 0.06696161470340964ITERATION : 4, loss : 0.0685830387038397ITERATION : 5, loss : 0.06962163291726202ITERATION : 6, loss : 0.07032537081755276ITERATION : 7, loss : 0.07081343317108439ITERATION : 8, loss : 0.07115499551264869ITERATION : 9, loss : 0.07139477347840374ITERATION : 10, loss : 0.07156321434768398ITERATION : 11, loss : 0.07168151716707068ITERATION : 12, loss : 0.07176456663342547ITERATION : 13, loss : 0.07182283954607885ITERATION : 14, loss : 0.07186371063889195ITERATION : 15, loss : 0.0718923672480453ITERATION : 16, loss : 0.07191245493584539ITERATION : 17, loss : 0.07192653359690739ITERATION : 18, loss : 0.07193639966782867ITERATION : 19, loss : 0.07194331309909047ITERATION : 20, loss : 0.07194815736564962ITERATION : 21, loss : 0.07195155165597904ITERATION : 22, loss : 0.07195392996718547ITERATION : 23, loss : 0.07195559640482684ITERATION : 24, loss : 0.07195676403907537ITERATION : 25, loss : 0.07195758222438425ITERATION : 26, loss : 0.07195815555092777ITERATION : 27, loss : 0.07195855732226494ITERATION : 28, loss : 0.0719588388666427ITERATION : 29, loss : 0.07195903617411446ITERATION : 30, loss : 0.07195917444930182ITERATION : 31, loss : 0.07195927136969266ITERATION : 32, loss : 0.07195933929590037ITERATION : 33, loss : 0.07195938688508452ITERATION : 34, loss : 0.07195942026103773ITERATION : 35, loss : 0.07195944366642157ITERATION : 36, loss : 0.07195946003854913ITERATION : 37, loss : 0.07195947155335476ITERATION : 38, loss : 0.071959479580104ITERATION : 39, loss : 0.07195948524333272ITERATION : 40, loss : 0.07195948917338314ITERATION : 41, loss : 0.07195949196247477ITERATION : 42, loss : 0.07195949388053022ITERATION : 43, loss : 0.07195949526507968ITERATION : 44, loss : 0.07195949618811551ITERATION : 45, loss : 0.07195949688400728ITERATION : 46, loss : 0.07195949733065359ITERATION : 47, loss : 0.07195949767368537ITERATION : 48, loss : 0.07195949788599192ITERATION : 49, loss : 0.07195949805592175ITERATION : 50, loss : 0.07195949815970754ITERATION : 51, loss : 0.07195949825125468ITERATION : 52, loss : 0.07195949827248714ITERATION : 53, loss : 0.07195949828305255ITERATION : 54, loss : 0.07195949828305255ITERATION : 55, loss : 0.07195949828305255ITERATION : 56, loss : 0.07195949828305255ITERATION : 57, loss : 0.07195949828305255ITERATION : 58, loss : 0.07195949828305255ITERATION : 59, loss : 0.07195949828305255ITERATION : 60, loss : 0.07195949828305255ITERATION : 61, loss : 0.07195949828305255ITERATION : 62, loss : 0.07195949828305255ITERATION : 63, loss : 0.07195949828305255ITERATION : 64, loss : 0.07195949828305255ITERATION : 65, loss : 0.07195949828305255ITERATION : 66, loss : 0.07195949828305255ITERATION : 67, loss : 0.07195949828305255ITERATION : 68, loss : 0.07195949828305255ITERATION : 69, loss : 0.07195949828305255ITERATION : 70, loss : 0.07195949828305255ITERATION : 71, loss : 0.07195949828305255ITERATION : 72, loss : 0.07195949828305255ITERATION : 73, loss : 0.07195949828305255ITERATION : 74, loss : 0.07195949828305255ITERATION : 75, loss : 0.07195949828305255ITERATION : 76, loss : 0.07195949828305255ITERATION : 77, loss : 0.07195949828305255ITERATION : 78, loss : 0.07195949828305255ITERATION : 79, loss : 0.07195949828305255ITERATION : 80, loss : 0.07195949828305255ITERATION : 81, loss : 0.07195949828305255ITERATION : 82, loss : 0.07195949828305255ITERATION : 83, loss : 0.07195949828305255ITERATION : 84, loss : 0.07195949828305255ITERATION : 85, loss : 0.07195949828305255ITERATION : 86, loss : 0.07195949828305255ITERATION : 87, loss : 0.07195949828305255ITERATION : 88, loss : 0.07195949828305255ITERATION : 89, loss : 0.07195949828305255ITERATION : 90, loss : 0.07195949828305255ITERATION : 91, loss : 0.07195949828305255ITERATION : 92, loss : 0.07195949828305255ITERATION : 93, loss : 0.07195949828305255ITERATION : 94, loss : 0.07195949828305255ITERATION : 95, loss : 0.07195949828305255ITERATION : 96, loss : 0.07195949828305255ITERATION : 97, loss : 0.07195949828305255ITERATION : 98, loss : 0.07195949828305255ITERATION : 99, loss : 0.07195949828305255ITERATION : 100, loss : 0.07195949828305255
gradient norm in None layer : 0.001625419500064491
gradient norm in None layer : 5.462993309201076e-05
gradient norm in None layer : 8.413862375898255e-05
gradient norm in None layer : 0.0009683647542291319
gradient norm in None layer : 7.165957173753695e-05
gradient norm in None layer : 8.732384675273197e-05
gradient norm in None layer : 0.0003738642973584339
gradient norm in None layer : 1.64343898537012e-05
gradient norm in None layer : 1.321567013940492e-05
gradient norm in None layer : 0.00034528559681594
gradient norm in None layer : 1.359493589898009e-05
gradient norm in None layer : 1.4394489685494407e-05
gradient norm in None layer : 0.00010450354344440714
gradient norm in None layer : 2.8481994818952874e-06
gradient norm in None layer : 2.3436450727630694e-06
gradient norm in None layer : 9.89419398587193e-05
gradient norm in None layer : 4.253183836201208e-06
gradient norm in None layer : 4.27215103803888e-06
gradient norm in None layer : 0.00014128000355501092
gradient norm in None layer : 1.841432773977433e-06
gradient norm in None layer : 0.0003477059448234891
gradient norm in None layer : 1.964466188840212e-05
gradient norm in None layer : 1.81035229970254e-05
gradient norm in None layer : 0.0003755625427606988
gradient norm in None layer : 3.798039209442144e-05
gradient norm in None layer : 5.158883711302364e-05
gradient norm in None layer : 0.0008486088167652852
gradient norm in None layer : 8.915437156846907e-06
gradient norm in None layer : 0.001153066914442962
gradient norm in None layer : 0.00010623250204879096
gradient norm in None layer : 0.00010969958329167216
gradient norm in None layer : 0.0013748666293529699
gradient norm in None layer : 0.00017424137418432656
gradient norm in None layer : 0.0002413677371568572
gradient norm in None layer : 0.00015967967021909374
gradient norm in None layer : 4.058912537418458e-05
Total gradient norm: 0.0028721047112735725
invariance loss : 4.634411141617356, avg_den : 0.38238525390625, density loss : 0.28238525390625, mse loss : 0.04350444903994037, solver time : 116.11855578422546 sec , total loss : 0.04842124543546398, running loss : 0.07214481241079101
Epoch 0/10 , batch 7/12500 
ITERATION : 1, loss : 0.02829996255281729ITERATION : 2, loss : 0.04358894757842766ITERATION : 3, loss : 0.056590278833416725ITERATION : 4, loss : 0.06593265609175927ITERATION : 5, loss : 0.07245031890419197ITERATION : 6, loss : 0.07698183511306235ITERATION : 7, loss : 0.08014253350506392ITERATION : 8, loss : 0.08235701792436742ITERATION : 9, loss : 0.08391505829621279ITERATION : 10, loss : 0.08501503873076827ITERATION : 11, loss : 0.0857937309298497ITERATION : 12, loss : 0.08634611428962662ITERATION : 13, loss : 0.08673856498276722ITERATION : 14, loss : 0.08701770759325653ITERATION : 15, loss : 0.08721642397413142ITERATION : 16, loss : 0.08735797388645182ITERATION : 17, loss : 0.08745884885395608ITERATION : 18, loss : 0.08753076083091282ITERATION : 19, loss : 0.0875820380839921ITERATION : 20, loss : 0.08761860819604436ITERATION : 21, loss : 0.08764469290825673ITERATION : 22, loss : 0.0876633002489644ITERATION : 23, loss : 0.08767657472970579ITERATION : 24, loss : 0.08768604524465502ITERATION : 25, loss : 0.08769280214589657ITERATION : 26, loss : 0.08769762303869386ITERATION : 27, loss : 0.08770106280321488ITERATION : 28, loss : 0.08770351711854166ITERATION : 29, loss : 0.08770526832645469ITERATION : 30, loss : 0.0877065178736292ITERATION : 31, loss : 0.08770740946765537ITERATION : 32, loss : 0.08770804566654027ITERATION : 33, loss : 0.08770849961039154ITERATION : 34, loss : 0.08770882354137222ITERATION : 35, loss : 0.08770905465962829ITERATION : 36, loss : 0.08770921960020847ITERATION : 37, loss : 0.08770933722954122ITERATION : 38, loss : 0.08770942123039538ITERATION : 39, loss : 0.0877094811373644ITERATION : 40, loss : 0.08770952399337667ITERATION : 41, loss : 0.08770955451013064ITERATION : 42, loss : 0.08770957628776686ITERATION : 43, loss : 0.08770959181377051ITERATION : 44, loss : 0.08770960285108119ITERATION : 45, loss : 0.08770961074788515ITERATION : 46, loss : 0.08770961643438537ITERATION : 47, loss : 0.08770962043377684ITERATION : 48, loss : 0.08770962331117955ITERATION : 49, loss : 0.08770962535666485ITERATION : 50, loss : 0.08770962681923686ITERATION : 51, loss : 0.08770962782244664ITERATION : 52, loss : 0.08770962851700063ITERATION : 53, loss : 0.08770962898476682ITERATION : 54, loss : 0.0877096293385209ITERATION : 55, loss : 0.0877096296685547ITERATION : 56, loss : 0.08770962982566874ITERATION : 57, loss : 0.087709629963655ITERATION : 58, loss : 0.08770963008631329ITERATION : 59, loss : 0.08770963009165107ITERATION : 60, loss : 0.08770963009165107ITERATION : 61, loss : 0.08770963009165107ITERATION : 62, loss : 0.08770963009165107ITERATION : 63, loss : 0.08770963009165107ITERATION : 64, loss : 0.08770963009165107ITERATION : 65, loss : 0.08770963009165107ITERATION : 66, loss : 0.08770963009165107ITERATION : 67, loss : 0.08770963009165107ITERATION : 68, loss : 0.08770963009165107ITERATION : 69, loss : 0.08770963009165107ITERATION : 70, loss : 0.08770963009165107ITERATION : 71, loss : 0.08770963009165107ITERATION : 72, loss : 0.08770963009165107ITERATION : 73, loss : 0.08770963009165107ITERATION : 74, loss : 0.08770963009165107ITERATION : 75, loss : 0.08770963009165107ITERATION : 76, loss : 0.08770963009165107ITERATION : 77, loss : 0.08770963009165107ITERATION : 78, loss : 0.08770963009165107ITERATION : 79, loss : 0.08770963009165107ITERATION : 80, loss : 0.08770963009165107ITERATION : 81, loss : 0.08770963009165107ITERATION : 82, loss : 0.08770963009165107ITERATION : 83, loss : 0.08770963009165107ITERATION : 84, loss : 0.08770963009165107ITERATION : 85, loss : 0.08770963009165107ITERATION : 86, loss : 0.08770963009165107ITERATION : 87, loss : 0.08770963009165107ITERATION : 88, loss : 0.08770963009165107ITERATION : 89, loss : 0.08770963009165107ITERATION : 90, loss : 0.08770963009165107ITERATION : 91, loss : 0.08770963009165107ITERATION : 92, loss : 0.08770963009165107ITERATION : 93, loss : 0.08770963009165107ITERATION : 94, loss : 0.08770963009165107ITERATION : 95, loss : 0.08770963009165107ITERATION : 96, loss : 0.08770963009165107ITERATION : 97, loss : 0.08770963009165107ITERATION : 98, loss : 0.08770963009165107ITERATION : 99, loss : 0.08770963009165107ITERATION : 100, loss : 0.08770963009165107
ITERATION : 1, loss : 0.03108845980649655ITERATION : 2, loss : 0.019893659773753788ITERATION : 3, loss : 0.013673415231043498ITERATION : 4, loss : 0.010831019660566087ITERATION : 5, loss : 0.009415413504807768ITERATION : 6, loss : 0.008681711060006562ITERATION : 7, loss : 0.00829071386937353ITERATION : 8, loss : 0.008077168645004334ITERATION : 9, loss : 0.007957680894414931ITERATION : 10, loss : 0.00788912636009649ITERATION : 11, loss : 0.007848750312237062ITERATION : 12, loss : 0.007824319142650762ITERATION : 13, loss : 0.007809130512556547ITERATION : 14, loss : 0.007799438221747208ITERATION : 15, loss : 0.007793102243791553ITERATION : 16, loss : 0.0077888707243658995ITERATION : 17, loss : 0.007785992570244207ITERATION : 18, loss : 0.007784005196262568ITERATION : 19, loss : 0.007782616195062471ITERATION : 20, loss : 0.007781636173956756ITERATION : 21, loss : 0.00778093963345046ITERATION : 22, loss : 0.007780441816196584ITERATION : 23, loss : 0.0077800845595525905ITERATION : 24, loss : 0.00777982737704809ITERATION : 25, loss : 0.007779641797954807ITERATION : 26, loss : 0.007779507669193426ITERATION : 27, loss : 0.007779410602970682ITERATION : 28, loss : 0.007779340294098488ITERATION : 29, loss : 0.007779289333022072ITERATION : 30, loss : 0.007779252378719766ITERATION : 31, loss : 0.007779225570655041ITERATION : 32, loss : 0.007779206113975246ITERATION : 33, loss : 0.0077791919934193385ITERATION : 34, loss : 0.007779181745822841ITERATION : 35, loss : 0.007779174302731409ITERATION : 36, loss : 0.007779168905755728ITERATION : 37, loss : 0.007779164982575577ITERATION : 38, loss : 0.0077791621374919285ITERATION : 39, loss : 0.007779160068281778ITERATION : 40, loss : 0.00777915856975664ITERATION : 41, loss : 0.0077791574788184294ITERATION : 42, loss : 0.007779156689634867ITERATION : 43, loss : 0.0077791561147285354ITERATION : 44, loss : 0.007779155698577148ITERATION : 45, loss : 0.007779155393450523ITERATION : 46, loss : 0.007779155174316261ITERATION : 47, loss : 0.007779155014276454ITERATION : 48, loss : 0.0077791548984868005ITERATION : 49, loss : 0.007779154814593568ITERATION : 50, loss : 0.007779154754486917ITERATION : 51, loss : 0.007779154712308792ITERATION : 52, loss : 0.007779154680022067ITERATION : 53, loss : 0.007779154657255948ITERATION : 54, loss : 0.007779154637485754ITERATION : 55, loss : 0.0077791546264436824ITERATION : 56, loss : 0.007779154616923047ITERATION : 57, loss : 0.007779154612387164ITERATION : 58, loss : 0.007779154604620317ITERATION : 59, loss : 0.007779154604871931ITERATION : 60, loss : 0.007779154600235276ITERATION : 61, loss : 0.0077791546003186844ITERATION : 62, loss : 0.0077791546003186844ITERATION : 63, loss : 0.0077791546003186844ITERATION : 64, loss : 0.0077791546003186844ITERATION : 65, loss : 0.0077791546003186844ITERATION : 66, loss : 0.0077791546003186844ITERATION : 67, loss : 0.0077791546003186844ITERATION : 68, loss : 0.0077791546003186844ITERATION : 69, loss : 0.0077791546003186844ITERATION : 70, loss : 0.0077791546003186844ITERATION : 71, loss : 0.0077791546003186844ITERATION : 72, loss : 0.0077791546003186844ITERATION : 73, loss : 0.0077791546003186844ITERATION : 74, loss : 0.0077791546003186844ITERATION : 75, loss : 0.0077791546003186844ITERATION : 76, loss : 0.0077791546003186844ITERATION : 77, loss : 0.0077791546003186844ITERATION : 78, loss : 0.0077791546003186844ITERATION : 79, loss : 0.0077791546003186844ITERATION : 80, loss : 0.0077791546003186844ITERATION : 81, loss : 0.0077791546003186844ITERATION : 82, loss : 0.0077791546003186844ITERATION : 83, loss : 0.0077791546003186844ITERATION : 84, loss : 0.0077791546003186844ITERATION : 85, loss : 0.0077791546003186844ITERATION : 86, loss : 0.0077791546003186844ITERATION : 87, loss : 0.0077791546003186844ITERATION : 88, loss : 0.0077791546003186844ITERATION : 89, loss : 0.0077791546003186844ITERATION : 90, loss : 0.0077791546003186844ITERATION : 91, loss : 0.0077791546003186844ITERATION : 92, loss : 0.0077791546003186844ITERATION : 93, loss : 0.0077791546003186844ITERATION : 94, loss : 0.0077791546003186844ITERATION : 95, loss : 0.0077791546003186844ITERATION : 96, loss : 0.0077791546003186844ITERATION : 97, loss : 0.0077791546003186844ITERATION : 98, loss : 0.0077791546003186844ITERATION : 99, loss : 0.0077791546003186844ITERATION : 100, loss : 0.0077791546003186844
ITERATION : 1, loss : 0.019434414102489382ITERATION : 2, loss : 0.017140805793289884ITERATION : 3, loss : 0.01662999072319812ITERATION : 4, loss : 0.016521807849050798ITERATION : 5, loss : 0.016553080931883995ITERATION : 6, loss : 0.01662652808203847ITERATION : 7, loss : 0.016703985234659484ITERATION : 8, loss : 0.01677177182737201ITERATION : 9, loss : 0.016826479833784533ITERATION : 10, loss : 0.016868762991930245ITERATION : 11, loss : 0.01690061488658945ITERATION : 12, loss : 0.016924223366592897ITERATION : 13, loss : 0.016941536871645287ITERATION : 14, loss : 0.01695414338055641ITERATION : 15, loss : 0.016963277692147026ITERATION : 16, loss : 0.016969873689075602ITERATION : 17, loss : 0.01697462542413439ITERATION : 18, loss : 0.01697804285600124ITERATION : 19, loss : 0.01698049775544944ITERATION : 20, loss : 0.016982259758433132ITERATION : 21, loss : 0.016983523686752468ITERATION : 22, loss : 0.016984429954653813ITERATION : 23, loss : 0.016985079571627194ITERATION : 24, loss : 0.016985545140100387ITERATION : 25, loss : 0.016985878744471758ITERATION : 26, loss : 0.01698611776483115ITERATION : 27, loss : 0.016986288999616312ITERATION : 28, loss : 0.01698641168158596ITERATION : 29, loss : 0.01698649956268137ITERATION : 30, loss : 0.016986562515323156ITERATION : 31, loss : 0.016986607604917846ITERATION : 32, loss : 0.016986639905799565ITERATION : 33, loss : 0.01698666304653989ITERATION : 34, loss : 0.016986679623429268ITERATION : 35, loss : 0.016986691501845293ITERATION : 36, loss : 0.016986700002613403ITERATION : 37, loss : 0.016986706098060085ITERATION : 38, loss : 0.01698671045823051ITERATION : 39, loss : 0.016986713603071885ITERATION : 40, loss : 0.016986715835285673ITERATION : 41, loss : 0.016986717452418948ITERATION : 42, loss : 0.01698671859681527ITERATION : 43, loss : 0.01698671940969745ITERATION : 44, loss : 0.016986719995147993ITERATION : 45, loss : 0.016986720417630857ITERATION : 46, loss : 0.016986720720827863ITERATION : 47, loss : 0.01698672093317438ITERATION : 48, loss : 0.016986721078427053ITERATION : 49, loss : 0.01698672117294739ITERATION : 50, loss : 0.016986721244196697ITERATION : 51, loss : 0.016986721288497313ITERATION : 52, loss : 0.016986721332517836ITERATION : 53, loss : 0.016986721365235564ITERATION : 54, loss : 0.016986721382150384ITERATION : 55, loss : 0.016986721392469237ITERATION : 56, loss : 0.016986721399502868ITERATION : 57, loss : 0.016986721409295837ITERATION : 58, loss : 0.016986721424468613ITERATION : 59, loss : 0.016986721424468613ITERATION : 60, loss : 0.016986721424468613ITERATION : 61, loss : 0.016986721424468613ITERATION : 62, loss : 0.016986721424468613ITERATION : 63, loss : 0.016986721424468613ITERATION : 64, loss : 0.016986721424468613ITERATION : 65, loss : 0.016986721424468613ITERATION : 66, loss : 0.016986721424468613ITERATION : 67, loss : 0.016986721424468613ITERATION : 68, loss : 0.016986721424468613ITERATION : 69, loss : 0.016986721424468613ITERATION : 70, loss : 0.016986721424468613ITERATION : 71, loss : 0.016986721424468613ITERATION : 72, loss : 0.016986721424468613ITERATION : 73, loss : 0.016986721424468613ITERATION : 74, loss : 0.016986721424468613ITERATION : 75, loss : 0.016986721424468613ITERATION : 76, loss : 0.016986721424468613ITERATION : 77, loss : 0.016986721424468613ITERATION : 78, loss : 0.016986721424468613ITERATION : 79, loss : 0.016986721424468613ITERATION : 80, loss : 0.016986721424468613ITERATION : 81, loss : 0.016986721424468613ITERATION : 82, loss : 0.016986721424468613ITERATION : 83, loss : 0.016986721424468613ITERATION : 84, loss : 0.016986721424468613ITERATION : 85, loss : 0.016986721424468613ITERATION : 86, loss : 0.016986721424468613ITERATION : 87, loss : 0.016986721424468613ITERATION : 88, loss : 0.016986721424468613ITERATION : 89, loss : 0.016986721424468613ITERATION : 90, loss : 0.016986721424468613ITERATION : 91, loss : 0.016986721424468613ITERATION : 92, loss : 0.016986721424468613ITERATION : 93, loss : 0.016986721424468613ITERATION : 94, loss : 0.016986721424468613ITERATION : 95, loss : 0.016986721424468613ITERATION : 96, loss : 0.016986721424468613ITERATION : 97, loss : 0.016986721424468613ITERATION : 98, loss : 0.016986721424468613ITERATION : 99, loss : 0.016986721424468613ITERATION : 100, loss : 0.016986721424468613
ITERATION : 1, loss : 0.033840110537866536ITERATION : 2, loss : 0.03258175934299227ITERATION : 3, loss : 0.03294385650901792ITERATION : 4, loss : 0.034940617070488934ITERATION : 5, loss : 0.03689334998375094ITERATION : 6, loss : 0.03644671005346284ITERATION : 7, loss : 0.03588318765992416ITERATION : 8, loss : 0.03552770869399046ITERATION : 9, loss : 0.03530183990358853ITERATION : 10, loss : 0.035157624513285544ITERATION : 11, loss : 0.03506534920428925ITERATION : 12, loss : 0.03500634276693685ITERATION : 13, loss : 0.034968730866289426ITERATION : 14, loss : 0.03494489415889638ITERATION : 15, loss : 0.03492991546109144ITERATION : 16, loss : 0.034920612884272464ITERATION : 17, loss : 0.03491492668139535ITERATION : 18, loss : 0.03491152605436906ITERATION : 19, loss : 0.03490955439656452ITERATION : 20, loss : 0.034908463453660186ITERATION : 21, loss : 0.03490790496458637ITERATION : 22, loss : 0.03490765997242944ITERATION : 23, loss : 0.034907592457407856ITERATION : 24, loss : 0.03490761947775069ITERATION : 25, loss : 0.03490769153227709ITERATION : 26, loss : 0.03490778006558504ITERATION : 27, loss : 0.03490786938441715ITERATION : 28, loss : 0.03490795159136524ITERATION : 29, loss : 0.03490802339204921ITERATION : 30, loss : 0.03490808390140111ITERATION : 31, loss : 0.034908133673447445ITERATION : 32, loss : 0.034908173813097805ITERATION : 33, loss : 0.03490820581723753ITERATION : 34, loss : 0.03490823100372227ITERATION : 35, loss : 0.03490825074550581ITERATION : 36, loss : 0.03490826602709675ITERATION : 37, loss : 0.03490827790724929ITERATION : 38, loss : 0.03490828692734007ITERATION : 39, loss : 0.03490829382387136ITERATION : 40, loss : 0.034908299067282204ITERATION : 41, loss : 0.03490830307816397ITERATION : 42, loss : 0.034908306093704246ITERATION : 43, loss : 0.034908308384288024ITERATION : 44, loss : 0.034908310116906205ITERATION : 45, loss : 0.03490831142745271ITERATION : 46, loss : 0.034908312452511164ITERATION : 47, loss : 0.034908313195678ITERATION : 48, loss : 0.03490831378265157ITERATION : 49, loss : 0.034908314217024054ITERATION : 50, loss : 0.03490831451416687ITERATION : 51, loss : 0.034908314722751445ITERATION : 52, loss : 0.03490831487330574ITERATION : 53, loss : 0.034908314985375326ITERATION : 54, loss : 0.034908315088237656ITERATION : 55, loss : 0.03490831514200288ITERATION : 56, loss : 0.03490831521301484ITERATION : 57, loss : 0.03490831523759869ITERATION : 58, loss : 0.03490831526886737ITERATION : 59, loss : 0.03490831527954931ITERATION : 60, loss : 0.0349083153056804ITERATION : 61, loss : 0.0349083153056804ITERATION : 62, loss : 0.0349083153056804ITERATION : 63, loss : 0.0349083153056804ITERATION : 64, loss : 0.0349083153056804ITERATION : 65, loss : 0.0349083153056804ITERATION : 66, loss : 0.0349083153056804ITERATION : 67, loss : 0.0349083153056804ITERATION : 68, loss : 0.0349083153056804ITERATION : 69, loss : 0.0349083153056804ITERATION : 70, loss : 0.0349083153056804ITERATION : 71, loss : 0.0349083153056804ITERATION : 72, loss : 0.0349083153056804ITERATION : 73, loss : 0.0349083153056804ITERATION : 74, loss : 0.0349083153056804ITERATION : 75, loss : 0.0349083153056804ITERATION : 76, loss : 0.0349083153056804ITERATION : 77, loss : 0.0349083153056804ITERATION : 78, loss : 0.0349083153056804ITERATION : 79, loss : 0.0349083153056804ITERATION : 80, loss : 0.0349083153056804ITERATION : 81, loss : 0.0349083153056804ITERATION : 82, loss : 0.0349083153056804ITERATION : 83, loss : 0.0349083153056804ITERATION : 84, loss : 0.0349083153056804ITERATION : 85, loss : 0.0349083153056804ITERATION : 86, loss : 0.0349083153056804ITERATION : 87, loss : 0.0349083153056804ITERATION : 88, loss : 0.0349083153056804ITERATION : 89, loss : 0.0349083153056804ITERATION : 90, loss : 0.0349083153056804ITERATION : 91, loss : 0.0349083153056804ITERATION : 92, loss : 0.0349083153056804ITERATION : 93, loss : 0.0349083153056804ITERATION : 94, loss : 0.0349083153056804ITERATION : 95, loss : 0.0349083153056804ITERATION : 96, loss : 0.0349083153056804ITERATION : 97, loss : 0.0349083153056804ITERATION : 98, loss : 0.0349083153056804ITERATION : 99, loss : 0.0349083153056804ITERATION : 100, loss : 0.0349083153056804
ITERATION : 1, loss : 0.05133982292566515ITERATION : 2, loss : 0.043223528442293616ITERATION : 3, loss : 0.04005981583476863ITERATION : 4, loss : 0.03813769072030302ITERATION : 5, loss : 0.036841504553652885ITERATION : 6, loss : 0.035949753535881576ITERATION : 7, loss : 0.035332012995284386ITERATION : 8, loss : 0.034902741336132644ITERATION : 9, loss : 0.03460384510278067ITERATION : 10, loss : 0.03439538992511534ITERATION : 11, loss : 0.034249790323881625ITERATION : 12, loss : 0.03414794433914183ITERATION : 13, loss : 0.03407660137815477ITERATION : 14, loss : 0.03402655510497488ITERATION : 15, loss : 0.03399140004796018ITERATION : 16, loss : 0.03396667256628555ITERATION : 17, loss : 0.033949257208651454ITERATION : 18, loss : 0.03393697668137084ITERATION : 19, loss : 0.03392830649814685ITERATION : 20, loss : 0.03392217836852952ITERATION : 21, loss : 0.03391784199615008ITERATION : 22, loss : 0.03391477042547537ITERATION : 23, loss : 0.033912592484057806ITERATION : 24, loss : 0.03391104670093122ITERATION : 25, loss : 0.033909948500750305ITERATION : 26, loss : 0.03390916759376634ITERATION : 27, loss : 0.033908611896124805ITERATION : 28, loss : 0.033908216177613165ITERATION : 29, loss : 0.033907933995643375ITERATION : 30, loss : 0.03390773272210648ITERATION : 31, loss : 0.03390758905574977ITERATION : 32, loss : 0.03390748647279571ITERATION : 33, loss : 0.033907413123923306ITERATION : 34, loss : 0.033907360660021674ITERATION : 35, loss : 0.033907323191306286ITERATION : 36, loss : 0.033907296321843076ITERATION : 37, loss : 0.033907277030062126ITERATION : 38, loss : 0.03390726317099345ITERATION : 39, loss : 0.033907253286231356ITERATION : 40, loss : 0.03390724625573195ITERATION : 41, loss : 0.03390724116200598ITERATION : 42, loss : 0.03390723750087003ITERATION : 43, loss : 0.03390723494846949ITERATION : 44, loss : 0.033907233067889446ITERATION : 45, loss : 0.033907231799954925ITERATION : 46, loss : 0.03390723082963755ITERATION : 47, loss : 0.033907230161182834ITERATION : 48, loss : 0.033907229639041955ITERATION : 49, loss : 0.03390722919503842ITERATION : 50, loss : 0.033907228997468015ITERATION : 51, loss : 0.03390722884902937ITERATION : 52, loss : 0.03390722876237764ITERATION : 53, loss : 0.03390722869716261ITERATION : 54, loss : 0.0339072286546001ITERATION : 55, loss : 0.03390722861033677ITERATION : 56, loss : 0.03390722858947654ITERATION : 57, loss : 0.03390722858876096ITERATION : 58, loss : 0.03390722858876096ITERATION : 59, loss : 0.03390722858876096ITERATION : 60, loss : 0.03390722858876096ITERATION : 61, loss : 0.03390722858876096ITERATION : 62, loss : 0.03390722858876096ITERATION : 63, loss : 0.03390722858876096ITERATION : 64, loss : 0.03390722858876096ITERATION : 65, loss : 0.03390722858876096ITERATION : 66, loss : 0.03390722858876096ITERATION : 67, loss : 0.03390722858876096ITERATION : 68, loss : 0.03390722858876096ITERATION : 69, loss : 0.03390722858876096ITERATION : 70, loss : 0.03390722858876096ITERATION : 71, loss : 0.03390722858876096ITERATION : 72, loss : 0.03390722858876096ITERATION : 73, loss : 0.03390722858876096ITERATION : 74, loss : 0.03390722858876096ITERATION : 75, loss : 0.03390722858876096ITERATION : 76, loss : 0.03390722858876096ITERATION : 77, loss : 0.03390722858876096ITERATION : 78, loss : 0.03390722858876096ITERATION : 79, loss : 0.03390722858876096ITERATION : 80, loss : 0.03390722858876096ITERATION : 81, loss : 0.03390722858876096ITERATION : 82, loss : 0.03390722858876096ITERATION : 83, loss : 0.03390722858876096ITERATION : 84, loss : 0.03390722858876096ITERATION : 85, loss : 0.03390722858876096ITERATION : 86, loss : 0.03390722858876096ITERATION : 87, loss : 0.03390722858876096ITERATION : 88, loss : 0.03390722858876096ITERATION : 89, loss : 0.03390722858876096ITERATION : 90, loss : 0.03390722858876096ITERATION : 91, loss : 0.03390722858876096ITERATION : 92, loss : 0.03390722858876096ITERATION : 93, loss : 0.03390722858876096ITERATION : 94, loss : 0.03390722858876096ITERATION : 95, loss : 0.03390722858876096ITERATION : 96, loss : 0.03390722858876096ITERATION : 97, loss : 0.03390722858876096ITERATION : 98, loss : 0.03390722858876096ITERATION : 99, loss : 0.03390722858876096ITERATION : 100, loss : 0.03390722858876096
ITERATION : 1, loss : 0.022373918825214067ITERATION : 2, loss : 0.019790591606100864ITERATION : 3, loss : 0.019000200179115502ITERATION : 4, loss : 0.018626775553317636ITERATION : 5, loss : 0.01840394406741635ITERATION : 6, loss : 0.01825597726234757ITERATION : 7, loss : 0.018153196178519727ITERATION : 8, loss : 0.018080611309498725ITERATION : 9, loss : 0.018029151070836295ITERATION : 10, loss : 0.01799271676721007ITERATION : 11, loss : 0.01796700387404907ITERATION : 12, loss : 0.017948922392174393ITERATION : 13, loss : 0.017936250166964134ITERATION : 14, loss : 0.017927394655518653ITERATION : 15, loss : 0.017921221367059557ITERATION : 16, loss : 0.017916926354564135ITERATION : 17, loss : 0.017913942885274634ITERATION : 18, loss : 0.017911873060083582ITERATION : 19, loss : 0.01791043851627051ITERATION : 20, loss : 0.017909445042706307ITERATION : 21, loss : 0.01790875746122408ITERATION : 22, loss : 0.01790828179295226ITERATION : 23, loss : 0.017907952864677442ITERATION : 24, loss : 0.017907725463296914ITERATION : 25, loss : 0.017907568304091296ITERATION : 26, loss : 0.01790745971063135ITERATION : 27, loss : 0.017907384656961834ITERATION : 28, loss : 0.017907332805590977ITERATION : 29, loss : 0.017907296996377598ITERATION : 30, loss : 0.01790727222979538ITERATION : 31, loss : 0.017907255159092184ITERATION : 32, loss : 0.017907243333235536ITERATION : 33, loss : 0.017907235205264792ITERATION : 34, loss : 0.0179072295497896ITERATION : 35, loss : 0.017907225686343096ITERATION : 36, loss : 0.017907222972397982ITERATION : 37, loss : 0.01790722114477128ITERATION : 38, loss : 0.017907219851480093ITERATION : 39, loss : 0.017907218998620043ITERATION : 40, loss : 0.017907218372740702ITERATION : 41, loss : 0.017907217960788584ITERATION : 42, loss : 0.017907217640492922ITERATION : 43, loss : 0.01790721747747279ITERATION : 44, loss : 0.017907217319306567ITERATION : 45, loss : 0.017907217267904757ITERATION : 46, loss : 0.01790721718257451ITERATION : 47, loss : 0.0179072171694801ITERATION : 48, loss : 0.01790721716641487ITERATION : 49, loss : 0.01790721716641487ITERATION : 50, loss : 0.01790721716641487ITERATION : 51, loss : 0.01790721716641487ITERATION : 52, loss : 0.01790721716641487ITERATION : 53, loss : 0.01790721716641487ITERATION : 54, loss : 0.01790721716641487ITERATION : 55, loss : 0.01790721716641487ITERATION : 56, loss : 0.01790721716641487ITERATION : 57, loss : 0.01790721716641487ITERATION : 58, loss : 0.01790721716641487ITERATION : 59, loss : 0.01790721716641487ITERATION : 60, loss : 0.01790721716641487ITERATION : 61, loss : 0.01790721716641487ITERATION : 62, loss : 0.01790721716641487ITERATION : 63, loss : 0.01790721716641487ITERATION : 64, loss : 0.01790721716641487ITERATION : 65, loss : 0.01790721716641487ITERATION : 66, loss : 0.01790721716641487ITERATION : 67, loss : 0.01790721716641487ITERATION : 68, loss : 0.01790721716641487ITERATION : 69, loss : 0.01790721716641487ITERATION : 70, loss : 0.01790721716641487ITERATION : 71, loss : 0.01790721716641487ITERATION : 72, loss : 0.01790721716641487ITERATION : 73, loss : 0.01790721716641487ITERATION : 74, loss : 0.01790721716641487ITERATION : 75, loss : 0.01790721716641487ITERATION : 76, loss : 0.01790721716641487ITERATION : 77, loss : 0.01790721716641487ITERATION : 78, loss : 0.01790721716641487ITERATION : 79, loss : 0.01790721716641487ITERATION : 80, loss : 0.01790721716641487ITERATION : 81, loss : 0.01790721716641487ITERATION : 82, loss : 0.01790721716641487ITERATION : 83, loss : 0.01790721716641487ITERATION : 84, loss : 0.01790721716641487ITERATION : 85, loss : 0.01790721716641487ITERATION : 86, loss : 0.01790721716641487ITERATION : 87, loss : 0.01790721716641487ITERATION : 88, loss : 0.01790721716641487ITERATION : 89, loss : 0.01790721716641487ITERATION : 90, loss : 0.01790721716641487ITERATION : 91, loss : 0.01790721716641487ITERATION : 92, loss : 0.01790721716641487ITERATION : 93, loss : 0.01790721716641487ITERATION : 94, loss : 0.01790721716641487ITERATION : 95, loss : 0.01790721716641487ITERATION : 96, loss : 0.01790721716641487ITERATION : 97, loss : 0.01790721716641487ITERATION : 98, loss : 0.01790721716641487ITERATION : 99, loss : 0.01790721716641487ITERATION : 100, loss : 0.01790721716641487
ITERATION : 1, loss : 0.1256957171286238ITERATION : 2, loss : 0.12863449940092825ITERATION : 3, loss : 0.1311230091673354ITERATION : 4, loss : 0.13288992298671404ITERATION : 5, loss : 0.1341632215274758ITERATION : 6, loss : 0.13507951278283747ITERATION : 7, loss : 0.13573627291172782ITERATION : 8, loss : 0.1362057268508169ITERATION : 9, loss : 0.13654088118132576ITERATION : 10, loss : 0.136780125426952ITERATION : 11, loss : 0.1369510004378697ITERATION : 12, loss : 0.13707315417802576ITERATION : 13, loss : 0.13716056810522753ITERATION : 14, loss : 0.137223185153116ITERATION : 15, loss : 0.13726808041438135ITERATION : 16, loss : 0.13730029458207163ITERATION : 17, loss : 0.1373234241821864ITERATION : 18, loss : 0.1373400396935304ITERATION : 19, loss : 0.13735198031893048ITERATION : 20, loss : 0.13736056384165635ITERATION : 21, loss : 0.1373667352347957ITERATION : 22, loss : 0.1373711731385065ITERATION : 23, loss : 0.13737436457404964ITERATION : 24, loss : 0.13737665981258868ITERATION : 25, loss : 0.1373783105483492ITERATION : 26, loss : 0.13737949768671295ITERATION : 27, loss : 0.13738035152747805ITERATION : 28, loss : 0.13738096554978096ITERATION : 29, loss : 0.13738140695896806ITERATION : 30, loss : 0.13738172441143026ITERATION : 31, loss : 0.13738195262794783ITERATION : 32, loss : 0.13738211670801118ITERATION : 33, loss : 0.13738223465279822ITERATION : 34, loss : 0.13738231956590008ITERATION : 35, loss : 0.13738238048979443ITERATION : 36, loss : 0.1373824243089258ITERATION : 37, loss : 0.13738245583837785ITERATION : 38, loss : 0.1373824784772204ITERATION : 39, loss : 0.13738249475127876ITERATION : 40, loss : 0.1373825064102048ITERATION : 41, loss : 0.13738251485907413ITERATION : 42, loss : 0.13738252087719077ITERATION : 43, loss : 0.13738252519225244ITERATION : 44, loss : 0.1373825282631174ITERATION : 45, loss : 0.13738253049663846ITERATION : 46, loss : 0.13738253210341003ITERATION : 47, loss : 0.13738253327098118ITERATION : 48, loss : 0.13738253408287313ITERATION : 49, loss : 0.1373825346359728ITERATION : 50, loss : 0.13738253512548235ITERATION : 51, loss : 0.1373825355806449ITERATION : 52, loss : 0.13738253574650627ITERATION : 53, loss : 0.13738253589114557ITERATION : 54, loss : 0.13738253591352426ITERATION : 55, loss : 0.1373825359330446ITERATION : 56, loss : 0.137382535934621ITERATION : 57, loss : 0.137382535934621ITERATION : 58, loss : 0.137382535934621ITERATION : 59, loss : 0.137382535934621ITERATION : 60, loss : 0.137382535934621ITERATION : 61, loss : 0.137382535934621ITERATION : 62, loss : 0.137382535934621ITERATION : 63, loss : 0.137382535934621ITERATION : 64, loss : 0.137382535934621ITERATION : 65, loss : 0.137382535934621ITERATION : 66, loss : 0.137382535934621ITERATION : 67, loss : 0.137382535934621ITERATION : 68, loss : 0.137382535934621ITERATION : 69, loss : 0.137382535934621ITERATION : 70, loss : 0.137382535934621ITERATION : 71, loss : 0.137382535934621ITERATION : 72, loss : 0.137382535934621ITERATION : 73, loss : 0.137382535934621ITERATION : 74, loss : 0.137382535934621ITERATION : 75, loss : 0.137382535934621ITERATION : 76, loss : 0.137382535934621ITERATION : 77, loss : 0.137382535934621ITERATION : 78, loss : 0.137382535934621ITERATION : 79, loss : 0.137382535934621ITERATION : 80, loss : 0.137382535934621ITERATION : 81, loss : 0.137382535934621ITERATION : 82, loss : 0.137382535934621ITERATION : 83, loss : 0.137382535934621ITERATION : 84, loss : 0.137382535934621ITERATION : 85, loss : 0.137382535934621ITERATION : 86, loss : 0.137382535934621ITERATION : 87, loss : 0.137382535934621ITERATION : 88, loss : 0.137382535934621ITERATION : 89, loss : 0.137382535934621ITERATION : 90, loss : 0.137382535934621ITERATION : 91, loss : 0.137382535934621ITERATION : 92, loss : 0.137382535934621ITERATION : 93, loss : 0.137382535934621ITERATION : 94, loss : 0.137382535934621ITERATION : 95, loss : 0.137382535934621ITERATION : 96, loss : 0.137382535934621ITERATION : 97, loss : 0.137382535934621ITERATION : 98, loss : 0.137382535934621ITERATION : 99, loss : 0.137382535934621ITERATION : 100, loss : 0.137382535934621
ITERATION : 1, loss : 0.011499186403089787ITERATION : 2, loss : 0.012048346958686949ITERATION : 3, loss : 0.01278866295008352ITERATION : 4, loss : 0.013308044555442503ITERATION : 5, loss : 0.013685084330027384ITERATION : 6, loss : 0.013955466613728354ITERATION : 7, loss : 0.014148207937384543ITERATION : 8, loss : 0.01428522847977419ITERATION : 9, loss : 0.014382525603083993ITERATION : 10, loss : 0.01445158757005205ITERATION : 11, loss : 0.014500604622173341ITERATION : 12, loss : 0.01453539776252039ITERATION : 13, loss : 0.014560098019514883ITERATION : 14, loss : 0.014577636305619012ITERATION : 15, loss : 0.014590091408292066ITERATION : 16, loss : 0.014598938122567948ITERATION : 17, loss : 0.014605222905778564ITERATION : 18, loss : 0.014609688360957116ITERATION : 19, loss : 0.014612861622408617ITERATION : 20, loss : 0.014615116925311668ITERATION : 21, loss : 0.014616720030812437ITERATION : 22, loss : 0.014617859679899883ITERATION : 23, loss : 0.014618669956815574ITERATION : 24, loss : 0.014619246119438007ITERATION : 25, loss : 0.0146196558620324ITERATION : 26, loss : 0.014619947266642952ITERATION : 27, loss : 0.014620154549340987ITERATION : 28, loss : 0.014620302003146266ITERATION : 29, loss : 0.014620406913471362ITERATION : 30, loss : 0.014620481569753917ITERATION : 31, loss : 0.01462053469658922ITERATION : 32, loss : 0.014620572502645555ITERATION : 33, loss : 0.014620599406397474ITERATION : 34, loss : 0.014620618545902448ITERATION : 35, loss : 0.014620632183736416ITERATION : 36, loss : 0.01462064188714493ITERATION : 37, loss : 0.014620648770569275ITERATION : 38, loss : 0.014620653657818316ITERATION : 39, loss : 0.014620657138110633ITERATION : 40, loss : 0.014620659767767545ITERATION : 41, loss : 0.014620661501295122ITERATION : 42, loss : 0.014620662731870062ITERATION : 43, loss : 0.014620663606354127ITERATION : 44, loss : 0.014620664238353532ITERATION : 45, loss : 0.014620664680645537ITERATION : 46, loss : 0.014620664939387374ITERATION : 47, loss : 0.014620665164667796ITERATION : 48, loss : 0.014620665330441289ITERATION : 49, loss : 0.014620665407963853ITERATION : 50, loss : 0.014620665489708272ITERATION : 51, loss : 0.01462066553085484ITERATION : 52, loss : 0.01462066553085484ITERATION : 53, loss : 0.01462066553085484ITERATION : 54, loss : 0.01462066553085484ITERATION : 55, loss : 0.01462066553085484ITERATION : 56, loss : 0.01462066553085484ITERATION : 57, loss : 0.01462066553085484ITERATION : 58, loss : 0.01462066553085484ITERATION : 59, loss : 0.01462066553085484ITERATION : 60, loss : 0.01462066553085484ITERATION : 61, loss : 0.01462066553085484ITERATION : 62, loss : 0.01462066553085484ITERATION : 63, loss : 0.01462066553085484ITERATION : 64, loss : 0.01462066553085484ITERATION : 65, loss : 0.01462066553085484ITERATION : 66, loss : 0.01462066553085484ITERATION : 67, loss : 0.01462066553085484ITERATION : 68, loss : 0.01462066553085484ITERATION : 69, loss : 0.01462066553085484ITERATION : 70, loss : 0.01462066553085484ITERATION : 71, loss : 0.01462066553085484ITERATION : 72, loss : 0.01462066553085484ITERATION : 73, loss : 0.01462066553085484ITERATION : 74, loss : 0.01462066553085484ITERATION : 75, loss : 0.01462066553085484ITERATION : 76, loss : 0.01462066553085484ITERATION : 77, loss : 0.01462066553085484ITERATION : 78, loss : 0.01462066553085484ITERATION : 79, loss : 0.01462066553085484ITERATION : 80, loss : 0.01462066553085484ITERATION : 81, loss : 0.01462066553085484ITERATION : 82, loss : 0.01462066553085484ITERATION : 83, loss : 0.01462066553085484ITERATION : 84, loss : 0.01462066553085484ITERATION : 85, loss : 0.01462066553085484ITERATION : 86, loss : 0.01462066553085484ITERATION : 87, loss : 0.01462066553085484ITERATION : 88, loss : 0.01462066553085484ITERATION : 89, loss : 0.01462066553085484ITERATION : 90, loss : 0.01462066553085484ITERATION : 91, loss : 0.01462066553085484ITERATION : 92, loss : 0.01462066553085484ITERATION : 93, loss : 0.01462066553085484ITERATION : 94, loss : 0.01462066553085484ITERATION : 95, loss : 0.01462066553085484ITERATION : 96, loss : 0.01462066553085484ITERATION : 97, loss : 0.01462066553085484ITERATION : 98, loss : 0.01462066553085484ITERATION : 99, loss : 0.01462066553085484ITERATION : 100, loss : 0.01462066553085484
gradient norm in None layer : 0.003350511465853328
gradient norm in None layer : 0.0001188032610299989
gradient norm in None layer : 0.00011988519009601815
gradient norm in None layer : 0.001969459647639642
gradient norm in None layer : 0.0001475677460085556
gradient norm in None layer : 0.0001623495326073933
gradient norm in None layer : 0.0008315231950878329
gradient norm in None layer : 3.481384266964011e-05
gradient norm in None layer : 2.3155119794544657e-05
gradient norm in None layer : 0.0007282783931579677
gradient norm in None layer : 3.5924089891996106e-05
gradient norm in None layer : 2.2268338774409436e-05
gradient norm in None layer : 0.00023331896166944646
gradient norm in None layer : 7.656620244992042e-06
gradient norm in None layer : 4.580387320626566e-06
gradient norm in None layer : 0.0002399785405973664
gradient norm in None layer : 1.159957487118832e-05
gradient norm in None layer : 7.354790485233155e-06
gradient norm in None layer : 0.0003813903022216489
gradient norm in None layer : 3.913713593785809e-06
gradient norm in None layer : 0.0007627789135854494
gradient norm in None layer : 4.912578729536476e-05
gradient norm in None layer : 3.570470714493316e-05
gradient norm in None layer : 0.0009729178531475593
gradient norm in None layer : 8.376340962351828e-05
gradient norm in None layer : 9.216383826201291e-05
gradient norm in None layer : 0.0018004924608354986
gradient norm in None layer : 1.301088920254221e-05
gradient norm in None layer : 0.0024682106263467373
gradient norm in None layer : 0.00020332797424462028
gradient norm in None layer : 0.00020741267482967256
gradient norm in None layer : 0.0029582788220799783
gradient norm in None layer : 0.0003175214462860041
gradient norm in None layer : 0.0004107642070259886
gradient norm in None layer : 0.0002822274560179273
gradient norm in None layer : 6.613270515226845e-05
Total gradient norm: 0.0060609733093946435
invariance loss : 4.9637535627750005, avg_den : 0.3873443603515625, density loss : 0.2873443603515625, mse loss : 0.0439001835803463, solver time : 117.98168969154358 sec , total loss : 0.049151281503472866, running loss : 0.06886002228117415
Epoch 0/10 , batch 8/12500 
ITERATION : 1, loss : 0.051559753817675455ITERATION : 2, loss : 0.042784340797281016ITERATION : 3, loss : 0.040203399210195ITERATION : 4, loss : 0.03941962670484208ITERATION : 5, loss : 0.03927430159998082ITERATION : 6, loss : 0.03937450392924418ITERATION : 7, loss : 0.03954551261877941ITERATION : 8, loss : 0.03971848354241333ITERATION : 9, loss : 0.03986905007499541ITERATION : 10, loss : 0.03999146015501436ITERATION : 11, loss : 0.04008733582054476ITERATION : 12, loss : 0.04015264147299986ITERATION : 13, loss : 0.04012137815755079ITERATION : 14, loss : 0.040100169659311846ITERATION : 15, loss : 0.040085682138787ITERATION : 16, loss : 0.04007573201784896ITERATION : 17, loss : 0.04006886967502215ITERATION : 18, loss : 0.040064121812489625ITERATION : 19, loss : 0.04006082903968886ITERATION : 20, loss : 0.04005854145540116ITERATION : 21, loss : 0.04005695022120974ITERATION : 22, loss : 0.04005584242828458ITERATION : 23, loss : 0.04005507081472071ITERATION : 24, loss : 0.04005453319303952ITERATION : 25, loss : 0.040054158635589726ITERATION : 26, loss : 0.040053897708558134ITERATION : 27, loss : 0.04005371597355724ITERATION : 28, loss : 0.040053589497078976ITERATION : 29, loss : 0.04005350149204422ITERATION : 30, loss : 0.040053440288628166ITERATION : 31, loss : 0.04005339774297422ITERATION : 32, loss : 0.040053368233398626ITERATION : 33, loss : 0.04005334773604681ITERATION : 34, loss : 0.04005333357175207ITERATION : 35, loss : 0.04005332377725674ITERATION : 36, loss : 0.04005331696484049ITERATION : 37, loss : 0.040053312255636984ITERATION : 38, loss : 0.040053309037799925ITERATION : 39, loss : 0.040053306777317534ITERATION : 40, loss : 0.04005330521789597ITERATION : 41, loss : 0.04005330415639157ITERATION : 42, loss : 0.04005330345504018ITERATION : 43, loss : 0.04005330297553275ITERATION : 44, loss : 0.04005330267475239ITERATION : 45, loss : 0.04005330245906239ITERATION : 46, loss : 0.0400533023319285ITERATION : 47, loss : 0.04005330223980558ITERATION : 48, loss : 0.04005330219836844ITERATION : 49, loss : 0.04005330213862604ITERATION : 50, loss : 0.040053302139185606ITERATION : 51, loss : 0.04005330210964009ITERATION : 52, loss : 0.040053302096644615ITERATION : 53, loss : 0.040053302096552515ITERATION : 54, loss : 0.04005330209399621ITERATION : 55, loss : 0.040053302083741284ITERATION : 56, loss : 0.04005330208378835ITERATION : 57, loss : 0.04005330208378835ITERATION : 58, loss : 0.04005330208378835ITERATION : 59, loss : 0.04005330208378835ITERATION : 60, loss : 0.04005330208378835ITERATION : 61, loss : 0.04005330208378835ITERATION : 62, loss : 0.04005330208378835ITERATION : 63, loss : 0.04005330208378835ITERATION : 64, loss : 0.04005330208378835ITERATION : 65, loss : 0.04005330208378835ITERATION : 66, loss : 0.04005330208378835ITERATION : 67, loss : 0.04005330208378835ITERATION : 68, loss : 0.04005330208378835ITERATION : 69, loss : 0.04005330208378835ITERATION : 70, loss : 0.04005330208378835ITERATION : 71, loss : 0.04005330208378835ITERATION : 72, loss : 0.04005330208378835ITERATION : 73, loss : 0.04005330208378835ITERATION : 74, loss : 0.04005330208378835ITERATION : 75, loss : 0.04005330208378835ITERATION : 76, loss : 0.04005330208378835ITERATION : 77, loss : 0.04005330208378835ITERATION : 78, loss : 0.04005330208378835ITERATION : 79, loss : 0.04005330208378835ITERATION : 80, loss : 0.04005330208378835ITERATION : 81, loss : 0.04005330208378835ITERATION : 82, loss : 0.04005330208378835ITERATION : 83, loss : 0.04005330208378835ITERATION : 84, loss : 0.04005330208378835ITERATION : 85, loss : 0.04005330208378835ITERATION : 86, loss : 0.04005330208378835ITERATION : 87, loss : 0.04005330208378835ITERATION : 88, loss : 0.04005330208378835ITERATION : 89, loss : 0.04005330208378835ITERATION : 90, loss : 0.04005330208378835ITERATION : 91, loss : 0.04005330208378835ITERATION : 92, loss : 0.04005330208378835ITERATION : 93, loss : 0.04005330208378835ITERATION : 94, loss : 0.04005330208378835ITERATION : 95, loss : 0.04005330208378835ITERATION : 96, loss : 0.04005330208378835ITERATION : 97, loss : 0.04005330208378835ITERATION : 98, loss : 0.04005330208378835ITERATION : 99, loss : 0.04005330208378835ITERATION : 100, loss : 0.04005330208378835
ITERATION : 1, loss : 0.024226588355734862ITERATION : 2, loss : 0.0231317488729436ITERATION : 3, loss : 0.0259630479576942ITERATION : 4, loss : 0.029581208234458558ITERATION : 5, loss : 0.03221144940377272ITERATION : 6, loss : 0.03277700913875048ITERATION : 7, loss : 0.03334251194687556ITERATION : 8, loss : 0.03363090051324318ITERATION : 9, loss : 0.033608577058545745ITERATION : 10, loss : 0.03360817029998645ITERATION : 11, loss : 0.03361550919696471ITERATION : 12, loss : 0.033624506707102826ITERATION : 13, loss : 0.033632781926374866ITERATION : 14, loss : 0.033639600207631966ITERATION : 15, loss : 0.033644912953626704ITERATION : 16, loss : 0.033648921511386286ITERATION : 17, loss : 0.03365188621758869ITERATION : 18, loss : 0.03365405079698344ITERATION : 19, loss : 0.033655617695686264ITERATION : 20, loss : 0.033656745497965844ITERATION : 21, loss : 0.033657554117328466ITERATION : 22, loss : 0.03365813231063189ITERATION : 23, loss : 0.033658545109131056ITERATION : 24, loss : 0.033658839484489424ITERATION : 25, loss : 0.03365904922094126ITERATION : 26, loss : 0.03365919862820697ITERATION : 27, loss : 0.03365930508562366ITERATION : 28, loss : 0.03365938090453862ITERATION : 29, loss : 0.03365943489790452ITERATION : 30, loss : 0.03365947339962976ITERATION : 31, loss : 0.03365950083389655ITERATION : 32, loss : 0.0336595203864299ITERATION : 33, loss : 0.03365953432544698ITERATION : 34, loss : 0.03365954425225989ITERATION : 35, loss : 0.03365955135564154ITERATION : 36, loss : 0.033659556428537206ITERATION : 37, loss : 0.03365956004911839ITERATION : 38, loss : 0.03365956262083287ITERATION : 39, loss : 0.033659564474001265ITERATION : 40, loss : 0.03365956577380685ITERATION : 41, loss : 0.03365956671725565ITERATION : 42, loss : 0.033659567376851066ITERATION : 43, loss : 0.033659567857909356ITERATION : 44, loss : 0.0336595681887763ITERATION : 45, loss : 0.03365956843327471ITERATION : 46, loss : 0.033659568595483295ITERATION : 47, loss : 0.03365956872760234ITERATION : 48, loss : 0.03365956880723861ITERATION : 49, loss : 0.03365956888010141ITERATION : 50, loss : 0.03365956891986002ITERATION : 51, loss : 0.03365956895689844ITERATION : 52, loss : 0.03365956897812781ITERATION : 53, loss : 0.03365956899062114ITERATION : 54, loss : 0.033659569003767924ITERATION : 55, loss : 0.03365956901146551ITERATION : 56, loss : 0.03365956901304202ITERATION : 57, loss : 0.03365956901777475ITERATION : 58, loss : 0.03365956903106678ITERATION : 59, loss : 0.033659569023491626ITERATION : 60, loss : 0.033659569023491626ITERATION : 61, loss : 0.033659569023491626ITERATION : 62, loss : 0.033659569023491626ITERATION : 63, loss : 0.033659569023491626ITERATION : 64, loss : 0.033659569023491626ITERATION : 65, loss : 0.033659569023491626ITERATION : 66, loss : 0.033659569023491626ITERATION : 67, loss : 0.033659569023491626ITERATION : 68, loss : 0.033659569023491626ITERATION : 69, loss : 0.033659569023491626ITERATION : 70, loss : 0.033659569023491626ITERATION : 71, loss : 0.033659569023491626ITERATION : 72, loss : 0.033659569023491626ITERATION : 73, loss : 0.033659569023491626ITERATION : 74, loss : 0.033659569023491626ITERATION : 75, loss : 0.033659569023491626ITERATION : 76, loss : 0.033659569023491626ITERATION : 77, loss : 0.033659569023491626ITERATION : 78, loss : 0.033659569023491626ITERATION : 79, loss : 0.033659569023491626ITERATION : 80, loss : 0.033659569023491626ITERATION : 81, loss : 0.033659569023491626ITERATION : 82, loss : 0.033659569023491626ITERATION : 83, loss : 0.033659569023491626ITERATION : 84, loss : 0.033659569023491626ITERATION : 85, loss : 0.033659569023491626ITERATION : 86, loss : 0.033659569023491626ITERATION : 87, loss : 0.033659569023491626ITERATION : 88, loss : 0.033659569023491626ITERATION : 89, loss : 0.033659569023491626ITERATION : 90, loss : 0.033659569023491626ITERATION : 91, loss : 0.033659569023491626ITERATION : 92, loss : 0.033659569023491626ITERATION : 93, loss : 0.033659569023491626ITERATION : 94, loss : 0.033659569023491626ITERATION : 95, loss : 0.033659569023491626ITERATION : 96, loss : 0.033659569023491626ITERATION : 97, loss : 0.033659569023491626ITERATION : 98, loss : 0.033659569023491626ITERATION : 99, loss : 0.033659569023491626ITERATION : 100, loss : 0.033659569023491626
ITERATION : 1, loss : 0.04850929567442283ITERATION : 2, loss : 0.050841737689044035ITERATION : 3, loss : 0.05099848853971523ITERATION : 4, loss : 0.05092219817295887ITERATION : 5, loss : 0.05086962846445688ITERATION : 6, loss : 0.050845916032701916ITERATION : 7, loss : 0.05083646780151921ITERATION : 8, loss : 0.050832780191485835ITERATION : 9, loss : 0.050831184560390846ITERATION : 10, loss : 0.0508302967289426ITERATION : 11, loss : 0.05082964954473771ITERATION : 12, loss : 0.05082910641295894ITERATION : 13, loss : 0.05082863587459175ITERATION : 14, loss : 0.05082823303383342ITERATION : 15, loss : 0.05082789582450694ITERATION : 16, loss : 0.0508276197361586ITERATION : 17, loss : 0.05082739786896919ITERATION : 18, loss : 0.05082722219495821ITERATION : 19, loss : 0.050827084892121135ITERATION : 20, loss : 0.05082697861385656ITERATION : 21, loss : 0.0508268969899783ITERATION : 22, loss : 0.050826834779274785ITERATION : 23, loss : 0.05082678760281836ITERATION : 24, loss : 0.050826752019566504ITERATION : 25, loss : 0.05082672526897575ITERATION : 26, loss : 0.050826705255774374ITERATION : 27, loss : 0.050826690354358654ITERATION : 28, loss : 0.050826679259624905ITERATION : 29, loss : 0.05082667102410807ITERATION : 30, loss : 0.050826664960182025ITERATION : 31, loss : 0.05082666047042787ITERATION : 32, loss : 0.05082665718451195ITERATION : 33, loss : 0.05082665479232207ITERATION : 34, loss : 0.050826652964419225ITERATION : 35, loss : 0.05082665165748343ITERATION : 36, loss : 0.050826650648873ITERATION : 37, loss : 0.05082664995897183ITERATION : 38, loss : 0.050826649394587455ITERATION : 39, loss : 0.05082664904056978ITERATION : 40, loss : 0.0508266487295645ITERATION : 41, loss : 0.05082664849966081ITERATION : 42, loss : 0.05082664834884473ITERATION : 43, loss : 0.05082664825146039ITERATION : 44, loss : 0.05082664817569925ITERATION : 45, loss : 0.05082664810473157ITERATION : 46, loss : 0.0508266480709911ITERATION : 47, loss : 0.05082664802623271ITERATION : 48, loss : 0.05082664802669236ITERATION : 49, loss : 0.050826648025498807ITERATION : 50, loss : 0.050826648025498807ITERATION : 51, loss : 0.050826648025498807ITERATION : 52, loss : 0.050826648025498807ITERATION : 53, loss : 0.050826648025498807ITERATION : 54, loss : 0.050826648025498807ITERATION : 55, loss : 0.050826648025498807ITERATION : 56, loss : 0.050826648025498807ITERATION : 57, loss : 0.050826648025498807ITERATION : 58, loss : 0.050826648025498807ITERATION : 59, loss : 0.050826648025498807ITERATION : 60, loss : 0.050826648025498807ITERATION : 61, loss : 0.050826648025498807ITERATION : 62, loss : 0.050826648025498807ITERATION : 63, loss : 0.050826648025498807ITERATION : 64, loss : 0.050826648025498807ITERATION : 65, loss : 0.050826648025498807ITERATION : 66, loss : 0.050826648025498807ITERATION : 67, loss : 0.050826648025498807ITERATION : 68, loss : 0.050826648025498807ITERATION : 69, loss : 0.050826648025498807ITERATION : 70, loss : 0.050826648025498807ITERATION : 71, loss : 0.050826648025498807ITERATION : 72, loss : 0.050826648025498807ITERATION : 73, loss : 0.050826648025498807ITERATION : 74, loss : 0.050826648025498807ITERATION : 75, loss : 0.050826648025498807ITERATION : 76, loss : 0.050826648025498807ITERATION : 77, loss : 0.050826648025498807ITERATION : 78, loss : 0.050826648025498807ITERATION : 79, loss : 0.050826648025498807ITERATION : 80, loss : 0.050826648025498807ITERATION : 81, loss : 0.050826648025498807ITERATION : 82, loss : 0.050826648025498807ITERATION : 83, loss : 0.050826648025498807ITERATION : 84, loss : 0.050826648025498807ITERATION : 85, loss : 0.050826648025498807ITERATION : 86, loss : 0.050826648025498807ITERATION : 87, loss : 0.050826648025498807ITERATION : 88, loss : 0.050826648025498807ITERATION : 89, loss : 0.050826648025498807ITERATION : 90, loss : 0.050826648025498807ITERATION : 91, loss : 0.050826648025498807ITERATION : 92, loss : 0.050826648025498807ITERATION : 93, loss : 0.050826648025498807ITERATION : 94, loss : 0.050826648025498807ITERATION : 95, loss : 0.050826648025498807ITERATION : 96, loss : 0.050826648025498807ITERATION : 97, loss : 0.050826648025498807ITERATION : 98, loss : 0.050826648025498807ITERATION : 99, loss : 0.050826648025498807ITERATION : 100, loss : 0.050826648025498807
ITERATION : 1, loss : 0.04529658610332903ITERATION : 2, loss : 0.0393282976156987ITERATION : 3, loss : 0.036901648210392955ITERATION : 4, loss : 0.035852942022907286ITERATION : 5, loss : 0.035384615048519ITERATION : 6, loss : 0.03516222870473904ITERATION : 7, loss : 0.035044225928560394ITERATION : 8, loss : 0.03497202028331514ITERATION : 9, loss : 0.03492182393109649ITERATION : 10, loss : 0.034883967738469156ITERATION : 11, loss : 0.03485431914618677ITERATION : 12, loss : 0.03483084781645093ITERATION : 13, loss : 0.03481231218927111ITERATION : 14, loss : 0.03479778830686998ITERATION : 15, loss : 0.034786513494105255ITERATION : 16, loss : 0.03477784113916323ITERATION : 17, loss : 0.034771226617001036ITERATION : 18, loss : 0.034766219370446924ITERATION : 19, loss : 0.034762453640508596ITERATION : 20, loss : 0.03475963779826329ITERATION : 21, loss : 0.034757542792879646ITERATION : 22, loss : 0.034755990914605815ITERATION : 23, loss : 0.03475484561040587ITERATION : 24, loss : 0.03475400327178506ITERATION : 25, loss : 0.034753385726882885ITERATION : 26, loss : 0.03475293398774642ITERATION : 27, loss : 0.034752604413726185ITERATION : 28, loss : 0.03475236456781027ITERATION : 29, loss : 0.03475219027652126ITERATION : 30, loss : 0.03475206375665542ITERATION : 31, loss : 0.03475197210977872ITERATION : 32, loss : 0.03475190591075874ITERATION : 33, loss : 0.034751858096416934ITERATION : 34, loss : 0.034751823567821784ITERATION : 35, loss : 0.03475179865686571ITERATION : 36, loss : 0.034751780761361074ITERATION : 37, loss : 0.03475176785834109ITERATION : 38, loss : 0.034751758545562755ITERATION : 39, loss : 0.034751751907772355ITERATION : 40, loss : 0.03475174715045282ITERATION : 41, loss : 0.03475174374553583ITERATION : 42, loss : 0.03475174127742766ITERATION : 43, loss : 0.03475173953132238ITERATION : 44, loss : 0.0347517382549908ITERATION : 45, loss : 0.034751737358140806ITERATION : 46, loss : 0.03475173670331166ITERATION : 47, loss : 0.034751736287672946ITERATION : 48, loss : 0.03475173592272004ITERATION : 49, loss : 0.03475173569346853ITERATION : 50, loss : 0.03475173553660302ITERATION : 51, loss : 0.034751735406760006ITERATION : 52, loss : 0.03475173535311542ITERATION : 53, loss : 0.034751735242888326ITERATION : 54, loss : 0.03475173523113554ITERATION : 55, loss : 0.034751735185587045ITERATION : 56, loss : 0.0347517351858552ITERATION : 57, loss : 0.0347517351858552ITERATION : 58, loss : 0.0347517351858552ITERATION : 59, loss : 0.0347517351858552ITERATION : 60, loss : 0.0347517351858552ITERATION : 61, loss : 0.0347517351858552ITERATION : 62, loss : 0.0347517351858552ITERATION : 63, loss : 0.0347517351858552ITERATION : 64, loss : 0.0347517351858552ITERATION : 65, loss : 0.0347517351858552ITERATION : 66, loss : 0.0347517351858552ITERATION : 67, loss : 0.0347517351858552ITERATION : 68, loss : 0.0347517351858552ITERATION : 69, loss : 0.0347517351858552ITERATION : 70, loss : 0.0347517351858552ITERATION : 71, loss : 0.0347517351858552ITERATION : 72, loss : 0.0347517351858552ITERATION : 73, loss : 0.0347517351858552ITERATION : 74, loss : 0.0347517351858552ITERATION : 75, loss : 0.0347517351858552ITERATION : 76, loss : 0.0347517351858552ITERATION : 77, loss : 0.0347517351858552ITERATION : 78, loss : 0.0347517351858552ITERATION : 79, loss : 0.0347517351858552ITERATION : 80, loss : 0.0347517351858552ITERATION : 81, loss : 0.0347517351858552ITERATION : 82, loss : 0.0347517351858552ITERATION : 83, loss : 0.0347517351858552ITERATION : 84, loss : 0.0347517351858552ITERATION : 85, loss : 0.0347517351858552ITERATION : 86, loss : 0.0347517351858552ITERATION : 87, loss : 0.0347517351858552ITERATION : 88, loss : 0.0347517351858552ITERATION : 89, loss : 0.0347517351858552ITERATION : 90, loss : 0.0347517351858552ITERATION : 91, loss : 0.0347517351858552ITERATION : 92, loss : 0.0347517351858552ITERATION : 93, loss : 0.0347517351858552ITERATION : 94, loss : 0.0347517351858552ITERATION : 95, loss : 0.0347517351858552ITERATION : 96, loss : 0.0347517351858552ITERATION : 97, loss : 0.0347517351858552ITERATION : 98, loss : 0.0347517351858552ITERATION : 99, loss : 0.0347517351858552ITERATION : 100, loss : 0.0347517351858552
ITERATION : 1, loss : 0.011429093028945054ITERATION : 2, loss : 0.008544332388632878ITERATION : 3, loss : 0.008165584313219771ITERATION : 4, loss : 0.008240037456464113ITERATION : 5, loss : 0.00841213150337939ITERATION : 6, loss : 0.008575627064145034ITERATION : 7, loss : 0.008702240585673952ITERATION : 8, loss : 0.00879123419312378ITERATION : 9, loss : 0.00885013377019859ITERATION : 10, loss : 0.008887379080570393ITERATION : 11, loss : 0.0089099628613121ITERATION : 12, loss : 0.008923029313860862ITERATION : 13, loss : 0.008930127928994211ITERATION : 14, loss : 0.00893360925996365ITERATION : 15, loss : 0.008934982315430528ITERATION : 16, loss : 0.008935189576153583ITERATION : 17, loss : 0.008934803337206788ITERATION : 18, loss : 0.008934159738587907ITERATION : 19, loss : 0.008933447298824101ITERATION : 20, loss : 0.008932765037970036ITERATION : 21, loss : 0.008932158860199005ITERATION : 22, loss : 0.008931644304056637ITERATION : 23, loss : 0.008931220998245174ITERATION : 24, loss : 0.008930880404856307ITERATION : 25, loss : 0.008930610831432664ITERATION : 26, loss : 0.008930400281978989ITERATION : 27, loss : 0.008930237537133776ITERATION : 28, loss : 0.008930112824559373ITERATION : 29, loss : 0.008930017906992285ITERATION : 30, loss : 0.008929946090782926ITERATION : 31, loss : 0.008929892095756363ITERATION : 32, loss : 0.008929851592915353ITERATION : 33, loss : 0.008929821345394927ITERATION : 34, loss : 0.00892979886086537ITERATION : 35, loss : 0.008929782180782754ITERATION : 36, loss : 0.00892976985782028ITERATION : 37, loss : 0.008929760750959094ITERATION : 38, loss : 0.008929753985083371ITERATION : 39, loss : 0.00892974903332655ITERATION : 40, loss : 0.008929745432080746ITERATION : 41, loss : 0.008929742774423837ITERATION : 42, loss : 0.00892974082302982ITERATION : 43, loss : 0.008929739406101143ITERATION : 44, loss : 0.008929738341323545ITERATION : 45, loss : 0.00892973757027863ITERATION : 46, loss : 0.00892973704641137ITERATION : 47, loss : 0.008929736622015324ITERATION : 48, loss : 0.00892973633540413ITERATION : 49, loss : 0.008929736124199335ITERATION : 50, loss : 0.008929736001701669ITERATION : 51, loss : 0.008929735936301078ITERATION : 52, loss : 0.008929735815988254ITERATION : 53, loss : 0.008929735771510187ITERATION : 54, loss : 0.00892973575897391ITERATION : 55, loss : 0.00892973574555119ITERATION : 56, loss : 0.00892973574555119ITERATION : 57, loss : 0.00892973574555119ITERATION : 58, loss : 0.00892973574555119ITERATION : 59, loss : 0.00892973574555119ITERATION : 60, loss : 0.00892973574555119ITERATION : 61, loss : 0.00892973574555119ITERATION : 62, loss : 0.00892973574555119ITERATION : 63, loss : 0.00892973574555119ITERATION : 64, loss : 0.00892973574555119ITERATION : 65, loss : 0.00892973574555119ITERATION : 66, loss : 0.00892973574555119ITERATION : 67, loss : 0.00892973574555119ITERATION : 68, loss : 0.00892973574555119ITERATION : 69, loss : 0.00892973574555119ITERATION : 70, loss : 0.00892973574555119ITERATION : 71, loss : 0.00892973574555119ITERATION : 72, loss : 0.00892973574555119ITERATION : 73, loss : 0.00892973574555119ITERATION : 74, loss : 0.00892973574555119ITERATION : 75, loss : 0.00892973574555119ITERATION : 76, loss : 0.00892973574555119ITERATION : 77, loss : 0.00892973574555119ITERATION : 78, loss : 0.00892973574555119ITERATION : 79, loss : 0.00892973574555119ITERATION : 80, loss : 0.00892973574555119ITERATION : 81, loss : 0.00892973574555119ITERATION : 82, loss : 0.00892973574555119ITERATION : 83, loss : 0.00892973574555119ITERATION : 84, loss : 0.00892973574555119ITERATION : 85, loss : 0.00892973574555119ITERATION : 86, loss : 0.00892973574555119ITERATION : 87, loss : 0.00892973574555119ITERATION : 88, loss : 0.00892973574555119ITERATION : 89, loss : 0.00892973574555119ITERATION : 90, loss : 0.00892973574555119ITERATION : 91, loss : 0.00892973574555119ITERATION : 92, loss : 0.00892973574555119ITERATION : 93, loss : 0.00892973574555119ITERATION : 94, loss : 0.00892973574555119ITERATION : 95, loss : 0.00892973574555119ITERATION : 96, loss : 0.00892973574555119ITERATION : 97, loss : 0.00892973574555119ITERATION : 98, loss : 0.00892973574555119ITERATION : 99, loss : 0.00892973574555119ITERATION : 100, loss : 0.00892973574555119
ITERATION : 1, loss : 0.03290215470680211ITERATION : 2, loss : 0.03190764190710956ITERATION : 3, loss : 0.0323053788920643ITERATION : 4, loss : 0.03307411186474915ITERATION : 5, loss : 0.033832888867610426ITERATION : 6, loss : 0.03445932534004778ITERATION : 7, loss : 0.034937484886335614ITERATION : 8, loss : 0.035287726397970905ITERATION : 9, loss : 0.03553823612426278ITERATION : 10, loss : 0.035714818921584165ITERATION : 11, loss : 0.0358381377556159ITERATION : 12, loss : 0.03592373232722937ITERATION : 13, loss : 0.035982897548650486ITERATION : 14, loss : 0.03602367779043728ITERATION : 15, loss : 0.036051730192349876ITERATION : 16, loss : 0.03607100027397427ITERATION : 17, loss : 0.03608422400251984ITERATION : 18, loss : 0.036093292152241115ITERATION : 19, loss : 0.03609950735123704ITERATION : 20, loss : 0.03610376544693738ITERATION : 21, loss : 0.036106682001184935ITERATION : 22, loss : 0.03610867915291464ITERATION : 23, loss : 0.03611004653493129ITERATION : 24, loss : 0.0361109826967154ITERATION : 25, loss : 0.03611162350332273ITERATION : 26, loss : 0.03611206208263235ITERATION : 27, loss : 0.03611236226616334ITERATION : 28, loss : 0.03611256798106558ITERATION : 29, loss : 0.036112708567485975ITERATION : 30, loss : 0.03611280485055452ITERATION : 31, loss : 0.03611287080547933ITERATION : 32, loss : 0.03611291594320955ITERATION : 33, loss : 0.03611294692224282ITERATION : 34, loss : 0.03611296803307134ITERATION : 35, loss : 0.036112982492078956ITERATION : 36, loss : 0.03611299246270506ITERATION : 37, loss : 0.03611299927379592ITERATION : 38, loss : 0.03611300386951962ITERATION : 39, loss : 0.03611300695281326ITERATION : 40, loss : 0.03611300902932542ITERATION : 41, loss : 0.036113010451610535ITERATION : 42, loss : 0.036113011372435944ITERATION : 43, loss : 0.036113012000926266ITERATION : 44, loss : 0.03611301242951154ITERATION : 45, loss : 0.03611301271449376ITERATION : 46, loss : 0.036113012903040274ITERATION : 47, loss : 0.03611301301561152ITERATION : 48, loss : 0.03611301311007034ITERATION : 49, loss : 0.03611301315934365ITERATION : 50, loss : 0.036113013201272144ITERATION : 51, loss : 0.036113013234902694ITERATION : 52, loss : 0.03611301324722817ITERATION : 53, loss : 0.03611301326091749ITERATION : 54, loss : 0.03611301326445303ITERATION : 55, loss : 0.03611301327029221ITERATION : 56, loss : 0.03611301327577442ITERATION : 57, loss : 0.03611301327776908ITERATION : 58, loss : 0.03611301327776908ITERATION : 59, loss : 0.03611301327776908ITERATION : 60, loss : 0.03611301327776908ITERATION : 61, loss : 0.03611301327776908ITERATION : 62, loss : 0.03611301327776908ITERATION : 63, loss : 0.03611301327776908ITERATION : 64, loss : 0.03611301327776908ITERATION : 65, loss : 0.03611301327776908ITERATION : 66, loss : 0.03611301327776908ITERATION : 67, loss : 0.03611301327776908ITERATION : 68, loss : 0.03611301327776908ITERATION : 69, loss : 0.03611301327776908ITERATION : 70, loss : 0.03611301327776908ITERATION : 71, loss : 0.03611301327776908ITERATION : 72, loss : 0.03611301327776908ITERATION : 73, loss : 0.03611301327776908ITERATION : 74, loss : 0.03611301327776908ITERATION : 75, loss : 0.03611301327776908ITERATION : 76, loss : 0.03611301327776908ITERATION : 77, loss : 0.03611301327776908ITERATION : 78, loss : 0.03611301327776908ITERATION : 79, loss : 0.03611301327776908ITERATION : 80, loss : 0.03611301327776908ITERATION : 81, loss : 0.03611301327776908ITERATION : 82, loss : 0.03611301327776908ITERATION : 83, loss : 0.03611301327776908ITERATION : 84, loss : 0.03611301327776908ITERATION : 85, loss : 0.03611301327776908ITERATION : 86, loss : 0.03611301327776908ITERATION : 87, loss : 0.03611301327776908ITERATION : 88, loss : 0.03611301327776908ITERATION : 89, loss : 0.03611301327776908ITERATION : 90, loss : 0.03611301327776908ITERATION : 91, loss : 0.03611301327776908ITERATION : 92, loss : 0.03611301327776908ITERATION : 93, loss : 0.03611301327776908ITERATION : 94, loss : 0.03611301327776908ITERATION : 95, loss : 0.03611301327776908ITERATION : 96, loss : 0.03611301327776908ITERATION : 97, loss : 0.03611301327776908ITERATION : 98, loss : 0.03611301327776908ITERATION : 99, loss : 0.03611301327776908ITERATION : 100, loss : 0.03611301327776908
ITERATION : 1, loss : 0.043584343430208825ITERATION : 2, loss : 0.05795427423330231ITERATION : 3, loss : 0.05403544615565895ITERATION : 4, loss : 0.051739529300030854ITERATION : 5, loss : 0.05036285876066352ITERATION : 6, loss : 0.04950225597610479ITERATION : 7, loss : 0.04894393409023784ITERATION : 8, loss : 0.04857147500567898ITERATION : 9, loss : 0.04831806995330354ITERATION : 10, loss : 0.04814331909640403ITERATION : 11, loss : 0.04802169848669131ITERATION : 12, loss : 0.04793653121250869ITERATION : 13, loss : 0.04787664578285281ITERATION : 14, loss : 0.04783442425627124ITERATION : 15, loss : 0.0478046060228557ITERATION : 16, loss : 0.0477835261960465ITERATION : 17, loss : 0.04776861589403343ITERATION : 18, loss : 0.047758067477124584ITERATION : 19, loss : 0.04775060481722789ITERATION : 20, loss : 0.04774532609636175ITERATION : 21, loss : 0.047741593162446035ITERATION : 22, loss : 0.04773895437816185ITERATION : 23, loss : 0.047737089507384374ITERATION : 24, loss : 0.047735772514769965ITERATION : 25, loss : 0.047734842454953096ITERATION : 26, loss : 0.047734186006476956ITERATION : 27, loss : 0.047733722766560543ITERATION : 28, loss : 0.04773339625043931ITERATION : 29, loss : 0.04773316600796026ITERATION : 30, loss : 0.04773300375749773ITERATION : 31, loss : 0.04773288952239217ITERATION : 32, loss : 0.04773280910504285ITERATION : 33, loss : 0.04773275237712864ITERATION : 34, loss : 0.047732712617383134ITERATION : 35, loss : 0.04773268457065344ITERATION : 36, loss : 0.047732664910692696ITERATION : 37, loss : 0.04773265105153442ITERATION : 38, loss : 0.047732641281221955ITERATION : 39, loss : 0.0477326345095041ITERATION : 40, loss : 0.04773262959191023ITERATION : 41, loss : 0.04773262620153485ITERATION : 42, loss : 0.04773262396854252ITERATION : 43, loss : 0.04773262229857365ITERATION : 44, loss : 0.047732621192698145ITERATION : 45, loss : 0.047732620293228764ITERATION : 46, loss : 0.047732619627954456ITERATION : 47, loss : 0.04773261931999426ITERATION : 48, loss : 0.04773261914573681ITERATION : 49, loss : 0.04773261882684983ITERATION : 50, loss : 0.047732618803505746ITERATION : 51, loss : 0.047732618803505746ITERATION : 52, loss : 0.047732618803505746ITERATION : 53, loss : 0.047732618803505746ITERATION : 54, loss : 0.047732618803505746ITERATION : 55, loss : 0.047732618803505746ITERATION : 56, loss : 0.047732618803505746ITERATION : 57, loss : 0.047732618803505746ITERATION : 58, loss : 0.047732618803505746ITERATION : 59, loss : 0.047732618803505746ITERATION : 60, loss : 0.047732618803505746ITERATION : 61, loss : 0.047732618803505746ITERATION : 62, loss : 0.047732618803505746ITERATION : 63, loss : 0.047732618803505746ITERATION : 64, loss : 0.047732618803505746ITERATION : 65, loss : 0.047732618803505746ITERATION : 66, loss : 0.047732618803505746ITERATION : 67, loss : 0.047732618803505746ITERATION : 68, loss : 0.047732618803505746ITERATION : 69, loss : 0.047732618803505746ITERATION : 70, loss : 0.047732618803505746ITERATION : 71, loss : 0.047732618803505746ITERATION : 72, loss : 0.047732618803505746ITERATION : 73, loss : 0.047732618803505746ITERATION : 74, loss : 0.047732618803505746ITERATION : 75, loss : 0.047732618803505746ITERATION : 76, loss : 0.047732618803505746ITERATION : 77, loss : 0.047732618803505746ITERATION : 78, loss : 0.047732618803505746ITERATION : 79, loss : 0.047732618803505746ITERATION : 80, loss : 0.047732618803505746ITERATION : 81, loss : 0.047732618803505746ITERATION : 82, loss : 0.047732618803505746ITERATION : 83, loss : 0.047732618803505746ITERATION : 84, loss : 0.047732618803505746ITERATION : 85, loss : 0.047732618803505746ITERATION : 86, loss : 0.047732618803505746ITERATION : 87, loss : 0.047732618803505746ITERATION : 88, loss : 0.047732618803505746ITERATION : 89, loss : 0.047732618803505746ITERATION : 90, loss : 0.047732618803505746ITERATION : 91, loss : 0.047732618803505746ITERATION : 92, loss : 0.047732618803505746ITERATION : 93, loss : 0.047732618803505746ITERATION : 94, loss : 0.047732618803505746ITERATION : 95, loss : 0.047732618803505746ITERATION : 96, loss : 0.047732618803505746ITERATION : 97, loss : 0.047732618803505746ITERATION : 98, loss : 0.047732618803505746ITERATION : 99, loss : 0.047732618803505746ITERATION : 100, loss : 0.047732618803505746
ITERATION : 1, loss : 0.04096485405182209ITERATION : 2, loss : 0.03588896929787719ITERATION : 3, loss : 0.03663853793632368ITERATION : 4, loss : 0.03529136042014592ITERATION : 5, loss : 0.03275142400344371ITERATION : 6, loss : 0.031305500655856605ITERATION : 7, loss : 0.03043835589589884ITERATION : 8, loss : 0.029897110277997106ITERATION : 9, loss : 0.029549327247836613ITERATION : 10, loss : 0.029321299845701466ITERATION : 11, loss : 0.029169748842832993ITERATION : 12, loss : 0.02906812553380534ITERATION : 13, loss : 0.02899959081512305ITERATION : 14, loss : 0.0289532047396586ITERATION : 15, loss : 0.02892174013742088ITERATION : 16, loss : 0.028900369593280916ITERATION : 17, loss : 0.028885844431715543ITERATION : 18, loss : 0.0288759688215886ITERATION : 19, loss : 0.0288692537737912ITERATION : 20, loss : 0.028864688085229247ITERATION : 21, loss : 0.028861584289308326ITERATION : 22, loss : 0.02885947471601491ITERATION : 23, loss : 0.028858041220491217ITERATION : 24, loss : 0.028857067447380542ITERATION : 25, loss : 0.028856406076284386ITERATION : 26, loss : 0.028855956987130855ITERATION : 27, loss : 0.028855652157445563ITERATION : 28, loss : 0.028855445253298737ITERATION : 29, loss : 0.02885530487202136ITERATION : 30, loss : 0.028855209639492865ITERATION : 31, loss : 0.028855145120952434ITERATION : 32, loss : 0.028855101357286295ITERATION : 33, loss : 0.028855071686271758ITERATION : 34, loss : 0.02885505165225963ITERATION : 35, loss : 0.028855037997497376ITERATION : 36, loss : 0.028855028745198897ITERATION : 37, loss : 0.028855022520638537ITERATION : 38, loss : 0.02885501826420696ITERATION : 39, loss : 0.02885501533561381ITERATION : 40, loss : 0.028855013413389773ITERATION : 41, loss : 0.028855012038946446ITERATION : 42, loss : 0.028855011195799894ITERATION : 43, loss : 0.028855010543732913ITERATION : 44, loss : 0.02885501012671441ITERATION : 45, loss : 0.028855009838364473ITERATION : 46, loss : 0.028855009709441735ITERATION : 47, loss : 0.02885500961266794ITERATION : 48, loss : 0.028855009491707388ITERATION : 49, loss : 0.0288550094899634ITERATION : 50, loss : 0.0288550094899634ITERATION : 51, loss : 0.0288550094899634ITERATION : 52, loss : 0.0288550094899634ITERATION : 53, loss : 0.0288550094899634ITERATION : 54, loss : 0.0288550094899634ITERATION : 55, loss : 0.0288550094899634ITERATION : 56, loss : 0.0288550094899634ITERATION : 57, loss : 0.0288550094899634ITERATION : 58, loss : 0.0288550094899634ITERATION : 59, loss : 0.0288550094899634ITERATION : 60, loss : 0.0288550094899634ITERATION : 61, loss : 0.0288550094899634ITERATION : 62, loss : 0.0288550094899634ITERATION : 63, loss : 0.0288550094899634ITERATION : 64, loss : 0.0288550094899634ITERATION : 65, loss : 0.0288550094899634ITERATION : 66, loss : 0.0288550094899634ITERATION : 67, loss : 0.0288550094899634ITERATION : 68, loss : 0.0288550094899634ITERATION : 69, loss : 0.0288550094899634ITERATION : 70, loss : 0.0288550094899634ITERATION : 71, loss : 0.0288550094899634ITERATION : 72, loss : 0.0288550094899634ITERATION : 73, loss : 0.0288550094899634ITERATION : 74, loss : 0.0288550094899634ITERATION : 75, loss : 0.0288550094899634ITERATION : 76, loss : 0.0288550094899634ITERATION : 77, loss : 0.0288550094899634ITERATION : 78, loss : 0.0288550094899634ITERATION : 79, loss : 0.0288550094899634ITERATION : 80, loss : 0.0288550094899634ITERATION : 81, loss : 0.0288550094899634ITERATION : 82, loss : 0.0288550094899634ITERATION : 83, loss : 0.0288550094899634ITERATION : 84, loss : 0.0288550094899634ITERATION : 85, loss : 0.0288550094899634ITERATION : 86, loss : 0.0288550094899634ITERATION : 87, loss : 0.0288550094899634ITERATION : 88, loss : 0.0288550094899634ITERATION : 89, loss : 0.0288550094899634ITERATION : 90, loss : 0.0288550094899634ITERATION : 91, loss : 0.0288550094899634ITERATION : 92, loss : 0.0288550094899634ITERATION : 93, loss : 0.0288550094899634ITERATION : 94, loss : 0.0288550094899634ITERATION : 95, loss : 0.0288550094899634ITERATION : 96, loss : 0.0288550094899634ITERATION : 97, loss : 0.0288550094899634ITERATION : 98, loss : 0.0288550094899634ITERATION : 99, loss : 0.0288550094899634ITERATION : 100, loss : 0.0288550094899634
gradient norm in None layer : 0.0008063787290905093
gradient norm in None layer : 3.6459303977373754e-05
gradient norm in None layer : 4.5364204706718625e-05
gradient norm in None layer : 0.0005972219869268139
gradient norm in None layer : 5.013296593477303e-05
gradient norm in None layer : 5.073999142077672e-05
gradient norm in None layer : 0.0003872548575880131
gradient norm in None layer : 1.423015493542678e-05
gradient norm in None layer : 1.2054283129144356e-05
gradient norm in None layer : 0.0003350440671977824
gradient norm in None layer : 1.321185263101101e-05
gradient norm in None layer : 1.0639039432292317e-05
gradient norm in None layer : 9.538209282267522e-05
gradient norm in None layer : 3.1615068600251904e-06
gradient norm in None layer : 2.2835256058295077e-06
gradient norm in None layer : 8.671058095615268e-05
gradient norm in None layer : 3.838269024260569e-06
gradient norm in None layer : 3.122580893577789e-06
gradient norm in None layer : 0.00012836158090299044
gradient norm in None layer : 2.8802150154363222e-06
gradient norm in None layer : 0.0003004098766140445
gradient norm in None layer : 1.961955235887226e-05
gradient norm in None layer : 1.568110463050683e-05
gradient norm in None layer : 0.0003413238599910126
gradient norm in None layer : 2.7767857381273292e-05
gradient norm in None layer : 4.718476126125841e-05
gradient norm in None layer : 0.000598577738849254
gradient norm in None layer : 8.212704606623929e-06
gradient norm in None layer : 0.0008042594517979905
gradient norm in None layer : 7.527371860889197e-05
gradient norm in None layer : 8.131555670144227e-05
gradient norm in None layer : 0.0009625571933187542
gradient norm in None layer : 8.766472649311517e-05
gradient norm in None layer : 0.00011369812813735681
gradient norm in None layer : 7.616253989832428e-05
gradient norm in None layer : 1.271390423410131e-05
Total gradient norm: 0.0018687790613735838
invariance loss : 4.405247434029366, avg_den : 0.40546417236328125, density loss : 0.30546417236328127, mse loss : 0.03511520395442792, solver time : 114.39377975463867 sec , total loss : 0.039825915560820574, running loss : 0.06523075894112995
Epoch 0/10 , batch 9/12500 
ITERATION : 1, loss : 0.06877249003936935ITERATION : 2, loss : 0.06742851768551127ITERATION : 3, loss : 0.06495521764698131ITERATION : 4, loss : 0.06320978570292167ITERATION : 5, loss : 0.062119558988261485ITERATION : 6, loss : 0.06140996935344822ITERATION : 7, loss : 0.06093245069104753ITERATION : 8, loss : 0.06060317163755098ITERATION : 9, loss : 0.06037224328500766ITERATION : 10, loss : 0.06020840705131424ITERATION : 11, loss : 0.06009124113310062ITERATION : 12, loss : 0.060006984459005354ITERATION : 13, loss : 0.059946155782205766ITERATION : 14, loss : 0.05990211914480112ITERATION : 15, loss : 0.05987017657177596ITERATION : 16, loss : 0.059846975330793556ITERATION : 17, loss : 0.05983010746185398ITERATION : 18, loss : 0.059817836822337135ITERATION : 19, loss : 0.059808907091172044ITERATION : 20, loss : 0.059802407684220654ITERATION : 21, loss : 0.05979767652079658ITERATION : 22, loss : 0.059794233081584776ITERATION : 23, loss : 0.05979172698419082ITERATION : 24, loss : 0.05978990344507791ITERATION : 25, loss : 0.059788576783946994ITERATION : 26, loss : 0.0597876118310086ITERATION : 27, loss : 0.05978691010934873ITERATION : 28, loss : 0.05978639997975832ITERATION : 29, loss : 0.05978602932046005ITERATION : 30, loss : 0.059785759871643265ITERATION : 31, loss : 0.0597855642169133ITERATION : 32, loss : 0.059785422051733955ITERATION : 33, loss : 0.05978531886721842ITERATION : 34, loss : 0.05978524402270652ITERATION : 35, loss : 0.05978518964621846ITERATION : 36, loss : 0.05978515026395411ITERATION : 37, loss : 0.05978512159126373ITERATION : 38, loss : 0.05978510091900486ITERATION : 39, loss : 0.05978508590023781ITERATION : 40, loss : 0.059785074950296516ITERATION : 41, loss : 0.05978506704151985ITERATION : 42, loss : 0.05978506128359822ITERATION : 43, loss : 0.059785057171659155ITERATION : 44, loss : 0.05978505418477371ITERATION : 45, loss : 0.05978505198212638ITERATION : 46, loss : 0.05978505036780312ITERATION : 47, loss : 0.05978504934019721ITERATION : 48, loss : 0.059785048668253ITERATION : 49, loss : 0.0597850479743074ITERATION : 50, loss : 0.05978504768132891ITERATION : 51, loss : 0.05978504728935091ITERATION : 52, loss : 0.05978504713216305ITERATION : 53, loss : 0.059785046926728626ITERATION : 54, loss : 0.05978504690605807ITERATION : 55, loss : 0.05978504690605807ITERATION : 56, loss : 0.05978504690605807ITERATION : 57, loss : 0.05978504690605807ITERATION : 58, loss : 0.05978504690605807ITERATION : 59, loss : 0.05978504690605807ITERATION : 60, loss : 0.05978504690605807ITERATION : 61, loss : 0.05978504690605807ITERATION : 62, loss : 0.05978504690605807ITERATION : 63, loss : 0.05978504690605807ITERATION : 64, loss : 0.05978504690605807ITERATION : 65, loss : 0.05978504690605807ITERATION : 66, loss : 0.05978504690605807ITERATION : 67, loss : 0.05978504690605807ITERATION : 68, loss : 0.05978504690605807ITERATION : 69, loss : 0.05978504690605807ITERATION : 70, loss : 0.05978504690605807ITERATION : 71, loss : 0.05978504690605807ITERATION : 72, loss : 0.05978504690605807ITERATION : 73, loss : 0.05978504690605807ITERATION : 74, loss : 0.05978504690605807ITERATION : 75, loss : 0.05978504690605807ITERATION : 76, loss : 0.05978504690605807ITERATION : 77, loss : 0.05978504690605807ITERATION : 78, loss : 0.05978504690605807ITERATION : 79, loss : 0.05978504690605807ITERATION : 80, loss : 0.05978504690605807ITERATION : 81, loss : 0.05978504690605807ITERATION : 82, loss : 0.05978504690605807ITERATION : 83, loss : 0.05978504690605807ITERATION : 84, loss : 0.05978504690605807ITERATION : 85, loss : 0.05978504690605807ITERATION : 86, loss : 0.05978504690605807ITERATION : 87, loss : 0.05978504690605807ITERATION : 88, loss : 0.05978504690605807ITERATION : 89, loss : 0.05978504690605807ITERATION : 90, loss : 0.05978504690605807ITERATION : 91, loss : 0.05978504690605807ITERATION : 92, loss : 0.05978504690605807ITERATION : 93, loss : 0.05978504690605807ITERATION : 94, loss : 0.05978504690605807ITERATION : 95, loss : 0.05978504690605807ITERATION : 96, loss : 0.05978504690605807ITERATION : 97, loss : 0.05978504690605807ITERATION : 98, loss : 0.05978504690605807ITERATION : 99, loss : 0.05978504690605807ITERATION : 100, loss : 0.05978504690605807
ITERATION : 1, loss : 0.023672718116343333ITERATION : 2, loss : 0.019712426113618564ITERATION : 3, loss : 0.018813362513435274ITERATION : 4, loss : 0.018484269628207702ITERATION : 5, loss : 0.018334821856232697ITERATION : 6, loss : 0.01825848468477271ITERATION : 7, loss : 0.018216294911381183ITERATION : 8, loss : 0.0181916203992632ITERATION : 9, loss : 0.018176601628143083ITERATION : 10, loss : 0.018167208177384535ITERATION : 11, loss : 0.018161226794555563ITERATION : 12, loss : 0.018157373749648188ITERATION : 13, loss : 0.018154873372862937ITERATION : 14, loss : 0.018153243187184014ITERATION : 15, loss : 0.018152177242180885ITERATION : 16, loss : 0.018151478985096735ITERATION : 17, loss : 0.018151021041474702ITERATION : 18, loss : 0.018150720505934043ITERATION : 19, loss : 0.018150523186807103ITERATION : 20, loss : 0.018150393604323238ITERATION : 21, loss : 0.01815030850769008ITERATION : 22, loss : 0.01815025260406015ITERATION : 23, loss : 0.018150215892212218ITERATION : 24, loss : 0.01815019178247313ITERATION : 25, loss : 0.018150175944171027ITERATION : 26, loss : 0.018150165537667913ITERATION : 27, loss : 0.018150158716640382ITERATION : 28, loss : 0.018150154232046117ITERATION : 29, loss : 0.018150151305245343ITERATION : 30, loss : 0.018150149384311844ITERATION : 31, loss : 0.01815014810871028ITERATION : 32, loss : 0.01815014726781986ITERATION : 33, loss : 0.018150146712146143ITERATION : 34, loss : 0.018150146351467118ITERATION : 35, loss : 0.01815014611763019ITERATION : 36, loss : 0.018150145966895587ITERATION : 37, loss : 0.01815014586882588ITERATION : 38, loss : 0.01815014580621701ITERATION : 39, loss : 0.018150145764044577ITERATION : 40, loss : 0.018150145743220172ITERATION : 41, loss : 0.018150145719032004ITERATION : 42, loss : 0.018150145716211655ITERATION : 43, loss : 0.018150145710834988ITERATION : 44, loss : 0.018150145709981014ITERATION : 45, loss : 0.018150145709981014ITERATION : 46, loss : 0.018150145709981014ITERATION : 47, loss : 0.018150145709981014ITERATION : 48, loss : 0.018150145709981014ITERATION : 49, loss : 0.018150145709981014ITERATION : 50, loss : 0.018150145709981014ITERATION : 51, loss : 0.018150145709981014ITERATION : 52, loss : 0.018150145709981014ITERATION : 53, loss : 0.018150145709981014ITERATION : 54, loss : 0.018150145709981014ITERATION : 55, loss : 0.018150145709981014ITERATION : 56, loss : 0.018150145709981014ITERATION : 57, loss : 0.018150145709981014ITERATION : 58, loss : 0.018150145709981014ITERATION : 59, loss : 0.018150145709981014ITERATION : 60, loss : 0.018150145709981014ITERATION : 61, loss : 0.018150145709981014ITERATION : 62, loss : 0.018150145709981014ITERATION : 63, loss : 0.018150145709981014ITERATION : 64, loss : 0.018150145709981014ITERATION : 65, loss : 0.018150145709981014ITERATION : 66, loss : 0.018150145709981014ITERATION : 67, loss : 0.018150145709981014ITERATION : 68, loss : 0.018150145709981014ITERATION : 69, loss : 0.018150145709981014ITERATION : 70, loss : 0.018150145709981014ITERATION : 71, loss : 0.018150145709981014ITERATION : 72, loss : 0.018150145709981014ITERATION : 73, loss : 0.018150145709981014ITERATION : 74, loss : 0.018150145709981014ITERATION : 75, loss : 0.018150145709981014ITERATION : 76, loss : 0.018150145709981014ITERATION : 77, loss : 0.018150145709981014ITERATION : 78, loss : 0.018150145709981014ITERATION : 79, loss : 0.018150145709981014ITERATION : 80, loss : 0.018150145709981014ITERATION : 81, loss : 0.018150145709981014ITERATION : 82, loss : 0.018150145709981014ITERATION : 83, loss : 0.018150145709981014ITERATION : 84, loss : 0.018150145709981014ITERATION : 85, loss : 0.018150145709981014ITERATION : 86, loss : 0.018150145709981014ITERATION : 87, loss : 0.018150145709981014ITERATION : 88, loss : 0.018150145709981014ITERATION : 89, loss : 0.018150145709981014ITERATION : 90, loss : 0.018150145709981014ITERATION : 91, loss : 0.018150145709981014ITERATION : 92, loss : 0.018150145709981014ITERATION : 93, loss : 0.018150145709981014ITERATION : 94, loss : 0.018150145709981014ITERATION : 95, loss : 0.018150145709981014ITERATION : 96, loss : 0.018150145709981014ITERATION : 97, loss : 0.018150145709981014ITERATION : 98, loss : 0.018150145709981014ITERATION : 99, loss : 0.018150145709981014ITERATION : 100, loss : 0.018150145709981014
ITERATION : 1, loss : 0.03693347407169324ITERATION : 2, loss : 0.0285807382287474ITERATION : 3, loss : 0.022740637316279795ITERATION : 4, loss : 0.020173210498854885ITERATION : 5, loss : 0.018930064042268154ITERATION : 6, loss : 0.01827908496035695ITERATION : 7, loss : 0.01791593216223512ITERATION : 8, loss : 0.017702697706704643ITERATION : 9, loss : 0.017572188310244836ITERATION : 10, loss : 0.017489596167731648ITERATION : 11, loss : 0.01743591706275463ITERATION : 12, loss : 0.01740029033140255ITERATION : 13, loss : 0.01737625687350374ITERATION : 14, loss : 0.01735984045799913ITERATION : 15, loss : 0.017348520169304976ITERATION : 16, loss : 0.017340658142656333ITERATION : 17, loss : 0.01733516860332412ITERATION : 18, loss : 0.017331320423795295ITERATION : 19, loss : 0.01732861487412939ITERATION : 20, loss : 0.017326708537638187ITERATION : 21, loss : 0.017325363170705536ITERATION : 22, loss : 0.01732441262415306ITERATION : 23, loss : 0.017323740385234807ITERATION : 24, loss : 0.017323264718619894ITERATION : 25, loss : 0.017322928019475815ITERATION : 26, loss : 0.017322689558664096ITERATION : 27, loss : 0.01732252064903785ITERATION : 28, loss : 0.017322400984227784ITERATION : 29, loss : 0.017322316174138847ITERATION : 30, loss : 0.017322256080627044ITERATION : 31, loss : 0.017322213505313086ITERATION : 32, loss : 0.017322183336805914ITERATION : 33, loss : 0.017322161960838887ITERATION : 34, loss : 0.017322146819937354ITERATION : 35, loss : 0.017322136094355633ITERATION : 36, loss : 0.017322128480802754ITERATION : 37, loss : 0.017322123098343793ITERATION : 38, loss : 0.01732211926853352ITERATION : 39, loss : 0.017322116559597457ITERATION : 40, loss : 0.017322114642511096ITERATION : 41, loss : 0.017322113285022048ITERATION : 42, loss : 0.017322112316605968ITERATION : 43, loss : 0.01732211164368599ITERATION : 44, loss : 0.017322111163432113ITERATION : 45, loss : 0.017322110834104544ITERATION : 46, loss : 0.017322110598546825ITERATION : 47, loss : 0.01732211043459525ITERATION : 48, loss : 0.017322110317102332ITERATION : 49, loss : 0.017322110239599904ITERATION : 50, loss : 0.017322110183228268ITERATION : 51, loss : 0.01732211013828931ITERATION : 52, loss : 0.01732211011132676ITERATION : 53, loss : 0.017322110087093733ITERATION : 54, loss : 0.01732211007789561ITERATION : 55, loss : 0.017322110068986922ITERATION : 56, loss : 0.01732211006879172ITERATION : 57, loss : 0.01732211006879172ITERATION : 58, loss : 0.01732211006879172ITERATION : 59, loss : 0.01732211006879172ITERATION : 60, loss : 0.01732211006879172ITERATION : 61, loss : 0.01732211006879172ITERATION : 62, loss : 0.01732211006879172ITERATION : 63, loss : 0.01732211006879172ITERATION : 64, loss : 0.01732211006879172ITERATION : 65, loss : 0.01732211006879172ITERATION : 66, loss : 0.01732211006879172ITERATION : 67, loss : 0.01732211006879172ITERATION : 68, loss : 0.01732211006879172ITERATION : 69, loss : 0.01732211006879172ITERATION : 70, loss : 0.01732211006879172ITERATION : 71, loss : 0.01732211006879172ITERATION : 72, loss : 0.01732211006879172ITERATION : 73, loss : 0.01732211006879172ITERATION : 74, loss : 0.01732211006879172ITERATION : 75, loss : 0.01732211006879172ITERATION : 76, loss : 0.01732211006879172ITERATION : 77, loss : 0.01732211006879172ITERATION : 78, loss : 0.01732211006879172ITERATION : 79, loss : 0.01732211006879172ITERATION : 80, loss : 0.01732211006879172ITERATION : 81, loss : 0.01732211006879172ITERATION : 82, loss : 0.01732211006879172ITERATION : 83, loss : 0.01732211006879172ITERATION : 84, loss : 0.01732211006879172ITERATION : 85, loss : 0.01732211006879172ITERATION : 86, loss : 0.01732211006879172ITERATION : 87, loss : 0.01732211006879172ITERATION : 88, loss : 0.01732211006879172ITERATION : 89, loss : 0.01732211006879172ITERATION : 90, loss : 0.01732211006879172ITERATION : 91, loss : 0.01732211006879172ITERATION : 92, loss : 0.01732211006879172ITERATION : 93, loss : 0.01732211006879172ITERATION : 94, loss : 0.01732211006879172ITERATION : 95, loss : 0.01732211006879172ITERATION : 96, loss : 0.01732211006879172ITERATION : 97, loss : 0.01732211006879172ITERATION : 98, loss : 0.01732211006879172ITERATION : 99, loss : 0.01732211006879172ITERATION : 100, loss : 0.01732211006879172
ITERATION : 1, loss : 0.024951411550224022ITERATION : 2, loss : 0.024645955812879896ITERATION : 3, loss : 0.020703833599382507ITERATION : 4, loss : 0.01915931799095749ITERATION : 5, loss : 0.018536844516418377ITERATION : 6, loss : 0.018293211051764152ITERATION : 7, loss : 0.018209927791040838ITERATION : 8, loss : 0.018194119558457966ITERATION : 9, loss : 0.018205085339363727ITERATION : 10, loss : 0.01822486958462254ITERATION : 11, loss : 0.018245742617618386ITERATION : 12, loss : 0.01826463509346965ITERATION : 13, loss : 0.018280586882818632ITERATION : 14, loss : 0.018293557151967888ITERATION : 15, loss : 0.01830386747976848ITERATION : 16, loss : 0.018311946129719914ITERATION : 17, loss : 0.018318215581382656ITERATION : 18, loss : 0.018323048813383574ITERATION : 19, loss : 0.01832675726743936ITERATION : 20, loss : 0.018329592804240135ITERATION : 21, loss : 0.0183317552693638ITERATION : 22, loss : 0.018333401158518565ITERATION : 23, loss : 0.018334651884282294ITERATION : 24, loss : 0.018335601105926424ITERATION : 25, loss : 0.018336320749131097ITERATION : 26, loss : 0.018336865864814002ITERATION : 27, loss : 0.018337278446084838ITERATION : 28, loss : 0.01833759051838073ITERATION : 29, loss : 0.018337826446551707ITERATION : 30, loss : 0.018338004708679474ITERATION : 31, loss : 0.01833813931765715ITERATION : 32, loss : 0.018338240966530556ITERATION : 33, loss : 0.018338317649982235ITERATION : 34, loss : 0.018338375492223592ITERATION : 35, loss : 0.01833841911015812ITERATION : 36, loss : 0.018338452007700585ITERATION : 37, loss : 0.01833847679633476ITERATION : 38, loss : 0.01833849546850442ITERATION : 39, loss : 0.018338509532937704ITERATION : 40, loss : 0.018338520121674955ITERATION : 41, loss : 0.018338528090826443ITERATION : 42, loss : 0.01833853409993704ITERATION : 43, loss : 0.018338538639089068ITERATION : 44, loss : 0.018338542036444902ITERATION : 45, loss : 0.01833854459645392ITERATION : 46, loss : 0.0183385465259782ITERATION : 47, loss : 0.018338547979029816ITERATION : 48, loss : 0.018338549066728237ITERATION : 49, loss : 0.018338549893247756ITERATION : 50, loss : 0.018338550491685687ITERATION : 51, loss : 0.01833855096515006ITERATION : 52, loss : 0.018338551312485823ITERATION : 53, loss : 0.018338551573791665ITERATION : 54, loss : 0.018338551770034562ITERATION : 55, loss : 0.01833855192972433ITERATION : 56, loss : 0.018338552026837755ITERATION : 57, loss : 0.018338552103817306ITERATION : 58, loss : 0.018338552154482185ITERATION : 59, loss : 0.01833855221900745ITERATION : 60, loss : 0.018338552251756558ITERATION : 61, loss : 0.01833855227605126ITERATION : 62, loss : 0.018338552298303418ITERATION : 63, loss : 0.018338552302886682ITERATION : 64, loss : 0.01833855230641664ITERATION : 65, loss : 0.0183385523235315ITERATION : 66, loss : 0.018338552323540352ITERATION : 67, loss : 0.018338552323540352ITERATION : 68, loss : 0.018338552323540352ITERATION : 69, loss : 0.018338552323540352ITERATION : 70, loss : 0.018338552323540352ITERATION : 71, loss : 0.018338552323540352ITERATION : 72, loss : 0.018338552323540352ITERATION : 73, loss : 0.018338552323540352ITERATION : 74, loss : 0.018338552323540352ITERATION : 75, loss : 0.018338552323540352ITERATION : 76, loss : 0.018338552323540352ITERATION : 77, loss : 0.018338552323540352ITERATION : 78, loss : 0.018338552323540352ITERATION : 79, loss : 0.018338552323540352ITERATION : 80, loss : 0.018338552323540352ITERATION : 81, loss : 0.018338552323540352ITERATION : 82, loss : 0.018338552323540352ITERATION : 83, loss : 0.018338552323540352ITERATION : 84, loss : 0.018338552323540352ITERATION : 85, loss : 0.018338552323540352ITERATION : 86, loss : 0.018338552323540352ITERATION : 87, loss : 0.018338552323540352ITERATION : 88, loss : 0.018338552323540352ITERATION : 89, loss : 0.018338552323540352ITERATION : 90, loss : 0.018338552323540352ITERATION : 91, loss : 0.018338552323540352ITERATION : 92, loss : 0.018338552323540352ITERATION : 93, loss : 0.018338552323540352ITERATION : 94, loss : 0.018338552323540352ITERATION : 95, loss : 0.018338552323540352ITERATION : 96, loss : 0.018338552323540352ITERATION : 97, loss : 0.018338552323540352ITERATION : 98, loss : 0.018338552323540352ITERATION : 99, loss : 0.018338552323540352ITERATION : 100, loss : 0.018338552323540352
ITERATION : 1, loss : 0.066717120695259ITERATION : 2, loss : 0.06786756066404268ITERATION : 3, loss : 0.06839773765520497ITERATION : 4, loss : 0.06863158280498952ITERATION : 5, loss : 0.06877972826674822ITERATION : 6, loss : 0.06889686429982259ITERATION : 7, loss : 0.06899391295252207ITERATION : 8, loss : 0.0690730698457064ITERATION : 9, loss : 0.0691359844946459ITERATION : 10, loss : 0.06918487303649079ITERATION : 11, loss : 0.06922220971717272ITERATION : 12, loss : 0.06925035939665661ITERATION : 13, loss : 0.06927138259385604ITERATION : 14, loss : 0.06928697274066685ITERATION : 15, loss : 0.06929847251898695ITERATION : 16, loss : 0.0693069201123542ITERATION : 17, loss : 0.06931310545484075ITERATION : 18, loss : 0.06931762240483501ITERATION : 19, loss : 0.06932091396707803ITERATION : 20, loss : 0.06932330803015042ITERATION : 21, loss : 0.06932504669298305ITERATION : 22, loss : 0.06932630751275137ITERATION : 23, loss : 0.06932722067437265ITERATION : 24, loss : 0.06932788132853705ITERATION : 25, loss : 0.06932835874750648ITERATION : 26, loss : 0.0693287034959185ITERATION : 27, loss : 0.0693289522738169ITERATION : 28, loss : 0.06932913141193812ITERATION : 29, loss : 0.06932926055876756ITERATION : 30, loss : 0.06932935354940647ITERATION : 31, loss : 0.06932942036731071ITERATION : 32, loss : 0.06932946826498368ITERATION : 33, loss : 0.06932950278635205ITERATION : 34, loss : 0.06932952754297275ITERATION : 35, loss : 0.06932954532207954ITERATION : 36, loss : 0.06932955795875924ITERATION : 37, loss : 0.06932956711248361ITERATION : 38, loss : 0.0693295736950394ITERATION : 39, loss : 0.06932957845888031ITERATION : 40, loss : 0.06932958188132318ITERATION : 41, loss : 0.06932958435620773ITERATION : 42, loss : 0.06932958612394717ITERATION : 43, loss : 0.0693295873890952ITERATION : 44, loss : 0.06932958831857533ITERATION : 45, loss : 0.06932958895703024ITERATION : 46, loss : 0.06932958942727804ITERATION : 47, loss : 0.06932958974646092ITERATION : 48, loss : 0.06932959000116823ITERATION : 49, loss : 0.06932959018163287ITERATION : 50, loss : 0.069329590281328ITERATION : 51, loss : 0.06932959036813817ITERATION : 52, loss : 0.06932959042166979ITERATION : 53, loss : 0.06932959047955492ITERATION : 54, loss : 0.06932959047892219ITERATION : 55, loss : 0.06932959047892219ITERATION : 56, loss : 0.06932959047892219ITERATION : 57, loss : 0.06932959047892219ITERATION : 58, loss : 0.06932959047892219ITERATION : 59, loss : 0.06932959047892219ITERATION : 60, loss : 0.06932959047892219ITERATION : 61, loss : 0.06932959047892219ITERATION : 62, loss : 0.06932959047892219ITERATION : 63, loss : 0.06932959047892219ITERATION : 64, loss : 0.06932959047892219ITERATION : 65, loss : 0.06932959047892219ITERATION : 66, loss : 0.06932959047892219ITERATION : 67, loss : 0.06932959047892219ITERATION : 68, loss : 0.06932959047892219ITERATION : 69, loss : 0.06932959047892219ITERATION : 70, loss : 0.06932959047892219ITERATION : 71, loss : 0.06932959047892219ITERATION : 72, loss : 0.06932959047892219ITERATION : 73, loss : 0.06932959047892219ITERATION : 74, loss : 0.06932959047892219ITERATION : 75, loss : 0.06932959047892219ITERATION : 76, loss : 0.06932959047892219ITERATION : 77, loss : 0.06932959047892219ITERATION : 78, loss : 0.06932959047892219ITERATION : 79, loss : 0.06932959047892219ITERATION : 80, loss : 0.06932959047892219ITERATION : 81, loss : 0.06932959047892219ITERATION : 82, loss : 0.06932959047892219ITERATION : 83, loss : 0.06932959047892219ITERATION : 84, loss : 0.06932959047892219ITERATION : 85, loss : 0.06932959047892219ITERATION : 86, loss : 0.06932959047892219ITERATION : 87, loss : 0.06932959047892219ITERATION : 88, loss : 0.06932959047892219ITERATION : 89, loss : 0.06932959047892219ITERATION : 90, loss : 0.06932959047892219ITERATION : 91, loss : 0.06932959047892219ITERATION : 92, loss : 0.06932959047892219ITERATION : 93, loss : 0.06932959047892219ITERATION : 94, loss : 0.06932959047892219ITERATION : 95, loss : 0.06932959047892219ITERATION : 96, loss : 0.06932959047892219ITERATION : 97, loss : 0.06932959047892219ITERATION : 98, loss : 0.06932959047892219ITERATION : 99, loss : 0.06932959047892219ITERATION : 100, loss : 0.06932959047892219
ITERATION : 1, loss : 0.0431465602982497ITERATION : 2, loss : 0.04468805994902065ITERATION : 3, loss : 0.04293453123012664ITERATION : 4, loss : 0.041020772563470156ITERATION : 5, loss : 0.039543112861317155ITERATION : 6, loss : 0.038480626791495526ITERATION : 7, loss : 0.037727779938385025ITERATION : 8, loss : 0.03719364191738682ITERATION : 9, loss : 0.03681248703998634ITERATION : 10, loss : 0.03653874920105019ITERATION : 11, loss : 0.03634101515713861ITERATION : 12, loss : 0.03619749388201021ITERATION : 13, loss : 0.036092921951669264ITERATION : 14, loss : 0.03601650157899358ITERATION : 15, loss : 0.03596052635176768ITERATION : 16, loss : 0.035919455405565084ITERATION : 17, loss : 0.0358892807725641ITERATION : 18, loss : 0.035867089816475364ITERATION : 19, loss : 0.035850758152129514ITERATION : 20, loss : 0.03583873204604122ITERATION : 21, loss : 0.03582987270515192ITERATION : 22, loss : 0.03582334426072587ITERATION : 23, loss : 0.035818532296747786ITERATION : 24, loss : 0.03581498489373433ITERATION : 25, loss : 0.03581236934588339ITERATION : 26, loss : 0.0358104407038936ITERATION : 27, loss : 0.03580901841503699ITERATION : 28, loss : 0.03580796950912148ITERATION : 29, loss : 0.03580719591658544ITERATION : 30, loss : 0.03580662534163759ITERATION : 31, loss : 0.035806204488646014ITERATION : 32, loss : 0.03580589407460239ITERATION : 33, loss : 0.035805665095625536ITERATION : 34, loss : 0.03580549618221113ITERATION : 35, loss : 0.035805371594611404ITERATION : 36, loss : 0.035805279705871225ITERATION : 37, loss : 0.03580521195417981ITERATION : 38, loss : 0.03580516194275808ITERATION : 39, loss : 0.03580512504072252ITERATION : 40, loss : 0.03580509782351795ITERATION : 41, loss : 0.03580507775468523ITERATION : 42, loss : 0.03580506295864592ITERATION : 43, loss : 0.03580505203009146ITERATION : 44, loss : 0.0358050439835643ITERATION : 45, loss : 0.03580503803045256ITERATION : 46, loss : 0.035805033644621606ITERATION : 47, loss : 0.035805030412518306ITERATION : 48, loss : 0.03580502802938432ITERATION : 49, loss : 0.03580502627068511ITERATION : 50, loss : 0.03580502496871437ITERATION : 51, loss : 0.03580502398518539ITERATION : 52, loss : 0.03580502333166923ITERATION : 53, loss : 0.035805022831407206ITERATION : 54, loss : 0.03580502246363286ITERATION : 55, loss : 0.0358050221583251ITERATION : 56, loss : 0.035805021952685694ITERATION : 57, loss : 0.03580502180342062ITERATION : 58, loss : 0.03580502166403046ITERATION : 59, loss : 0.03580502163378292ITERATION : 60, loss : 0.03580502153926009ITERATION : 61, loss : 0.03580502154268156ITERATION : 62, loss : 0.03580502154268156ITERATION : 63, loss : 0.03580502154268156ITERATION : 64, loss : 0.03580502154268156ITERATION : 65, loss : 0.03580502154268156ITERATION : 66, loss : 0.03580502154268156ITERATION : 67, loss : 0.03580502154268156ITERATION : 68, loss : 0.03580502154268156ITERATION : 69, loss : 0.03580502154268156ITERATION : 70, loss : 0.03580502154268156ITERATION : 71, loss : 0.03580502154268156ITERATION : 72, loss : 0.03580502154268156ITERATION : 73, loss : 0.03580502154268156ITERATION : 74, loss : 0.03580502154268156ITERATION : 75, loss : 0.03580502154268156ITERATION : 76, loss : 0.03580502154268156ITERATION : 77, loss : 0.03580502154268156ITERATION : 78, loss : 0.03580502154268156ITERATION : 79, loss : 0.03580502154268156ITERATION : 80, loss : 0.03580502154268156ITERATION : 81, loss : 0.03580502154268156ITERATION : 82, loss : 0.03580502154268156ITERATION : 83, loss : 0.03580502154268156ITERATION : 84, loss : 0.03580502154268156ITERATION : 85, loss : 0.03580502154268156ITERATION : 86, loss : 0.03580502154268156ITERATION : 87, loss : 0.03580502154268156ITERATION : 88, loss : 0.03580502154268156ITERATION : 89, loss : 0.03580502154268156ITERATION : 90, loss : 0.03580502154268156ITERATION : 91, loss : 0.03580502154268156ITERATION : 92, loss : 0.03580502154268156ITERATION : 93, loss : 0.03580502154268156ITERATION : 94, loss : 0.03580502154268156ITERATION : 95, loss : 0.03580502154268156ITERATION : 96, loss : 0.03580502154268156ITERATION : 97, loss : 0.03580502154268156ITERATION : 98, loss : 0.03580502154268156ITERATION : 99, loss : 0.03580502154268156ITERATION : 100, loss : 0.03580502154268156
ITERATION : 1, loss : 0.026205482470786873ITERATION : 2, loss : 0.02467866256311578ITERATION : 3, loss : 0.024572736244130148ITERATION : 4, loss : 0.02475870400261813ITERATION : 5, loss : 0.025487907986064564ITERATION : 6, loss : 0.026294992337755084ITERATION : 7, loss : 0.027031873906863457ITERATION : 8, loss : 0.027657591692267912ITERATION : 9, loss : 0.028170334624513533ITERATION : 10, loss : 0.028582140178540542ITERATION : 11, loss : 0.028908803562185875ITERATION : 12, loss : 0.029165830010621174ITERATION : 13, loss : 0.029366940006215127ITERATION : 14, loss : 0.029523676663756694ITERATION : 15, loss : 0.02964547890405192ITERATION : 16, loss : 0.02973993017118277ITERATION : 17, loss : 0.029813053019401473ITERATION : 18, loss : 0.0298695927697597ITERATION : 19, loss : 0.029913267506851773ITERATION : 20, loss : 0.029946978593816866ITERATION : 21, loss : 0.029972982919926634ITERATION : 22, loss : 0.029993032449902746ITERATION : 23, loss : 0.030008484464140856ITERATION : 24, loss : 0.030020389227340405ITERATION : 25, loss : 0.03002955850591524ITERATION : 26, loss : 0.0300366192146066ITERATION : 27, loss : 0.030042055155066482ITERATION : 28, loss : 0.030046239465886224ITERATION : 29, loss : 0.030049459878113423ITERATION : 30, loss : 0.03005193813213665ITERATION : 31, loss : 0.03005384504006731ITERATION : 32, loss : 0.03005531216181223ITERATION : 33, loss : 0.030056440828758177ITERATION : 34, loss : 0.03005730906461973ITERATION : 35, loss : 0.030057976897855106ITERATION : 36, loss : 0.030058490540961852ITERATION : 37, loss : 0.03005888562808502ITERATION : 38, loss : 0.030059189478070267ITERATION : 39, loss : 0.030059423112142145ITERATION : 40, loss : 0.03005960278939019ITERATION : 41, loss : 0.030059740919522546ITERATION : 42, loss : 0.03005984717050801ITERATION : 43, loss : 0.030059928832857387ITERATION : 44, loss : 0.030059991627693426ITERATION : 45, loss : 0.03006003995289311ITERATION : 46, loss : 0.0300600770495345ITERATION : 47, loss : 0.03006010561153718ITERATION : 48, loss : 0.030060127583755165ITERATION : 49, loss : 0.030060144447173236ITERATION : 50, loss : 0.03006015742265836ITERATION : 51, loss : 0.030060167400995225ITERATION : 52, loss : 0.030060175049624643ITERATION : 53, loss : 0.030060180936860584ITERATION : 54, loss : 0.03006018545635474ITERATION : 55, loss : 0.030060188939017275ITERATION : 56, loss : 0.03006019162332927ITERATION : 57, loss : 0.03006019375331752ITERATION : 58, loss : 0.030060195299350983ITERATION : 59, loss : 0.03006019648985322ITERATION : 60, loss : 0.030060197402493438ITERATION : 61, loss : 0.030060198083758972ITERATION : 62, loss : 0.030060198618791886ITERATION : 63, loss : 0.0300601991039202ITERATION : 64, loss : 0.030060199390666634ITERATION : 65, loss : 0.030060199577366827ITERATION : 66, loss : 0.030060199771099ITERATION : 67, loss : 0.030060199866271575ITERATION : 68, loss : 0.030060199997303118ITERATION : 69, loss : 0.03006020007396072ITERATION : 70, loss : 0.030060200100612673ITERATION : 71, loss : 0.030060200123947545ITERATION : 72, loss : 0.03006020021703111ITERATION : 73, loss : 0.030060200219490667ITERATION : 74, loss : 0.030060200219490667ITERATION : 75, loss : 0.030060200219490667ITERATION : 76, loss : 0.030060200219490667ITERATION : 77, loss : 0.030060200219490667ITERATION : 78, loss : 0.030060200219490667ITERATION : 79, loss : 0.030060200219490667ITERATION : 80, loss : 0.030060200219490667ITERATION : 81, loss : 0.030060200219490667ITERATION : 82, loss : 0.030060200219490667ITERATION : 83, loss : 0.030060200219490667ITERATION : 84, loss : 0.030060200219490667ITERATION : 85, loss : 0.030060200219490667ITERATION : 86, loss : 0.030060200219490667ITERATION : 87, loss : 0.030060200219490667ITERATION : 88, loss : 0.030060200219490667ITERATION : 89, loss : 0.030060200219490667ITERATION : 90, loss : 0.030060200219490667ITERATION : 91, loss : 0.030060200219490667ITERATION : 92, loss : 0.030060200219490667ITERATION : 93, loss : 0.030060200219490667ITERATION : 94, loss : 0.030060200219490667ITERATION : 95, loss : 0.030060200219490667ITERATION : 96, loss : 0.030060200219490667ITERATION : 97, loss : 0.030060200219490667ITERATION : 98, loss : 0.030060200219490667ITERATION : 99, loss : 0.030060200219490667ITERATION : 100, loss : 0.030060200219490667
ITERATION : 1, loss : 0.057702550693982634ITERATION : 2, loss : 0.04809592229628836ITERATION : 3, loss : 0.04520274654679049ITERATION : 4, loss : 0.04540200436062016ITERATION : 5, loss : 0.04640786267695532ITERATION : 6, loss : 0.047346378839316694ITERATION : 7, loss : 0.048179589363729264ITERATION : 8, loss : 0.048900836059718365ITERATION : 9, loss : 0.049515520043010255ITERATION : 10, loss : 0.050033871259550035ITERATION : 11, loss : 0.05046767860477028ITERATION : 12, loss : 0.05082868984619071ITERATION : 13, loss : 0.051127836010274375ITERATION : 14, loss : 0.05137489750559428ITERATION : 15, loss : 0.05157841127807097ITERATION : 16, loss : 0.05174570487446602ITERATION : 17, loss : 0.05188299348131793ITERATION : 18, loss : 0.05199550472423363ITERATION : 19, loss : 0.05208760594879361ITERATION : 20, loss : 0.05216292889868967ITERATION : 21, loss : 0.052224481521861606ITERATION : 22, loss : 0.05227474758555881ITERATION : 23, loss : 0.052315773294381085ITERATION : 24, loss : 0.052349241069473966ITERATION : 25, loss : 0.052376531945979805ITERATION : 26, loss : 0.05239877780858368ITERATION : 27, loss : 0.05241690560632913ITERATION : 28, loss : 0.05243167377162033ITERATION : 29, loss : 0.052443702100093345ITERATION : 30, loss : 0.05245349660541095ITERATION : 31, loss : 0.05246147095642277ITERATION : 32, loss : 0.05246796227976ITERATION : 33, loss : 0.05247324561083287ITERATION : 34, loss : 0.0524775451591157ITERATION : 35, loss : 0.05248104388840174ITERATION : 36, loss : 0.052483890574452054ITERATION : 37, loss : 0.05248620647344499ITERATION : 38, loss : 0.05248809049827524ITERATION : 39, loss : 0.05248962315920667ITERATION : 40, loss : 0.05249086985026845ITERATION : 41, loss : 0.052491883775832256ITERATION : 42, loss : 0.052492708427434455ITERATION : 43, loss : 0.05249337915165647ITERATION : 44, loss : 0.052493924572222905ITERATION : 45, loss : 0.05249436812234413ITERATION : 46, loss : 0.05249472877202278ITERATION : 47, loss : 0.05249502209884063ITERATION : 48, loss : 0.052495260747129846ITERATION : 49, loss : 0.052495454791341344ITERATION : 50, loss : 0.052495612493109994ITERATION : 51, loss : 0.05249574085309007ITERATION : 52, loss : 0.05249584513456047ITERATION : 53, loss : 0.05249592990168625ITERATION : 54, loss : 0.05249599882346188ITERATION : 55, loss : 0.05249605497827074ITERATION : 56, loss : 0.05249610038723526ITERATION : 57, loss : 0.05249613746097733ITERATION : 58, loss : 0.0524961675324338ITERATION : 59, loss : 0.052496192036726216ITERATION : 60, loss : 0.052496211975287706ITERATION : 61, loss : 0.05249622828874218ITERATION : 62, loss : 0.05249624143415118ITERATION : 63, loss : 0.05249625211710461ITERATION : 64, loss : 0.0524962607180721ITERATION : 65, loss : 0.05249626794437976ITERATION : 66, loss : 0.05249627366879242ITERATION : 67, loss : 0.05249627843128086ITERATION : 68, loss : 0.05249628217794783ITERATION : 69, loss : 0.0524962851112931ITERATION : 70, loss : 0.0524962876149182ITERATION : 71, loss : 0.052496289659071535ITERATION : 72, loss : 0.05249629128614429ITERATION : 73, loss : 0.05249629259788498ITERATION : 74, loss : 0.05249629371341662ITERATION : 75, loss : 0.052496294752948196ITERATION : 76, loss : 0.052496295370457094ITERATION : 77, loss : 0.052496295934509954ITERATION : 78, loss : 0.052496296366779466ITERATION : 79, loss : 0.05249629665267339ITERATION : 80, loss : 0.05249629704676491ITERATION : 81, loss : 0.05249629727407119ITERATION : 82, loss : 0.052496297433672186ITERATION : 83, loss : 0.05249629751908954ITERATION : 84, loss : 0.052496297584186905ITERATION : 85, loss : 0.05249629774185178ITERATION : 86, loss : 0.05249629778354359ITERATION : 87, loss : 0.05249629791788738ITERATION : 88, loss : 0.05249629792152469ITERATION : 89, loss : 0.05249629792152469ITERATION : 90, loss : 0.05249629792152469ITERATION : 91, loss : 0.05249629792152469ITERATION : 92, loss : 0.05249629792152469ITERATION : 93, loss : 0.05249629792152469ITERATION : 94, loss : 0.05249629792152469ITERATION : 95, loss : 0.05249629792152469ITERATION : 96, loss : 0.05249629792152469ITERATION : 97, loss : 0.05249629792152469ITERATION : 98, loss : 0.05249629792152469ITERATION : 99, loss : 0.05249629792152469ITERATION : 100, loss : 0.05249629792152469
gradient norm in None layer : 0.002163202398240351
gradient norm in None layer : 0.00010987511876607707
gradient norm in None layer : 0.0001509785282756887
gradient norm in None layer : 0.0020447509284216158
gradient norm in None layer : 0.00014212576359928062
gradient norm in None layer : 0.00016296937844878802
gradient norm in None layer : 0.0009493477014916999
gradient norm in None layer : 4.268083196362247e-05
gradient norm in None layer : 3.6566674355209025e-05
gradient norm in None layer : 0.0009485859925842555
gradient norm in None layer : 4.372330849025979e-05
gradient norm in None layer : 3.8095637285064144e-05
gradient norm in None layer : 0.0003040067382138564
gradient norm in None layer : 8.006606251667665e-06
gradient norm in None layer : 6.328731497114988e-06
gradient norm in None layer : 0.0002758353592291051
gradient norm in None layer : 1.0746933740739105e-05
gradient norm in None layer : 1.0047569611076236e-05
gradient norm in None layer : 0.00039646987765930546
gradient norm in None layer : 2.362670865149178e-06
gradient norm in None layer : 0.0009074136877205519
gradient norm in None layer : 6.502672922880249e-05
gradient norm in None layer : 5.175377729314633e-05
gradient norm in None layer : 0.001093975982092299
gradient norm in None layer : 7.121314219308456e-05
gradient norm in None layer : 5.8535064549571276e-05
gradient norm in None layer : 0.0015590579956490749
gradient norm in None layer : 4.937926089985899e-06
gradient norm in None layer : 0.002281559609820169
gradient norm in None layer : 0.0001654858255151643
gradient norm in None layer : 0.0001572903168851288
gradient norm in None layer : 0.0024890462866135744
gradient norm in None layer : 0.00027923984885928703
gradient norm in None layer : 0.0003817040373955457
gradient norm in None layer : 0.00025224631101302905
gradient norm in None layer : 6.53329562228613e-05
Total gradient norm: 0.005223650983802789
invariance loss : 5.260982790174877, avg_den : 0.404296875, density loss : 0.30579833984375004, mse loss : 0.03766087064637379, solver time : 139.32132649421692 sec , total loss : 0.043227651776392415, running loss : 0.06278596925615912
Epoch 0/10 , batch 10/12500 
ITERATION : 1, loss : 0.033525201293229764ITERATION : 2, loss : 0.030998972063794497ITERATION : 3, loss : 0.030307967643695397ITERATION : 4, loss : 0.029991903155764606ITERATION : 5, loss : 0.029807375425034945ITERATION : 6, loss : 0.029686500655340686ITERATION : 7, loss : 0.029602739925043076ITERATION : 8, loss : 0.029543025317378024ITERATION : 9, loss : 0.02949984606627549ITERATION : 10, loss : 0.02946840873705366ITERATION : 11, loss : 0.029445447253681593ITERATION : 12, loss : 0.029428652896727688ITERATION : 13, loss : 0.029416362116769738ITERATION : 14, loss : 0.02940736529586717ITERATION : 15, loss : 0.029400779166205433ITERATION : 16, loss : 0.029396237106373953ITERATION : 17, loss : 0.029393006026080172ITERATION : 18, loss : 0.029390642277739163ITERATION : 19, loss : 0.029388912798305224ITERATION : 20, loss : 0.02938764720011165ITERATION : 21, loss : 0.02938672090440279ITERATION : 22, loss : 0.029386042849307396ITERATION : 23, loss : 0.029385546389000938ITERATION : 24, loss : 0.029385182892409926ITERATION : 25, loss : 0.02938491666404615ITERATION : 26, loss : 0.029384721701166988ITERATION : 27, loss : 0.02938457885000262ITERATION : 28, loss : 0.02938447422599041ITERATION : 29, loss : 0.02938439757317012ITERATION : 30, loss : 0.029384341351569577ITERATION : 31, loss : 0.02938430020779529ITERATION : 32, loss : 0.02938427002889687ITERATION : 33, loss : 0.029384247914124976ITERATION : 34, loss : 0.029384231710929163ITERATION : 35, loss : 0.02938421984337508ITERATION : 36, loss : 0.02938421113073158ITERATION : 37, loss : 0.029384204738087734ITERATION : 38, loss : 0.02938420005271121ITERATION : 39, loss : 0.029384196623507ITERATION : 40, loss : 0.029384194149412967ITERATION : 41, loss : 0.029384192308225074ITERATION : 42, loss : 0.02938419097368487ITERATION : 43, loss : 0.029384189992348833ITERATION : 44, loss : 0.029384189287361816ITERATION : 45, loss : 0.02938418877090158ITERATION : 46, loss : 0.02938418839365749ITERATION : 47, loss : 0.029384188110212754ITERATION : 48, loss : 0.029384187903748738ITERATION : 49, loss : 0.029384187707861158ITERATION : 50, loss : 0.02938418761152948ITERATION : 51, loss : 0.029384187518265284ITERATION : 52, loss : 0.02938418747172467ITERATION : 53, loss : 0.029384187376142012ITERATION : 54, loss : 0.02938418736923037ITERATION : 55, loss : 0.029384187269328444ITERATION : 56, loss : 0.029384187269328444ITERATION : 57, loss : 0.029384187269328444ITERATION : 58, loss : 0.029384187269328444ITERATION : 59, loss : 0.029384187269328444ITERATION : 60, loss : 0.029384187269328444ITERATION : 61, loss : 0.029384187269328444ITERATION : 62, loss : 0.029384187269328444ITERATION : 63, loss : 0.029384187269328444ITERATION : 64, loss : 0.029384187269328444ITERATION : 65, loss : 0.029384187269328444ITERATION : 66, loss : 0.029384187269328444ITERATION : 67, loss : 0.029384187269328444ITERATION : 68, loss : 0.029384187269328444ITERATION : 69, loss : 0.029384187269328444ITERATION : 70, loss : 0.029384187269328444ITERATION : 71, loss : 0.029384187269328444ITERATION : 72, loss : 0.029384187269328444ITERATION : 73, loss : 0.029384187269328444ITERATION : 74, loss : 0.029384187269328444ITERATION : 75, loss : 0.029384187269328444ITERATION : 76, loss : 0.029384187269328444ITERATION : 77, loss : 0.029384187269328444ITERATION : 78, loss : 0.029384187269328444ITERATION : 79, loss : 0.029384187269328444ITERATION : 80, loss : 0.029384187269328444ITERATION : 81, loss : 0.029384187269328444ITERATION : 82, loss : 0.029384187269328444ITERATION : 83, loss : 0.029384187269328444ITERATION : 84, loss : 0.029384187269328444ITERATION : 85, loss : 0.029384187269328444ITERATION : 86, loss : 0.029384187269328444ITERATION : 87, loss : 0.029384187269328444ITERATION : 88, loss : 0.029384187269328444ITERATION : 89, loss : 0.029384187269328444ITERATION : 90, loss : 0.029384187269328444ITERATION : 91, loss : 0.029384187269328444ITERATION : 92, loss : 0.029384187269328444ITERATION : 93, loss : 0.029384187269328444ITERATION : 94, loss : 0.029384187269328444ITERATION : 95, loss : 0.029384187269328444ITERATION : 96, loss : 0.029384187269328444ITERATION : 97, loss : 0.029384187269328444ITERATION : 98, loss : 0.029384187269328444ITERATION : 99, loss : 0.029384187269328444ITERATION : 100, loss : 0.029384187269328444
ITERATION : 1, loss : 0.016779419278523226ITERATION : 2, loss : 0.014779916020757775ITERATION : 3, loss : 0.014354978536562178ITERATION : 4, loss : 0.014335706301495367ITERATION : 5, loss : 0.01446726061792619ITERATION : 6, loss : 0.014645570808731223ITERATION : 7, loss : 0.014821273452704169ITERATION : 8, loss : 0.01497364054077143ITERATION : 9, loss : 0.015096992738812357ITERATION : 10, loss : 0.015192763975298671ITERATION : 11, loss : 0.015265131286149307ITERATION : 12, loss : 0.015318824720496881ITERATION : 13, loss : 0.015358165100114441ITERATION : 14, loss : 0.015386736825712092ITERATION : 15, loss : 0.015407358884381267ITERATION : 16, loss : 0.01542217742490912ITERATION : 17, loss : 0.01543279197619171ITERATION : 18, loss : 0.015440377831606272ITERATION : 19, loss : 0.015445790389199776ITERATION : 20, loss : 0.015449647625911413ITERATION : 21, loss : 0.015452394119682182ITERATION : 22, loss : 0.015454348568697325ITERATION : 23, loss : 0.01545573880300316ITERATION : 24, loss : 0.015456727302556064ITERATION : 25, loss : 0.015457430037824075ITERATION : 26, loss : 0.015457929533708746ITERATION : 27, loss : 0.015458284519348522ITERATION : 28, loss : 0.015458536768328826ITERATION : 29, loss : 0.015458716028939619ITERATION : 30, loss : 0.015458843408505233ITERATION : 31, loss : 0.015458933923934971ITERATION : 32, loss : 0.015458998246310383ITERATION : 33, loss : 0.015459043943606771ITERATION : 34, loss : 0.015459076405172262ITERATION : 35, loss : 0.015459099470788142ITERATION : 36, loss : 0.015459115864293193ITERATION : 37, loss : 0.015459127513921663ITERATION : 38, loss : 0.015459135807747115ITERATION : 39, loss : 0.015459141668219812ITERATION : 40, loss : 0.015459145867645446ITERATION : 41, loss : 0.01545914882008651ITERATION : 42, loss : 0.015459150956657414ITERATION : 43, loss : 0.015459152416596018ITERATION : 44, loss : 0.015459153505262441ITERATION : 45, loss : 0.015459154232293028ITERATION : 46, loss : 0.015459154787768457ITERATION : 47, loss : 0.015459155119451673ITERATION : 48, loss : 0.015459155417633362ITERATION : 49, loss : 0.01545915559384478ITERATION : 50, loss : 0.01545915576774919ITERATION : 51, loss : 0.015459155838764158ITERATION : 52, loss : 0.015459155924736125ITERATION : 53, loss : 0.015459155953906348ITERATION : 54, loss : 0.015459155984534212ITERATION : 55, loss : 0.015459155985096302ITERATION : 56, loss : 0.015459155985096302ITERATION : 57, loss : 0.015459155985096302ITERATION : 58, loss : 0.015459155985096302ITERATION : 59, loss : 0.015459155985096302ITERATION : 60, loss : 0.015459155985096302ITERATION : 61, loss : 0.015459155985096302ITERATION : 62, loss : 0.015459155985096302ITERATION : 63, loss : 0.015459155985096302ITERATION : 64, loss : 0.015459155985096302ITERATION : 65, loss : 0.015459155985096302ITERATION : 66, loss : 0.015459155985096302ITERATION : 67, loss : 0.015459155985096302ITERATION : 68, loss : 0.015459155985096302ITERATION : 69, loss : 0.015459155985096302ITERATION : 70, loss : 0.015459155985096302ITERATION : 71, loss : 0.015459155985096302ITERATION : 72, loss : 0.015459155985096302ITERATION : 73, loss : 0.015459155985096302ITERATION : 74, loss : 0.015459155985096302ITERATION : 75, loss : 0.015459155985096302ITERATION : 76, loss : 0.015459155985096302ITERATION : 77, loss : 0.015459155985096302ITERATION : 78, loss : 0.015459155985096302ITERATION : 79, loss : 0.015459155985096302ITERATION : 80, loss : 0.015459155985096302ITERATION : 81, loss : 0.015459155985096302ITERATION : 82, loss : 0.015459155985096302ITERATION : 83, loss : 0.015459155985096302ITERATION : 84, loss : 0.015459155985096302ITERATION : 85, loss : 0.015459155985096302ITERATION : 86, loss : 0.015459155985096302ITERATION : 87, loss : 0.015459155985096302ITERATION : 88, loss : 0.015459155985096302ITERATION : 89, loss : 0.015459155985096302ITERATION : 90, loss : 0.015459155985096302ITERATION : 91, loss : 0.015459155985096302ITERATION : 92, loss : 0.015459155985096302ITERATION : 93, loss : 0.015459155985096302ITERATION : 94, loss : 0.015459155985096302ITERATION : 95, loss : 0.015459155985096302ITERATION : 96, loss : 0.015459155985096302ITERATION : 97, loss : 0.015459155985096302ITERATION : 98, loss : 0.015459155985096302ITERATION : 99, loss : 0.015459155985096302ITERATION : 100, loss : 0.015459155985096302
ITERATION : 1, loss : 0.0609751731151613ITERATION : 2, loss : 0.04762067169516914ITERATION : 3, loss : 0.043656078011678895ITERATION : 4, loss : 0.04222130347629987ITERATION : 5, loss : 0.04165903190735008ITERATION : 6, loss : 0.04143001818754949ITERATION : 7, loss : 0.04133543794848194ITERATION : 8, loss : 0.04129718795044865ITERATION : 9, loss : 0.041283197847701646ITERATION : 10, loss : 0.041279774325168626ITERATION : 11, loss : 0.0412808564864532ITERATION : 12, loss : 0.0412836841593244ITERATION : 13, loss : 0.04128698509521818ITERATION : 14, loss : 0.041290184447206786ITERATION : 15, loss : 0.04129304343949658ITERATION : 16, loss : 0.041295487162213186ITERATION : 17, loss : 0.04129751865342078ITERATION : 18, loss : 0.04129917634178635ITERATION : 19, loss : 0.041300511127735556ITERATION : 20, loss : 0.04130157518567ITERATION : 21, loss : 0.041302416967329454ITERATION : 22, loss : 0.04130307919021054ITERATION : 23, loss : 0.04130359716706131ITERATION : 24, loss : 0.04130400094580594ITERATION : 25, loss : 0.0413043146123666ITERATION : 26, loss : 0.04130455757117414ITERATION : 27, loss : 0.041304745407174ITERATION : 28, loss : 0.0413048902940185ITERATION : 29, loss : 0.04130500182531252ITERATION : 30, loss : 0.04130508758642357ITERATION : 31, loss : 0.04130515336913886ITERATION : 32, loss : 0.04130520382640572ITERATION : 33, loss : 0.04130524247288288ITERATION : 34, loss : 0.04130527204823643ITERATION : 35, loss : 0.041305294610210354ITERATION : 36, loss : 0.041305311813356116ITERATION : 37, loss : 0.041305325006449345ITERATION : 38, loss : 0.04130533507075556ITERATION : 39, loss : 0.041305342788098116ITERATION : 40, loss : 0.04130534864141424ITERATION : 41, loss : 0.041305353146536494ITERATION : 42, loss : 0.0413053565410987ITERATION : 43, loss : 0.041305359162066235ITERATION : 44, loss : 0.04130536111796454ITERATION : 45, loss : 0.04130536264744363ITERATION : 46, loss : 0.04130536377045439ITERATION : 47, loss : 0.04130536466086998ITERATION : 48, loss : 0.0413053653036896ITERATION : 49, loss : 0.04130536583039838ITERATION : 50, loss : 0.041305366187748487ITERATION : 51, loss : 0.041305366503709355ITERATION : 52, loss : 0.04130536670659755ITERATION : 53, loss : 0.04130536689507884ITERATION : 54, loss : 0.04130536701160758ITERATION : 55, loss : 0.04130536710473642ITERATION : 56, loss : 0.0413053671691351ITERATION : 57, loss : 0.04130536722967255ITERATION : 58, loss : 0.04130536726366674ITERATION : 59, loss : 0.041305367304854256ITERATION : 60, loss : 0.04130536732038113ITERATION : 61, loss : 0.041305367351665276ITERATION : 62, loss : 0.041305367353810206ITERATION : 63, loss : 0.041305367375377315ITERATION : 64, loss : 0.04130536736989618ITERATION : 65, loss : 0.04130536737000769ITERATION : 66, loss : 0.04130536737000769ITERATION : 67, loss : 0.04130536737000769ITERATION : 68, loss : 0.04130536737000769ITERATION : 69, loss : 0.04130536737000769ITERATION : 70, loss : 0.04130536737000769ITERATION : 71, loss : 0.04130536737000769ITERATION : 72, loss : 0.04130536737000769ITERATION : 73, loss : 0.04130536737000769ITERATION : 74, loss : 0.04130536737000769ITERATION : 75, loss : 0.04130536737000769ITERATION : 76, loss : 0.04130536737000769ITERATION : 77, loss : 0.04130536737000769ITERATION : 78, loss : 0.04130536737000769ITERATION : 79, loss : 0.04130536737000769ITERATION : 80, loss : 0.04130536737000769ITERATION : 81, loss : 0.04130536737000769ITERATION : 82, loss : 0.04130536737000769ITERATION : 83, loss : 0.04130536737000769ITERATION : 84, loss : 0.04130536737000769ITERATION : 85, loss : 0.04130536737000769ITERATION : 86, loss : 0.04130536737000769ITERATION : 87, loss : 0.04130536737000769ITERATION : 88, loss : 0.04130536737000769ITERATION : 89, loss : 0.04130536737000769ITERATION : 90, loss : 0.04130536737000769ITERATION : 91, loss : 0.04130536737000769ITERATION : 92, loss : 0.04130536737000769ITERATION : 93, loss : 0.04130536737000769ITERATION : 94, loss : 0.04130536737000769ITERATION : 95, loss : 0.04130536737000769ITERATION : 96, loss : 0.04130536737000769ITERATION : 97, loss : 0.04130536737000769ITERATION : 98, loss : 0.04130536737000769ITERATION : 99, loss : 0.04130536737000769ITERATION : 100, loss : 0.04130536737000769
ITERATION : 1, loss : 0.06739865341927033ITERATION : 2, loss : 0.04625034166108484ITERATION : 3, loss : 0.033175866089022586ITERATION : 4, loss : 0.025353400803177304ITERATION : 5, loss : 0.021320717112114355ITERATION : 6, loss : 0.01923676446820359ITERATION : 7, loss : 0.01794141498067143ITERATION : 8, loss : 0.017106372786065558ITERATION : 9, loss : 0.016551618965084027ITERATION : 10, loss : 0.01617390254567769ITERATION : 11, loss : 0.015911598020844894ITERATION : 12, loss : 0.0157265786260915ITERATION : 13, loss : 0.015594482119705108ITERATION : 14, loss : 0.015499290161134792ITERATION : 15, loss : 0.01543020791559304ITERATION : 16, loss : 0.015379808457745748ITERATION : 17, loss : 0.01534289435643923ITERATION : 18, loss : 0.01531577874569698ITERATION : 19, loss : 0.015295818300123493ITERATION : 20, loss : 0.01528110220054167ITERATION : 21, loss : 0.01527024042741761ITERATION : 22, loss : 0.015262216994871005ITERATION : 23, loss : 0.015256286947775212ITERATION : 24, loss : 0.015251902386131303ITERATION : 25, loss : 0.015248659592098916ITERATION : 26, loss : 0.015246260863803776ITERATION : 27, loss : 0.015244486290617306ITERATION : 28, loss : 0.015243173398624755ITERATION : 29, loss : 0.015242202079276559ITERATION : 30, loss : 0.01524148348009643ITERATION : 31, loss : 0.015240951768695075ITERATION : 32, loss : 0.01524055840417875ITERATION : 33, loss : 0.01524026738009453ITERATION : 34, loss : 0.015240052121577767ITERATION : 35, loss : 0.015239892888159332ITERATION : 36, loss : 0.015239775129372444ITERATION : 37, loss : 0.015239688007836988ITERATION : 38, loss : 0.015239623552525748ITERATION : 39, loss : 0.015239575872010748ITERATION : 40, loss : 0.01523954060501978ITERATION : 41, loss : 0.015239514506738524ITERATION : 42, loss : 0.015239495214508967ITERATION : 43, loss : 0.015239480923292298ITERATION : 44, loss : 0.015239470376765044ITERATION : 45, loss : 0.015239462565521496ITERATION : 46, loss : 0.01523945680093702ITERATION : 47, loss : 0.015239452521654073ITERATION : 48, loss : 0.015239449378797707ITERATION : 49, loss : 0.01523944704397059ITERATION : 50, loss : 0.01523944532740034ITERATION : 51, loss : 0.015239444041766817ITERATION : 52, loss : 0.015239443108067205ITERATION : 53, loss : 0.015239442405592081ITERATION : 54, loss : 0.015239441903713487ITERATION : 55, loss : 0.015239441535939329ITERATION : 56, loss : 0.015239441258816013ITERATION : 57, loss : 0.015239441043676687ITERATION : 58, loss : 0.015239440891758169ITERATION : 59, loss : 0.015239440784695312ITERATION : 60, loss : 0.015239440722669337ITERATION : 61, loss : 0.0152394406503261ITERATION : 62, loss : 0.015239440606946847ITERATION : 63, loss : 0.015239440570439373ITERATION : 64, loss : 0.015239440559746785ITERATION : 65, loss : 0.015239440546433761ITERATION : 66, loss : 0.015239440546440908ITERATION : 67, loss : 0.015239440546440908ITERATION : 68, loss : 0.015239440546440908ITERATION : 69, loss : 0.015239440546440908ITERATION : 70, loss : 0.015239440546440908ITERATION : 71, loss : 0.015239440546440908ITERATION : 72, loss : 0.015239440546440908ITERATION : 73, loss : 0.015239440546440908ITERATION : 74, loss : 0.015239440546440908ITERATION : 75, loss : 0.015239440546440908ITERATION : 76, loss : 0.015239440546440908ITERATION : 77, loss : 0.015239440546440908ITERATION : 78, loss : 0.015239440546440908ITERATION : 79, loss : 0.015239440546440908ITERATION : 80, loss : 0.015239440546440908ITERATION : 81, loss : 0.015239440546440908ITERATION : 82, loss : 0.015239440546440908ITERATION : 83, loss : 0.015239440546440908ITERATION : 84, loss : 0.015239440546440908ITERATION : 85, loss : 0.015239440546440908ITERATION : 86, loss : 0.015239440546440908ITERATION : 87, loss : 0.015239440546440908ITERATION : 88, loss : 0.015239440546440908ITERATION : 89, loss : 0.015239440546440908ITERATION : 90, loss : 0.015239440546440908ITERATION : 91, loss : 0.015239440546440908ITERATION : 92, loss : 0.015239440546440908ITERATION : 93, loss : 0.015239440546440908ITERATION : 94, loss : 0.015239440546440908ITERATION : 95, loss : 0.015239440546440908ITERATION : 96, loss : 0.015239440546440908ITERATION : 97, loss : 0.015239440546440908ITERATION : 98, loss : 0.015239440546440908ITERATION : 99, loss : 0.015239440546440908ITERATION : 100, loss : 0.015239440546440908
ITERATION : 1, loss : 0.022013935862149475ITERATION : 2, loss : 0.02194333433300509ITERATION : 3, loss : 0.02386928388964331ITERATION : 4, loss : 0.02601740258990386ITERATION : 5, loss : 0.025001479975172537ITERATION : 6, loss : 0.02416810486005524ITERATION : 7, loss : 0.023608782963449003ITERATION : 8, loss : 0.023228712696381437ITERATION : 9, loss : 0.02296701634460541ITERATION : 10, loss : 0.02278453431352089ITERATION : 11, loss : 0.0226558509337439ITERATION : 12, loss : 0.022564238762500227ITERATION : 13, loss : 0.022498508695150864ITERATION : 14, loss : 0.022451053555053904ITERATION : 15, loss : 0.0224166230689681ITERATION : 16, loss : 0.022391545434087126ITERATION : 17, loss : 0.02237322424215357ITERATION : 18, loss : 0.022359807038639348ITERATION : 19, loss : 0.02234996227600067ITERATION : 20, loss : 0.022342727597966596ITERATION : 21, loss : 0.02233740424572583ITERATION : 22, loss : 0.022333483095274775ITERATION : 23, loss : 0.022330592165044957ITERATION : 24, loss : 0.022328459099073286ITERATION : 25, loss : 0.02232688406065981ITERATION : 26, loss : 0.02232572034024369ITERATION : 27, loss : 0.022324860025371984ITERATION : 28, loss : 0.022324223611543775ITERATION : 29, loss : 0.022323752543358454ITERATION : 30, loss : 0.022323403725566536ITERATION : 31, loss : 0.02232314528192521ITERATION : 32, loss : 0.022322953725671883ITERATION : 33, loss : 0.02232281167479509ITERATION : 34, loss : 0.022322706252995504ITERATION : 35, loss : 0.022322627986723385ITERATION : 36, loss : 0.022322569828262105ITERATION : 37, loss : 0.022322526595359302ITERATION : 38, loss : 0.022322494465868824ITERATION : 39, loss : 0.022322470579622434ITERATION : 40, loss : 0.02232245282968241ITERATION : 41, loss : 0.022322439582452884ITERATION : 42, loss : 0.022322429728376964ITERATION : 43, loss : 0.02232242239280892ITERATION : 44, loss : 0.022322416917806277ITERATION : 45, loss : 0.022322412822774355ITERATION : 46, loss : 0.022322409786957674ITERATION : 47, loss : 0.022322407518627983ITERATION : 48, loss : 0.02232240584275056ITERATION : 49, loss : 0.022322404537521573ITERATION : 50, loss : 0.02232240362671926ITERATION : 51, loss : 0.022322402931843423ITERATION : 52, loss : 0.02232240243478628ITERATION : 53, loss : 0.022322402024888794ITERATION : 54, loss : 0.02232240169542271ITERATION : 55, loss : 0.02232240152252013ITERATION : 56, loss : 0.022322401343188154ITERATION : 57, loss : 0.02232240124871205ITERATION : 58, loss : 0.02232240113987564ITERATION : 59, loss : 0.02232240109121964ITERATION : 60, loss : 0.02232240103381558ITERATION : 61, loss : 0.022322401013341715ITERATION : 62, loss : 0.02232240098195014ITERATION : 63, loss : 0.02232240097682462ITERATION : 64, loss : 0.02232240097682462ITERATION : 65, loss : 0.02232240097682462ITERATION : 66, loss : 0.02232240097682462ITERATION : 67, loss : 0.02232240097682462ITERATION : 68, loss : 0.02232240097682462ITERATION : 69, loss : 0.02232240097682462ITERATION : 70, loss : 0.02232240097682462ITERATION : 71, loss : 0.02232240097682462ITERATION : 72, loss : 0.02232240097682462ITERATION : 73, loss : 0.02232240097682462ITERATION : 74, loss : 0.02232240097682462ITERATION : 75, loss : 0.02232240097682462ITERATION : 76, loss : 0.02232240097682462ITERATION : 77, loss : 0.02232240097682462ITERATION : 78, loss : 0.02232240097682462ITERATION : 79, loss : 0.02232240097682462ITERATION : 80, loss : 0.02232240097682462ITERATION : 81, loss : 0.02232240097682462ITERATION : 82, loss : 0.02232240097682462ITERATION : 83, loss : 0.02232240097682462ITERATION : 84, loss : 0.02232240097682462ITERATION : 85, loss : 0.02232240097682462ITERATION : 86, loss : 0.02232240097682462ITERATION : 87, loss : 0.02232240097682462ITERATION : 88, loss : 0.02232240097682462ITERATION : 89, loss : 0.02232240097682462ITERATION : 90, loss : 0.02232240097682462ITERATION : 91, loss : 0.02232240097682462ITERATION : 92, loss : 0.02232240097682462ITERATION : 93, loss : 0.02232240097682462ITERATION : 94, loss : 0.02232240097682462ITERATION : 95, loss : 0.02232240097682462ITERATION : 96, loss : 0.02232240097682462ITERATION : 97, loss : 0.02232240097682462ITERATION : 98, loss : 0.02232240097682462ITERATION : 99, loss : 0.02232240097682462ITERATION : 100, loss : 0.02232240097682462
ITERATION : 1, loss : 0.03632473217431037ITERATION : 2, loss : 0.026053061433630408ITERATION : 3, loss : 0.023818327908361518ITERATION : 4, loss : 0.023356267201831767ITERATION : 5, loss : 0.023332952243211934ITERATION : 6, loss : 0.02341649834191746ITERATION : 7, loss : 0.02351265500306798ITERATION : 8, loss : 0.023595606710020565ITERATION : 9, loss : 0.023660834030261063ITERATION : 10, loss : 0.02371014073926865ITERATION : 11, loss : 0.023746718615559853ITERATION : 12, loss : 0.023773597128449175ITERATION : 13, loss : 0.023793249839479117ITERATION : 14, loss : 0.023807580572249167ITERATION : 15, loss : 0.023818014734730438ITERATION : 16, loss : 0.023825605309658132ITERATION : 17, loss : 0.023831124335463985ITERATION : 18, loss : 0.02383513593846947ITERATION : 19, loss : 0.023838051292848432ITERATION : 20, loss : 0.023840169610880695ITERATION : 21, loss : 0.023841708685282903ITERATION : 22, loss : 0.023842826872549003ITERATION : 23, loss : 0.023843639226121233ITERATION : 24, loss : 0.023844229362636547ITERATION : 25, loss : 0.02384465806476147ITERATION : 26, loss : 0.023844969489349357ITERATION : 27, loss : 0.023845195748217758ITERATION : 28, loss : 0.023845360090740727ITERATION : 29, loss : 0.02384547945164464ITERATION : 30, loss : 0.023845566190512042ITERATION : 31, loss : 0.023845629171898544ITERATION : 32, loss : 0.0238456749657115ITERATION : 33, loss : 0.023845708188258877ITERATION : 34, loss : 0.023845732343403033ITERATION : 35, loss : 0.023845749867930074ITERATION : 36, loss : 0.02384576261627469ITERATION : 37, loss : 0.023845771850864672ITERATION : 38, loss : 0.023845778583867834ITERATION : 39, loss : 0.023845783458311558ITERATION : 40, loss : 0.02384578700187594ITERATION : 41, loss : 0.023845789559241347ITERATION : 42, loss : 0.023845791427140647ITERATION : 43, loss : 0.023845792769472525ITERATION : 44, loss : 0.023845793769819684ITERATION : 45, loss : 0.023845794480859932ITERATION : 46, loss : 0.023845794983199418ITERATION : 47, loss : 0.02384579532883439ITERATION : 48, loss : 0.023845795626054184ITERATION : 49, loss : 0.023845795799936828ITERATION : 50, loss : 0.02384579596737231ITERATION : 51, loss : 0.023845796015582964ITERATION : 52, loss : 0.023845796108741883ITERATION : 53, loss : 0.023845796123244178ITERATION : 54, loss : 0.02384579617783474ITERATION : 55, loss : 0.02384579617825365ITERATION : 56, loss : 0.02384579617825365ITERATION : 57, loss : 0.02384579617825365ITERATION : 58, loss : 0.02384579617825365ITERATION : 59, loss : 0.02384579617825365ITERATION : 60, loss : 0.02384579617825365ITERATION : 61, loss : 0.02384579617825365ITERATION : 62, loss : 0.02384579617825365ITERATION : 63, loss : 0.02384579617825365ITERATION : 64, loss : 0.02384579617825365ITERATION : 65, loss : 0.02384579617825365ITERATION : 66, loss : 0.02384579617825365ITERATION : 67, loss : 0.02384579617825365ITERATION : 68, loss : 0.02384579617825365ITERATION : 69, loss : 0.02384579617825365ITERATION : 70, loss : 0.02384579617825365ITERATION : 71, loss : 0.02384579617825365ITERATION : 72, loss : 0.02384579617825365ITERATION : 73, loss : 0.02384579617825365ITERATION : 74, loss : 0.02384579617825365ITERATION : 75, loss : 0.02384579617825365ITERATION : 76, loss : 0.02384579617825365ITERATION : 77, loss : 0.02384579617825365ITERATION : 78, loss : 0.02384579617825365ITERATION : 79, loss : 0.02384579617825365ITERATION : 80, loss : 0.02384579617825365ITERATION : 81, loss : 0.02384579617825365ITERATION : 82, loss : 0.02384579617825365ITERATION : 83, loss : 0.02384579617825365ITERATION : 84, loss : 0.02384579617825365ITERATION : 85, loss : 0.02384579617825365ITERATION : 86, loss : 0.02384579617825365ITERATION : 87, loss : 0.02384579617825365ITERATION : 88, loss : 0.02384579617825365ITERATION : 89, loss : 0.02384579617825365ITERATION : 90, loss : 0.02384579617825365ITERATION : 91, loss : 0.02384579617825365ITERATION : 92, loss : 0.02384579617825365ITERATION : 93, loss : 0.02384579617825365ITERATION : 94, loss : 0.02384579617825365ITERATION : 95, loss : 0.02384579617825365ITERATION : 96, loss : 0.02384579617825365ITERATION : 97, loss : 0.02384579617825365ITERATION : 98, loss : 0.02384579617825365ITERATION : 99, loss : 0.02384579617825365ITERATION : 100, loss : 0.02384579617825365
ITERATION : 1, loss : 0.05868288161025917ITERATION : 2, loss : 0.0495069921602135ITERATION : 3, loss : 0.04464075063403415ITERATION : 4, loss : 0.041768066354491484ITERATION : 5, loss : 0.04003673413353554ITERATION : 6, loss : 0.03898107976050564ITERATION : 7, loss : 0.038327198833900046ITERATION : 8, loss : 0.03791411074213453ITERATION : 9, loss : 0.037614181713502864ITERATION : 10, loss : 0.03739075587389358ITERATION : 11, loss : 0.03723576163637537ITERATION : 12, loss : 0.03712715048047446ITERATION : 13, loss : 0.03705043718835007ITERATION : 14, loss : 0.036995919932690906ITERATION : 15, loss : 0.0369569929560773ITERATION : 16, loss : 0.03692909699744805ITERATION : 17, loss : 0.03690905050694525ITERATION : 18, loss : 0.0368946139548596ITERATION : 19, loss : 0.03688420021067514ITERATION : 20, loss : 0.03687667858494565ITERATION : 21, loss : 0.03687124032866182ITERATION : 22, loss : 0.03686730517650928ITERATION : 23, loss : 0.03686445575582501ITERATION : 24, loss : 0.03686239130300966ITERATION : 25, loss : 0.036860894902461944ITERATION : 26, loss : 0.0368598097756834ITERATION : 27, loss : 0.036859022575584335ITERATION : 28, loss : 0.03685845141618749ITERATION : 29, loss : 0.036858036846425864ITERATION : 30, loss : 0.03685773582219005ITERATION : 31, loss : 0.036857517253002195ITERATION : 32, loss : 0.03685735847954112ITERATION : 33, loss : 0.03685724313599298ITERATION : 34, loss : 0.03685715931492917ITERATION : 35, loss : 0.036857098348393534ITERATION : 36, loss : 0.03685705405301654ITERATION : 37, loss : 0.0368570218260275ITERATION : 38, loss : 0.03685699836743414ITERATION : 39, loss : 0.03685698131263135ITERATION : 40, loss : 0.03685696890478935ITERATION : 41, loss : 0.03685695989799499ITERATION : 42, loss : 0.03685695335174389ITERATION : 43, loss : 0.03685694860060312ITERATION : 44, loss : 0.036856945138317505ITERATION : 45, loss : 0.03685694260827844ITERATION : 46, loss : 0.03685694070447707ITERATION : 47, loss : 0.036856939349722756ITERATION : 48, loss : 0.03685693839570135ITERATION : 49, loss : 0.03685693769575551ITERATION : 50, loss : 0.03685693723516048ITERATION : 51, loss : 0.0368569368825037ITERATION : 52, loss : 0.036856936645237176ITERATION : 53, loss : 0.03685693640939654ITERATION : 54, loss : 0.03685693627096062ITERATION : 55, loss : 0.036856936199844904ITERATION : 56, loss : 0.036856936144943626ITERATION : 57, loss : 0.03685693610191835ITERATION : 58, loss : 0.036856936058449426ITERATION : 59, loss : 0.0368569360445683ITERATION : 60, loss : 0.03685693603403064ITERATION : 61, loss : 0.03685693591874208ITERATION : 62, loss : 0.03685693591874208ITERATION : 63, loss : 0.03685693591874208ITERATION : 64, loss : 0.03685693591874208ITERATION : 65, loss : 0.03685693591874208ITERATION : 66, loss : 0.03685693591874208ITERATION : 67, loss : 0.03685693591874208ITERATION : 68, loss : 0.03685693591874208ITERATION : 69, loss : 0.03685693591874208ITERATION : 70, loss : 0.03685693591874208ITERATION : 71, loss : 0.03685693591874208ITERATION : 72, loss : 0.03685693591874208ITERATION : 73, loss : 0.03685693591874208ITERATION : 74, loss : 0.03685693591874208ITERATION : 75, loss : 0.03685693591874208ITERATION : 76, loss : 0.03685693591874208ITERATION : 77, loss : 0.03685693591874208ITERATION : 78, loss : 0.03685693591874208ITERATION : 79, loss : 0.03685693591874208ITERATION : 80, loss : 0.03685693591874208ITERATION : 81, loss : 0.03685693591874208ITERATION : 82, loss : 0.03685693591874208ITERATION : 83, loss : 0.03685693591874208ITERATION : 84, loss : 0.03685693591874208ITERATION : 85, loss : 0.03685693591874208ITERATION : 86, loss : 0.03685693591874208ITERATION : 87, loss : 0.03685693591874208ITERATION : 88, loss : 0.03685693591874208ITERATION : 89, loss : 0.03685693591874208ITERATION : 90, loss : 0.03685693591874208ITERATION : 91, loss : 0.03685693591874208ITERATION : 92, loss : 0.03685693591874208ITERATION : 93, loss : 0.03685693591874208ITERATION : 94, loss : 0.03685693591874208ITERATION : 95, loss : 0.03685693591874208ITERATION : 96, loss : 0.03685693591874208ITERATION : 97, loss : 0.03685693591874208ITERATION : 98, loss : 0.03685693591874208ITERATION : 99, loss : 0.03685693591874208ITERATION : 100, loss : 0.03685693591874208
ITERATION : 1, loss : 0.04351733407517059ITERATION : 2, loss : 0.03601385260658559ITERATION : 3, loss : 0.037246138428596955ITERATION : 4, loss : 0.03738751739836693ITERATION : 5, loss : 0.03708284030060542ITERATION : 6, loss : 0.03686706812912408ITERATION : 7, loss : 0.03675162755742858ITERATION : 8, loss : 0.03668670455570897ITERATION : 9, loss : 0.03664810254930048ITERATION : 10, loss : 0.036623836083715486ITERATION : 11, loss : 0.036607813717897744ITERATION : 12, loss : 0.03659682106000264ITERATION : 13, loss : 0.03658907336571949ITERATION : 14, loss : 0.03658351851042494ITERATION : 15, loss : 0.03657949654263547ITERATION : 16, loss : 0.036576570429711235ITERATION : 17, loss : 0.03657443804214642ITERATION : 18, loss : 0.0365728844882609ITERATION : 19, loss : 0.03657175418621198ITERATION : 20, loss : 0.036570933440374126ITERATION : 21, loss : 0.03657033879244932ITERATION : 22, loss : 0.03656990888842512ITERATION : 23, loss : 0.036569598806484666ITERATION : 24, loss : 0.03656937563019825ITERATION : 25, loss : 0.036569215330550076ITERATION : 26, loss : 0.03656910042042876ITERATION : 27, loss : 0.03656901817176513ITERATION : 28, loss : 0.036568959402708955ITERATION : 29, loss : 0.036568917488878604ITERATION : 30, loss : 0.03656888762380199ITERATION : 31, loss : 0.03656886637449798ITERATION : 32, loss : 0.03656885123836558ITERATION : 33, loss : 0.03656884047573632ITERATION : 34, loss : 0.036568832900171086ITERATION : 35, loss : 0.03656882756018506ITERATION : 36, loss : 0.03656882376166105ITERATION : 37, loss : 0.03656882108235361ITERATION : 38, loss : 0.03656881917344065ITERATION : 39, loss : 0.03656881782245198ITERATION : 40, loss : 0.036568816866418906ITERATION : 41, loss : 0.036568816188529106ITERATION : 42, loss : 0.03656881570693222ITERATION : 43, loss : 0.03656881536737664ITERATION : 44, loss : 0.03656881513088962ITERATION : 45, loss : 0.03656881497357479ITERATION : 46, loss : 0.03656881484691079ITERATION : 47, loss : 0.03656881477970261ITERATION : 48, loss : 0.03656881470722877ITERATION : 49, loss : 0.03656881467888544ITERATION : 50, loss : 0.03656881464914924ITERATION : 51, loss : 0.0365688146221939ITERATION : 52, loss : 0.03656881462222022ITERATION : 53, loss : 0.03656881462222022ITERATION : 54, loss : 0.03656881462222022ITERATION : 55, loss : 0.03656881462222022ITERATION : 56, loss : 0.03656881462222022ITERATION : 57, loss : 0.03656881462222022ITERATION : 58, loss : 0.03656881462222022ITERATION : 59, loss : 0.03656881462222022ITERATION : 60, loss : 0.03656881462222022ITERATION : 61, loss : 0.03656881462222022ITERATION : 62, loss : 0.03656881462222022ITERATION : 63, loss : 0.03656881462222022ITERATION : 64, loss : 0.03656881462222022ITERATION : 65, loss : 0.03656881462222022ITERATION : 66, loss : 0.03656881462222022ITERATION : 67, loss : 0.03656881462222022ITERATION : 68, loss : 0.03656881462222022ITERATION : 69, loss : 0.03656881462222022ITERATION : 70, loss : 0.03656881462222022ITERATION : 71, loss : 0.03656881462222022ITERATION : 72, loss : 0.03656881462222022ITERATION : 73, loss : 0.03656881462222022ITERATION : 74, loss : 0.03656881462222022ITERATION : 75, loss : 0.03656881462222022ITERATION : 76, loss : 0.03656881462222022ITERATION : 77, loss : 0.03656881462222022ITERATION : 78, loss : 0.03656881462222022ITERATION : 79, loss : 0.03656881462222022ITERATION : 80, loss : 0.03656881462222022ITERATION : 81, loss : 0.03656881462222022ITERATION : 82, loss : 0.03656881462222022ITERATION : 83, loss : 0.03656881462222022ITERATION : 84, loss : 0.03656881462222022ITERATION : 85, loss : 0.03656881462222022ITERATION : 86, loss : 0.03656881462222022ITERATION : 87, loss : 0.03656881462222022ITERATION : 88, loss : 0.03656881462222022ITERATION : 89, loss : 0.03656881462222022ITERATION : 90, loss : 0.03656881462222022ITERATION : 91, loss : 0.03656881462222022ITERATION : 92, loss : 0.03656881462222022ITERATION : 93, loss : 0.03656881462222022ITERATION : 94, loss : 0.03656881462222022ITERATION : 95, loss : 0.03656881462222022ITERATION : 96, loss : 0.03656881462222022ITERATION : 97, loss : 0.03656881462222022ITERATION : 98, loss : 0.03656881462222022ITERATION : 99, loss : 0.03656881462222022ITERATION : 100, loss : 0.03656881462222022
gradient norm in None layer : 0.0007129774798018643
gradient norm in None layer : 3.37592295092893e-05
gradient norm in None layer : 4.647532123937621e-05
gradient norm in None layer : 0.0005574789976291079
gradient norm in None layer : 5.5137592193339014e-05
gradient norm in None layer : 6.457633736257497e-05
gradient norm in None layer : 0.00020236689164168938
gradient norm in None layer : 9.332212965045895e-06
gradient norm in None layer : 7.3828224586461225e-06
gradient norm in None layer : 0.0001937600316444636
gradient norm in None layer : 8.617292370873508e-06
gradient norm in None layer : 7.158749283435758e-06
gradient norm in None layer : 6.491603531235024e-05
gradient norm in None layer : 2.1349038404654315e-06
gradient norm in None layer : 1.6258832348731035e-06
gradient norm in None layer : 5.833342779791555e-05
gradient norm in None layer : 2.943146195900284e-06
gradient norm in None layer : 2.5887162092731433e-06
gradient norm in None layer : 8.949759672213161e-05
gradient norm in None layer : 1.5385908316987436e-06
gradient norm in None layer : 0.00018481376252778991
gradient norm in None layer : 1.2518977114482068e-05
gradient norm in None layer : 9.919740721953161e-06
gradient norm in None layer : 0.00023201329244852334
gradient norm in None layer : 2.40002124739325e-05
gradient norm in None layer : 2.894918025247729e-05
gradient norm in None layer : 0.00041016397081268443
gradient norm in None layer : 5.07750043629604e-06
gradient norm in None layer : 0.0007366971818153106
gradient norm in None layer : 5.715340316249289e-05
gradient norm in None layer : 6.471452583919711e-05
gradient norm in None layer : 0.000853165526271032
gradient norm in None layer : 7.334748389935135e-05
gradient norm in None layer : 9.832658882037455e-05
gradient norm in None layer : 6.778701696061318e-05
gradient norm in None layer : 1.4102463293665769e-05
Total gradient norm: 0.001574760291458655
invariance loss : 4.465161928083464, avg_den : 0.3932647705078125, density loss : 0.2932647705078125, mse loss : 0.027622762358364236, solver time : 130.03030586242676 sec , total loss : 0.032381189056955514, running loss : 0.05974549123623876
saving checkpoint
Epoch 0/10 , batch 11/12500 
ITERATION : 1, loss : 0.017783411711336922ITERATION : 2, loss : 0.027551757500198105ITERATION : 3, loss : 0.02522794703480111ITERATION : 4, loss : 0.023407180274825745ITERATION : 5, loss : 0.022212675065876025ITERATION : 6, loss : 0.021463398066565987ITERATION : 7, loss : 0.020996415427893902ITERATION : 8, loss : 0.020703804859085757ITERATION : 9, loss : 0.020518943943694284ITERATION : 10, loss : 0.02040123789162078ITERATION : 11, loss : 0.020325815190895727ITERATION : 12, loss : 0.020277265331197357ITERATION : 13, loss : 0.020245924455265563ITERATION : 14, loss : 0.02022566654619633ITERATION : 15, loss : 0.020212573814258793ITERATION : 16, loss : 0.020204123602733867ITERATION : 17, loss : 0.02019868379488835ITERATION : 18, loss : 0.02019519509598717ITERATION : 19, loss : 0.02019296894257763ITERATION : 20, loss : 0.02019155759839269ITERATION : 21, loss : 0.02019067012876993ITERATION : 22, loss : 0.02019011779510691ITERATION : 23, loss : 0.02018977858875065ITERATION : 24, loss : 0.020189573820132475ITERATION : 25, loss : 0.0201894531191058ITERATION : 26, loss : 0.020189384189214506ITERATION : 27, loss : 0.020189346769622217ITERATION : 28, loss : 0.020189328077886638ITERATION : 29, loss : 0.020189320184766334ITERATION : 30, loss : 0.020189318275614487ITERATION : 31, loss : 0.020189319418623983ITERATION : 32, loss : 0.020189321896789153ITERATION : 33, loss : 0.02018932484701505ITERATION : 34, loss : 0.020189327872810907ITERATION : 35, loss : 0.020189330574867378ITERATION : 36, loss : 0.02018933284337473ITERATION : 37, loss : 0.020189334696973392ITERATION : 38, loss : 0.02018933618010643ITERATION : 39, loss : 0.020189337309441953ITERATION : 40, loss : 0.020189338222306275ITERATION : 41, loss : 0.020189338956772145ITERATION : 42, loss : 0.020189339540605564ITERATION : 43, loss : 0.020189339990839154ITERATION : 44, loss : 0.020189340326435913ITERATION : 45, loss : 0.02018934060759581ITERATION : 46, loss : 0.02018934079789603ITERATION : 47, loss : 0.020189340965350928ITERATION : 48, loss : 0.02018934105344016ITERATION : 49, loss : 0.020189341115929727ITERATION : 50, loss : 0.02018934117955289ITERATION : 51, loss : 0.020189341198243463ITERATION : 52, loss : 0.020189341236893803ITERATION : 53, loss : 0.020189341243610336ITERATION : 54, loss : 0.020189341243610336ITERATION : 55, loss : 0.020189341243610336ITERATION : 56, loss : 0.020189341243610336ITERATION : 57, loss : 0.020189341243610336ITERATION : 58, loss : 0.020189341243610336ITERATION : 59, loss : 0.020189341243610336ITERATION : 60, loss : 0.020189341243610336ITERATION : 61, loss : 0.020189341243610336ITERATION : 62, loss : 0.020189341243610336ITERATION : 63, loss : 0.020189341243610336ITERATION : 64, loss : 0.020189341243610336ITERATION : 65, loss : 0.020189341243610336ITERATION : 66, loss : 0.020189341243610336ITERATION : 67, loss : 0.020189341243610336ITERATION : 68, loss : 0.020189341243610336ITERATION : 69, loss : 0.020189341243610336ITERATION : 70, loss : 0.020189341243610336ITERATION : 71, loss : 0.020189341243610336ITERATION : 72, loss : 0.020189341243610336ITERATION : 73, loss : 0.020189341243610336ITERATION : 74, loss : 0.020189341243610336ITERATION : 75, loss : 0.020189341243610336ITERATION : 76, loss : 0.020189341243610336ITERATION : 77, loss : 0.020189341243610336ITERATION : 78, loss : 0.020189341243610336ITERATION : 79, loss : 0.020189341243610336ITERATION : 80, loss : 0.020189341243610336ITERATION : 81, loss : 0.020189341243610336ITERATION : 82, loss : 0.020189341243610336ITERATION : 83, loss : 0.020189341243610336ITERATION : 84, loss : 0.020189341243610336ITERATION : 85, loss : 0.020189341243610336ITERATION : 86, loss : 0.020189341243610336ITERATION : 87, loss : 0.020189341243610336ITERATION : 88, loss : 0.020189341243610336ITERATION : 89, loss : 0.020189341243610336ITERATION : 90, loss : 0.020189341243610336ITERATION : 91, loss : 0.020189341243610336ITERATION : 92, loss : 0.020189341243610336ITERATION : 93, loss : 0.020189341243610336ITERATION : 94, loss : 0.020189341243610336ITERATION : 95, loss : 0.020189341243610336ITERATION : 96, loss : 0.020189341243610336ITERATION : 97, loss : 0.020189341243610336ITERATION : 98, loss : 0.020189341243610336ITERATION : 99, loss : 0.020189341243610336ITERATION : 100, loss : 0.020189341243610336
ITERATION : 1, loss : 0.07122976391665567ITERATION : 2, loss : 0.06578987120630445ITERATION : 3, loss : 0.06441621252792469ITERATION : 4, loss : 0.06381579427311398ITERATION : 5, loss : 0.0634960119136788ITERATION : 6, loss : 0.06331415593584887ITERATION : 7, loss : 0.06320486049770595ITERATION : 8, loss : 0.06314417986663842ITERATION : 9, loss : 0.06312722898554084ITERATION : 10, loss : 0.06311720685420327ITERATION : 11, loss : 0.06311090843679913ITERATION : 12, loss : 0.06310673649907751ITERATION : 13, loss : 0.06310385515991093ITERATION : 14, loss : 0.06310180239749846ITERATION : 15, loss : 0.06310030759976293ITERATION : 16, loss : 0.06309920299312513ITERATION : 17, loss : 0.06309837873613182ITERATION : 18, loss : 0.06309775998926269ITERATION : 19, loss : 0.06309729376959568ITERATION : 20, loss : 0.06309694168322433ITERATION : 21, loss : 0.06309667557762991ITERATION : 22, loss : 0.06309647432939007ITERATION : 23, loss : 0.06309632215376965ITERATION : 24, loss : 0.0630962071139864ITERATION : 25, loss : 0.0630961202033693ITERATION : 26, loss : 0.06309605457401787ITERATION : 27, loss : 0.06309600501727393ITERATION : 28, loss : 0.06309596764187571ITERATION : 29, loss : 0.06309593944443516ITERATION : 30, loss : 0.06309591820993968ITERATION : 31, loss : 0.0630959022318839ITERATION : 32, loss : 0.06309589020233151ITERATION : 33, loss : 0.06309588119314821ITERATION : 34, loss : 0.06309587440943491ITERATION : 35, loss : 0.06309586930561281ITERATION : 36, loss : 0.06309586548126252ITERATION : 37, loss : 0.06309586260898027ITERATION : 38, loss : 0.06309586045987149ITERATION : 39, loss : 0.06309585885458253ITERATION : 40, loss : 0.06309585765886308ITERATION : 41, loss : 0.06309585674111363ITERATION : 42, loss : 0.06309585608021302ITERATION : 43, loss : 0.06309585556346947ITERATION : 44, loss : 0.06309585519050794ITERATION : 45, loss : 0.06309585487825548ITERATION : 46, loss : 0.06309585467514718ITERATION : 47, loss : 0.06309585452445385ITERATION : 48, loss : 0.0630958544052953ITERATION : 49, loss : 0.06309585430791284ITERATION : 50, loss : 0.06309585423920111ITERATION : 51, loss : 0.06309585418326115ITERATION : 52, loss : 0.06309585416190093ITERATION : 53, loss : 0.06309585413370375ITERATION : 54, loss : 0.0630958540946503ITERATION : 55, loss : 0.06309585409840594ITERATION : 56, loss : 0.06309585407672175ITERATION : 57, loss : 0.06309585407737928ITERATION : 58, loss : 0.0630958540776564ITERATION : 59, loss : 0.0630958540776564ITERATION : 60, loss : 0.0630958540776564ITERATION : 61, loss : 0.0630958540776564ITERATION : 62, loss : 0.0630958540776564ITERATION : 63, loss : 0.0630958540776564ITERATION : 64, loss : 0.0630958540776564ITERATION : 65, loss : 0.0630958540776564ITERATION : 66, loss : 0.0630958540776564ITERATION : 67, loss : 0.0630958540776564ITERATION : 68, loss : 0.0630958540776564ITERATION : 69, loss : 0.0630958540776564ITERATION : 70, loss : 0.0630958540776564ITERATION : 71, loss : 0.0630958540776564ITERATION : 72, loss : 0.0630958540776564ITERATION : 73, loss : 0.0630958540776564ITERATION : 74, loss : 0.0630958540776564ITERATION : 75, loss : 0.0630958540776564ITERATION : 76, loss : 0.0630958540776564ITERATION : 77, loss : 0.0630958540776564ITERATION : 78, loss : 0.0630958540776564ITERATION : 79, loss : 0.0630958540776564ITERATION : 80, loss : 0.0630958540776564ITERATION : 81, loss : 0.0630958540776564ITERATION : 82, loss : 0.0630958540776564ITERATION : 83, loss : 0.0630958540776564ITERATION : 84, loss : 0.0630958540776564ITERATION : 85, loss : 0.0630958540776564ITERATION : 86, loss : 0.0630958540776564ITERATION : 87, loss : 0.0630958540776564ITERATION : 88, loss : 0.0630958540776564ITERATION : 89, loss : 0.0630958540776564ITERATION : 90, loss : 0.0630958540776564ITERATION : 91, loss : 0.0630958540776564ITERATION : 92, loss : 0.0630958540776564ITERATION : 93, loss : 0.0630958540776564ITERATION : 94, loss : 0.0630958540776564ITERATION : 95, loss : 0.0630958540776564ITERATION : 96, loss : 0.0630958540776564ITERATION : 97, loss : 0.0630958540776564ITERATION : 98, loss : 0.0630958540776564ITERATION : 99, loss : 0.0630958540776564ITERATION : 100, loss : 0.0630958540776564
ITERATION : 1, loss : 0.03954890036445155ITERATION : 2, loss : 0.03682928545035953ITERATION : 3, loss : 0.03556771111712187ITERATION : 4, loss : 0.03473731484584068ITERATION : 5, loss : 0.03415357946370467ITERATION : 6, loss : 0.03374045794409827ITERATION : 7, loss : 0.033448176323706225ITERATION : 8, loss : 0.03324115928857522ITERATION : 9, loss : 0.03309414122931693ITERATION : 10, loss : 0.0329893846301487ITERATION : 11, loss : 0.03291449340019122ITERATION : 12, loss : 0.03286079492142627ITERATION : 13, loss : 0.03282219762377129ITERATION : 14, loss : 0.03279440013078175ITERATION : 15, loss : 0.03277435008491462ITERATION : 16, loss : 0.03275987141307168ITERATION : 17, loss : 0.032749406872683975ITERATION : 18, loss : 0.032741838653796165ITERATION : 19, loss : 0.03273636258318992ITERATION : 20, loss : 0.03273239889248494ITERATION : 21, loss : 0.03272952917458549ITERATION : 22, loss : 0.03272745110252015ITERATION : 23, loss : 0.03272594610503661ITERATION : 24, loss : 0.03272485602994685ITERATION : 25, loss : 0.03272406643263042ITERATION : 26, loss : 0.03272349446428028ITERATION : 27, loss : 0.03272308009649291ITERATION : 28, loss : 0.032722779944643975ITERATION : 29, loss : 0.03272256250806426ITERATION : 30, loss : 0.032722404991819554ITERATION : 31, loss : 0.03272229086670006ITERATION : 32, loss : 0.03272220820879988ITERATION : 33, loss : 0.03272214831775523ITERATION : 34, loss : 0.032722104910523214ITERATION : 35, loss : 0.03272207347986323ITERATION : 36, loss : 0.03272205068918585ITERATION : 37, loss : 0.032722034192505266ITERATION : 38, loss : 0.032722022261013ITERATION : 39, loss : 0.03272201359854489ITERATION : 40, loss : 0.03272200731205432ITERATION : 41, loss : 0.03272200277153372ITERATION : 42, loss : 0.03272199947414782ITERATION : 43, loss : 0.03272199713635665ITERATION : 44, loss : 0.03272199538649741ITERATION : 45, loss : 0.03272199415120837ITERATION : 46, loss : 0.03272199323425667ITERATION : 47, loss : 0.03272199262884948ITERATION : 48, loss : 0.03272199216867688ITERATION : 49, loss : 0.03272199181303816ITERATION : 50, loss : 0.03272199153877414ITERATION : 51, loss : 0.03272199136795664ITERATION : 52, loss : 0.032721991227332096ITERATION : 53, loss : 0.032721991151421125ITERATION : 54, loss : 0.0327219910736521ITERATION : 55, loss : 0.032721991071874165ITERATION : 56, loss : 0.032721991071874165ITERATION : 57, loss : 0.032721991071874165ITERATION : 58, loss : 0.032721991071874165ITERATION : 59, loss : 0.032721991071874165ITERATION : 60, loss : 0.032721991071874165ITERATION : 61, loss : 0.032721991071874165ITERATION : 62, loss : 0.032721991071874165ITERATION : 63, loss : 0.032721991071874165ITERATION : 64, loss : 0.032721991071874165ITERATION : 65, loss : 0.032721991071874165ITERATION : 66, loss : 0.032721991071874165ITERATION : 67, loss : 0.032721991071874165ITERATION : 68, loss : 0.032721991071874165ITERATION : 69, loss : 0.032721991071874165ITERATION : 70, loss : 0.032721991071874165ITERATION : 71, loss : 0.032721991071874165ITERATION : 72, loss : 0.032721991071874165ITERATION : 73, loss : 0.032721991071874165ITERATION : 74, loss : 0.032721991071874165ITERATION : 75, loss : 0.032721991071874165ITERATION : 76, loss : 0.032721991071874165ITERATION : 77, loss : 0.032721991071874165ITERATION : 78, loss : 0.032721991071874165ITERATION : 79, loss : 0.032721991071874165ITERATION : 80, loss : 0.032721991071874165ITERATION : 81, loss : 0.032721991071874165ITERATION : 82, loss : 0.032721991071874165ITERATION : 83, loss : 0.032721991071874165ITERATION : 84, loss : 0.032721991071874165ITERATION : 85, loss : 0.032721991071874165ITERATION : 86, loss : 0.032721991071874165ITERATION : 87, loss : 0.032721991071874165ITERATION : 88, loss : 0.032721991071874165ITERATION : 89, loss : 0.032721991071874165ITERATION : 90, loss : 0.032721991071874165ITERATION : 91, loss : 0.032721991071874165ITERATION : 92, loss : 0.032721991071874165ITERATION : 93, loss : 0.032721991071874165ITERATION : 94, loss : 0.032721991071874165ITERATION : 95, loss : 0.032721991071874165ITERATION : 96, loss : 0.032721991071874165ITERATION : 97, loss : 0.032721991071874165ITERATION : 98, loss : 0.032721991071874165ITERATION : 99, loss : 0.032721991071874165ITERATION : 100, loss : 0.032721991071874165
ITERATION : 1, loss : 0.02260945910270978ITERATION : 2, loss : 0.027598938661217595ITERATION : 3, loss : 0.0318958813568815ITERATION : 4, loss : 0.03284999475700589ITERATION : 5, loss : 0.032836130864975104ITERATION : 6, loss : 0.032906366119619265ITERATION : 7, loss : 0.03298797673176865ITERATION : 8, loss : 0.03305796856992452ITERATION : 9, loss : 0.03311190396720846ITERATION : 10, loss : 0.03315144294254504ITERATION : 11, loss : 0.033179677045372266ITERATION : 12, loss : 0.033199543280060605ITERATION : 13, loss : 0.033213401594290884ITERATION : 14, loss : 0.033223019204424185ITERATION : 15, loss : 0.033229672792737ITERATION : 16, loss : 0.033234266842909393ITERATION : 17, loss : 0.03323743501062802ITERATION : 18, loss : 0.03323961839105166ITERATION : 19, loss : 0.03324112237493612ITERATION : 20, loss : 0.03324215796875945ITERATION : 21, loss : 0.03324287106029636ITERATION : 22, loss : 0.033243361922867906ITERATION : 23, loss : 0.03324369980112049ITERATION : 24, loss : 0.03324393230705926ITERATION : 25, loss : 0.0332440923103777ITERATION : 26, loss : 0.0332442024537408ITERATION : 27, loss : 0.03324427826671636ITERATION : 28, loss : 0.03324433042901025ITERATION : 29, loss : 0.033244366349478936ITERATION : 30, loss : 0.03324439103980892ITERATION : 31, loss : 0.03324440802236597ITERATION : 32, loss : 0.033244419656616966ITERATION : 33, loss : 0.03324442770567482ITERATION : 34, loss : 0.0332444331011082ITERATION : 35, loss : 0.033244436945853745ITERATION : 36, loss : 0.03324443942899312ITERATION : 37, loss : 0.03324444127886498ITERATION : 38, loss : 0.03324444241380833ITERATION : 39, loss : 0.03324444333293574ITERATION : 40, loss : 0.033244443799128554ITERATION : 41, loss : 0.03324444425550127ITERATION : 42, loss : 0.03324444451406434ITERATION : 43, loss : 0.03324444471672574ITERATION : 44, loss : 0.033244444802956345ITERATION : 45, loss : 0.03324444493643557ITERATION : 46, loss : 0.033244444943252426ITERATION : 47, loss : 0.03324444503628922ITERATION : 48, loss : 0.03324444499162071ITERATION : 49, loss : 0.033244445050494544ITERATION : 50, loss : 0.033244445050494544ITERATION : 51, loss : 0.033244445050494544ITERATION : 52, loss : 0.033244445050494544ITERATION : 53, loss : 0.033244445050494544ITERATION : 54, loss : 0.033244445050494544ITERATION : 55, loss : 0.033244445050494544ITERATION : 56, loss : 0.033244445050494544ITERATION : 57, loss : 0.033244445050494544ITERATION : 58, loss : 0.033244445050494544ITERATION : 59, loss : 0.033244445050494544ITERATION : 60, loss : 0.033244445050494544ITERATION : 61, loss : 0.033244445050494544ITERATION : 62, loss : 0.033244445050494544ITERATION : 63, loss : 0.033244445050494544ITERATION : 64, loss : 0.033244445050494544ITERATION : 65, loss : 0.033244445050494544ITERATION : 66, loss : 0.033244445050494544ITERATION : 67, loss : 0.033244445050494544ITERATION : 68, loss : 0.033244445050494544ITERATION : 69, loss : 0.033244445050494544ITERATION : 70, loss : 0.033244445050494544ITERATION : 71, loss : 0.033244445050494544ITERATION : 72, loss : 0.033244445050494544ITERATION : 73, loss : 0.033244445050494544ITERATION : 74, loss : 0.033244445050494544ITERATION : 75, loss : 0.033244445050494544ITERATION : 76, loss : 0.033244445050494544ITERATION : 77, loss : 0.033244445050494544ITERATION : 78, loss : 0.033244445050494544ITERATION : 79, loss : 0.033244445050494544ITERATION : 80, loss : 0.033244445050494544ITERATION : 81, loss : 0.033244445050494544ITERATION : 82, loss : 0.033244445050494544ITERATION : 83, loss : 0.033244445050494544ITERATION : 84, loss : 0.033244445050494544ITERATION : 85, loss : 0.033244445050494544ITERATION : 86, loss : 0.033244445050494544ITERATION : 87, loss : 0.033244445050494544ITERATION : 88, loss : 0.033244445050494544ITERATION : 89, loss : 0.033244445050494544ITERATION : 90, loss : 0.033244445050494544ITERATION : 91, loss : 0.033244445050494544ITERATION : 92, loss : 0.033244445050494544ITERATION : 93, loss : 0.033244445050494544ITERATION : 94, loss : 0.033244445050494544ITERATION : 95, loss : 0.033244445050494544ITERATION : 96, loss : 0.033244445050494544ITERATION : 97, loss : 0.033244445050494544ITERATION : 98, loss : 0.033244445050494544ITERATION : 99, loss : 0.033244445050494544ITERATION : 100, loss : 0.033244445050494544
ITERATION : 1, loss : 0.011220284080076394ITERATION : 2, loss : 0.013536700712639913ITERATION : 3, loss : 0.014079420581054786ITERATION : 4, loss : 0.014105736700576467ITERATION : 5, loss : 0.01401614090956272ITERATION : 6, loss : 0.0139169825060764ITERATION : 7, loss : 0.013833834605440604ITERATION : 8, loss : 0.01374475064501573ITERATION : 9, loss : 0.01366665191506434ITERATION : 10, loss : 0.013608938863882716ITERATION : 11, loss : 0.013566904138095923ITERATION : 12, loss : 0.01353657957328904ITERATION : 13, loss : 0.013514846459174024ITERATION : 14, loss : 0.013499343716011523ITERATION : 15, loss : 0.013488323109332576ITERATION : 16, loss : 0.013480508750907767ITERATION : 17, loss : 0.013474978539834738ITERATION : 18, loss : 0.013471070582026694ITERATION : 19, loss : 0.013468312148157696ITERATION : 20, loss : 0.013466366703416981ITERATION : 21, loss : 0.013464995535571879ITERATION : 22, loss : 0.013464029670714505ITERATION : 23, loss : 0.013463349630220143ITERATION : 24, loss : 0.013462870949500398ITERATION : 25, loss : 0.013462533981930307ITERATION : 26, loss : 0.013462296933314712ITERATION : 27, loss : 0.013462130145308065ITERATION : 28, loss : 0.013462012820649068ITERATION : 29, loss : 0.013461930305722779ITERATION : 30, loss : 0.013461872245880723ITERATION : 31, loss : 0.01346183139737111ITERATION : 32, loss : 0.013461802600806183ITERATION : 33, loss : 0.013461782377122271ITERATION : 34, loss : 0.01346176816092774ITERATION : 35, loss : 0.013461758198255597ITERATION : 36, loss : 0.013461751212693475ITERATION : 37, loss : 0.013461746298088512ITERATION : 38, loss : 0.013461742829219896ITERATION : 39, loss : 0.013461740408614819ITERATION : 40, loss : 0.013461738728096174ITERATION : 41, loss : 0.013461737538100177ITERATION : 42, loss : 0.013461736680596572ITERATION : 43, loss : 0.013461736111520281ITERATION : 44, loss : 0.013461735689703947ITERATION : 45, loss : 0.013461735472905034ITERATION : 46, loss : 0.013461735253873485ITERATION : 47, loss : 0.01346173515862798ITERATION : 48, loss : 0.013461735072425549ITERATION : 49, loss : 0.01346173506224594ITERATION : 50, loss : 0.01346173506224594ITERATION : 51, loss : 0.01346173506224594ITERATION : 52, loss : 0.01346173506224594ITERATION : 53, loss : 0.01346173506224594ITERATION : 54, loss : 0.01346173506224594ITERATION : 55, loss : 0.01346173506224594ITERATION : 56, loss : 0.01346173506224594ITERATION : 57, loss : 0.01346173506224594ITERATION : 58, loss : 0.01346173506224594ITERATION : 59, loss : 0.01346173506224594ITERATION : 60, loss : 0.01346173506224594ITERATION : 61, loss : 0.01346173506224594ITERATION : 62, loss : 0.01346173506224594ITERATION : 63, loss : 0.01346173506224594ITERATION : 64, loss : 0.01346173506224594ITERATION : 65, loss : 0.01346173506224594ITERATION : 66, loss : 0.01346173506224594ITERATION : 67, loss : 0.01346173506224594ITERATION : 68, loss : 0.01346173506224594ITERATION : 69, loss : 0.01346173506224594ITERATION : 70, loss : 0.01346173506224594ITERATION : 71, loss : 0.01346173506224594ITERATION : 72, loss : 0.01346173506224594ITERATION : 73, loss : 0.01346173506224594ITERATION : 74, loss : 0.01346173506224594ITERATION : 75, loss : 0.01346173506224594ITERATION : 76, loss : 0.01346173506224594ITERATION : 77, loss : 0.01346173506224594ITERATION : 78, loss : 0.01346173506224594ITERATION : 79, loss : 0.01346173506224594ITERATION : 80, loss : 0.01346173506224594ITERATION : 81, loss : 0.01346173506224594ITERATION : 82, loss : 0.01346173506224594ITERATION : 83, loss : 0.01346173506224594ITERATION : 84, loss : 0.01346173506224594ITERATION : 85, loss : 0.01346173506224594ITERATION : 86, loss : 0.01346173506224594ITERATION : 87, loss : 0.01346173506224594ITERATION : 88, loss : 0.01346173506224594ITERATION : 89, loss : 0.01346173506224594ITERATION : 90, loss : 0.01346173506224594ITERATION : 91, loss : 0.01346173506224594ITERATION : 92, loss : 0.01346173506224594ITERATION : 93, loss : 0.01346173506224594ITERATION : 94, loss : 0.01346173506224594ITERATION : 95, loss : 0.01346173506224594ITERATION : 96, loss : 0.01346173506224594ITERATION : 97, loss : 0.01346173506224594ITERATION : 98, loss : 0.01346173506224594ITERATION : 99, loss : 0.01346173506224594ITERATION : 100, loss : 0.01346173506224594
ITERATION : 1, loss : 0.06796234283195385ITERATION : 2, loss : 0.04957626297283991ITERATION : 3, loss : 0.04311414772611388ITERATION : 4, loss : 0.04088869325827401ITERATION : 5, loss : 0.04352591713795307ITERATION : 6, loss : 0.04582577236423445ITERATION : 7, loss : 0.04774488188510472ITERATION : 8, loss : 0.049312123226989454ITERATION : 9, loss : 0.05057760907181619ITERATION : 10, loss : 0.0515931224185081ITERATION : 11, loss : 0.05240520231537633ITERATION : 12, loss : 0.05305331735271672ITERATION : 13, loss : 0.053570003934267967ITERATION : 14, loss : 0.05398167790441889ITERATION : 15, loss : 0.05430960218039142ITERATION : 16, loss : 0.05457080560122948ITERATION : 17, loss : 0.05477888413027903ITERATION : 18, loss : 0.05494467283793021ITERATION : 19, loss : 0.05507679844441106ITERATION : 20, loss : 0.05518212455075826ITERATION : 21, loss : 0.05526611081609174ITERATION : 22, loss : 0.05533310037547459ITERATION : 23, loss : 0.05538654851962788ITERATION : 24, loss : 0.05542920506234238ITERATION : 25, loss : 0.055463258008538865ITERATION : 26, loss : 0.0554904502539219ITERATION : 27, loss : 0.055512169357980194ITERATION : 28, loss : 0.05552952109103584ITERATION : 29, loss : 0.05554338690833595ITERATION : 30, loss : 0.05555446932282857ITERATION : 31, loss : 0.05556332894574934ITERATION : 32, loss : 0.055570412874647146ITERATION : 33, loss : 0.055576078044941565ITERATION : 34, loss : 0.055580609498688184ITERATION : 35, loss : 0.055584234392674396ITERATION : 36, loss : 0.0555871344886424ITERATION : 37, loss : 0.055589454976954784ITERATION : 38, loss : 0.055591312189220705ITERATION : 39, loss : 0.055592798772524765ITERATION : 40, loss : 0.05559398854145531ITERATION : 41, loss : 0.05559494109385954ITERATION : 42, loss : 0.055595703652072545ITERATION : 43, loss : 0.0555963142102907ITERATION : 44, loss : 0.05559680308866876ITERATION : 45, loss : 0.055597194562906765ITERATION : 46, loss : 0.05559750806710931ITERATION : 47, loss : 0.05559775915895394ITERATION : 48, loss : 0.05559796028131022ITERATION : 49, loss : 0.05559812141296324ITERATION : 50, loss : 0.055598250410401014ITERATION : 51, loss : 0.05559835371768502ITERATION : 52, loss : 0.0555984364284582ITERATION : 53, loss : 0.05559850272099117ITERATION : 54, loss : 0.055598555839605435ITERATION : 55, loss : 0.055598598392177956ITERATION : 56, loss : 0.05559863232756648ITERATION : 57, loss : 0.05559865970926117ITERATION : 58, loss : 0.05559868163043112ITERATION : 59, loss : 0.05559869919026759ITERATION : 60, loss : 0.05559871323702303ITERATION : 61, loss : 0.055598724435321455ITERATION : 62, loss : 0.055598733549628146ITERATION : 63, loss : 0.05559874083549931ITERATION : 64, loss : 0.05559874668907537ITERATION : 65, loss : 0.05559875146195181ITERATION : 66, loss : 0.055598754990720145ITERATION : 67, loss : 0.05559875798172429ITERATION : 68, loss : 0.055598760331147454ITERATION : 69, loss : 0.055598762373500185ITERATION : 70, loss : 0.05559876399903481ITERATION : 71, loss : 0.05559876502968745ITERATION : 72, loss : 0.05559876621668357ITERATION : 73, loss : 0.05559876686438752ITERATION : 74, loss : 0.055598767504741006ITERATION : 75, loss : 0.05559876801151527ITERATION : 76, loss : 0.055598768233981286ITERATION : 77, loss : 0.05559876842914241ITERATION : 78, loss : 0.055598768757851275ITERATION : 79, loss : 0.05559876902436355ITERATION : 80, loss : 0.05559876907931658ITERATION : 81, loss : 0.05559876910127183ITERATION : 82, loss : 0.05559876910889532ITERATION : 83, loss : 0.05559876910889532ITERATION : 84, loss : 0.05559876910889532ITERATION : 85, loss : 0.05559876910889532ITERATION : 86, loss : 0.05559876910889532ITERATION : 87, loss : 0.05559876910889532ITERATION : 88, loss : 0.05559876910889532ITERATION : 89, loss : 0.05559876910889532ITERATION : 90, loss : 0.05559876910889532ITERATION : 91, loss : 0.05559876910889532ITERATION : 92, loss : 0.05559876910889532ITERATION : 93, loss : 0.05559876910889532ITERATION : 94, loss : 0.05559876910889532ITERATION : 95, loss : 0.05559876910889532ITERATION : 96, loss : 0.05559876910889532ITERATION : 97, loss : 0.05559876910889532ITERATION : 98, loss : 0.05559876910889532ITERATION : 99, loss : 0.05559876910889532ITERATION : 100, loss : 0.05559876910889532
ITERATION : 1, loss : 0.05025558810497653ITERATION : 2, loss : 0.041099083422155624ITERATION : 3, loss : 0.03568529596737567ITERATION : 4, loss : 0.03284617827266199ITERATION : 5, loss : 0.031201201836655593ITERATION : 6, loss : 0.030179027296435887ITERATION : 7, loss : 0.029516318709545252ITERATION : 8, loss : 0.02907448375637357ITERATION : 9, loss : 0.028774242511544014ITERATION : 10, loss : 0.028567520220644783ITERATION : 11, loss : 0.028423889015092355ITERATION : 12, loss : 0.028323467307620465ITERATION : 13, loss : 0.028252955301709935ITERATION : 14, loss : 0.028203301474187713ITERATION : 15, loss : 0.02816826898113048ITERATION : 16, loss : 0.028143522042882528ITERATION : 17, loss : 0.02812602779936905ITERATION : 18, loss : 0.028113655773710527ITERATION : 19, loss : 0.02810490461410543ITERATION : 20, loss : 0.02809871461189366ITERATION : 21, loss : 0.028094336854853134ITERATION : 22, loss : 0.02809124123728536ITERATION : 23, loss : 0.028089052841018914ITERATION : 24, loss : 0.02808750622458468ITERATION : 25, loss : 0.028086413563693544ITERATION : 26, loss : 0.028085641847864086ITERATION : 27, loss : 0.02808509699215253ITERATION : 28, loss : 0.028084712452647407ITERATION : 29, loss : 0.028084441165469546ITERATION : 30, loss : 0.028084249896105418ITERATION : 31, loss : 0.028084115006424992ITERATION : 32, loss : 0.02808401995707994ITERATION : 33, loss : 0.028083953009107963ITERATION : 34, loss : 0.028083905893085497ITERATION : 35, loss : 0.028083872726141646ITERATION : 36, loss : 0.02808384940310881ITERATION : 37, loss : 0.02808383298883343ITERATION : 38, loss : 0.028083821448933916ITERATION : 39, loss : 0.028083813341120333ITERATION : 40, loss : 0.028083807632147583ITERATION : 41, loss : 0.028083803621569245ITERATION : 42, loss : 0.028083800822845752ITERATION : 43, loss : 0.028083798926608984ITERATION : 44, loss : 0.028083797527159015ITERATION : 45, loss : 0.028083796604382903ITERATION : 46, loss : 0.028083795895471775ITERATION : 47, loss : 0.028083795492790248ITERATION : 48, loss : 0.028083795142241876ITERATION : 49, loss : 0.028083794930415865ITERATION : 50, loss : 0.028083794777838823ITERATION : 51, loss : 0.02808379454831424ITERATION : 52, loss : 0.02808379454126255ITERATION : 53, loss : 0.028083794523599295ITERATION : 54, loss : 0.028083794523599295ITERATION : 55, loss : 0.028083794523599295ITERATION : 56, loss : 0.028083794523599295ITERATION : 57, loss : 0.028083794523599295ITERATION : 58, loss : 0.028083794523599295ITERATION : 59, loss : 0.028083794523599295ITERATION : 60, loss : 0.028083794523599295ITERATION : 61, loss : 0.028083794523599295ITERATION : 62, loss : 0.028083794523599295ITERATION : 63, loss : 0.028083794523599295ITERATION : 64, loss : 0.028083794523599295ITERATION : 65, loss : 0.028083794523599295ITERATION : 66, loss : 0.028083794523599295ITERATION : 67, loss : 0.028083794523599295ITERATION : 68, loss : 0.028083794523599295ITERATION : 69, loss : 0.028083794523599295ITERATION : 70, loss : 0.028083794523599295ITERATION : 71, loss : 0.028083794523599295ITERATION : 72, loss : 0.028083794523599295ITERATION : 73, loss : 0.028083794523599295ITERATION : 74, loss : 0.028083794523599295ITERATION : 75, loss : 0.028083794523599295ITERATION : 76, loss : 0.028083794523599295ITERATION : 77, loss : 0.028083794523599295ITERATION : 78, loss : 0.028083794523599295ITERATION : 79, loss : 0.028083794523599295ITERATION : 80, loss : 0.028083794523599295ITERATION : 81, loss : 0.028083794523599295ITERATION : 82, loss : 0.028083794523599295ITERATION : 83, loss : 0.028083794523599295ITERATION : 84, loss : 0.028083794523599295ITERATION : 85, loss : 0.028083794523599295ITERATION : 86, loss : 0.028083794523599295ITERATION : 87, loss : 0.028083794523599295ITERATION : 88, loss : 0.028083794523599295ITERATION : 89, loss : 0.028083794523599295ITERATION : 90, loss : 0.028083794523599295ITERATION : 91, loss : 0.028083794523599295ITERATION : 92, loss : 0.028083794523599295ITERATION : 93, loss : 0.028083794523599295ITERATION : 94, loss : 0.028083794523599295ITERATION : 95, loss : 0.028083794523599295ITERATION : 96, loss : 0.028083794523599295ITERATION : 97, loss : 0.028083794523599295ITERATION : 98, loss : 0.028083794523599295ITERATION : 99, loss : 0.028083794523599295ITERATION : 100, loss : 0.028083794523599295
ITERATION : 1, loss : 0.02769264221171827ITERATION : 2, loss : 0.02098840006377862ITERATION : 3, loss : 0.019945044375924804ITERATION : 4, loss : 0.019994629663104398ITERATION : 5, loss : 0.020293642024460982ITERATION : 6, loss : 0.02061217296520173ITERATION : 7, loss : 0.020885417892901225ITERATION : 8, loss : 0.02110141358368678ITERATION : 9, loss : 0.02126531441331618ITERATION : 10, loss : 0.021268488369097283ITERATION : 11, loss : 0.021242549018450844ITERATION : 12, loss : 0.021225499698510064ITERATION : 13, loss : 0.021214055997773124ITERATION : 14, loss : 0.02120625408706262ITERATION : 15, loss : 0.021200875266106232ITERATION : 16, loss : 0.02119713805448284ITERATION : 17, loss : 0.021194527717515524ITERATION : 18, loss : 0.02119269811891818ITERATION : 19, loss : 0.021191412822207493ITERATION : 20, loss : 0.021190508606294782ITERATION : 21, loss : 0.02118987192319292ITERATION : 22, loss : 0.021189423362203686ITERATION : 23, loss : 0.021189107247092706ITERATION : 24, loss : 0.021188884467118266ITERATION : 25, loss : 0.02118872740730472ITERATION : 26, loss : 0.021188616717384808ITERATION : 27, loss : 0.021188538698623194ITERATION : 28, loss : 0.021188483725179983ITERATION : 29, loss : 0.021188444977845153ITERATION : 30, loss : 0.021188417661398853ITERATION : 31, loss : 0.021188398419214143ITERATION : 32, loss : 0.021188384857467327ITERATION : 33, loss : 0.021188375302545446ITERATION : 34, loss : 0.021188368561033386ITERATION : 35, loss : 0.02118836381434665ITERATION : 36, loss : 0.021188360470387604ITERATION : 37, loss : 0.021188358117800312ITERATION : 38, loss : 0.02118835646184677ITERATION : 39, loss : 0.021188355287732802ITERATION : 40, loss : 0.021188354465770606ITERATION : 41, loss : 0.021188353864315796ITERATION : 42, loss : 0.021188353466907246ITERATION : 43, loss : 0.02118835316506489ITERATION : 44, loss : 0.021188352942740643ITERATION : 45, loss : 0.021188352807299696ITERATION : 46, loss : 0.021188352706989905ITERATION : 47, loss : 0.021188352648689564ITERATION : 48, loss : 0.02118835260429536ITERATION : 49, loss : 0.021188352570708432ITERATION : 50, loss : 0.021188352533155465ITERATION : 51, loss : 0.021188352523620328ITERATION : 52, loss : 0.02118835250703054ITERATION : 53, loss : 0.02118835250864957ITERATION : 54, loss : 0.02118835250864957ITERATION : 55, loss : 0.02118835250864957ITERATION : 56, loss : 0.02118835250864957ITERATION : 57, loss : 0.02118835250864957ITERATION : 58, loss : 0.02118835250864957ITERATION : 59, loss : 0.02118835250864957ITERATION : 60, loss : 0.02118835250864957ITERATION : 61, loss : 0.02118835250864957ITERATION : 62, loss : 0.02118835250864957ITERATION : 63, loss : 0.02118835250864957ITERATION : 64, loss : 0.02118835250864957ITERATION : 65, loss : 0.02118835250864957ITERATION : 66, loss : 0.02118835250864957ITERATION : 67, loss : 0.02118835250864957ITERATION : 68, loss : 0.02118835250864957ITERATION : 69, loss : 0.02118835250864957ITERATION : 70, loss : 0.02118835250864957ITERATION : 71, loss : 0.02118835250864957ITERATION : 72, loss : 0.02118835250864957ITERATION : 73, loss : 0.02118835250864957ITERATION : 74, loss : 0.02118835250864957ITERATION : 75, loss : 0.02118835250864957ITERATION : 76, loss : 0.02118835250864957ITERATION : 77, loss : 0.02118835250864957ITERATION : 78, loss : 0.02118835250864957ITERATION : 79, loss : 0.02118835250864957ITERATION : 80, loss : 0.02118835250864957ITERATION : 81, loss : 0.02118835250864957ITERATION : 82, loss : 0.02118835250864957ITERATION : 83, loss : 0.02118835250864957ITERATION : 84, loss : 0.02118835250864957ITERATION : 85, loss : 0.02118835250864957ITERATION : 86, loss : 0.02118835250864957ITERATION : 87, loss : 0.02118835250864957ITERATION : 88, loss : 0.02118835250864957ITERATION : 89, loss : 0.02118835250864957ITERATION : 90, loss : 0.02118835250864957ITERATION : 91, loss : 0.02118835250864957ITERATION : 92, loss : 0.02118835250864957ITERATION : 93, loss : 0.02118835250864957ITERATION : 94, loss : 0.02118835250864957ITERATION : 95, loss : 0.02118835250864957ITERATION : 96, loss : 0.02118835250864957ITERATION : 97, loss : 0.02118835250864957ITERATION : 98, loss : 0.02118835250864957ITERATION : 99, loss : 0.02118835250864957ITERATION : 100, loss : 0.02118835250864957
gradient norm in None layer : 0.0013924098400013047
gradient norm in None layer : 6.182145509651037e-05
gradient norm in None layer : 8.089992471105691e-05
gradient norm in None layer : 0.0010376875279795867
gradient norm in None layer : 6.540582226720117e-05
gradient norm in None layer : 8.4174757834986e-05
gradient norm in None layer : 0.00038570017378262614
gradient norm in None layer : 1.6290281124781065e-05
gradient norm in None layer : 1.3326632558132299e-05
gradient norm in None layer : 0.00035639656148959384
gradient norm in None layer : 1.5364583260720782e-05
gradient norm in None layer : 1.3387492658109861e-05
gradient norm in None layer : 0.0001261006471336254
gradient norm in None layer : 4.221094609476888e-06
gradient norm in None layer : 2.8098476145959316e-06
gradient norm in None layer : 0.0001046650208944646
gradient norm in None layer : 4.432925591808978e-06
gradient norm in None layer : 3.400012212556271e-06
gradient norm in None layer : 0.00014159304525618514
gradient norm in None layer : 1.3633194013264456e-06
gradient norm in None layer : 0.0003636609312798893
gradient norm in None layer : 2.4315658726005437e-05
gradient norm in None layer : 1.9650309728551766e-05
gradient norm in None layer : 0.00043476365783239214
gradient norm in None layer : 4.0340475250667904e-05
gradient norm in None layer : 5.6826441726170874e-05
gradient norm in None layer : 0.000850366343907633
gradient norm in None layer : 4.386404630077986e-06
gradient norm in None layer : 0.0011907415511851086
gradient norm in None layer : 0.00010928330992477313
gradient norm in None layer : 0.00010661502391709997
gradient norm in None layer : 0.0013946430990957783
gradient norm in None layer : 0.00012018385443962335
gradient norm in None layer : 0.00014528550181491847
gradient norm in None layer : 0.00010460574666171909
gradient norm in None layer : 1.6761661040882068e-05
Total gradient norm: 0.002800742778338596
invariance loss : 4.410663126884987, avg_den : 0.41407012939453125, density loss : 0.31407012939453127, mse loss : 0.0334480353308782, solver time : 145.76080179214478 sec , total loss : 0.03817276858715772, running loss : 0.057784334631776846
Epoch 0/10 , batch 12/12500 
ITERATION : 1, loss : 0.02675505584220591ITERATION : 2, loss : 0.023323610478930997ITERATION : 3, loss : 0.02212683542282043ITERATION : 4, loss : 0.021542861127343382ITERATION : 5, loss : 0.021214816510951147ITERATION : 6, loss : 0.021014830135539706ITERATION : 7, loss : 0.020886438522397372ITERATION : 8, loss : 0.020801277019457345ITERATION : 9, loss : 0.020743617940990906ITERATION : 10, loss : 0.020704062797234087ITERATION : 11, loss : 0.020676689658765186ITERATION : 12, loss : 0.020657631924450855ITERATION : 13, loss : 0.02064430501588438ITERATION : 14, loss : 0.02063495422239987ITERATION : 15, loss : 0.0206283755842894ITERATION : 16, loss : 0.020623736862508874ITERATION : 17, loss : 0.020620459650052707ITERATION : 18, loss : 0.020618140312371774ITERATION : 19, loss : 0.02061649625577346ITERATION : 20, loss : 0.020615329144511755ITERATION : 21, loss : 0.020614499450543427ITERATION : 22, loss : 0.020613908830507132ITERATION : 23, loss : 0.020613487841518206ITERATION : 24, loss : 0.020613187382286645ITERATION : 25, loss : 0.020612972674279605ITERATION : 26, loss : 0.020612819050156606ITERATION : 27, loss : 0.020612709019454726ITERATION : 28, loss : 0.020612630110915534ITERATION : 29, loss : 0.020612573452102143ITERATION : 30, loss : 0.02061253270066068ITERATION : 31, loss : 0.020612503371771916ITERATION : 32, loss : 0.02061248225493304ITERATION : 33, loss : 0.020612467017013584ITERATION : 34, loss : 0.02061245602287892ITERATION : 35, loss : 0.02061244808268901ITERATION : 36, loss : 0.020612442319590606ITERATION : 37, loss : 0.02061243814751221ITERATION : 38, loss : 0.020612435131228155ITERATION : 39, loss : 0.020612432941585736ITERATION : 40, loss : 0.020612431347394527ITERATION : 41, loss : 0.020612430201663607ITERATION : 42, loss : 0.020612429368306998ITERATION : 43, loss : 0.020612428762199212ITERATION : 44, loss : 0.020612428331769017ITERATION : 45, loss : 0.020612428007884705ITERATION : 46, loss : 0.020612427781566512ITERATION : 47, loss : 0.020612427609772135ITERATION : 48, loss : 0.020612427496945102ITERATION : 49, loss : 0.020612427412070453ITERATION : 50, loss : 0.020612427356149387ITERATION : 51, loss : 0.020612427299617955ITERATION : 52, loss : 0.02061242727361631ITERATION : 53, loss : 0.02061242725183014ITERATION : 54, loss : 0.02061242722600586ITERATION : 55, loss : 0.02061242720508008ITERATION : 56, loss : 0.020612427202493726ITERATION : 57, loss : 0.02061242719802871ITERATION : 58, loss : 0.020612427196575543ITERATION : 59, loss : 0.020612427196575543ITERATION : 60, loss : 0.020612427196575543ITERATION : 61, loss : 0.020612427196575543ITERATION : 62, loss : 0.020612427196575543ITERATION : 63, loss : 0.020612427196575543ITERATION : 64, loss : 0.020612427196575543ITERATION : 65, loss : 0.020612427196575543ITERATION : 66, loss : 0.020612427196575543ITERATION : 67, loss : 0.020612427196575543ITERATION : 68, loss : 0.020612427196575543ITERATION : 69, loss : 0.020612427196575543ITERATION : 70, loss : 0.020612427196575543ITERATION : 71, loss : 0.020612427196575543ITERATION : 72, loss : 0.020612427196575543ITERATION : 73, loss : 0.020612427196575543ITERATION : 74, loss : 0.020612427196575543ITERATION : 75, loss : 0.020612427196575543ITERATION : 76, loss : 0.020612427196575543ITERATION : 77, loss : 0.020612427196575543ITERATION : 78, loss : 0.020612427196575543ITERATION : 79, loss : 0.020612427196575543ITERATION : 80, loss : 0.020612427196575543ITERATION : 81, loss : 0.020612427196575543ITERATION : 82, loss : 0.020612427196575543ITERATION : 83, loss : 0.020612427196575543ITERATION : 84, loss : 0.020612427196575543ITERATION : 85, loss : 0.020612427196575543ITERATION : 86, loss : 0.020612427196575543ITERATION : 87, loss : 0.020612427196575543ITERATION : 88, loss : 0.020612427196575543ITERATION : 89, loss : 0.020612427196575543ITERATION : 90, loss : 0.020612427196575543ITERATION : 91, loss : 0.020612427196575543ITERATION : 92, loss : 0.020612427196575543ITERATION : 93, loss : 0.020612427196575543ITERATION : 94, loss : 0.020612427196575543ITERATION : 95, loss : 0.020612427196575543ITERATION : 96, loss : 0.020612427196575543ITERATION : 97, loss : 0.020612427196575543ITERATION : 98, loss : 0.020612427196575543ITERATION : 99, loss : 0.020612427196575543ITERATION : 100, loss : 0.020612427196575543
ITERATION : 1, loss : 0.016480296645697584ITERATION : 2, loss : 0.0150747149779722ITERATION : 3, loss : 0.014835346787791576ITERATION : 4, loss : 0.01500209735295311ITERATION : 5, loss : 0.015185339119928592ITERATION : 6, loss : 0.015254881660647921ITERATION : 7, loss : 0.015314700308147862ITERATION : 8, loss : 0.015362130632364435ITERATION : 9, loss : 0.015398303007186897ITERATION : 10, loss : 0.015425339977986353ITERATION : 11, loss : 0.015445335499644079ITERATION : 12, loss : 0.015460043240637641ITERATION : 13, loss : 0.015470834079133836ITERATION : 14, loss : 0.015478743905724674ITERATION : 15, loss : 0.015484541927681008ITERATION : 16, loss : 0.015488794031395121ITERATION : 17, loss : 0.015491914715620054ITERATION : 18, loss : 0.015494206962904258ITERATION : 19, loss : 0.015495892172672119ITERATION : 20, loss : 0.015497132134087287ITERATION : 21, loss : 0.01549804522570527ITERATION : 22, loss : 0.01549871813781361ITERATION : 23, loss : 0.015499214390451047ITERATION : 24, loss : 0.015499580664626445ITERATION : 25, loss : 0.015499851106053144ITERATION : 26, loss : 0.015500050955879875ITERATION : 27, loss : 0.015500198726439767ITERATION : 28, loss : 0.015500308005838691ITERATION : 29, loss : 0.015500388904177414ITERATION : 30, loss : 0.015500448783327264ITERATION : 31, loss : 0.015500493144212105ITERATION : 32, loss : 0.015500526015559957ITERATION : 33, loss : 0.015500550389833135ITERATION : 34, loss : 0.015500568460995082ITERATION : 35, loss : 0.01550058186778088ITERATION : 36, loss : 0.015500591824586829ITERATION : 37, loss : 0.015500599220566926ITERATION : 38, loss : 0.015500604716119427ITERATION : 39, loss : 0.015500608796281219ITERATION : 40, loss : 0.015500611824276419ITERATION : 41, loss : 0.015500614073882759ITERATION : 42, loss : 0.015500615747447115ITERATION : 43, loss : 0.015500616989717688ITERATION : 44, loss : 0.015500617916768059ITERATION : 45, loss : 0.015500618599880693ITERATION : 46, loss : 0.01550061911446ITERATION : 47, loss : 0.0155006195032056ITERATION : 48, loss : 0.015500619797587096ITERATION : 49, loss : 0.015500620011832804ITERATION : 50, loss : 0.015500620179978768ITERATION : 51, loss : 0.015500620291118946ITERATION : 52, loss : 0.0155006203742311ITERATION : 53, loss : 0.015500620444888117ITERATION : 54, loss : 0.015500620492032576ITERATION : 55, loss : 0.01550062051968599ITERATION : 56, loss : 0.015500620533248561ITERATION : 57, loss : 0.015500620540954574ITERATION : 58, loss : 0.015500620546813717ITERATION : 59, loss : 0.015500620570105665ITERATION : 60, loss : 0.015500620570860725ITERATION : 61, loss : 0.015500620570860725ITERATION : 62, loss : 0.015500620570860725ITERATION : 63, loss : 0.015500620570860725ITERATION : 64, loss : 0.015500620570860725ITERATION : 65, loss : 0.015500620570860725ITERATION : 66, loss : 0.015500620570860725ITERATION : 67, loss : 0.015500620570860725ITERATION : 68, loss : 0.015500620570860725ITERATION : 69, loss : 0.015500620570860725ITERATION : 70, loss : 0.015500620570860725ITERATION : 71, loss : 0.015500620570860725ITERATION : 72, loss : 0.015500620570860725ITERATION : 73, loss : 0.015500620570860725ITERATION : 74, loss : 0.015500620570860725ITERATION : 75, loss : 0.015500620570860725ITERATION : 76, loss : 0.015500620570860725ITERATION : 77, loss : 0.015500620570860725ITERATION : 78, loss : 0.015500620570860725ITERATION : 79, loss : 0.015500620570860725ITERATION : 80, loss : 0.015500620570860725ITERATION : 81, loss : 0.015500620570860725ITERATION : 82, loss : 0.015500620570860725ITERATION : 83, loss : 0.015500620570860725ITERATION : 84, loss : 0.015500620570860725ITERATION : 85, loss : 0.015500620570860725ITERATION : 86, loss : 0.015500620570860725ITERATION : 87, loss : 0.015500620570860725ITERATION : 88, loss : 0.015500620570860725ITERATION : 89, loss : 0.015500620570860725ITERATION : 90, loss : 0.015500620570860725ITERATION : 91, loss : 0.015500620570860725ITERATION : 92, loss : 0.015500620570860725ITERATION : 93, loss : 0.015500620570860725ITERATION : 94, loss : 0.015500620570860725ITERATION : 95, loss : 0.015500620570860725ITERATION : 96, loss : 0.015500620570860725ITERATION : 97, loss : 0.015500620570860725ITERATION : 98, loss : 0.015500620570860725ITERATION : 99, loss : 0.015500620570860725ITERATION : 100, loss : 0.015500620570860725
ITERATION : 1, loss : 0.03133342435942539ITERATION : 2, loss : 0.02578748044685835ITERATION : 3, loss : 0.023384721316802623ITERATION : 4, loss : 0.0221969756083198ITERATION : 5, loss : 0.02154494222099239ITERATION : 6, loss : 0.021159848012452298ITERATION : 7, loss : 0.020920247348787658ITERATION : 8, loss : 0.020765456118823845ITERATION : 9, loss : 0.020662706730588915ITERATION : 10, loss : 0.020593163239136136ITERATION : 11, loss : 0.02054543537297381ITERATION : 12, loss : 0.020512352350752248ITERATION : 13, loss : 0.02048925646739212ITERATION : 14, loss : 0.020473049854182798ITERATION : 15, loss : 0.020461635150602116ITERATION : 16, loss : 0.020453573662130374ITERATION : 17, loss : 0.020447868929932094ITERATION : 18, loss : 0.020443825821300023ITERATION : 19, loss : 0.0204409571311269ITERATION : 20, loss : 0.02043891990972955ITERATION : 21, loss : 0.02043747217759346ITERATION : 22, loss : 0.020436442787315025ITERATION : 23, loss : 0.02043571046573758ITERATION : 24, loss : 0.0204351893048061ITERATION : 25, loss : 0.020434818262469987ITERATION : 26, loss : 0.0204345540828884ITERATION : 27, loss : 0.020434365854078453ITERATION : 28, loss : 0.020434231722445084ITERATION : 29, loss : 0.020434136126748038ITERATION : 30, loss : 0.020434067989273292ITERATION : 31, loss : 0.020434019413397776ITERATION : 32, loss : 0.020433984755569776ITERATION : 33, loss : 0.020433960094237768ITERATION : 34, loss : 0.020433942483229207ITERATION : 35, loss : 0.02043392989158644ITERATION : 36, loss : 0.020433920899241332ITERATION : 37, loss : 0.020433914490250924ITERATION : 38, loss : 0.02043390991327306ITERATION : 39, loss : 0.020433906669458442ITERATION : 40, loss : 0.020433904337327478ITERATION : 41, loss : 0.020433902668415174ITERATION : 42, loss : 0.02043390149055693ITERATION : 43, loss : 0.020433900651798443ITERATION : 44, loss : 0.020433900051221702ITERATION : 45, loss : 0.02043389962524085ITERATION : 46, loss : 0.020433899324692084ITERATION : 47, loss : 0.02043389911005165ITERATION : 48, loss : 0.020433898957650225ITERATION : 49, loss : 0.02043389885609962ITERATION : 50, loss : 0.020433898768461706ITERATION : 51, loss : 0.02043389871077502ITERATION : 52, loss : 0.020433898670781693ITERATION : 53, loss : 0.020433898650188777ITERATION : 54, loss : 0.020433898608560135ITERATION : 55, loss : 0.02043389859614042ITERATION : 56, loss : 0.020433898594452802ITERATION : 57, loss : 0.020433898594452802ITERATION : 58, loss : 0.020433898594452802ITERATION : 59, loss : 0.020433898594452802ITERATION : 60, loss : 0.020433898594452802ITERATION : 61, loss : 0.020433898594452802ITERATION : 62, loss : 0.020433898594452802ITERATION : 63, loss : 0.020433898594452802ITERATION : 64, loss : 0.020433898594452802ITERATION : 65, loss : 0.020433898594452802ITERATION : 66, loss : 0.020433898594452802ITERATION : 67, loss : 0.020433898594452802ITERATION : 68, loss : 0.020433898594452802ITERATION : 69, loss : 0.020433898594452802ITERATION : 70, loss : 0.020433898594452802ITERATION : 71, loss : 0.020433898594452802ITERATION : 72, loss : 0.020433898594452802ITERATION : 73, loss : 0.020433898594452802ITERATION : 74, loss : 0.020433898594452802ITERATION : 75, loss : 0.020433898594452802ITERATION : 76, loss : 0.020433898594452802ITERATION : 77, loss : 0.020433898594452802ITERATION : 78, loss : 0.020433898594452802ITERATION : 79, loss : 0.020433898594452802ITERATION : 80, loss : 0.020433898594452802ITERATION : 81, loss : 0.020433898594452802ITERATION : 82, loss : 0.020433898594452802ITERATION : 83, loss : 0.020433898594452802ITERATION : 84, loss : 0.020433898594452802ITERATION : 85, loss : 0.020433898594452802ITERATION : 86, loss : 0.020433898594452802ITERATION : 87, loss : 0.020433898594452802ITERATION : 88, loss : 0.020433898594452802ITERATION : 89, loss : 0.020433898594452802ITERATION : 90, loss : 0.020433898594452802ITERATION : 91, loss : 0.020433898594452802ITERATION : 92, loss : 0.020433898594452802ITERATION : 93, loss : 0.020433898594452802ITERATION : 94, loss : 0.020433898594452802ITERATION : 95, loss : 0.020433898594452802ITERATION : 96, loss : 0.020433898594452802ITERATION : 97, loss : 0.020433898594452802ITERATION : 98, loss : 0.020433898594452802ITERATION : 99, loss : 0.020433898594452802ITERATION : 100, loss : 0.020433898594452802
ITERATION : 1, loss : 0.08038678271395255ITERATION : 2, loss : 0.0820760888204915ITERATION : 3, loss : 0.07443283334875625ITERATION : 4, loss : 0.06713496973154044ITERATION : 5, loss : 0.06236484227370188ITERATION : 6, loss : 0.05918424792895289ITERATION : 7, loss : 0.057032951897734024ITERATION : 8, loss : 0.0555626004549138ITERATION : 9, loss : 0.05455004895659205ITERATION : 10, loss : 0.05384898775654887ITERATION : 11, loss : 0.05336173584528637ITERATION : 12, loss : 0.05302217763661291ITERATION : 13, loss : 0.05278510401822713ITERATION : 14, loss : 0.052619370701026236ITERATION : 15, loss : 0.05250340879218286ITERATION : 16, loss : 0.05242222362568577ITERATION : 17, loss : 0.052365363433971565ITERATION : 18, loss : 0.05232552968022383ITERATION : 19, loss : 0.05229761962733137ITERATION : 20, loss : 0.05227806219566535ITERATION : 21, loss : 0.05226435688809757ITERATION : 22, loss : 0.05225274918471325ITERATION : 23, loss : 0.05224349331063372ITERATION : 24, loss : 0.05223695533553185ITERATION : 25, loss : 0.05223233672967977ITERATION : 26, loss : 0.05222907378306879ITERATION : 27, loss : 0.052226768547824036ITERATION : 28, loss : 0.05222513983448863ITERATION : 29, loss : 0.052223989057677896ITERATION : 30, loss : 0.05222317606345262ITERATION : 31, loss : 0.052222601635015395ITERATION : 32, loss : 0.05222219563302089ITERATION : 33, loss : 0.0522219087664257ITERATION : 34, loss : 0.052221706118266015ITERATION : 35, loss : 0.05222156297296989ITERATION : 36, loss : 0.05222146184346364ITERATION : 37, loss : 0.05222139032745191ITERATION : 38, loss : 0.052221339832078294ITERATION : 39, loss : 0.05222130413110141ITERATION : 40, loss : 0.05222127883373733ITERATION : 41, loss : 0.052221260889947906ITERATION : 42, loss : 0.05222124825687119ITERATION : 43, loss : 0.052221239372511226ITERATION : 44, loss : 0.05222123302715376ITERATION : 45, loss : 0.0522212286435396ITERATION : 46, loss : 0.05222122542875241ITERATION : 47, loss : 0.05222122345343347ITERATION : 48, loss : 0.05222122192133985ITERATION : 49, loss : 0.052221220767543514ITERATION : 50, loss : 0.05222121994144668ITERATION : 51, loss : 0.05222121938174335ITERATION : 52, loss : 0.05222121898749366ITERATION : 53, loss : 0.05222121873974339ITERATION : 54, loss : 0.05222121873176576ITERATION : 55, loss : 0.05222121872689573ITERATION : 56, loss : 0.05222121872689573ITERATION : 57, loss : 0.05222121872689573ITERATION : 58, loss : 0.05222121872689573ITERATION : 59, loss : 0.05222121872689573ITERATION : 60, loss : 0.05222121872689573ITERATION : 61, loss : 0.05222121872689573ITERATION : 62, loss : 0.05222121872689573ITERATION : 63, loss : 0.05222121872689573ITERATION : 64, loss : 0.05222121872689573ITERATION : 65, loss : 0.05222121872689573ITERATION : 66, loss : 0.05222121872689573ITERATION : 67, loss : 0.05222121872689573ITERATION : 68, loss : 0.05222121872689573ITERATION : 69, loss : 0.05222121872689573ITERATION : 70, loss : 0.05222121872689573ITERATION : 71, loss : 0.05222121872689573ITERATION : 72, loss : 0.05222121872689573ITERATION : 73, loss : 0.05222121872689573ITERATION : 74, loss : 0.05222121872689573ITERATION : 75, loss : 0.05222121872689573ITERATION : 76, loss : 0.05222121872689573ITERATION : 77, loss : 0.05222121872689573ITERATION : 78, loss : 0.05222121872689573ITERATION : 79, loss : 0.05222121872689573ITERATION : 80, loss : 0.05222121872689573ITERATION : 81, loss : 0.05222121872689573ITERATION : 82, loss : 0.05222121872689573ITERATION : 83, loss : 0.05222121872689573ITERATION : 84, loss : 0.05222121872689573ITERATION : 85, loss : 0.05222121872689573ITERATION : 86, loss : 0.05222121872689573ITERATION : 87, loss : 0.05222121872689573ITERATION : 88, loss : 0.05222121872689573ITERATION : 89, loss : 0.05222121872689573ITERATION : 90, loss : 0.05222121872689573ITERATION : 91, loss : 0.05222121872689573ITERATION : 92, loss : 0.05222121872689573ITERATION : 93, loss : 0.05222121872689573ITERATION : 94, loss : 0.05222121872689573ITERATION : 95, loss : 0.05222121872689573ITERATION : 96, loss : 0.05222121872689573ITERATION : 97, loss : 0.05222121872689573ITERATION : 98, loss : 0.05222121872689573ITERATION : 99, loss : 0.05222121872689573ITERATION : 100, loss : 0.05222121872689573
ITERATION : 1, loss : 0.03974553731606258ITERATION : 2, loss : 0.0319770979697879ITERATION : 3, loss : 0.029124906406887367ITERATION : 4, loss : 0.027956444532931217ITERATION : 5, loss : 0.027424746276192607ITERATION : 6, loss : 0.027164854921257205ITERATION : 7, loss : 0.0270315819117984ITERATION : 8, loss : 0.026961093317772734ITERATION : 9, loss : 0.026923211778051053ITERATION : 10, loss : 0.02690284062516846ITERATION : 11, loss : 0.026892074221966326ITERATION : 12, loss : 0.026886622346038197ITERATION : 13, loss : 0.026884097186574394ITERATION : 14, loss : 0.02688315067457951ITERATION : 15, loss : 0.02688301972004662ITERATION : 16, loss : 0.02688327600137831ITERATION : 17, loss : 0.026883684785549825ITERATION : 18, loss : 0.026884122496529758ITERATION : 19, loss : 0.02688452936366223ITERATION : 20, loss : 0.026884880712347802ITERATION : 21, loss : 0.026885170669669747ITERATION : 22, loss : 0.02688540273545404ITERATION : 23, loss : 0.026885584568297864ITERATION : 24, loss : 0.026885724446157517ITERATION : 25, loss : 0.02688583089813101ITERATION : 26, loss : 0.026885910801876ITERATION : 27, loss : 0.026885970402154284ITERATION : 28, loss : 0.026886014472893224ITERATION : 29, loss : 0.026886046898610967ITERATION : 30, loss : 0.026886070521230158ITERATION : 31, loss : 0.026886087764291303ITERATION : 32, loss : 0.026886100334765375ITERATION : 33, loss : 0.026886109386496205ITERATION : 34, loss : 0.02688611588830726ITERATION : 35, loss : 0.02688612048557237ITERATION : 36, loss : 0.026886123787521723ITERATION : 37, loss : 0.026886126149006522ITERATION : 38, loss : 0.026886127845073653ITERATION : 39, loss : 0.026886129057140613ITERATION : 40, loss : 0.02688612994602553ITERATION : 41, loss : 0.02688613058398289ITERATION : 42, loss : 0.026886131022151344ITERATION : 43, loss : 0.026886131375713683ITERATION : 44, loss : 0.02688613157961707ITERATION : 45, loss : 0.026886131742071804ITERATION : 46, loss : 0.026886131831678216ITERATION : 47, loss : 0.026886131955619882ITERATION : 48, loss : 0.02688613199416705ITERATION : 49, loss : 0.026886132017303263ITERATION : 50, loss : 0.026886132024963216ITERATION : 51, loss : 0.026886132024963216ITERATION : 52, loss : 0.026886132024963216ITERATION : 53, loss : 0.026886132024963216ITERATION : 54, loss : 0.026886132024963216ITERATION : 55, loss : 0.026886132024963216ITERATION : 56, loss : 0.026886132024963216ITERATION : 57, loss : 0.026886132024963216ITERATION : 58, loss : 0.026886132024963216ITERATION : 59, loss : 0.026886132024963216ITERATION : 60, loss : 0.026886132024963216ITERATION : 61, loss : 0.026886132024963216ITERATION : 62, loss : 0.026886132024963216ITERATION : 63, loss : 0.026886132024963216ITERATION : 64, loss : 0.026886132024963216ITERATION : 65, loss : 0.026886132024963216ITERATION : 66, loss : 0.026886132024963216ITERATION : 67, loss : 0.026886132024963216ITERATION : 68, loss : 0.026886132024963216ITERATION : 69, loss : 0.026886132024963216ITERATION : 70, loss : 0.026886132024963216ITERATION : 71, loss : 0.026886132024963216ITERATION : 72, loss : 0.026886132024963216ITERATION : 73, loss : 0.026886132024963216ITERATION : 74, loss : 0.026886132024963216ITERATION : 75, loss : 0.026886132024963216ITERATION : 76, loss : 0.026886132024963216ITERATION : 77, loss : 0.026886132024963216ITERATION : 78, loss : 0.026886132024963216ITERATION : 79, loss : 0.026886132024963216ITERATION : 80, loss : 0.026886132024963216ITERATION : 81, loss : 0.026886132024963216ITERATION : 82, loss : 0.026886132024963216ITERATION : 83, loss : 0.026886132024963216ITERATION : 84, loss : 0.026886132024963216ITERATION : 85, loss : 0.026886132024963216ITERATION : 86, loss : 0.026886132024963216ITERATION : 87, loss : 0.026886132024963216ITERATION : 88, loss : 0.026886132024963216ITERATION : 89, loss : 0.026886132024963216ITERATION : 90, loss : 0.026886132024963216ITERATION : 91, loss : 0.026886132024963216ITERATION : 92, loss : 0.026886132024963216ITERATION : 93, loss : 0.026886132024963216ITERATION : 94, loss : 0.026886132024963216ITERATION : 95, loss : 0.026886132024963216ITERATION : 96, loss : 0.026886132024963216ITERATION : 97, loss : 0.026886132024963216ITERATION : 98, loss : 0.026886132024963216ITERATION : 99, loss : 0.026886132024963216ITERATION : 100, loss : 0.026886132024963216
ITERATION : 1, loss : 0.08732631144434821ITERATION : 2, loss : 0.09083521078741455ITERATION : 3, loss : 0.09039663355394563ITERATION : 4, loss : 0.0881898136240326ITERATION : 5, loss : 0.08579514542793504ITERATION : 6, loss : 0.08371352808417545ITERATION : 7, loss : 0.08204251419641045ITERATION : 8, loss : 0.08074636397045186ITERATION : 9, loss : 0.07975672558516064ITERATION : 10, loss : 0.07900667048198803ITERATION : 11, loss : 0.07844010884407306ITERATION : 12, loss : 0.0780127475502265ITERATION : 13, loss : 0.07769051677887433ITERATION : 14, loss : 0.07744753010310591ITERATION : 15, loss : 0.07726423391891475ITERATION : 16, loss : 0.07712589684952151ITERATION : 17, loss : 0.07702143330256855ITERATION : 18, loss : 0.0769425033676273ITERATION : 19, loss : 0.07688283117386349ITERATION : 20, loss : 0.07683769204950726ITERATION : 21, loss : 0.07680352704654206ITERATION : 22, loss : 0.07677765417190294ITERATION : 23, loss : 0.0767580507807282ITERATION : 24, loss : 0.07674319004659977ITERATION : 25, loss : 0.07673191968002634ITERATION : 26, loss : 0.07672336803765371ITERATION : 27, loss : 0.07671687658265927ITERATION : 28, loss : 0.07671194729034622ITERATION : 29, loss : 0.07670820260734124ITERATION : 30, loss : 0.07670535691154573ITERATION : 31, loss : 0.07670319373778361ITERATION : 32, loss : 0.07670154876043077ITERATION : 33, loss : 0.07670029747840573ITERATION : 34, loss : 0.07669934540020476ITERATION : 35, loss : 0.07669862082232497ITERATION : 36, loss : 0.07669806924825083ITERATION : 37, loss : 0.07669764928396179ITERATION : 38, loss : 0.07669732943401918ITERATION : 39, loss : 0.07669708571346237ITERATION : 40, loss : 0.07669690015368646ITERATION : 41, loss : 0.07669675868574562ITERATION : 42, loss : 0.0766966508614632ITERATION : 43, loss : 0.07669656857985566ITERATION : 44, loss : 0.0766965059451698ITERATION : 45, loss : 0.07669645818772378ITERATION : 46, loss : 0.07669642182849985ITERATION : 47, loss : 0.07669639409530636ITERATION : 48, loss : 0.07669637294411119ITERATION : 49, loss : 0.07669635685400902ITERATION : 50, loss : 0.07669634459987575ITERATION : 51, loss : 0.07669633534397276ITERATION : 52, loss : 0.07669632821305844ITERATION : 53, loss : 0.0766963227674136ITERATION : 54, loss : 0.07669631870496874ITERATION : 55, loss : 0.07669631559404241ITERATION : 56, loss : 0.07669631287556791ITERATION : 57, loss : 0.07669631102363335ITERATION : 58, loss : 0.07669630973599223ITERATION : 59, loss : 0.07669630853600842ITERATION : 60, loss : 0.07669630765209522ITERATION : 61, loss : 0.07669630718540899ITERATION : 62, loss : 0.07669630670514478ITERATION : 63, loss : 0.07669630643698983ITERATION : 64, loss : 0.0766963061108177ITERATION : 65, loss : 0.07669630595241926ITERATION : 66, loss : 0.07669630585577336ITERATION : 67, loss : 0.07669630584556207ITERATION : 68, loss : 0.07669630584058472ITERATION : 69, loss : 0.07669630584058472ITERATION : 70, loss : 0.07669630584058472ITERATION : 71, loss : 0.07669630584058472ITERATION : 72, loss : 0.07669630584058472ITERATION : 73, loss : 0.07669630584058472ITERATION : 74, loss : 0.07669630584058472ITERATION : 75, loss : 0.07669630584058472ITERATION : 76, loss : 0.07669630584058472ITERATION : 77, loss : 0.07669630584058472ITERATION : 78, loss : 0.07669630584058472ITERATION : 79, loss : 0.07669630584058472ITERATION : 80, loss : 0.07669630584058472ITERATION : 81, loss : 0.07669630584058472ITERATION : 82, loss : 0.07669630584058472ITERATION : 83, loss : 0.07669630584058472ITERATION : 84, loss : 0.07669630584058472ITERATION : 85, loss : 0.07669630584058472ITERATION : 86, loss : 0.07669630584058472ITERATION : 87, loss : 0.07669630584058472ITERATION : 88, loss : 0.07669630584058472ITERATION : 89, loss : 0.07669630584058472ITERATION : 90, loss : 0.07669630584058472ITERATION : 91, loss : 0.07669630584058472ITERATION : 92, loss : 0.07669630584058472ITERATION : 93, loss : 0.07669630584058472ITERATION : 94, loss : 0.07669630584058472ITERATION : 95, loss : 0.07669630584058472ITERATION : 96, loss : 0.07669630584058472ITERATION : 97, loss : 0.07669630584058472ITERATION : 98, loss : 0.07669630584058472ITERATION : 99, loss : 0.07669630584058472ITERATION : 100, loss : 0.07669630584058472
ITERATION : 1, loss : 0.023015525821605754ITERATION : 2, loss : 0.01874893062454779ITERATION : 3, loss : 0.019060846062116974ITERATION : 4, loss : 0.0200913541817042ITERATION : 5, loss : 0.021098624117024574ITERATION : 6, loss : 0.02193372284451483ITERATION : 7, loss : 0.022587823691926783ITERATION : 8, loss : 0.02308681315849682ITERATION : 9, loss : 0.023462017357789182ITERATION : 10, loss : 0.02374168410960869ITERATION : 11, loss : 0.02394895826677117ITERATION : 12, loss : 0.02410198666595727ITERATION : 13, loss : 0.024214660316573888ITERATION : 14, loss : 0.02429746014533166ITERATION : 15, loss : 0.024358220656662672ITERATION : 16, loss : 0.024402761539301966ITERATION : 17, loss : 0.024435386918165096ITERATION : 18, loss : 0.024459270315243968ITERATION : 19, loss : 0.024476746184981007ITERATION : 20, loss : 0.024489529025463558ITERATION : 21, loss : 0.024498876562398363ITERATION : 22, loss : 0.024505710496157324ITERATION : 23, loss : 0.024510705842233093ITERATION : 24, loss : 0.024514356641838625ITERATION : 25, loss : 0.024517024476105397ITERATION : 26, loss : 0.024518973738507956ITERATION : 27, loss : 0.02452039786915823ITERATION : 28, loss : 0.02452143825503458ITERATION : 29, loss : 0.024522198188620503ITERATION : 30, loss : 0.02452275324158583ITERATION : 31, loss : 0.024523158640385068ITERATION : 32, loss : 0.02452345467808325ITERATION : 33, loss : 0.02452367087416647ITERATION : 34, loss : 0.0245238287159629ITERATION : 35, loss : 0.024523943963454406ITERATION : 36, loss : 0.024524028064873034ITERATION : 37, loss : 0.0245240894862847ITERATION : 38, loss : 0.024524134313414922ITERATION : 39, loss : 0.024524167030647366ITERATION : 40, loss : 0.024524190941144247ITERATION : 41, loss : 0.024524208363864118ITERATION : 42, loss : 0.02452422108859952ITERATION : 43, loss : 0.024524230397328792ITERATION : 44, loss : 0.0245242371754956ITERATION : 45, loss : 0.024524242077929043ITERATION : 46, loss : 0.024524245661788575ITERATION : 47, loss : 0.024524248246732935ITERATION : 48, loss : 0.02452425013532023ITERATION : 49, loss : 0.024524251509810725ITERATION : 50, loss : 0.024524252525594082ITERATION : 51, loss : 0.024524253205401095ITERATION : 52, loss : 0.024524253770206145ITERATION : 53, loss : 0.0245242541666229ITERATION : 54, loss : 0.024524254465802687ITERATION : 55, loss : 0.02452425465271386ITERATION : 56, loss : 0.02452425479986448ITERATION : 57, loss : 0.02452425489492745ITERATION : 58, loss : 0.024524254981058376ITERATION : 59, loss : 0.02452425499660271ITERATION : 60, loss : 0.024524255041404777ITERATION : 61, loss : 0.02452425505482063ITERATION : 62, loss : 0.02452425505482063ITERATION : 63, loss : 0.02452425505482063ITERATION : 64, loss : 0.02452425505482063ITERATION : 65, loss : 0.02452425505482063ITERATION : 66, loss : 0.02452425505482063ITERATION : 67, loss : 0.02452425505482063ITERATION : 68, loss : 0.02452425505482063ITERATION : 69, loss : 0.02452425505482063ITERATION : 70, loss : 0.02452425505482063ITERATION : 71, loss : 0.02452425505482063ITERATION : 72, loss : 0.02452425505482063ITERATION : 73, loss : 0.02452425505482063ITERATION : 74, loss : 0.02452425505482063ITERATION : 75, loss : 0.02452425505482063ITERATION : 76, loss : 0.02452425505482063ITERATION : 77, loss : 0.02452425505482063ITERATION : 78, loss : 0.02452425505482063ITERATION : 79, loss : 0.02452425505482063ITERATION : 80, loss : 0.02452425505482063ITERATION : 81, loss : 0.02452425505482063ITERATION : 82, loss : 0.02452425505482063ITERATION : 83, loss : 0.02452425505482063ITERATION : 84, loss : 0.02452425505482063ITERATION : 85, loss : 0.02452425505482063ITERATION : 86, loss : 0.02452425505482063ITERATION : 87, loss : 0.02452425505482063ITERATION : 88, loss : 0.02452425505482063ITERATION : 89, loss : 0.02452425505482063ITERATION : 90, loss : 0.02452425505482063ITERATION : 91, loss : 0.02452425505482063ITERATION : 92, loss : 0.02452425505482063ITERATION : 93, loss : 0.02452425505482063ITERATION : 94, loss : 0.02452425505482063ITERATION : 95, loss : 0.02452425505482063ITERATION : 96, loss : 0.02452425505482063ITERATION : 97, loss : 0.02452425505482063ITERATION : 98, loss : 0.02452425505482063ITERATION : 99, loss : 0.02452425505482063ITERATION : 100, loss : 0.02452425505482063
ITERATION : 1, loss : 0.031045136478831958ITERATION : 2, loss : 0.025239395925752386ITERATION : 3, loss : 0.02370576768936309ITERATION : 4, loss : 0.023173852432707014ITERATION : 5, loss : 0.022960742350268383ITERATION : 6, loss : 0.02286421333373573ITERATION : 7, loss : 0.022814521501113023ITERATION : 8, loss : 0.02278554515118279ITERATION : 9, loss : 0.02276685047515926ITERATION : 10, loss : 0.022753945172880045ITERATION : 11, loss : 0.02274468723492944ITERATION : 12, loss : 0.022737918608782055ITERATION : 13, loss : 0.022732930659273436ITERATION : 14, loss : 0.022729246777139667ITERATION : 15, loss : 0.022726527421327324ITERATION : 16, loss : 0.02272452314031927ITERATION : 17, loss : 0.02272304889282751ITERATION : 18, loss : 0.022721966540350084ITERATION : 19, loss : 0.022721173290255005ITERATION : 20, loss : 0.022720592981685238ITERATION : 21, loss : 0.022720169032010008ITERATION : 22, loss : 0.022719859624978724ITERATION : 23, loss : 0.02271963408719377ITERATION : 24, loss : 0.022719469862659124ITERATION : 25, loss : 0.02271935033442571ITERATION : 26, loss : 0.02271926346087832ITERATION : 27, loss : 0.022719200286424285ITERATION : 28, loss : 0.0227191543624995ITERATION : 29, loss : 0.02271912113072223ITERATION : 30, loss : 0.022719097006543596ITERATION : 31, loss : 0.022719079479266862ITERATION : 32, loss : 0.022719066792256964ITERATION : 33, loss : 0.02271905760619697ITERATION : 34, loss : 0.022719050958518675ITERATION : 35, loss : 0.02271904612702099ITERATION : 36, loss : 0.022719042612023642ITERATION : 37, loss : 0.022719040062181423ITERATION : 38, loss : 0.02271903823214777ITERATION : 39, loss : 0.02271903687439612ITERATION : 40, loss : 0.022719035899720037ITERATION : 41, loss : 0.02271903521622798ITERATION : 42, loss : 0.02271903471853602ITERATION : 43, loss : 0.02271903436286407ITERATION : 44, loss : 0.022719034078235924ITERATION : 45, loss : 0.022719033894919537ITERATION : 46, loss : 0.0227190337779664ITERATION : 47, loss : 0.022719033686688074ITERATION : 48, loss : 0.02271903362259112ITERATION : 49, loss : 0.022719033594813143ITERATION : 50, loss : 0.022719033564084813ITERATION : 51, loss : 0.022719033550446334ITERATION : 52, loss : 0.02271903353799642ITERATION : 53, loss : 0.022719033534673114ITERATION : 54, loss : 0.022719033509165917ITERATION : 55, loss : 0.02271903350900642ITERATION : 56, loss : 0.02271903350900642ITERATION : 57, loss : 0.02271903350900642ITERATION : 58, loss : 0.02271903350900642ITERATION : 59, loss : 0.02271903350900642ITERATION : 60, loss : 0.02271903350900642ITERATION : 61, loss : 0.02271903350900642ITERATION : 62, loss : 0.02271903350900642ITERATION : 63, loss : 0.02271903350900642ITERATION : 64, loss : 0.02271903350900642ITERATION : 65, loss : 0.02271903350900642ITERATION : 66, loss : 0.02271903350900642ITERATION : 67, loss : 0.02271903350900642ITERATION : 68, loss : 0.02271903350900642ITERATION : 69, loss : 0.02271903350900642ITERATION : 70, loss : 0.02271903350900642ITERATION : 71, loss : 0.02271903350900642ITERATION : 72, loss : 0.02271903350900642ITERATION : 73, loss : 0.02271903350900642ITERATION : 74, loss : 0.02271903350900642ITERATION : 75, loss : 0.02271903350900642ITERATION : 76, loss : 0.02271903350900642ITERATION : 77, loss : 0.02271903350900642ITERATION : 78, loss : 0.02271903350900642ITERATION : 79, loss : 0.02271903350900642ITERATION : 80, loss : 0.02271903350900642ITERATION : 81, loss : 0.02271903350900642ITERATION : 82, loss : 0.02271903350900642ITERATION : 83, loss : 0.02271903350900642ITERATION : 84, loss : 0.02271903350900642ITERATION : 85, loss : 0.02271903350900642ITERATION : 86, loss : 0.02271903350900642ITERATION : 87, loss : 0.02271903350900642ITERATION : 88, loss : 0.02271903350900642ITERATION : 89, loss : 0.02271903350900642ITERATION : 90, loss : 0.02271903350900642ITERATION : 91, loss : 0.02271903350900642ITERATION : 92, loss : 0.02271903350900642ITERATION : 93, loss : 0.02271903350900642ITERATION : 94, loss : 0.02271903350900642ITERATION : 95, loss : 0.02271903350900642ITERATION : 96, loss : 0.02271903350900642ITERATION : 97, loss : 0.02271903350900642ITERATION : 98, loss : 0.02271903350900642ITERATION : 99, loss : 0.02271903350900642ITERATION : 100, loss : 0.02271903350900642
gradient norm in None layer : 0.001104302111161977
gradient norm in None layer : 4.4348292037827246e-05
gradient norm in None layer : 6.23671269956594e-05
gradient norm in None layer : 0.0006919284285300716
gradient norm in None layer : 5.7120409884656397e-05
gradient norm in None layer : 7.362977230519017e-05
gradient norm in None layer : 0.00025005971017955894
gradient norm in None layer : 1.1347282715641674e-05
gradient norm in None layer : 1.0104844721205355e-05
gradient norm in None layer : 0.00022432564757243493
gradient norm in None layer : 1.1769175283821983e-05
gradient norm in None layer : 9.714190536142376e-06
gradient norm in None layer : 9.79451212193466e-05
gradient norm in None layer : 3.5330821431560262e-06
gradient norm in None layer : 2.8916172005588513e-06
gradient norm in None layer : 7.692623384677826e-05
gradient norm in None layer : 4.104686806185056e-06
gradient norm in None layer : 3.3106072243209396e-06
gradient norm in None layer : 0.00011171556502702522
gradient norm in None layer : 1.4049687764315704e-06
gradient norm in None layer : 0.00023160910647026164
gradient norm in None layer : 1.6904111236621547e-05
gradient norm in None layer : 1.415700441224917e-05
gradient norm in None layer : 0.000281154983241079
gradient norm in None layer : 2.767012200501511e-05
gradient norm in None layer : 3.436055575357129e-05
gradient norm in None layer : 0.00047775624006458835
gradient norm in None layer : 3.1022556563653368e-06
gradient norm in None layer : 0.0008125385189701091
gradient norm in None layer : 6.57168367672338e-05
gradient norm in None layer : 7.676497437080798e-05
gradient norm in None layer : 0.0009738700445804504
gradient norm in None layer : 0.00010077223544290343
gradient norm in None layer : 0.00013297202310903162
gradient norm in None layer : 8.801333916420575e-05
gradient norm in None layer : 2.0068800262798423e-05
Total gradient norm: 0.001967833603291757
invariance loss : 4.370085311180665, avg_den : 0.39966583251953125, density loss : 0.29966583251953127, mse loss : 0.03244923643976998, solver time : 143.25081634521484 sec , total loss : 0.037118987583470174, running loss : 0.056062222377751283
Epoch 0/10 , batch 13/12500 
ITERATION : 1, loss : 0.01012996542693879ITERATION : 2, loss : 0.009670359670610976ITERATION : 3, loss : 0.009901987734695717ITERATION : 4, loss : 0.010188233610775872ITERATION : 5, loss : 0.010429525838502226ITERATION : 6, loss : 0.010614128518572638ITERATION : 7, loss : 0.010749839922055988ITERATION : 8, loss : 0.010847637850482211ITERATION : 9, loss : 0.010917337569906323ITERATION : 10, loss : 0.010966687492640553ITERATION : 11, loss : 0.011001487934651997ITERATION : 12, loss : 0.011025965463304992ITERATION : 13, loss : 0.011043153473896844ITERATION : 14, loss : 0.01105520950033021ITERATION : 15, loss : 0.011063659608395171ITERATION : 16, loss : 0.011069579304883835ITERATION : 17, loss : 0.011073724881015665ITERATION : 18, loss : 0.011076627314050857ITERATION : 19, loss : 0.0110786590351118ITERATION : 20, loss : 0.011080081060394703ITERATION : 21, loss : 0.011081076209390098ITERATION : 22, loss : 0.011081772545685821ITERATION : 23, loss : 0.011082259813511191ITERATION : 24, loss : 0.011082600765840712ITERATION : 25, loss : 0.01108283932113431ITERATION : 26, loss : 0.011083006170417062ITERATION : 27, loss : 0.01108312293733997ITERATION : 28, loss : 0.011083204588611405ITERATION : 29, loss : 0.011083261670717744ITERATION : 30, loss : 0.011083301596322299ITERATION : 31, loss : 0.011083329507939662ITERATION : 32, loss : 0.011083349041316966ITERATION : 33, loss : 0.011083362707255573ITERATION : 34, loss : 0.011083372312670842ITERATION : 35, loss : 0.011083379007312337ITERATION : 36, loss : 0.011083383664073576ITERATION : 37, loss : 0.011083386900154235ITERATION : 38, loss : 0.011083389158786584ITERATION : 39, loss : 0.011083390722382578ITERATION : 40, loss : 0.011083391807919975ITERATION : 41, loss : 0.011083392559919417ITERATION : 42, loss : 0.011083393077170057ITERATION : 43, loss : 0.011083393440125919ITERATION : 44, loss : 0.011083393692137811ITERATION : 45, loss : 0.011083393852041821ITERATION : 46, loss : 0.011083393970012371ITERATION : 47, loss : 0.01108339407053929ITERATION : 48, loss : 0.011083394123042472ITERATION : 49, loss : 0.01108339417636008ITERATION : 50, loss : 0.011083394209592817ITERATION : 51, loss : 0.011083394218710785ITERATION : 52, loss : 0.011083394218793723ITERATION : 53, loss : 0.011083394218793723ITERATION : 54, loss : 0.011083394218793723ITERATION : 55, loss : 0.011083394218793723ITERATION : 56, loss : 0.011083394218793723ITERATION : 57, loss : 0.011083394218793723ITERATION : 58, loss : 0.011083394218793723ITERATION : 59, loss : 0.011083394218793723ITERATION : 60, loss : 0.011083394218793723ITERATION : 61, loss : 0.011083394218793723ITERATION : 62, loss : 0.011083394218793723ITERATION : 63, loss : 0.011083394218793723ITERATION : 64, loss : 0.011083394218793723ITERATION : 65, loss : 0.011083394218793723ITERATION : 66, loss : 0.011083394218793723ITERATION : 67, loss : 0.011083394218793723ITERATION : 68, loss : 0.011083394218793723ITERATION : 69, loss : 0.011083394218793723ITERATION : 70, loss : 0.011083394218793723ITERATION : 71, loss : 0.011083394218793723ITERATION : 72, loss : 0.011083394218793723ITERATION : 73, loss : 0.011083394218793723ITERATION : 74, loss : 0.011083394218793723ITERATION : 75, loss : 0.011083394218793723ITERATION : 76, loss : 0.011083394218793723ITERATION : 77, loss : 0.011083394218793723ITERATION : 78, loss : 0.011083394218793723ITERATION : 79, loss : 0.011083394218793723ITERATION : 80, loss : 0.011083394218793723ITERATION : 81, loss : 0.011083394218793723ITERATION : 82, loss : 0.011083394218793723ITERATION : 83, loss : 0.011083394218793723ITERATION : 84, loss : 0.011083394218793723ITERATION : 85, loss : 0.011083394218793723ITERATION : 86, loss : 0.011083394218793723ITERATION : 87, loss : 0.011083394218793723ITERATION : 88, loss : 0.011083394218793723ITERATION : 89, loss : 0.011083394218793723ITERATION : 90, loss : 0.011083394218793723ITERATION : 91, loss : 0.011083394218793723ITERATION : 92, loss : 0.011083394218793723ITERATION : 93, loss : 0.011083394218793723ITERATION : 94, loss : 0.011083394218793723ITERATION : 95, loss : 0.011083394218793723ITERATION : 96, loss : 0.011083394218793723ITERATION : 97, loss : 0.011083394218793723ITERATION : 98, loss : 0.011083394218793723ITERATION : 99, loss : 0.011083394218793723ITERATION : 100, loss : 0.011083394218793723
ITERATION : 1, loss : 0.015714002299632917ITERATION : 2, loss : 0.021208474022784987ITERATION : 3, loss : 0.02211419799681926ITERATION : 4, loss : 0.021863915677819812ITERATION : 5, loss : 0.021432132999211383ITERATION : 6, loss : 0.020862036959265407ITERATION : 7, loss : 0.020456026852822676ITERATION : 8, loss : 0.0201694657325006ITERATION : 9, loss : 0.01996725454958605ITERATION : 10, loss : 0.019824124953973836ITERATION : 11, loss : 0.01972241394691216ITERATION : 12, loss : 0.019649860208538202ITERATION : 13, loss : 0.019597934412461236ITERATION : 14, loss : 0.019560670701043163ITERATION : 15, loss : 0.019533870948141727ITERATION : 16, loss : 0.0195145637241519ITERATION : 17, loss : 0.019500635607394692ITERATION : 18, loss : 0.019490577424404527ITERATION : 19, loss : 0.019483307895550264ITERATION : 20, loss : 0.01947805044073195ITERATION : 21, loss : 0.019474246153660456ITERATION : 22, loss : 0.019471492108695056ITERATION : 23, loss : 0.019469497805588317ITERATION : 24, loss : 0.019468053105982534ITERATION : 25, loss : 0.019467006376273816ITERATION : 26, loss : 0.019466247739637416ITERATION : 27, loss : 0.01946569779449925ITERATION : 28, loss : 0.01946529909063614ITERATION : 29, loss : 0.019465010040658978ITERATION : 30, loss : 0.019464800391242244ITERATION : 31, loss : 0.019464648255291293ITERATION : 32, loss : 0.01946453794689664ITERATION : 33, loss : 0.019464457942305034ITERATION : 34, loss : 0.019464399824596794ITERATION : 35, loss : 0.019464357665471735ITERATION : 36, loss : 0.019464327088972947ITERATION : 37, loss : 0.019464304933317394ITERATION : 38, loss : 0.01946428886696711ITERATION : 39, loss : 0.01946427715549345ITERATION : 40, loss : 0.01946426871766545ITERATION : 41, loss : 0.019464262604699387ITERATION : 42, loss : 0.019464258179534726ITERATION : 43, loss : 0.019464254985400182ITERATION : 44, loss : 0.0194642526684272ITERATION : 45, loss : 0.019464250965069537ITERATION : 46, loss : 0.019464249714803518ITERATION : 47, loss : 0.019464248803181285ITERATION : 48, loss : 0.019464248147016035ITERATION : 49, loss : 0.01946424768542929ITERATION : 50, loss : 0.019464247332692267ITERATION : 51, loss : 0.01946424709743874ITERATION : 52, loss : 0.019464246922808803ITERATION : 53, loss : 0.019464246841985566ITERATION : 54, loss : 0.019464246806797623ITERATION : 55, loss : 0.019464246804672607ITERATION : 56, loss : 0.019464246804672607ITERATION : 57, loss : 0.019464246804672607ITERATION : 58, loss : 0.019464246804672607ITERATION : 59, loss : 0.019464246804672607ITERATION : 60, loss : 0.019464246804672607ITERATION : 61, loss : 0.019464246804672607ITERATION : 62, loss : 0.019464246804672607ITERATION : 63, loss : 0.019464246804672607ITERATION : 64, loss : 0.019464246804672607ITERATION : 65, loss : 0.019464246804672607ITERATION : 66, loss : 0.019464246804672607ITERATION : 67, loss : 0.019464246804672607ITERATION : 68, loss : 0.019464246804672607ITERATION : 69, loss : 0.019464246804672607ITERATION : 70, loss : 0.019464246804672607ITERATION : 71, loss : 0.019464246804672607ITERATION : 72, loss : 0.019464246804672607ITERATION : 73, loss : 0.019464246804672607ITERATION : 74, loss : 0.019464246804672607ITERATION : 75, loss : 0.019464246804672607ITERATION : 76, loss : 0.019464246804672607ITERATION : 77, loss : 0.019464246804672607ITERATION : 78, loss : 0.019464246804672607ITERATION : 79, loss : 0.019464246804672607ITERATION : 80, loss : 0.019464246804672607ITERATION : 81, loss : 0.019464246804672607ITERATION : 82, loss : 0.019464246804672607ITERATION : 83, loss : 0.019464246804672607ITERATION : 84, loss : 0.019464246804672607ITERATION : 85, loss : 0.019464246804672607ITERATION : 86, loss : 0.019464246804672607ITERATION : 87, loss : 0.019464246804672607ITERATION : 88, loss : 0.019464246804672607ITERATION : 89, loss : 0.019464246804672607ITERATION : 90, loss : 0.019464246804672607ITERATION : 91, loss : 0.019464246804672607ITERATION : 92, loss : 0.019464246804672607ITERATION : 93, loss : 0.019464246804672607ITERATION : 94, loss : 0.019464246804672607ITERATION : 95, loss : 0.019464246804672607ITERATION : 96, loss : 0.019464246804672607ITERATION : 97, loss : 0.019464246804672607ITERATION : 98, loss : 0.019464246804672607ITERATION : 99, loss : 0.019464246804672607ITERATION : 100, loss : 0.019464246804672607
ITERATION : 1, loss : 0.02725901188002807ITERATION : 2, loss : 0.023406773772975422ITERATION : 3, loss : 0.021305846090340343ITERATION : 4, loss : 0.020169208593602113ITERATION : 5, loss : 0.019472151708531085ITERATION : 6, loss : 0.01901744551943017ITERATION : 7, loss : 0.018710873128399887ITERATION : 8, loss : 0.018500137021216556ITERATION : 9, loss : 0.018353541545240014ITERATION : 10, loss : 0.018250797455349285ITERATION : 11, loss : 0.01817844373148561ITERATION : 12, loss : 0.018127335419881378ITERATION : 13, loss : 0.018091162322252514ITERATION : 14, loss : 0.018065526264820355ITERATION : 15, loss : 0.018047341633040302ITERATION : 16, loss : 0.0180344346261499ITERATION : 17, loss : 0.018025269339084594ITERATION : 18, loss : 0.018018758861168986ITERATION : 19, loss : 0.018014132951794774ITERATION : 20, loss : 0.018010845383443674ITERATION : 21, loss : 0.018008508504311016ITERATION : 22, loss : 0.018006847115987715ITERATION : 23, loss : 0.018005665773630743ITERATION : 24, loss : 0.01800482567467845ITERATION : 25, loss : 0.01800422813846994ITERATION : 26, loss : 0.018003803082982855ITERATION : 27, loss : 0.018003500670904497ITERATION : 28, loss : 0.018003285474739405ITERATION : 29, loss : 0.018003132342394076ITERATION : 30, loss : 0.01800302335793873ITERATION : 31, loss : 0.018002945774142357ITERATION : 32, loss : 0.018002890527352135ITERATION : 33, loss : 0.018002851230886213ITERATION : 34, loss : 0.018002823217905632ITERATION : 35, loss : 0.01800280325785695ITERATION : 36, loss : 0.018002789051141544ITERATION : 37, loss : 0.018002778940569595ITERATION : 38, loss : 0.018002771655552616ITERATION : 39, loss : 0.018002766537214956ITERATION : 40, loss : 0.01800276289284121ITERATION : 41, loss : 0.018002760279564142ITERATION : 42, loss : 0.018002758445980628ITERATION : 43, loss : 0.018002757131103782ITERATION : 44, loss : 0.018002756244067636ITERATION : 45, loss : 0.018002755565875096ITERATION : 46, loss : 0.01800275513762658ITERATION : 47, loss : 0.0180027547933662ITERATION : 48, loss : 0.01800275455758665ITERATION : 49, loss : 0.01800275437454783ITERATION : 50, loss : 0.018002754289114225ITERATION : 51, loss : 0.018002754190891736ITERATION : 52, loss : 0.018002754130700627ITERATION : 53, loss : 0.018002754124235326ITERATION : 54, loss : 0.018002754124235326ITERATION : 55, loss : 0.018002754124235326ITERATION : 56, loss : 0.018002754124235326ITERATION : 57, loss : 0.018002754124235326ITERATION : 58, loss : 0.018002754124235326ITERATION : 59, loss : 0.018002754124235326ITERATION : 60, loss : 0.018002754124235326ITERATION : 61, loss : 0.018002754124235326ITERATION : 62, loss : 0.018002754124235326ITERATION : 63, loss : 0.018002754124235326ITERATION : 64, loss : 0.018002754124235326ITERATION : 65, loss : 0.018002754124235326ITERATION : 66, loss : 0.018002754124235326ITERATION : 67, loss : 0.018002754124235326ITERATION : 68, loss : 0.018002754124235326ITERATION : 69, loss : 0.018002754124235326ITERATION : 70, loss : 0.018002754124235326ITERATION : 71, loss : 0.018002754124235326ITERATION : 72, loss : 0.018002754124235326ITERATION : 73, loss : 0.018002754124235326ITERATION : 74, loss : 0.018002754124235326ITERATION : 75, loss : 0.018002754124235326ITERATION : 76, loss : 0.018002754124235326ITERATION : 77, loss : 0.018002754124235326ITERATION : 78, loss : 0.018002754124235326ITERATION : 79, loss : 0.018002754124235326ITERATION : 80, loss : 0.018002754124235326ITERATION : 81, loss : 0.018002754124235326ITERATION : 82, loss : 0.018002754124235326ITERATION : 83, loss : 0.018002754124235326ITERATION : 84, loss : 0.018002754124235326ITERATION : 85, loss : 0.018002754124235326ITERATION : 86, loss : 0.018002754124235326ITERATION : 87, loss : 0.018002754124235326ITERATION : 88, loss : 0.018002754124235326ITERATION : 89, loss : 0.018002754124235326ITERATION : 90, loss : 0.018002754124235326ITERATION : 91, loss : 0.018002754124235326ITERATION : 92, loss : 0.018002754124235326ITERATION : 93, loss : 0.018002754124235326ITERATION : 94, loss : 0.018002754124235326ITERATION : 95, loss : 0.018002754124235326ITERATION : 96, loss : 0.018002754124235326ITERATION : 97, loss : 0.018002754124235326ITERATION : 98, loss : 0.018002754124235326ITERATION : 99, loss : 0.018002754124235326ITERATION : 100, loss : 0.018002754124235326
ITERATION : 1, loss : 0.03567872370993229ITERATION : 2, loss : 0.034590658324528685ITERATION : 3, loss : 0.03404972275018861ITERATION : 4, loss : 0.03388189908407818ITERATION : 5, loss : 0.03381818395224226ITERATION : 6, loss : 0.03380079083341288ITERATION : 7, loss : 0.03380481837832716ITERATION : 8, loss : 0.033817050566226095ITERATION : 9, loss : 0.033830865171494216ITERATION : 10, loss : 0.033843375179503025ITERATION : 11, loss : 0.03385363244004741ITERATION : 12, loss : 0.03386159085450496ITERATION : 13, loss : 0.033867559391557445ITERATION : 14, loss : 0.03387193719829979ITERATION : 15, loss : 0.033875100356101764ITERATION : 16, loss : 0.0338773624355108ITERATION : 17, loss : 0.033878968040273696ITERATION : 18, loss : 0.03388010184255976ITERATION : 19, loss : 0.0338808994715214ITERATION : 20, loss : 0.03388145902657994ITERATION : 21, loss : 0.033881850669742294ITERATION : 22, loss : 0.0338821245787327ITERATION : 23, loss : 0.03388231577179156ITERATION : 24, loss : 0.033882449255512724ITERATION : 25, loss : 0.03388254232607746ITERATION : 26, loss : 0.03388260717292265ITERATION : 27, loss : 0.03388265245290162ITERATION : 28, loss : 0.03388268402285037ITERATION : 29, loss : 0.03388270606308037ITERATION : 30, loss : 0.033882721402784796ITERATION : 31, loss : 0.03388273212727827ITERATION : 32, loss : 0.03388273958872358ITERATION : 33, loss : 0.03388274478509982ITERATION : 34, loss : 0.0338827483885704ITERATION : 35, loss : 0.03388275093355974ITERATION : 36, loss : 0.033882752671442445ITERATION : 37, loss : 0.03388275391985253ITERATION : 38, loss : 0.033882754752762534ITERATION : 39, loss : 0.03388275537238283ITERATION : 40, loss : 0.03388275576554553ITERATION : 41, loss : 0.03388275607665259ITERATION : 42, loss : 0.03388275628345805ITERATION : 43, loss : 0.03388275640971267ITERATION : 44, loss : 0.033882756518840644ITERATION : 45, loss : 0.033882756575874376ITERATION : 46, loss : 0.03388275662391677ITERATION : 47, loss : 0.03388275665282994ITERATION : 48, loss : 0.03388275668536652ITERATION : 49, loss : 0.0338827566877684ITERATION : 50, loss : 0.03388275670021564ITERATION : 51, loss : 0.03388275670038097ITERATION : 52, loss : 0.03388275670038097ITERATION : 53, loss : 0.03388275670038097ITERATION : 54, loss : 0.03388275670038097ITERATION : 55, loss : 0.03388275670038097ITERATION : 56, loss : 0.03388275670038097ITERATION : 57, loss : 0.03388275670038097ITERATION : 58, loss : 0.03388275670038097ITERATION : 59, loss : 0.03388275670038097ITERATION : 60, loss : 0.03388275670038097ITERATION : 61, loss : 0.03388275670038097ITERATION : 62, loss : 0.03388275670038097ITERATION : 63, loss : 0.03388275670038097ITERATION : 64, loss : 0.03388275670038097ITERATION : 65, loss : 0.03388275670038097ITERATION : 66, loss : 0.03388275670038097ITERATION : 67, loss : 0.03388275670038097ITERATION : 68, loss : 0.03388275670038097ITERATION : 69, loss : 0.03388275670038097ITERATION : 70, loss : 0.03388275670038097ITERATION : 71, loss : 0.03388275670038097ITERATION : 72, loss : 0.03388275670038097ITERATION : 73, loss : 0.03388275670038097ITERATION : 74, loss : 0.03388275670038097ITERATION : 75, loss : 0.03388275670038097ITERATION : 76, loss : 0.03388275670038097ITERATION : 77, loss : 0.03388275670038097ITERATION : 78, loss : 0.03388275670038097ITERATION : 79, loss : 0.03388275670038097ITERATION : 80, loss : 0.03388275670038097ITERATION : 81, loss : 0.03388275670038097ITERATION : 82, loss : 0.03388275670038097ITERATION : 83, loss : 0.03388275670038097ITERATION : 84, loss : 0.03388275670038097ITERATION : 85, loss : 0.03388275670038097ITERATION : 86, loss : 0.03388275670038097ITERATION : 87, loss : 0.03388275670038097ITERATION : 88, loss : 0.03388275670038097ITERATION : 89, loss : 0.03388275670038097ITERATION : 90, loss : 0.03388275670038097ITERATION : 91, loss : 0.03388275670038097ITERATION : 92, loss : 0.03388275670038097ITERATION : 93, loss : 0.03388275670038097ITERATION : 94, loss : 0.03388275670038097ITERATION : 95, loss : 0.03388275670038097ITERATION : 96, loss : 0.03388275670038097ITERATION : 97, loss : 0.03388275670038097ITERATION : 98, loss : 0.03388275670038097ITERATION : 99, loss : 0.03388275670038097ITERATION : 100, loss : 0.03388275670038097
ITERATION : 1, loss : 0.06654614753190868ITERATION : 2, loss : 0.046926687553839656ITERATION : 3, loss : 0.035804944355033536ITERATION : 4, loss : 0.030468141126294923ITERATION : 5, loss : 0.02809066436026784ITERATION : 6, loss : 0.02669451121068568ITERATION : 7, loss : 0.025804436560868383ITERATION : 8, loss : 0.02522863088768719ITERATION : 9, loss : 0.024852647548720932ITERATION : 10, loss : 0.02460573622360373ITERATION : 11, loss : 0.0244430760342612ITERATION : 12, loss : 0.024335780900334204ITERATION : 13, loss : 0.024265013220758785ITERATION : 14, loss : 0.02421839242941926ITERATION : 15, loss : 0.024187742235187538ITERATION : 16, loss : 0.024167648520560022ITERATION : 17, loss : 0.02415452228769681ITERATION : 18, loss : 0.024145984809021113ITERATION : 19, loss : 0.024140460756612067ITERATION : 20, loss : 0.024136908849524973ITERATION : 21, loss : 0.02413464214138714ITERATION : 22, loss : 0.024133208828032753ITERATION : 23, loss : 0.024132312859665675ITERATION : 24, loss : 0.02413176090321862ITERATION : 25, loss : 0.024131427181876938ITERATION : 26, loss : 0.024131230541234318ITERATION : 27, loss : 0.02413111890809552ITERATION : 28, loss : 0.02413105903661251ITERATION : 29, loss : 0.024131030025614602ITERATION : 30, loss : 0.024131018795608545ITERATION : 31, loss : 0.02413101743975708ITERATION : 32, loss : 0.024131020935565198ITERATION : 33, loss : 0.024131026588695488ITERATION : 34, loss : 0.02413103286266782ITERATION : 35, loss : 0.024131038877568626ITERATION : 36, loss : 0.02413104420167014ITERATION : 37, loss : 0.024131048706688973ITERATION : 38, loss : 0.024131052509950346ITERATION : 39, loss : 0.024131055496713474ITERATION : 40, loss : 0.024131057921425184ITERATION : 41, loss : 0.024131059810926715ITERATION : 42, loss : 0.024131061321577822ITERATION : 43, loss : 0.02413106246365659ITERATION : 44, loss : 0.0241310633600357ITERATION : 45, loss : 0.02413106403169322ITERATION : 46, loss : 0.024131064565121898ITERATION : 47, loss : 0.024131064977052485ITERATION : 48, loss : 0.024131065275865657ITERATION : 49, loss : 0.024131065495187363ITERATION : 50, loss : 0.024131065701967096ITERATION : 51, loss : 0.02413106580420242ITERATION : 52, loss : 0.024131065885322496ITERATION : 53, loss : 0.024131065935939843ITERATION : 54, loss : 0.024131065992996706ITERATION : 55, loss : 0.02413106604097675ITERATION : 56, loss : 0.024131066045975313ITERATION : 57, loss : 0.024131066045975313ITERATION : 58, loss : 0.024131066045975313ITERATION : 59, loss : 0.024131066045975313ITERATION : 60, loss : 0.024131066045975313ITERATION : 61, loss : 0.024131066045975313ITERATION : 62, loss : 0.024131066045975313ITERATION : 63, loss : 0.024131066045975313ITERATION : 64, loss : 0.024131066045975313ITERATION : 65, loss : 0.024131066045975313ITERATION : 66, loss : 0.024131066045975313ITERATION : 67, loss : 0.024131066045975313ITERATION : 68, loss : 0.024131066045975313ITERATION : 69, loss : 0.024131066045975313ITERATION : 70, loss : 0.024131066045975313ITERATION : 71, loss : 0.024131066045975313ITERATION : 72, loss : 0.024131066045975313ITERATION : 73, loss : 0.024131066045975313ITERATION : 74, loss : 0.024131066045975313ITERATION : 75, loss : 0.024131066045975313ITERATION : 76, loss : 0.024131066045975313ITERATION : 77, loss : 0.024131066045975313ITERATION : 78, loss : 0.024131066045975313ITERATION : 79, loss : 0.024131066045975313ITERATION : 80, loss : 0.024131066045975313ITERATION : 81, loss : 0.024131066045975313ITERATION : 82, loss : 0.024131066045975313ITERATION : 83, loss : 0.024131066045975313ITERATION : 84, loss : 0.024131066045975313ITERATION : 85, loss : 0.024131066045975313ITERATION : 86, loss : 0.024131066045975313ITERATION : 87, loss : 0.024131066045975313ITERATION : 88, loss : 0.024131066045975313ITERATION : 89, loss : 0.024131066045975313ITERATION : 90, loss : 0.024131066045975313ITERATION : 91, loss : 0.024131066045975313ITERATION : 92, loss : 0.024131066045975313ITERATION : 93, loss : 0.024131066045975313ITERATION : 94, loss : 0.024131066045975313ITERATION : 95, loss : 0.024131066045975313ITERATION : 96, loss : 0.024131066045975313ITERATION : 97, loss : 0.024131066045975313ITERATION : 98, loss : 0.024131066045975313ITERATION : 99, loss : 0.024131066045975313ITERATION : 100, loss : 0.024131066045975313
ITERATION : 1, loss : 0.07710714163794363ITERATION : 2, loss : 0.050346641242607716ITERATION : 3, loss : 0.03548688238818397ITERATION : 4, loss : 0.027584535100665556ITERATION : 5, loss : 0.023195277070948445ITERATION : 6, loss : 0.020624973338106502ITERATION : 7, loss : 0.019050473655406328ITERATION : 8, loss : 0.018051896755420325ITERATION : 9, loss : 0.017402028520663138ITERATION : 10, loss : 0.016971043248824702ITERATION : 11, loss : 0.01668128360533738ITERATION : 12, loss : 0.01648454378716367ITERATION : 13, loss : 0.016350014045179585ITERATION : 14, loss : 0.016257556202317686ITERATION : 15, loss : 0.016193781832715207ITERATION : 16, loss : 0.01614967781240364ITERATION : 17, loss : 0.01611911996891274ITERATION : 18, loss : 0.016097919051819818ITERATION : 19, loss : 0.01608319540543182ITERATION : 20, loss : 0.01607296277114111ITERATION : 21, loss : 0.01606584750346542ITERATION : 22, loss : 0.016060897919013297ITERATION : 23, loss : 0.01605745382219918ITERATION : 24, loss : 0.016055056712737718ITERATION : 25, loss : 0.016053388008553736ITERATION : 26, loss : 0.016052226117515295ITERATION : 27, loss : 0.01605141713844315ITERATION : 28, loss : 0.016050853739798185ITERATION : 29, loss : 0.016050461326192977ITERATION : 30, loss : 0.016050187952630187ITERATION : 31, loss : 0.0160499974774218ITERATION : 32, loss : 0.01604986478334976ITERATION : 33, loss : 0.016049772292270954ITERATION : 34, loss : 0.016049707861394025ITERATION : 35, loss : 0.01604966292709248ITERATION : 36, loss : 0.016049631629957737ITERATION : 37, loss : 0.01604960977356764ITERATION : 38, loss : 0.01604959456900967ITERATION : 39, loss : 0.016049583949946867ITERATION : 40, loss : 0.01604957656397031ITERATION : 41, loss : 0.01604957137249806ITERATION : 42, loss : 0.01604956775527036ITERATION : 43, loss : 0.01604956525615087ITERATION : 44, loss : 0.016049563505471324ITERATION : 45, loss : 0.016049562299421283ITERATION : 46, loss : 0.016049561438117935ITERATION : 47, loss : 0.01604956088160278ITERATION : 48, loss : 0.016049560531277027ITERATION : 49, loss : 0.01604956027433087ITERATION : 50, loss : 0.01604956006053989ITERATION : 51, loss : 0.016049559966577767ITERATION : 52, loss : 0.016049559861446287ITERATION : 53, loss : 0.016049559807976253ITERATION : 54, loss : 0.016049559772598823ITERATION : 55, loss : 0.01604955975426037ITERATION : 56, loss : 0.01604955975426037ITERATION : 57, loss : 0.01604955975426037ITERATION : 58, loss : 0.01604955975426037ITERATION : 59, loss : 0.01604955975426037ITERATION : 60, loss : 0.01604955975426037ITERATION : 61, loss : 0.01604955975426037ITERATION : 62, loss : 0.01604955975426037ITERATION : 63, loss : 0.01604955975426037ITERATION : 64, loss : 0.01604955975426037ITERATION : 65, loss : 0.01604955975426037ITERATION : 66, loss : 0.01604955975426037ITERATION : 67, loss : 0.01604955975426037ITERATION : 68, loss : 0.01604955975426037ITERATION : 69, loss : 0.01604955975426037ITERATION : 70, loss : 0.01604955975426037ITERATION : 71, loss : 0.01604955975426037ITERATION : 72, loss : 0.01604955975426037ITERATION : 73, loss : 0.01604955975426037ITERATION : 74, loss : 0.01604955975426037ITERATION : 75, loss : 0.01604955975426037ITERATION : 76, loss : 0.01604955975426037ITERATION : 77, loss : 0.01604955975426037ITERATION : 78, loss : 0.01604955975426037ITERATION : 79, loss : 0.01604955975426037ITERATION : 80, loss : 0.01604955975426037ITERATION : 81, loss : 0.01604955975426037ITERATION : 82, loss : 0.01604955975426037ITERATION : 83, loss : 0.01604955975426037ITERATION : 84, loss : 0.01604955975426037ITERATION : 85, loss : 0.01604955975426037ITERATION : 86, loss : 0.01604955975426037ITERATION : 87, loss : 0.01604955975426037ITERATION : 88, loss : 0.01604955975426037ITERATION : 89, loss : 0.01604955975426037ITERATION : 90, loss : 0.01604955975426037ITERATION : 91, loss : 0.01604955975426037ITERATION : 92, loss : 0.01604955975426037ITERATION : 93, loss : 0.01604955975426037ITERATION : 94, loss : 0.01604955975426037ITERATION : 95, loss : 0.01604955975426037ITERATION : 96, loss : 0.01604955975426037ITERATION : 97, loss : 0.01604955975426037ITERATION : 98, loss : 0.01604955975426037ITERATION : 99, loss : 0.01604955975426037ITERATION : 100, loss : 0.01604955975426037
ITERATION : 1, loss : 0.044646684137413975ITERATION : 2, loss : 0.03648779320247436ITERATION : 3, loss : 0.03302123656397868ITERATION : 4, loss : 0.0317390029558483ITERATION : 5, loss : 0.03111631465723546ITERATION : 6, loss : 0.030752082046148918ITERATION : 7, loss : 0.03051816626210006ITERATION : 8, loss : 0.030362109041839277ITERATION : 9, loss : 0.030256583504497718ITERATION : 10, loss : 0.030184947114325705ITERATION : 11, loss : 0.030136291745367597ITERATION : 12, loss : 0.030103264431167974ITERATION : 13, loss : 0.03008086469954417ITERATION : 14, loss : 0.030065686040318812ITERATION : 15, loss : 0.030055409083039598ITERATION : 16, loss : 0.030048456192970543ITERATION : 17, loss : 0.03004375574587743ITERATION : 18, loss : 0.030040580533190652ITERATION : 19, loss : 0.030038437149847087ITERATION : 20, loss : 0.030036991495629703ITERATION : 21, loss : 0.03003601725079167ITERATION : 22, loss : 0.030035361266648276ITERATION : 23, loss : 0.030034920070752195ITERATION : 24, loss : 0.03003462360766642ITERATION : 25, loss : 0.030034424673205308ITERATION : 26, loss : 0.030034291557562168ITERATION : 27, loss : 0.030034202308833333ITERATION : 28, loss : 0.0300341427800429ITERATION : 29, loss : 0.03003410307580868ITERATION : 30, loss : 0.030034076696809003ITERATION : 31, loss : 0.030034059162485985ITERATION : 32, loss : 0.030034047432419317ITERATION : 33, loss : 0.030034039709102543ITERATION : 34, loss : 0.03003403490791188ITERATION : 35, loss : 0.030034031502452575ITERATION : 36, loss : 0.030034029273826115ITERATION : 37, loss : 0.03003402788906054ITERATION : 38, loss : 0.030034026979780505ITERATION : 39, loss : 0.030034026499661144ITERATION : 40, loss : 0.030034026140091527ITERATION : 41, loss : 0.030034025969412865ITERATION : 42, loss : 0.03003402587146489ITERATION : 43, loss : 0.03003402582022585ITERATION : 44, loss : 0.03003402576409572ITERATION : 45, loss : 0.030034025755364957ITERATION : 46, loss : 0.03003402575110405ITERATION : 47, loss : 0.03003402575110405ITERATION : 48, loss : 0.03003402575110405ITERATION : 49, loss : 0.03003402575110405ITERATION : 50, loss : 0.03003402575110405ITERATION : 51, loss : 0.03003402575110405ITERATION : 52, loss : 0.03003402575110405ITERATION : 53, loss : 0.03003402575110405ITERATION : 54, loss : 0.03003402575110405ITERATION : 55, loss : 0.03003402575110405ITERATION : 56, loss : 0.03003402575110405ITERATION : 57, loss : 0.03003402575110405ITERATION : 58, loss : 0.03003402575110405ITERATION : 59, loss : 0.03003402575110405ITERATION : 60, loss : 0.03003402575110405ITERATION : 61, loss : 0.03003402575110405ITERATION : 62, loss : 0.03003402575110405ITERATION : 63, loss : 0.03003402575110405ITERATION : 64, loss : 0.03003402575110405ITERATION : 65, loss : 0.03003402575110405ITERATION : 66, loss : 0.03003402575110405ITERATION : 67, loss : 0.03003402575110405ITERATION : 68, loss : 0.03003402575110405ITERATION : 69, loss : 0.03003402575110405ITERATION : 70, loss : 0.03003402575110405ITERATION : 71, loss : 0.03003402575110405ITERATION : 72, loss : 0.03003402575110405ITERATION : 73, loss : 0.03003402575110405ITERATION : 74, loss : 0.03003402575110405ITERATION : 75, loss : 0.03003402575110405ITERATION : 76, loss : 0.03003402575110405ITERATION : 77, loss : 0.03003402575110405ITERATION : 78, loss : 0.03003402575110405ITERATION : 79, loss : 0.03003402575110405ITERATION : 80, loss : 0.03003402575110405ITERATION : 81, loss : 0.03003402575110405ITERATION : 82, loss : 0.03003402575110405ITERATION : 83, loss : 0.03003402575110405ITERATION : 84, loss : 0.03003402575110405ITERATION : 85, loss : 0.03003402575110405ITERATION : 86, loss : 0.03003402575110405ITERATION : 87, loss : 0.03003402575110405ITERATION : 88, loss : 0.03003402575110405ITERATION : 89, loss : 0.03003402575110405ITERATION : 90, loss : 0.03003402575110405ITERATION : 91, loss : 0.03003402575110405ITERATION : 92, loss : 0.03003402575110405ITERATION : 93, loss : 0.03003402575110405ITERATION : 94, loss : 0.03003402575110405ITERATION : 95, loss : 0.03003402575110405ITERATION : 96, loss : 0.03003402575110405ITERATION : 97, loss : 0.03003402575110405ITERATION : 98, loss : 0.03003402575110405ITERATION : 99, loss : 0.03003402575110405ITERATION : 100, loss : 0.03003402575110405
ITERATION : 1, loss : 0.04846142273368393ITERATION : 2, loss : 0.04608666358936806ITERATION : 3, loss : 0.045146266628252156ITERATION : 4, loss : 0.04460843262594909ITERATION : 5, loss : 0.04425358166316005ITERATION : 6, loss : 0.044010081694143624ITERATION : 7, loss : 0.04384143736494686ITERATION : 8, loss : 0.043724245953504624ITERATION : 9, loss : 0.043642585999796916ITERATION : 10, loss : 0.04358552479791965ITERATION : 11, loss : 0.043545544537414264ITERATION : 12, loss : 0.04351746421966913ITERATION : 13, loss : 0.04349770143501927ITERATION : 14, loss : 0.04348376906208262ITERATION : 15, loss : 0.04347393401651048ITERATION : 16, loss : 0.04346698391233236ITERATION : 17, loss : 0.043462068595502515ITERATION : 18, loss : 0.043458590209524195ITERATION : 19, loss : 0.043456127452249096ITERATION : 20, loss : 0.04345438319360711ITERATION : 21, loss : 0.04345314747464793ITERATION : 22, loss : 0.04345227183478015ITERATION : 23, loss : 0.043451651288721985ITERATION : 24, loss : 0.0434512114741584ITERATION : 25, loss : 0.043450899702936564ITERATION : 26, loss : 0.04345067868700996ITERATION : 27, loss : 0.04345052200649521ITERATION : 28, loss : 0.04345041092882404ITERATION : 29, loss : 0.0434503321957389ITERATION : 30, loss : 0.043450276373739265ITERATION : 31, loss : 0.04345023679600459ITERATION : 32, loss : 0.0434502087312658ITERATION : 33, loss : 0.04345018884258284ITERATION : 34, loss : 0.04345017472987275ITERATION : 35, loss : 0.043450164731743374ITERATION : 36, loss : 0.043450157632208455ITERATION : 37, loss : 0.043450152613953893ITERATION : 38, loss : 0.04345014905504303ITERATION : 39, loss : 0.043450146526659815ITERATION : 40, loss : 0.043450144742244214ITERATION : 41, loss : 0.043450143465047414ITERATION : 42, loss : 0.043450142564510824ITERATION : 43, loss : 0.04345014196386826ITERATION : 44, loss : 0.043450141512486204ITERATION : 45, loss : 0.043450141167309456ITERATION : 46, loss : 0.04345014096791861ITERATION : 47, loss : 0.043450140788364705ITERATION : 48, loss : 0.04345014070789906ITERATION : 49, loss : 0.0434501406093938ITERATION : 50, loss : 0.043450140572428804ITERATION : 51, loss : 0.04345014056199434ITERATION : 52, loss : 0.04345014052528911ITERATION : 53, loss : 0.043450140522946795ITERATION : 54, loss : 0.043450140522946795ITERATION : 55, loss : 0.043450140522946795ITERATION : 56, loss : 0.043450140522946795ITERATION : 57, loss : 0.043450140522946795ITERATION : 58, loss : 0.043450140522946795ITERATION : 59, loss : 0.043450140522946795ITERATION : 60, loss : 0.043450140522946795ITERATION : 61, loss : 0.043450140522946795ITERATION : 62, loss : 0.043450140522946795ITERATION : 63, loss : 0.043450140522946795ITERATION : 64, loss : 0.043450140522946795ITERATION : 65, loss : 0.043450140522946795ITERATION : 66, loss : 0.043450140522946795ITERATION : 67, loss : 0.043450140522946795ITERATION : 68, loss : 0.043450140522946795ITERATION : 69, loss : 0.043450140522946795ITERATION : 70, loss : 0.043450140522946795ITERATION : 71, loss : 0.043450140522946795ITERATION : 72, loss : 0.043450140522946795ITERATION : 73, loss : 0.043450140522946795ITERATION : 74, loss : 0.043450140522946795ITERATION : 75, loss : 0.043450140522946795ITERATION : 76, loss : 0.043450140522946795ITERATION : 77, loss : 0.043450140522946795ITERATION : 78, loss : 0.043450140522946795ITERATION : 79, loss : 0.043450140522946795ITERATION : 80, loss : 0.043450140522946795ITERATION : 81, loss : 0.043450140522946795ITERATION : 82, loss : 0.043450140522946795ITERATION : 83, loss : 0.043450140522946795ITERATION : 84, loss : 0.043450140522946795ITERATION : 85, loss : 0.043450140522946795ITERATION : 86, loss : 0.043450140522946795ITERATION : 87, loss : 0.043450140522946795ITERATION : 88, loss : 0.043450140522946795ITERATION : 89, loss : 0.043450140522946795ITERATION : 90, loss : 0.043450140522946795ITERATION : 91, loss : 0.043450140522946795ITERATION : 92, loss : 0.043450140522946795ITERATION : 93, loss : 0.043450140522946795ITERATION : 94, loss : 0.043450140522946795ITERATION : 95, loss : 0.043450140522946795ITERATION : 96, loss : 0.043450140522946795ITERATION : 97, loss : 0.043450140522946795ITERATION : 98, loss : 0.043450140522946795ITERATION : 99, loss : 0.043450140522946795ITERATION : 100, loss : 0.043450140522946795
gradient norm in None layer : 0.0010681934834369832
gradient norm in None layer : 4.795408239694018e-05
gradient norm in None layer : 5.445315445186239e-05
gradient norm in None layer : 0.0007665539305719085
gradient norm in None layer : 7.568962163886323e-05
gradient norm in None layer : 8.678946378159101e-05
gradient norm in None layer : 0.00033195763666723333
gradient norm in None layer : 1.4990515837994127e-05
gradient norm in None layer : 1.1456572232504964e-05
gradient norm in None layer : 0.0003117912284453858
gradient norm in None layer : 1.5350759011874186e-05
gradient norm in None layer : 1.0765194881421025e-05
gradient norm in None layer : 0.00011399510066306886
gradient norm in None layer : 3.4374167684283003e-06
gradient norm in None layer : 2.28153932358186e-06
gradient norm in None layer : 9.789754094588069e-05
gradient norm in None layer : 4.144951725864047e-06
gradient norm in None layer : 2.683755009359884e-06
gradient norm in None layer : 0.00013920127668715632
gradient norm in None layer : 1.6668557889257285e-06
gradient norm in None layer : 0.0002985225478596949
gradient norm in None layer : 2.1461131040276994e-05
gradient norm in None layer : 1.660097475869716e-05
gradient norm in None layer : 0.000377266923516564
gradient norm in None layer : 3.53122445086206e-05
gradient norm in None layer : 4.7802703346214265e-05
gradient norm in None layer : 0.0006581892889916444
gradient norm in None layer : 5.853121732548337e-06
gradient norm in None layer : 0.0010381430413750048
gradient norm in None layer : 9.648760387873936e-05
gradient norm in None layer : 0.00010462476764939642
gradient norm in None layer : 0.0013758487217392437
gradient norm in None layer : 0.00012413835093172894
gradient norm in None layer : 0.00015072023131849221
gradient norm in None layer : 0.00010895449273777226
gradient norm in None layer : 1.9832326082699666e-05
Total gradient norm: 0.002389057412605845
invariance loss : 4.524672274262137, avg_den : 0.4194183349609375, density loss : 0.3194183349609375, mse loss : 0.024512242990296144, solver time : 118.63215827941895 sec , total loss : 0.029356333599519217, running loss : 0.054007923240964206
Epoch 0/10 , batch 14/12500 
ITERATION : 1, loss : 0.037545060561865455ITERATION : 2, loss : 0.028667007584830522ITERATION : 3, loss : 0.02423881433530552ITERATION : 4, loss : 0.02232573070281709ITERATION : 5, loss : 0.02142032907615975ITERATION : 6, loss : 0.021155614642339147ITERATION : 7, loss : 0.02137217716882706ITERATION : 8, loss : 0.02169879103029659ITERATION : 9, loss : 0.021994009961508295ITERATION : 10, loss : 0.02223690621685883ITERATION : 11, loss : 0.02242675883843753ITERATION : 12, loss : 0.02257062034506837ITERATION : 13, loss : 0.02267748871913373ITERATION : 14, loss : 0.022755837946268145ITERATION : 15, loss : 0.02281276797805755ITERATION : 16, loss : 0.0228538808273099ITERATION : 17, loss : 0.022883444269270078ITERATION : 18, loss : 0.022904639133957305ITERATION : 19, loss : 0.022919802299116877ITERATION : 20, loss : 0.022930634033698837ITERATION : 21, loss : 0.02293836357247816ITERATION : 22, loss : 0.022943875033528807ITERATION : 23, loss : 0.022947802857412443ITERATION : 24, loss : 0.022950601017745945ITERATION : 25, loss : 0.02295259390724826ITERATION : 26, loss : 0.022954012920954447ITERATION : 27, loss : 0.022955023171661208ITERATION : 28, loss : 0.022955742464513068ITERATION : 29, loss : 0.022956254384796947ITERATION : 30, loss : 0.02295661873958725ITERATION : 31, loss : 0.022956878078192373ITERATION : 32, loss : 0.022957062710230146ITERATION : 33, loss : 0.022957194112227713ITERATION : 34, loss : 0.022957287625325132ITERATION : 35, loss : 0.022957354196605256ITERATION : 36, loss : 0.02295740151218536ITERATION : 37, loss : 0.02295743522000849ITERATION : 38, loss : 0.022957459186749213ITERATION : 39, loss : 0.022957476272725222ITERATION : 40, loss : 0.022957488391007176ITERATION : 41, loss : 0.022957497045986566ITERATION : 42, loss : 0.022957503185754333ITERATION : 43, loss : 0.02295750768380903ITERATION : 44, loss : 0.02295751080747101ITERATION : 45, loss : 0.02295751293143694ITERATION : 46, loss : 0.022957514518678727ITERATION : 47, loss : 0.022957515623148773ITERATION : 48, loss : 0.022957516456999283ITERATION : 49, loss : 0.022957517035151462ITERATION : 50, loss : 0.022957517466415864ITERATION : 51, loss : 0.022957517715588398ITERATION : 52, loss : 0.022957517890240912ITERATION : 53, loss : 0.022957518032040623ITERATION : 54, loss : 0.022957518095039042ITERATION : 55, loss : 0.022957518141214224ITERATION : 56, loss : 0.022957518217305288ITERATION : 57, loss : 0.022957518217305288ITERATION : 58, loss : 0.022957518217305288ITERATION : 59, loss : 0.022957518217305288ITERATION : 60, loss : 0.022957518217305288ITERATION : 61, loss : 0.022957518217305288ITERATION : 62, loss : 0.022957518217305288ITERATION : 63, loss : 0.022957518217305288ITERATION : 64, loss : 0.022957518217305288ITERATION : 65, loss : 0.022957518217305288ITERATION : 66, loss : 0.022957518217305288ITERATION : 67, loss : 0.022957518217305288ITERATION : 68, loss : 0.022957518217305288ITERATION : 69, loss : 0.022957518217305288ITERATION : 70, loss : 0.022957518217305288ITERATION : 71, loss : 0.022957518217305288ITERATION : 72, loss : 0.022957518217305288ITERATION : 73, loss : 0.022957518217305288ITERATION : 74, loss : 0.022957518217305288ITERATION : 75, loss : 0.022957518217305288ITERATION : 76, loss : 0.022957518217305288ITERATION : 77, loss : 0.022957518217305288ITERATION : 78, loss : 0.022957518217305288ITERATION : 79, loss : 0.022957518217305288ITERATION : 80, loss : 0.022957518217305288ITERATION : 81, loss : 0.022957518217305288ITERATION : 82, loss : 0.022957518217305288ITERATION : 83, loss : 0.022957518217305288ITERATION : 84, loss : 0.022957518217305288ITERATION : 85, loss : 0.022957518217305288ITERATION : 86, loss : 0.022957518217305288ITERATION : 87, loss : 0.022957518217305288ITERATION : 88, loss : 0.022957518217305288ITERATION : 89, loss : 0.022957518217305288ITERATION : 90, loss : 0.022957518217305288ITERATION : 91, loss : 0.022957518217305288ITERATION : 92, loss : 0.022957518217305288ITERATION : 93, loss : 0.022957518217305288ITERATION : 94, loss : 0.022957518217305288ITERATION : 95, loss : 0.022957518217305288ITERATION : 96, loss : 0.022957518217305288ITERATION : 97, loss : 0.022957518217305288ITERATION : 98, loss : 0.022957518217305288ITERATION : 99, loss : 0.022957518217305288ITERATION : 100, loss : 0.022957518217305288
ITERATION : 1, loss : 0.027804155567170058ITERATION : 2, loss : 0.023805285826310866ITERATION : 3, loss : 0.02691485988969652ITERATION : 4, loss : 0.028256562177765898ITERATION : 5, loss : 0.02612979493856787ITERATION : 6, loss : 0.02482311702111375ITERATION : 7, loss : 0.023996156218367877ITERATION : 8, loss : 0.023460207510233ITERATION : 9, loss : 0.023106363861936034ITERATION : 10, loss : 0.022869443608059797ITERATION : 11, loss : 0.022709151796574806ITERATION : 12, loss : 0.022599880790041195ITERATION : 13, loss : 0.02252498571312027ITERATION : 14, loss : 0.02247345422989816ITERATION : 15, loss : 0.022437901871497967ITERATION : 16, loss : 0.022413327210593845ITERATION : 17, loss : 0.022396318147570905ITERATION : 18, loss : 0.022384534781989998ITERATION : 19, loss : 0.022376366419053145ITERATION : 20, loss : 0.022370701737995206ITERATION : 21, loss : 0.022366772154251112ITERATION : 22, loss : 0.022364045699107167ITERATION : 23, loss : 0.022362153769277172ITERATION : 24, loss : 0.022360840825773283ITERATION : 25, loss : 0.02235992964077115ITERATION : 26, loss : 0.02235929740666202ITERATION : 27, loss : 0.022358858509232644ITERATION : 28, loss : 0.022358553940099835ITERATION : 29, loss : 0.022358342555860576ITERATION : 30, loss : 0.022358195885329925ITERATION : 31, loss : 0.022358094130148883ITERATION : 32, loss : 0.022358023575574443ITERATION : 33, loss : 0.02235797460956479ITERATION : 34, loss : 0.022357940691352295ITERATION : 35, loss : 0.022357917113558206ITERATION : 36, loss : 0.02235790070873477ITERATION : 37, loss : 0.022357889300213412ITERATION : 38, loss : 0.02235788137782869ITERATION : 39, loss : 0.022357875871300607ITERATION : 40, loss : 0.022357872037575544ITERATION : 41, loss : 0.022357869392611907ITERATION : 42, loss : 0.022357867560002655ITERATION : 43, loss : 0.022357866200744047ITERATION : 44, loss : 0.02235786532574037ITERATION : 45, loss : 0.022357864711103667ITERATION : 46, loss : 0.022357864290438283ITERATION : 47, loss : 0.022357863993482944ITERATION : 48, loss : 0.02235786379230999ITERATION : 49, loss : 0.02235786367802155ITERATION : 50, loss : 0.022357863568665662ITERATION : 51, loss : 0.022357863517058967ITERATION : 52, loss : 0.022357863496714064ITERATION : 53, loss : 0.02235786349569537ITERATION : 54, loss : 0.02235786349569537ITERATION : 55, loss : 0.02235786349569537ITERATION : 56, loss : 0.02235786349569537ITERATION : 57, loss : 0.02235786349569537ITERATION : 58, loss : 0.02235786349569537ITERATION : 59, loss : 0.02235786349569537ITERATION : 60, loss : 0.02235786349569537ITERATION : 61, loss : 0.02235786349569537ITERATION : 62, loss : 0.02235786349569537ITERATION : 63, loss : 0.02235786349569537ITERATION : 64, loss : 0.02235786349569537ITERATION : 65, loss : 0.02235786349569537ITERATION : 66, loss : 0.02235786349569537ITERATION : 67, loss : 0.02235786349569537ITERATION : 68, loss : 0.02235786349569537ITERATION : 69, loss : 0.02235786349569537ITERATION : 70, loss : 0.02235786349569537ITERATION : 71, loss : 0.02235786349569537ITERATION : 72, loss : 0.02235786349569537ITERATION : 73, loss : 0.02235786349569537ITERATION : 74, loss : 0.02235786349569537ITERATION : 75, loss : 0.02235786349569537ITERATION : 76, loss : 0.02235786349569537ITERATION : 77, loss : 0.02235786349569537ITERATION : 78, loss : 0.02235786349569537ITERATION : 79, loss : 0.02235786349569537ITERATION : 80, loss : 0.02235786349569537ITERATION : 81, loss : 0.02235786349569537ITERATION : 82, loss : 0.02235786349569537ITERATION : 83, loss : 0.02235786349569537ITERATION : 84, loss : 0.02235786349569537ITERATION : 85, loss : 0.02235786349569537ITERATION : 86, loss : 0.02235786349569537ITERATION : 87, loss : 0.02235786349569537ITERATION : 88, loss : 0.02235786349569537ITERATION : 89, loss : 0.02235786349569537ITERATION : 90, loss : 0.02235786349569537ITERATION : 91, loss : 0.02235786349569537ITERATION : 92, loss : 0.02235786349569537ITERATION : 93, loss : 0.02235786349569537ITERATION : 94, loss : 0.02235786349569537ITERATION : 95, loss : 0.02235786349569537ITERATION : 96, loss : 0.02235786349569537ITERATION : 97, loss : 0.02235786349569537ITERATION : 98, loss : 0.02235786349569537ITERATION : 99, loss : 0.02235786349569537ITERATION : 100, loss : 0.02235786349569537
ITERATION : 1, loss : 0.11117171359054753ITERATION : 2, loss : 0.08310163316780307ITERATION : 3, loss : 0.07282881938879282ITERATION : 4, loss : 0.06889924972989249ITERATION : 5, loss : 0.067262983580375ITERATION : 6, loss : 0.06652096025249847ITERATION : 7, loss : 0.06615440363769289ITERATION : 8, loss : 0.06595890041670388ITERATION : 9, loss : 0.06584835422693615ITERATION : 10, loss : 0.06578350541917609ITERATION : 11, loss : 0.06574482691816831ITERATION : 12, loss : 0.06572177671207698ITERATION : 13, loss : 0.06570827572587234ITERATION : 14, loss : 0.0657006549199965ITERATION : 15, loss : 0.06569663866882658ITERATION : 16, loss : 0.06569479737178324ITERATION : 17, loss : 0.06569423088511585ITERATION : 18, loss : 0.06569437622816676ITERATION : 19, loss : 0.06569488582163761ITERATION : 20, loss : 0.06569555029468778ITERATION : 21, loss : 0.06569624790779167ITERATION : 22, loss : 0.06569691195785905ITERATION : 23, loss : 0.06569750962117463ITERATION : 24, loss : 0.06569802841345854ITERATION : 25, loss : 0.06569846751021804ITERATION : 26, loss : 0.06569883224491822ITERATION : 27, loss : 0.06569913088763482ITERATION : 28, loss : 0.06569937271200459ITERATION : 29, loss : 0.06569956670055015ITERATION : 30, loss : 0.06569972114435566ITERATION : 31, loss : 0.06569984339429889ITERATION : 32, loss : 0.06569993958259394ITERATION : 33, loss : 0.06570001494295957ITERATION : 34, loss : 0.06570007374787466ITERATION : 35, loss : 0.06570011947361905ITERATION : 36, loss : 0.06570015491911022ITERATION : 37, loss : 0.06570018228542912ITERATION : 38, loss : 0.0657002033766709ITERATION : 39, loss : 0.06570021961454664ITERATION : 40, loss : 0.06570023205596705ITERATION : 41, loss : 0.06570024166011813ITERATION : 42, loss : 0.06570024900170662ITERATION : 43, loss : 0.06570025453186766ITERATION : 44, loss : 0.06570025881059498ITERATION : 45, loss : 0.06570026206815292ITERATION : 46, loss : 0.06570026459336546ITERATION : 47, loss : 0.06570026647721958ITERATION : 48, loss : 0.06570026795165333ITERATION : 49, loss : 0.06570026904292908ITERATION : 50, loss : 0.06570026990390325ITERATION : 51, loss : 0.06570027053775994ITERATION : 52, loss : 0.06570027104404844ITERATION : 53, loss : 0.06570027138232738ITERATION : 54, loss : 0.06570027167898396ITERATION : 55, loss : 0.06570027186112777ITERATION : 56, loss : 0.06570027204288752ITERATION : 57, loss : 0.06570027215140432ITERATION : 58, loss : 0.06570027224987529ITERATION : 59, loss : 0.06570027230524501ITERATION : 60, loss : 0.06570027234777064ITERATION : 61, loss : 0.06570027240531298ITERATION : 62, loss : 0.06570027241178661ITERATION : 63, loss : 0.06570027241188843ITERATION : 64, loss : 0.06570027241188843ITERATION : 65, loss : 0.06570027241188843ITERATION : 66, loss : 0.06570027241188843ITERATION : 67, loss : 0.06570027241188843ITERATION : 68, loss : 0.06570027241188843ITERATION : 69, loss : 0.06570027241188843ITERATION : 70, loss : 0.06570027241188843ITERATION : 71, loss : 0.06570027241188843ITERATION : 72, loss : 0.06570027241188843ITERATION : 73, loss : 0.06570027241188843ITERATION : 74, loss : 0.06570027241188843ITERATION : 75, loss : 0.06570027241188843ITERATION : 76, loss : 0.06570027241188843ITERATION : 77, loss : 0.06570027241188843ITERATION : 78, loss : 0.06570027241188843ITERATION : 79, loss : 0.06570027241188843ITERATION : 80, loss : 0.06570027241188843ITERATION : 81, loss : 0.06570027241188843ITERATION : 82, loss : 0.06570027241188843ITERATION : 83, loss : 0.06570027241188843ITERATION : 84, loss : 0.06570027241188843ITERATION : 85, loss : 0.06570027241188843ITERATION : 86, loss : 0.06570027241188843ITERATION : 87, loss : 0.06570027241188843ITERATION : 88, loss : 0.06570027241188843ITERATION : 89, loss : 0.06570027241188843ITERATION : 90, loss : 0.06570027241188843ITERATION : 91, loss : 0.06570027241188843ITERATION : 92, loss : 0.06570027241188843ITERATION : 93, loss : 0.06570027241188843ITERATION : 94, loss : 0.06570027241188843ITERATION : 95, loss : 0.06570027241188843ITERATION : 96, loss : 0.06570027241188843ITERATION : 97, loss : 0.06570027241188843ITERATION : 98, loss : 0.06570027241188843ITERATION : 99, loss : 0.06570027241188843ITERATION : 100, loss : 0.06570027241188843
ITERATION : 1, loss : 0.026189281803354347ITERATION : 2, loss : 0.021128775432594376ITERATION : 3, loss : 0.019333233125951386ITERATION : 4, loss : 0.017771789477607637ITERATION : 5, loss : 0.015801076656511363ITERATION : 6, loss : 0.014568241873562367ITERATION : 7, loss : 0.013777513783099295ITERATION : 8, loss : 0.013260001594923203ITERATION : 9, loss : 0.012915847905009603ITERATION : 10, loss : 0.012684150314912651ITERATION : 11, loss : 0.012526714323226155ITERATION : 12, loss : 0.012419004419740233ITERATION : 13, loss : 0.012344945020567996ITERATION : 14, loss : 0.012293837612979763ITERATION : 15, loss : 0.012258476057536738ITERATION : 16, loss : 0.012233962534388831ITERATION : 17, loss : 0.012216945660453884ITERATION : 18, loss : 0.012205120947484225ITERATION : 19, loss : 0.012196898159074022ITERATION : 20, loss : 0.012191177000394387ITERATION : 21, loss : 0.012187194774604058ITERATION : 22, loss : 0.012184422101398498ITERATION : 23, loss : 0.012182491117069113ITERATION : 24, loss : 0.01218114605526044ITERATION : 25, loss : 0.012180208988358728ITERATION : 26, loss : 0.012179556070914584ITERATION : 27, loss : 0.012179101074601226ITERATION : 28, loss : 0.01217878397535419ITERATION : 29, loss : 0.012178562957550857ITERATION : 30, loss : 0.012178408896629873ITERATION : 31, loss : 0.012178301484394796ITERATION : 32, loss : 0.012178226608444394ITERATION : 33, loss : 0.012178174394862913ITERATION : 34, loss : 0.012178137985736172ITERATION : 35, loss : 0.01217811260501809ITERATION : 36, loss : 0.012178094892977524ITERATION : 37, loss : 0.012178082553480143ITERATION : 38, loss : 0.012178073936853122ITERATION : 39, loss : 0.012178067929980376ITERATION : 40, loss : 0.012178063732511181ITERATION : 41, loss : 0.012178060810287782ITERATION : 42, loss : 0.01217805876178354ITERATION : 43, loss : 0.012178057331393632ITERATION : 44, loss : 0.012178056329434996ITERATION : 45, loss : 0.012178055692190926ITERATION : 46, loss : 0.012178055194873433ITERATION : 47, loss : 0.012178054868747594ITERATION : 48, loss : 0.01217805457035265ITERATION : 49, loss : 0.012178054407793416ITERATION : 50, loss : 0.012178054327911357ITERATION : 51, loss : 0.012178054268652852ITERATION : 52, loss : 0.012178054231510707ITERATION : 53, loss : 0.012178054214269539ITERATION : 54, loss : 0.012178054214133758ITERATION : 55, loss : 0.012178054214133758ITERATION : 56, loss : 0.012178054214133758ITERATION : 57, loss : 0.012178054214133758ITERATION : 58, loss : 0.012178054214133758ITERATION : 59, loss : 0.012178054214133758ITERATION : 60, loss : 0.012178054214133758ITERATION : 61, loss : 0.012178054214133758ITERATION : 62, loss : 0.012178054214133758ITERATION : 63, loss : 0.012178054214133758ITERATION : 64, loss : 0.012178054214133758ITERATION : 65, loss : 0.012178054214133758ITERATION : 66, loss : 0.012178054214133758ITERATION : 67, loss : 0.012178054214133758ITERATION : 68, loss : 0.012178054214133758ITERATION : 69, loss : 0.012178054214133758ITERATION : 70, loss : 0.012178054214133758ITERATION : 71, loss : 0.012178054214133758ITERATION : 72, loss : 0.012178054214133758ITERATION : 73, loss : 0.012178054214133758ITERATION : 74, loss : 0.012178054214133758ITERATION : 75, loss : 0.012178054214133758ITERATION : 76, loss : 0.012178054214133758ITERATION : 77, loss : 0.012178054214133758ITERATION : 78, loss : 0.012178054214133758ITERATION : 79, loss : 0.012178054214133758ITERATION : 80, loss : 0.012178054214133758ITERATION : 81, loss : 0.012178054214133758ITERATION : 82, loss : 0.012178054214133758ITERATION : 83, loss : 0.012178054214133758ITERATION : 84, loss : 0.012178054214133758ITERATION : 85, loss : 0.012178054214133758ITERATION : 86, loss : 0.012178054214133758ITERATION : 87, loss : 0.012178054214133758ITERATION : 88, loss : 0.012178054214133758ITERATION : 89, loss : 0.012178054214133758ITERATION : 90, loss : 0.012178054214133758ITERATION : 91, loss : 0.012178054214133758ITERATION : 92, loss : 0.012178054214133758ITERATION : 93, loss : 0.012178054214133758ITERATION : 94, loss : 0.012178054214133758ITERATION : 95, loss : 0.012178054214133758ITERATION : 96, loss : 0.012178054214133758ITERATION : 97, loss : 0.012178054214133758ITERATION : 98, loss : 0.012178054214133758ITERATION : 99, loss : 0.012178054214133758ITERATION : 100, loss : 0.012178054214133758
ITERATION : 1, loss : 0.055538410286581325ITERATION : 2, loss : 0.047818557984652295ITERATION : 3, loss : 0.043740459424150585ITERATION : 4, loss : 0.04141166996669663ITERATION : 5, loss : 0.03994273022955667ITERATION : 6, loss : 0.03896092982704974ITERATION : 7, loss : 0.038281300468566204ITERATION : 8, loss : 0.03780048956416538ITERATION : 9, loss : 0.0374556127995937ITERATION : 10, loss : 0.03720602185263667ITERATION : 11, loss : 0.03702432009820258ITERATION : 12, loss : 0.03689151273532745ITERATION : 13, loss : 0.0367941759979791ITERATION : 14, loss : 0.036722699914961174ITERATION : 15, loss : 0.036670143300847495ITERATION : 16, loss : 0.03663146169650918ITERATION : 17, loss : 0.03660297311093569ITERATION : 18, loss : 0.036581981812958866ITERATION : 19, loss : 0.036566509793074804ITERATION : 20, loss : 0.03655510325031738ITERATION : 21, loss : 0.03654669285918489ITERATION : 22, loss : 0.03654049103929188ITERATION : 23, loss : 0.0365359175637731ITERATION : 24, loss : 0.03653254479173273ITERATION : 25, loss : 0.03653005757229806ITERATION : 26, loss : 0.036528223417120635ITERATION : 27, loss : 0.03652687085420178ITERATION : 28, loss : 0.03652587344817664ITERATION : 29, loss : 0.03652513797072551ITERATION : 30, loss : 0.036524595662578103ITERATION : 31, loss : 0.03652419580903576ITERATION : 32, loss : 0.03652390099785928ITERATION : 33, loss : 0.03652368366264873ITERATION : 34, loss : 0.03652352343640123ITERATION : 35, loss : 0.036523405302220036ITERATION : 36, loss : 0.03652331821965235ITERATION : 37, loss : 0.03652325402541851ITERATION : 38, loss : 0.03652320672457742ITERATION : 39, loss : 0.036523171851282406ITERATION : 40, loss : 0.03652314615664514ITERATION : 41, loss : 0.036523127207531834ITERATION : 42, loss : 0.03652311323646174ITERATION : 43, loss : 0.03652310294776884ITERATION : 44, loss : 0.03652309536756585ITERATION : 45, loss : 0.03652308978514705ITERATION : 46, loss : 0.036523085654198706ITERATION : 47, loss : 0.03652308256652497ITERATION : 48, loss : 0.03652308035344314ITERATION : 49, loss : 0.03652307874854079ITERATION : 50, loss : 0.036523077531573266ITERATION : 51, loss : 0.036523076629948616ITERATION : 52, loss : 0.03652307597168657ITERATION : 53, loss : 0.03652307548231565ITERATION : 54, loss : 0.03652307514987241ITERATION : 55, loss : 0.03652307488387008ITERATION : 56, loss : 0.03652307466615886ITERATION : 57, loss : 0.03652307454126964ITERATION : 58, loss : 0.036523074538208876ITERATION : 59, loss : 0.036523074538208876ITERATION : 60, loss : 0.036523074538208876ITERATION : 61, loss : 0.036523074538208876ITERATION : 62, loss : 0.036523074538208876ITERATION : 63, loss : 0.036523074538208876ITERATION : 64, loss : 0.036523074538208876ITERATION : 65, loss : 0.036523074538208876ITERATION : 66, loss : 0.036523074538208876ITERATION : 67, loss : 0.036523074538208876ITERATION : 68, loss : 0.036523074538208876ITERATION : 69, loss : 0.036523074538208876ITERATION : 70, loss : 0.036523074538208876ITERATION : 71, loss : 0.036523074538208876ITERATION : 72, loss : 0.036523074538208876ITERATION : 73, loss : 0.036523074538208876ITERATION : 74, loss : 0.036523074538208876ITERATION : 75, loss : 0.036523074538208876ITERATION : 76, loss : 0.036523074538208876ITERATION : 77, loss : 0.036523074538208876ITERATION : 78, loss : 0.036523074538208876ITERATION : 79, loss : 0.036523074538208876ITERATION : 80, loss : 0.036523074538208876ITERATION : 81, loss : 0.036523074538208876ITERATION : 82, loss : 0.036523074538208876ITERATION : 83, loss : 0.036523074538208876ITERATION : 84, loss : 0.036523074538208876ITERATION : 85, loss : 0.036523074538208876ITERATION : 86, loss : 0.036523074538208876ITERATION : 87, loss : 0.036523074538208876ITERATION : 88, loss : 0.036523074538208876ITERATION : 89, loss : 0.036523074538208876ITERATION : 90, loss : 0.036523074538208876ITERATION : 91, loss : 0.036523074538208876ITERATION : 92, loss : 0.036523074538208876ITERATION : 93, loss : 0.036523074538208876ITERATION : 94, loss : 0.036523074538208876ITERATION : 95, loss : 0.036523074538208876ITERATION : 96, loss : 0.036523074538208876ITERATION : 97, loss : 0.036523074538208876ITERATION : 98, loss : 0.036523074538208876ITERATION : 99, loss : 0.036523074538208876ITERATION : 100, loss : 0.036523074538208876
ITERATION : 1, loss : 0.02920325151564451ITERATION : 2, loss : 0.02434811165900417ITERATION : 3, loss : 0.022827129979111363ITERATION : 4, loss : 0.021991547589519073ITERATION : 5, loss : 0.0214602151457956ITERATION : 6, loss : 0.021107842548245746ITERATION : 7, loss : 0.020870477751130708ITERATION : 8, loss : 0.02070937667055267ITERATION : 9, loss : 0.020599549339162092ITERATION : 10, loss : 0.020524453985307656ITERATION : 11, loss : 0.020472998160933974ITERATION : 12, loss : 0.020437685672794716ITERATION : 13, loss : 0.02041342413820288ITERATION : 14, loss : 0.020396741082148603ITERATION : 15, loss : 0.020385261981612127ITERATION : 16, loss : 0.020377359944327884ITERATION : 17, loss : 0.02037191847729044ITERATION : 18, loss : 0.020368170394840245ITERATION : 19, loss : 0.020365588255626663ITERATION : 20, loss : 0.020363809124079115ITERATION : 21, loss : 0.0203625831428042ITERATION : 22, loss : 0.020361738246824995ITERATION : 23, loss : 0.020361155959300332ITERATION : 24, loss : 0.020360754637628594ITERATION : 25, loss : 0.020360478004873134ITERATION : 26, loss : 0.020360287348152796ITERATION : 27, loss : 0.02036015593357039ITERATION : 28, loss : 0.02036006533609452ITERATION : 29, loss : 0.020360002887166915ITERATION : 30, loss : 0.02035995984229201ITERATION : 31, loss : 0.020359930193578717ITERATION : 32, loss : 0.02035990972535518ITERATION : 33, loss : 0.02035989564342297ITERATION : 34, loss : 0.020359885926197925ITERATION : 35, loss : 0.020359879240323304ITERATION : 36, loss : 0.020359874634409383ITERATION : 37, loss : 0.02035987146883234ITERATION : 38, loss : 0.020359869303070748ITERATION : 39, loss : 0.02035986788569343ITERATION : 40, loss : 0.020359866736465736ITERATION : 41, loss : 0.02035986600331998ITERATION : 42, loss : 0.020359865586803538ITERATION : 43, loss : 0.02035986528505436ITERATION : 44, loss : 0.020359865051570402ITERATION : 45, loss : 0.020359864888462822ITERATION : 46, loss : 0.020359864792124253ITERATION : 47, loss : 0.020359864719843922ITERATION : 48, loss : 0.02035986471533403ITERATION : 49, loss : 0.02035986471533403ITERATION : 50, loss : 0.02035986471533403ITERATION : 51, loss : 0.02035986471533403ITERATION : 52, loss : 0.02035986471533403ITERATION : 53, loss : 0.02035986471533403ITERATION : 54, loss : 0.02035986471533403ITERATION : 55, loss : 0.02035986471533403ITERATION : 56, loss : 0.02035986471533403ITERATION : 57, loss : 0.02035986471533403ITERATION : 58, loss : 0.02035986471533403ITERATION : 59, loss : 0.02035986471533403ITERATION : 60, loss : 0.02035986471533403ITERATION : 61, loss : 0.02035986471533403ITERATION : 62, loss : 0.02035986471533403ITERATION : 63, loss : 0.02035986471533403ITERATION : 64, loss : 0.02035986471533403ITERATION : 65, loss : 0.02035986471533403ITERATION : 66, loss : 0.02035986471533403ITERATION : 67, loss : 0.02035986471533403ITERATION : 68, loss : 0.02035986471533403ITERATION : 69, loss : 0.02035986471533403ITERATION : 70, loss : 0.02035986471533403ITERATION : 71, loss : 0.02035986471533403ITERATION : 72, loss : 0.02035986471533403ITERATION : 73, loss : 0.02035986471533403ITERATION : 74, loss : 0.02035986471533403ITERATION : 75, loss : 0.02035986471533403ITERATION : 76, loss : 0.02035986471533403ITERATION : 77, loss : 0.02035986471533403ITERATION : 78, loss : 0.02035986471533403ITERATION : 79, loss : 0.02035986471533403ITERATION : 80, loss : 0.02035986471533403ITERATION : 81, loss : 0.02035986471533403ITERATION : 82, loss : 0.02035986471533403ITERATION : 83, loss : 0.02035986471533403ITERATION : 84, loss : 0.02035986471533403ITERATION : 85, loss : 0.02035986471533403ITERATION : 86, loss : 0.02035986471533403ITERATION : 87, loss : 0.02035986471533403ITERATION : 88, loss : 0.02035986471533403ITERATION : 89, loss : 0.02035986471533403ITERATION : 90, loss : 0.02035986471533403ITERATION : 91, loss : 0.02035986471533403ITERATION : 92, loss : 0.02035986471533403ITERATION : 93, loss : 0.02035986471533403ITERATION : 94, loss : 0.02035986471533403ITERATION : 95, loss : 0.02035986471533403ITERATION : 96, loss : 0.02035986471533403ITERATION : 97, loss : 0.02035986471533403ITERATION : 98, loss : 0.02035986471533403ITERATION : 99, loss : 0.02035986471533403ITERATION : 100, loss : 0.02035986471533403
ITERATION : 1, loss : 0.02753627917469007ITERATION : 2, loss : 0.027987207120976635ITERATION : 3, loss : 0.029215693763396458ITERATION : 4, loss : 0.030216351800243086ITERATION : 5, loss : 0.030855178262901767ITERATION : 6, loss : 0.03128443983812399ITERATION : 7, loss : 0.031571645921720126ITERATION : 8, loss : 0.031765388393453144ITERATION : 9, loss : 0.03189776197567196ITERATION : 10, loss : 0.03198945893354547ITERATION : 11, loss : 0.032053808462523634ITERATION : 12, loss : 0.03209948255954405ITERATION : 13, loss : 0.032132208864427275ITERATION : 14, loss : 0.032155835144477195ITERATION : 15, loss : 0.03217299164924232ITERATION : 16, loss : 0.03218550424752681ITERATION : 17, loss : 0.03219465898706449ITERATION : 18, loss : 0.03220137207850141ITERATION : 19, loss : 0.032206301831285304ITERATION : 20, loss : 0.0322099252857394ITERATION : 21, loss : 0.032212590018723876ITERATION : 22, loss : 0.03221455010423714ITERATION : 23, loss : 0.03221599178099507ITERATION : 24, loss : 0.032217051930850754ITERATION : 25, loss : 0.03221783125838709ITERATION : 26, loss : 0.03221840402748223ITERATION : 27, loss : 0.03221882476479629ITERATION : 28, loss : 0.032219133814478766ITERATION : 29, loss : 0.03221936061711263ITERATION : 30, loss : 0.032219526902121945ITERATION : 31, loss : 0.032219648783988114ITERATION : 32, loss : 0.03221973817550279ITERATION : 33, loss : 0.032219803652597426ITERATION : 34, loss : 0.03221985169743555ITERATION : 35, loss : 0.032219886797952915ITERATION : 36, loss : 0.032219912442025704ITERATION : 37, loss : 0.03221993115106756ITERATION : 38, loss : 0.032219944889931004ITERATION : 39, loss : 0.03221995493202077ITERATION : 40, loss : 0.03221996221208181ITERATION : 41, loss : 0.032219967592729395ITERATION : 42, loss : 0.032219971549744604ITERATION : 43, loss : 0.03221997435319684ITERATION : 44, loss : 0.03221997645024227ITERATION : 45, loss : 0.03221997795728112ITERATION : 46, loss : 0.03221997902864331ITERATION : 47, loss : 0.03221997987817053ITERATION : 48, loss : 0.03221998042426667ITERATION : 49, loss : 0.03221998081311081ITERATION : 50, loss : 0.03221998112129064ITERATION : 51, loss : 0.03221998131832052ITERATION : 52, loss : 0.03221998144424714ITERATION : 53, loss : 0.0322199815639378ITERATION : 54, loss : 0.03221998164139991ITERATION : 55, loss : 0.03221998171547291ITERATION : 56, loss : 0.032219981772413524ITERATION : 57, loss : 0.032219981772413524ITERATION : 58, loss : 0.032219981772413524ITERATION : 59, loss : 0.032219981772413524ITERATION : 60, loss : 0.032219981772413524ITERATION : 61, loss : 0.032219981772413524ITERATION : 62, loss : 0.032219981772413524ITERATION : 63, loss : 0.032219981772413524ITERATION : 64, loss : 0.032219981772413524ITERATION : 65, loss : 0.032219981772413524ITERATION : 66, loss : 0.032219981772413524ITERATION : 67, loss : 0.032219981772413524ITERATION : 68, loss : 0.032219981772413524ITERATION : 69, loss : 0.032219981772413524ITERATION : 70, loss : 0.032219981772413524ITERATION : 71, loss : 0.032219981772413524ITERATION : 72, loss : 0.032219981772413524ITERATION : 73, loss : 0.032219981772413524ITERATION : 74, loss : 0.032219981772413524ITERATION : 75, loss : 0.032219981772413524ITERATION : 76, loss : 0.032219981772413524ITERATION : 77, loss : 0.032219981772413524ITERATION : 78, loss : 0.032219981772413524ITERATION : 79, loss : 0.032219981772413524ITERATION : 80, loss : 0.032219981772413524ITERATION : 81, loss : 0.032219981772413524ITERATION : 82, loss : 0.032219981772413524ITERATION : 83, loss : 0.032219981772413524ITERATION : 84, loss : 0.032219981772413524ITERATION : 85, loss : 0.032219981772413524ITERATION : 86, loss : 0.032219981772413524ITERATION : 87, loss : 0.032219981772413524ITERATION : 88, loss : 0.032219981772413524ITERATION : 89, loss : 0.032219981772413524ITERATION : 90, loss : 0.032219981772413524ITERATION : 91, loss : 0.032219981772413524ITERATION : 92, loss : 0.032219981772413524ITERATION : 93, loss : 0.032219981772413524ITERATION : 94, loss : 0.032219981772413524ITERATION : 95, loss : 0.032219981772413524ITERATION : 96, loss : 0.032219981772413524ITERATION : 97, loss : 0.032219981772413524ITERATION : 98, loss : 0.032219981772413524ITERATION : 99, loss : 0.032219981772413524ITERATION : 100, loss : 0.032219981772413524
ITERATION : 1, loss : 0.020144591084974718ITERATION : 2, loss : 0.018813883837996034ITERATION : 3, loss : 0.018073362747024563ITERATION : 4, loss : 0.01813106964672621ITERATION : 5, loss : 0.018461763304039012ITERATION : 6, loss : 0.018618545774883817ITERATION : 7, loss : 0.0186326438947116ITERATION : 8, loss : 0.018667715841210058ITERATION : 9, loss : 0.018704489870619116ITERATION : 10, loss : 0.018736260307800623ITERATION : 11, loss : 0.01876152519696262ITERATION : 12, loss : 0.018780756299765194ITERATION : 13, loss : 0.018795022189101507ITERATION : 14, loss : 0.01880543469730819ITERATION : 15, loss : 0.01881295432614036ITERATION : 16, loss : 0.01881834585540789ITERATION : 17, loss : 0.01882219250258298ITERATION : 18, loss : 0.018824927370439726ITERATION : 19, loss : 0.018826867021882045ITERATION : 20, loss : 0.018828240343881166ITERATION : 21, loss : 0.01882921144072439ITERATION : 22, loss : 0.018829897547204693ITERATION : 23, loss : 0.01883038197450469ITERATION : 24, loss : 0.01883072382175093ITERATION : 25, loss : 0.01883096503580571ITERATION : 26, loss : 0.01883113513230151ITERATION : 27, loss : 0.0188312551098428ITERATION : 28, loss : 0.018831339697134675ITERATION : 29, loss : 0.018831399342902795ITERATION : 30, loss : 0.018831441409652556ITERATION : 31, loss : 0.01883147105827808ITERATION : 32, loss : 0.018831491965603773ITERATION : 33, loss : 0.018831506692254838ITERATION : 34, loss : 0.01883151706511202ITERATION : 35, loss : 0.018831524395833556ITERATION : 36, loss : 0.018831529527177003ITERATION : 37, loss : 0.01883153318269588ITERATION : 38, loss : 0.018831535718265957ITERATION : 39, loss : 0.018831537561330738ITERATION : 40, loss : 0.018831538794509557ITERATION : 41, loss : 0.018831539720866177ITERATION : 42, loss : 0.01883154031892807ITERATION : 43, loss : 0.018831540800688402ITERATION : 44, loss : 0.018831541084713264ITERATION : 45, loss : 0.018831541349389722ITERATION : 46, loss : 0.01883154149268426ITERATION : 47, loss : 0.018831541618588087ITERATION : 48, loss : 0.01883154167766487ITERATION : 49, loss : 0.01883154169481691ITERATION : 50, loss : 0.018831541752997154ITERATION : 51, loss : 0.01883154175507588ITERATION : 52, loss : 0.0188315417836207ITERATION : 53, loss : 0.018831541784540044ITERATION : 54, loss : 0.018831541784540044ITERATION : 55, loss : 0.018831541784540044ITERATION : 56, loss : 0.018831541784540044ITERATION : 57, loss : 0.018831541784540044ITERATION : 58, loss : 0.018831541784540044ITERATION : 59, loss : 0.018831541784540044ITERATION : 60, loss : 0.018831541784540044ITERATION : 61, loss : 0.018831541784540044ITERATION : 62, loss : 0.018831541784540044ITERATION : 63, loss : 0.018831541784540044ITERATION : 64, loss : 0.018831541784540044ITERATION : 65, loss : 0.018831541784540044ITERATION : 66, loss : 0.018831541784540044ITERATION : 67, loss : 0.018831541784540044ITERATION : 68, loss : 0.018831541784540044ITERATION : 69, loss : 0.018831541784540044ITERATION : 70, loss : 0.018831541784540044ITERATION : 71, loss : 0.018831541784540044ITERATION : 72, loss : 0.018831541784540044ITERATION : 73, loss : 0.018831541784540044ITERATION : 74, loss : 0.018831541784540044ITERATION : 75, loss : 0.018831541784540044ITERATION : 76, loss : 0.018831541784540044ITERATION : 77, loss : 0.018831541784540044ITERATION : 78, loss : 0.018831541784540044ITERATION : 79, loss : 0.018831541784540044ITERATION : 80, loss : 0.018831541784540044ITERATION : 81, loss : 0.018831541784540044ITERATION : 82, loss : 0.018831541784540044ITERATION : 83, loss : 0.018831541784540044ITERATION : 84, loss : 0.018831541784540044ITERATION : 85, loss : 0.018831541784540044ITERATION : 86, loss : 0.018831541784540044ITERATION : 87, loss : 0.018831541784540044ITERATION : 88, loss : 0.018831541784540044ITERATION : 89, loss : 0.018831541784540044ITERATION : 90, loss : 0.018831541784540044ITERATION : 91, loss : 0.018831541784540044ITERATION : 92, loss : 0.018831541784540044ITERATION : 93, loss : 0.018831541784540044ITERATION : 94, loss : 0.018831541784540044ITERATION : 95, loss : 0.018831541784540044ITERATION : 96, loss : 0.018831541784540044ITERATION : 97, loss : 0.018831541784540044ITERATION : 98, loss : 0.018831541784540044ITERATION : 99, loss : 0.018831541784540044ITERATION : 100, loss : 0.018831541784540044
gradient norm in None layer : 0.000993654948251246
gradient norm in None layer : 4.089928690132762e-05
gradient norm in None layer : 3.5997486872271247e-05
gradient norm in None layer : 0.0006468467574676518
gradient norm in None layer : 4.501155597323326e-05
gradient norm in None layer : 5.3259580087010436e-05
gradient norm in None layer : 0.0003886578686921948
gradient norm in None layer : 1.4448585349044198e-05
gradient norm in None layer : 9.6848355719635e-06
gradient norm in None layer : 0.0003036383920987743
gradient norm in None layer : 1.1562661796860721e-05
gradient norm in None layer : 8.000709665330581e-06
gradient norm in None layer : 0.00012170206689435195
gradient norm in None layer : 3.509255756430503e-06
gradient norm in None layer : 1.9852217227569704e-06
gradient norm in None layer : 9.734164649319517e-05
gradient norm in None layer : 3.6679699462765274e-06
gradient norm in None layer : 2.661659562598533e-06
gradient norm in None layer : 0.0001265084399092061
gradient norm in None layer : 9.706201947587019e-07
gradient norm in None layer : 0.00023785884886042935
gradient norm in None layer : 1.2116550953900814e-05
gradient norm in None layer : 8.663000583872338e-06
gradient norm in None layer : 0.0002497734131145121
gradient norm in None layer : 2.023427183260486e-05
gradient norm in None layer : 2.3691365305740943e-05
gradient norm in None layer : 0.0004022576327752796
gradient norm in None layer : 2.1473685632550865e-06
gradient norm in None layer : 0.000680975151259542
gradient norm in None layer : 6.250695964880713e-05
gradient norm in None layer : 6.351339367935103e-05
gradient norm in None layer : 0.0008417413220396257
gradient norm in None layer : 7.009933189817833e-05
gradient norm in None layer : 8.057081470300422e-05
gradient norm in None layer : 6.789169538347685e-05
gradient norm in None layer : 9.974489010406397e-06
Total gradient norm: 0.0017821210565938375
invariance loss : 4.437999647558858, avg_den : 0.42301177978515625, density loss : 0.32301177978515627, mse loss : 0.028891021393689917, solver time : 122.7388117313385 sec , total loss : 0.03365203282103393, running loss : 0.052553931068112035
Epoch 0/10 , batch 15/12500 
ITERATION : 1, loss : 0.04786400655366243ITERATION : 2, loss : 0.04302841202484972ITERATION : 3, loss : 0.036397614118189515ITERATION : 4, loss : 0.03253355845274264ITERATION : 5, loss : 0.030161484093441163ITERATION : 6, loss : 0.02865599116127691ITERATION : 7, loss : 0.027677213544828504ITERATION : 8, loss : 0.027029459061092418ITERATION : 9, loss : 0.026595154333850655ITERATION : 10, loss : 0.02630120566359813ITERATION : 11, loss : 0.026100907919898275ITERATION : 12, loss : 0.025963770184440144ITERATION : 13, loss : 0.02586955924288076ITERATION : 14, loss : 0.02580468503411337ITERATION : 15, loss : 0.025759938319605883ITERATION : 16, loss : 0.025729038819405078ITERATION : 17, loss : 0.0257076842823086ITERATION : 18, loss : 0.025692918019461667ITERATION : 19, loss : 0.02568270348493318ITERATION : 20, loss : 0.025675635715765567ITERATION : 21, loss : 0.025670744398566297ITERATION : 22, loss : 0.02566735893560585ITERATION : 23, loss : 0.025665015492876725ITERATION : 24, loss : 0.02566339329033302ITERATION : 25, loss : 0.025662270292072343ITERATION : 26, loss : 0.02566149286487573ITERATION : 27, loss : 0.02566095465737821ITERATION : 28, loss : 0.02566058206903871ITERATION : 29, loss : 0.025660324110194546ITERATION : 30, loss : 0.025660145531051073ITERATION : 31, loss : 0.02566002192208544ITERATION : 32, loss : 0.02565993635369965ITERATION : 33, loss : 0.025659877114226428ITERATION : 34, loss : 0.025659836094597634ITERATION : 35, loss : 0.02565980769728026ITERATION : 36, loss : 0.025659788044521813ITERATION : 37, loss : 0.025659774439288796ITERATION : 38, loss : 0.025659765039238617ITERATION : 39, loss : 0.025659758537128166ITERATION : 40, loss : 0.025659754004693707ITERATION : 41, loss : 0.02565975090175176ITERATION : 42, loss : 0.025659748754498175ITERATION : 43, loss : 0.02565974728577574ITERATION : 44, loss : 0.025659746262190263ITERATION : 45, loss : 0.025659745600322532ITERATION : 46, loss : 0.025659745077403498ITERATION : 47, loss : 0.02565974477669032ITERATION : 48, loss : 0.025659744567435427ITERATION : 49, loss : 0.025659744385324985ITERATION : 50, loss : 0.025659744289125406ITERATION : 51, loss : 0.025659744248949366ITERATION : 52, loss : 0.02565974420766851ITERATION : 53, loss : 0.02565974419215875ITERATION : 54, loss : 0.02565974411185343ITERATION : 55, loss : 0.02565974411185343ITERATION : 56, loss : 0.02565974411185343ITERATION : 57, loss : 0.02565974411185343ITERATION : 58, loss : 0.02565974411185343ITERATION : 59, loss : 0.02565974411185343ITERATION : 60, loss : 0.02565974411185343ITERATION : 61, loss : 0.02565974411185343ITERATION : 62, loss : 0.02565974411185343ITERATION : 63, loss : 0.02565974411185343ITERATION : 64, loss : 0.02565974411185343ITERATION : 65, loss : 0.02565974411185343ITERATION : 66, loss : 0.02565974411185343ITERATION : 67, loss : 0.02565974411185343ITERATION : 68, loss : 0.02565974411185343ITERATION : 69, loss : 0.02565974411185343ITERATION : 70, loss : 0.02565974411185343ITERATION : 71, loss : 0.02565974411185343ITERATION : 72, loss : 0.02565974411185343ITERATION : 73, loss : 0.02565974411185343ITERATION : 74, loss : 0.02565974411185343ITERATION : 75, loss : 0.02565974411185343ITERATION : 76, loss : 0.02565974411185343ITERATION : 77, loss : 0.02565974411185343ITERATION : 78, loss : 0.02565974411185343ITERATION : 79, loss : 0.02565974411185343ITERATION : 80, loss : 0.02565974411185343ITERATION : 81, loss : 0.02565974411185343ITERATION : 82, loss : 0.02565974411185343ITERATION : 83, loss : 0.02565974411185343ITERATION : 84, loss : 0.02565974411185343ITERATION : 85, loss : 0.02565974411185343ITERATION : 86, loss : 0.02565974411185343ITERATION : 87, loss : 0.02565974411185343ITERATION : 88, loss : 0.02565974411185343ITERATION : 89, loss : 0.02565974411185343ITERATION : 90, loss : 0.02565974411185343ITERATION : 91, loss : 0.02565974411185343ITERATION : 92, loss : 0.02565974411185343ITERATION : 93, loss : 0.02565974411185343ITERATION : 94, loss : 0.02565974411185343ITERATION : 95, loss : 0.02565974411185343ITERATION : 96, loss : 0.02565974411185343ITERATION : 97, loss : 0.02565974411185343ITERATION : 98, loss : 0.02565974411185343ITERATION : 99, loss : 0.02565974411185343ITERATION : 100, loss : 0.02565974411185343
ITERATION : 1, loss : 0.03905880148009272ITERATION : 2, loss : 0.03268565005105262ITERATION : 3, loss : 0.028949603789194537ITERATION : 4, loss : 0.02714192018215107ITERATION : 5, loss : 0.02625238513237294ITERATION : 6, loss : 0.02579148425374153ITERATION : 7, loss : 0.025538252792136514ITERATION : 8, loss : 0.025391162856008568ITERATION : 9, loss : 0.025301600044791962ITERATION : 10, loss : 0.025245020211295467ITERATION : 11, loss : 0.02520829352707235ITERATION : 12, loss : 0.025183989509777252ITERATION : 13, loss : 0.025167688627972855ITERATION : 14, loss : 0.025156653548618502ITERATION : 15, loss : 0.02514913522807572ITERATION : 16, loss : 0.025143989908960003ITERATION : 17, loss : 0.025140457599737057ITERATION : 18, loss : 0.025138027349709175ITERATION : 19, loss : 0.02513635258158484ITERATION : 20, loss : 0.0251351970797356ITERATION : 21, loss : 0.025134399171345044ITERATION : 22, loss : 0.025133847771225105ITERATION : 23, loss : 0.025133466523999167ITERATION : 24, loss : 0.025133202821335535ITERATION : 25, loss : 0.02513302033639447ITERATION : 26, loss : 0.025132894043957462ITERATION : 27, loss : 0.025132806569450335ITERATION : 28, loss : 0.025132745996839106ITERATION : 29, loss : 0.025132704020528827ITERATION : 30, loss : 0.025132674933211363ITERATION : 31, loss : 0.02513265477088908ITERATION : 32, loss : 0.02513264079934232ITERATION : 33, loss : 0.025132631104831275ITERATION : 34, loss : 0.025132624381291456ITERATION : 35, loss : 0.025132619713320162ITERATION : 36, loss : 0.025132616466804338ITERATION : 37, loss : 0.02513261422480105ITERATION : 38, loss : 0.025132612671395434ITERATION : 39, loss : 0.02513261157525648ITERATION : 40, loss : 0.025132610805113944ITERATION : 41, loss : 0.025132610304410583ITERATION : 42, loss : 0.02513260992348322ITERATION : 43, loss : 0.025132609695275455ITERATION : 44, loss : 0.025132609503648606ITERATION : 45, loss : 0.025132609406106222ITERATION : 46, loss : 0.02513260928741238ITERATION : 47, loss : 0.025132609236119357ITERATION : 48, loss : 0.02513260920227653ITERATION : 49, loss : 0.02513260917313342ITERATION : 50, loss : 0.025132609163198365ITERATION : 51, loss : 0.02513260914445427ITERATION : 52, loss : 0.025132609139224573ITERATION : 53, loss : 0.025132609139224573ITERATION : 54, loss : 0.025132609139224573ITERATION : 55, loss : 0.025132609139224573ITERATION : 56, loss : 0.025132609139224573ITERATION : 57, loss : 0.025132609139224573ITERATION : 58, loss : 0.025132609139224573ITERATION : 59, loss : 0.025132609139224573ITERATION : 60, loss : 0.025132609139224573ITERATION : 61, loss : 0.025132609139224573ITERATION : 62, loss : 0.025132609139224573ITERATION : 63, loss : 0.025132609139224573ITERATION : 64, loss : 0.025132609139224573ITERATION : 65, loss : 0.025132609139224573ITERATION : 66, loss : 0.025132609139224573ITERATION : 67, loss : 0.025132609139224573ITERATION : 68, loss : 0.025132609139224573ITERATION : 69, loss : 0.025132609139224573ITERATION : 70, loss : 0.025132609139224573ITERATION : 71, loss : 0.025132609139224573ITERATION : 72, loss : 0.025132609139224573ITERATION : 73, loss : 0.025132609139224573ITERATION : 74, loss : 0.025132609139224573ITERATION : 75, loss : 0.025132609139224573ITERATION : 76, loss : 0.025132609139224573ITERATION : 77, loss : 0.025132609139224573ITERATION : 78, loss : 0.025132609139224573ITERATION : 79, loss : 0.025132609139224573ITERATION : 80, loss : 0.025132609139224573ITERATION : 81, loss : 0.025132609139224573ITERATION : 82, loss : 0.025132609139224573ITERATION : 83, loss : 0.025132609139224573ITERATION : 84, loss : 0.025132609139224573ITERATION : 85, loss : 0.025132609139224573ITERATION : 86, loss : 0.025132609139224573ITERATION : 87, loss : 0.025132609139224573ITERATION : 88, loss : 0.025132609139224573ITERATION : 89, loss : 0.025132609139224573ITERATION : 90, loss : 0.025132609139224573ITERATION : 91, loss : 0.025132609139224573ITERATION : 92, loss : 0.025132609139224573ITERATION : 93, loss : 0.025132609139224573ITERATION : 94, loss : 0.025132609139224573ITERATION : 95, loss : 0.025132609139224573ITERATION : 96, loss : 0.025132609139224573ITERATION : 97, loss : 0.025132609139224573ITERATION : 98, loss : 0.025132609139224573ITERATION : 99, loss : 0.025132609139224573ITERATION : 100, loss : 0.025132609139224573
ITERATION : 1, loss : 0.021108988643984677ITERATION : 2, loss : 0.018829100175487844ITERATION : 3, loss : 0.01881964924081484ITERATION : 4, loss : 0.019136853279145268ITERATION : 5, loss : 0.01944965954407894ITERATION : 6, loss : 0.019679568857859524ITERATION : 7, loss : 0.01977391173680053ITERATION : 8, loss : 0.019804934445581953ITERATION : 9, loss : 0.019832925483585184ITERATION : 10, loss : 0.01985585907986985ITERATION : 11, loss : 0.019873824009692058ITERATION : 12, loss : 0.019887562299211608ITERATION : 13, loss : 0.019897921834281795ITERATION : 14, loss : 0.019905665831745432ITERATION : 15, loss : 0.019911422114276902ITERATION : 16, loss : 0.01991568480126331ITERATION : 17, loss : 0.019918833344417623ITERATION : 18, loss : 0.019921154890881705ITERATION : 19, loss : 0.01992286454801925ITERATION : 20, loss : 0.01992412248326762ITERATION : 21, loss : 0.019925047501155684ITERATION : 22, loss : 0.019925727456020132ITERATION : 23, loss : 0.019926227123430597ITERATION : 24, loss : 0.019926594229279407ITERATION : 25, loss : 0.019926863932247105ITERATION : 26, loss : 0.019927062054391025ITERATION : 27, loss : 0.01992720758091916ITERATION : 28, loss : 0.019927314488595114ITERATION : 29, loss : 0.019927393031251496ITERATION : 30, loss : 0.019927450733622235ITERATION : 31, loss : 0.019927493127462902ITERATION : 32, loss : 0.019927524263255402ITERATION : 33, loss : 0.019927547177376787ITERATION : 34, loss : 0.019927563987827395ITERATION : 35, loss : 0.019927576332379533ITERATION : 36, loss : 0.019927585407775807ITERATION : 37, loss : 0.019927592070165243ITERATION : 38, loss : 0.01992759695445826ITERATION : 39, loss : 0.019927600558284103ITERATION : 40, loss : 0.019927603213329995ITERATION : 41, loss : 0.019927605171071033ITERATION : 42, loss : 0.019927606610716228ITERATION : 43, loss : 0.019927607671620964ITERATION : 44, loss : 0.019927608445724512ITERATION : 45, loss : 0.019927609015141262ITERATION : 46, loss : 0.019927609482338425ITERATION : 47, loss : 0.019927609818506758ITERATION : 48, loss : 0.01992761004642603ITERATION : 49, loss : 0.019927610201873558ITERATION : 50, loss : 0.01992761030837017ITERATION : 51, loss : 0.019927610391690644ITERATION : 52, loss : 0.019927610435235662ITERATION : 53, loss : 0.01992761048810298ITERATION : 54, loss : 0.019927610508751414ITERATION : 55, loss : 0.019927610532665854ITERATION : 56, loss : 0.01992761053308684ITERATION : 57, loss : 0.01992761053308684ITERATION : 58, loss : 0.01992761053308684ITERATION : 59, loss : 0.01992761053308684ITERATION : 60, loss : 0.01992761053308684ITERATION : 61, loss : 0.01992761053308684ITERATION : 62, loss : 0.01992761053308684ITERATION : 63, loss : 0.01992761053308684ITERATION : 64, loss : 0.01992761053308684ITERATION : 65, loss : 0.01992761053308684ITERATION : 66, loss : 0.01992761053308684ITERATION : 67, loss : 0.01992761053308684ITERATION : 68, loss : 0.01992761053308684ITERATION : 69, loss : 0.01992761053308684ITERATION : 70, loss : 0.01992761053308684ITERATION : 71, loss : 0.01992761053308684ITERATION : 72, loss : 0.01992761053308684ITERATION : 73, loss : 0.01992761053308684ITERATION : 74, loss : 0.01992761053308684ITERATION : 75, loss : 0.01992761053308684ITERATION : 76, loss : 0.01992761053308684ITERATION : 77, loss : 0.01992761053308684ITERATION : 78, loss : 0.01992761053308684ITERATION : 79, loss : 0.01992761053308684ITERATION : 80, loss : 0.01992761053308684ITERATION : 81, loss : 0.01992761053308684ITERATION : 82, loss : 0.01992761053308684ITERATION : 83, loss : 0.01992761053308684ITERATION : 84, loss : 0.01992761053308684ITERATION : 85, loss : 0.01992761053308684ITERATION : 86, loss : 0.01992761053308684ITERATION : 87, loss : 0.01992761053308684ITERATION : 88, loss : 0.01992761053308684ITERATION : 89, loss : 0.01992761053308684ITERATION : 90, loss : 0.01992761053308684ITERATION : 91, loss : 0.01992761053308684ITERATION : 92, loss : 0.01992761053308684ITERATION : 93, loss : 0.01992761053308684ITERATION : 94, loss : 0.01992761053308684ITERATION : 95, loss : 0.01992761053308684ITERATION : 96, loss : 0.01992761053308684ITERATION : 97, loss : 0.01992761053308684ITERATION : 98, loss : 0.01992761053308684ITERATION : 99, loss : 0.01992761053308684ITERATION : 100, loss : 0.01992761053308684
ITERATION : 1, loss : 0.030860558606004774ITERATION : 2, loss : 0.029050872701826217ITERATION : 3, loss : 0.025802720709010882ITERATION : 4, loss : 0.024118424771449005ITERATION : 5, loss : 0.023142967972970076ITERATION : 6, loss : 0.022548038122663937ITERATION : 7, loss : 0.022172442135619166ITERATION : 8, loss : 0.021929071671954153ITERATION : 9, loss : 0.021768204290803294ITERATION : 10, loss : 0.021660249324880372ITERATION : 11, loss : 0.021586975140006175ITERATION : 12, loss : 0.021536819333735448ITERATION : 13, loss : 0.021502273982450776ITERATION : 14, loss : 0.02147837184537905ITERATION : 15, loss : 0.02146177872329956ITERATION : 16, loss : 0.021450231494639985ITERATION : 17, loss : 0.0214421815091473ITERATION : 18, loss : 0.02143656230137638ITERATION : 19, loss : 0.021432636121347517ITERATION : 20, loss : 0.02142989090124135ITERATION : 21, loss : 0.02142797049809578ITERATION : 22, loss : 0.021426626449365653ITERATION : 23, loss : 0.021425685570120593ITERATION : 24, loss : 0.021425026811556315ITERATION : 25, loss : 0.021424565437556855ITERATION : 26, loss : 0.02142424226117697ITERATION : 27, loss : 0.021424015870930437ITERATION : 28, loss : 0.021423857237376935ITERATION : 29, loss : 0.021423746131395156ITERATION : 30, loss : 0.02142366827729321ITERATION : 31, loss : 0.021423613722303236ITERATION : 32, loss : 0.02142357551749566ITERATION : 33, loss : 0.02142354871408303ITERATION : 34, loss : 0.021423529931515285ITERATION : 35, loss : 0.021423516775488833ITERATION : 36, loss : 0.021423507535237654ITERATION : 37, loss : 0.021423501100903363ITERATION : 38, loss : 0.0214234965499276ITERATION : 39, loss : 0.021423493389063412ITERATION : 40, loss : 0.021423491134866305ITERATION : 41, loss : 0.021423489600207703ITERATION : 42, loss : 0.021423488449437977ITERATION : 43, loss : 0.02142348772460713ITERATION : 44, loss : 0.0214234871731943ITERATION : 45, loss : 0.021423486815945303ITERATION : 46, loss : 0.021423486583209597ITERATION : 47, loss : 0.02142348638623388ITERATION : 48, loss : 0.021423486254540148ITERATION : 49, loss : 0.021423486136390325ITERATION : 50, loss : 0.021423486103087606ITERATION : 51, loss : 0.021423486042479546ITERATION : 52, loss : 0.021423486036051716ITERATION : 53, loss : 0.021423486036051716ITERATION : 54, loss : 0.021423486036051716ITERATION : 55, loss : 0.021423486036051716ITERATION : 56, loss : 0.021423486036051716ITERATION : 57, loss : 0.021423486036051716ITERATION : 58, loss : 0.021423486036051716ITERATION : 59, loss : 0.021423486036051716ITERATION : 60, loss : 0.021423486036051716ITERATION : 61, loss : 0.021423486036051716ITERATION : 62, loss : 0.021423486036051716ITERATION : 63, loss : 0.021423486036051716ITERATION : 64, loss : 0.021423486036051716ITERATION : 65, loss : 0.021423486036051716ITERATION : 66, loss : 0.021423486036051716ITERATION : 67, loss : 0.021423486036051716ITERATION : 68, loss : 0.021423486036051716ITERATION : 69, loss : 0.021423486036051716ITERATION : 70, loss : 0.021423486036051716ITERATION : 71, loss : 0.021423486036051716ITERATION : 72, loss : 0.021423486036051716ITERATION : 73, loss : 0.021423486036051716ITERATION : 74, loss : 0.021423486036051716ITERATION : 75, loss : 0.021423486036051716ITERATION : 76, loss : 0.021423486036051716ITERATION : 77, loss : 0.021423486036051716ITERATION : 78, loss : 0.021423486036051716ITERATION : 79, loss : 0.021423486036051716ITERATION : 80, loss : 0.021423486036051716ITERATION : 81, loss : 0.021423486036051716ITERATION : 82, loss : 0.021423486036051716ITERATION : 83, loss : 0.021423486036051716ITERATION : 84, loss : 0.021423486036051716ITERATION : 85, loss : 0.021423486036051716ITERATION : 86, loss : 0.021423486036051716ITERATION : 87, loss : 0.021423486036051716ITERATION : 88, loss : 0.021423486036051716ITERATION : 89, loss : 0.021423486036051716ITERATION : 90, loss : 0.021423486036051716ITERATION : 91, loss : 0.021423486036051716ITERATION : 92, loss : 0.021423486036051716ITERATION : 93, loss : 0.021423486036051716ITERATION : 94, loss : 0.021423486036051716ITERATION : 95, loss : 0.021423486036051716ITERATION : 96, loss : 0.021423486036051716ITERATION : 97, loss : 0.021423486036051716ITERATION : 98, loss : 0.021423486036051716ITERATION : 99, loss : 0.021423486036051716ITERATION : 100, loss : 0.021423486036051716
ITERATION : 1, loss : 0.014746488552702941ITERATION : 2, loss : 0.013527279290537056ITERATION : 3, loss : 0.012807985046542698ITERATION : 4, loss : 0.012323152591954689ITERATION : 5, loss : 0.012008959423733005ITERATION : 6, loss : 0.01180570913374777ITERATION : 7, loss : 0.011672519601850407ITERATION : 8, loss : 0.011583955830868464ITERATION : 9, loss : 0.011524322517588967ITERATION : 10, loss : 0.011483767700322715ITERATION : 11, loss : 0.01145597408184569ITERATION : 12, loss : 0.011436812420564177ITERATION : 13, loss : 0.011423540667218117ITERATION : 14, loss : 0.011414315205472411ITERATION : 15, loss : 0.011407884206711913ITERATION : 16, loss : 0.011403391137883792ITERATION : 17, loss : 0.011400246387207645ITERATION : 18, loss : 0.011398042181831132ITERATION : 19, loss : 0.011396495396470694ITERATION : 20, loss : 0.011395408922394101ITERATION : 21, loss : 0.011394645201260743ITERATION : 22, loss : 0.011394107922354318ITERATION : 23, loss : 0.011393729753661645ITERATION : 24, loss : 0.011393463462613407ITERATION : 25, loss : 0.011393275877091429ITERATION : 26, loss : 0.011393143609665993ITERATION : 27, loss : 0.011393050400596674ITERATION : 28, loss : 0.011392984667310944ITERATION : 29, loss : 0.011392938303528485ITERATION : 30, loss : 0.011392905540384824ITERATION : 31, loss : 0.011392882419095684ITERATION : 32, loss : 0.011392866109537634ITERATION : 33, loss : 0.01139285459200823ITERATION : 34, loss : 0.011392846441274024ITERATION : 35, loss : 0.011392840715950954ITERATION : 36, loss : 0.011392836674744278ITERATION : 37, loss : 0.011392833783476036ITERATION : 38, loss : 0.011392831739403397ITERATION : 39, loss : 0.011392830295557491ITERATION : 40, loss : 0.011392829300477544ITERATION : 41, loss : 0.011392828573948747ITERATION : 42, loss : 0.01139282811550409ITERATION : 43, loss : 0.011392827748532458ITERATION : 44, loss : 0.011392827476900838ITERATION : 45, loss : 0.011392827306085764ITERATION : 46, loss : 0.011392827195735719ITERATION : 47, loss : 0.011392827086566209ITERATION : 48, loss : 0.01139282701335343ITERATION : 49, loss : 0.011392827005901237ITERATION : 50, loss : 0.011392827005901237ITERATION : 51, loss : 0.011392827005901237ITERATION : 52, loss : 0.011392827005901237ITERATION : 53, loss : 0.011392827005901237ITERATION : 54, loss : 0.011392827005901237ITERATION : 55, loss : 0.011392827005901237ITERATION : 56, loss : 0.011392827005901237ITERATION : 57, loss : 0.011392827005901237ITERATION : 58, loss : 0.011392827005901237ITERATION : 59, loss : 0.011392827005901237ITERATION : 60, loss : 0.011392827005901237ITERATION : 61, loss : 0.011392827005901237ITERATION : 62, loss : 0.011392827005901237ITERATION : 63, loss : 0.011392827005901237ITERATION : 64, loss : 0.011392827005901237ITERATION : 65, loss : 0.011392827005901237ITERATION : 66, loss : 0.011392827005901237ITERATION : 67, loss : 0.011392827005901237ITERATION : 68, loss : 0.011392827005901237ITERATION : 69, loss : 0.011392827005901237ITERATION : 70, loss : 0.011392827005901237ITERATION : 71, loss : 0.011392827005901237ITERATION : 72, loss : 0.011392827005901237ITERATION : 73, loss : 0.011392827005901237ITERATION : 74, loss : 0.011392827005901237ITERATION : 75, loss : 0.011392827005901237ITERATION : 76, loss : 0.011392827005901237ITERATION : 77, loss : 0.011392827005901237ITERATION : 78, loss : 0.011392827005901237ITERATION : 79, loss : 0.011392827005901237ITERATION : 80, loss : 0.011392827005901237ITERATION : 81, loss : 0.011392827005901237ITERATION : 82, loss : 0.011392827005901237ITERATION : 83, loss : 0.011392827005901237ITERATION : 84, loss : 0.011392827005901237ITERATION : 85, loss : 0.011392827005901237ITERATION : 86, loss : 0.011392827005901237ITERATION : 87, loss : 0.011392827005901237ITERATION : 88, loss : 0.011392827005901237ITERATION : 89, loss : 0.011392827005901237ITERATION : 90, loss : 0.011392827005901237ITERATION : 91, loss : 0.011392827005901237ITERATION : 92, loss : 0.011392827005901237ITERATION : 93, loss : 0.011392827005901237ITERATION : 94, loss : 0.011392827005901237ITERATION : 95, loss : 0.011392827005901237ITERATION : 96, loss : 0.011392827005901237ITERATION : 97, loss : 0.011392827005901237ITERATION : 98, loss : 0.011392827005901237ITERATION : 99, loss : 0.011392827005901237ITERATION : 100, loss : 0.011392827005901237
ITERATION : 1, loss : 0.040286402311520454ITERATION : 2, loss : 0.029141350777240557ITERATION : 3, loss : 0.026616458302573934ITERATION : 4, loss : 0.02576517151796417ITERATION : 5, loss : 0.025314161364813858ITERATION : 6, loss : 0.02516071275523693ITERATION : 7, loss : 0.02511421676855075ITERATION : 8, loss : 0.025105515632599457ITERATION : 9, loss : 0.025071745605035285ITERATION : 10, loss : 0.02503508770473258ITERATION : 11, loss : 0.025009352706927845ITERATION : 12, loss : 0.024991280310648695ITERATION : 13, loss : 0.024978590242066317ITERATION : 14, loss : 0.024969680891657298ITERATION : 15, loss : 0.02496342614795978ITERATION : 16, loss : 0.02495903565643341ITERATION : 17, loss : 0.024955953498623517ITERATION : 18, loss : 0.024953790376606312ITERATION : 19, loss : 0.024952272343325244ITERATION : 20, loss : 0.02495120692558932ITERATION : 21, loss : 0.024950459693915367ITERATION : 22, loss : 0.024949935589058905ITERATION : 23, loss : 0.024949568027488017ITERATION : 24, loss : 0.024949310356220777ITERATION : 25, loss : 0.024949129780887223ITERATION : 26, loss : 0.024949003210842313ITERATION : 27, loss : 0.02494891467124468ITERATION : 28, loss : 0.02494885260853472ITERATION : 29, loss : 0.02494880915045599ITERATION : 30, loss : 0.024948778745047417ITERATION : 31, loss : 0.024948757490144035ITERATION : 32, loss : 0.024948742671224593ITERATION : 33, loss : 0.02494873220768811ITERATION : 34, loss : 0.024948725012795412ITERATION : 35, loss : 0.02494871990783486ITERATION : 36, loss : 0.024948716435989574ITERATION : 37, loss : 0.024948713923663667ITERATION : 38, loss : 0.024948712143972856ITERATION : 39, loss : 0.024948711013201372ITERATION : 40, loss : 0.024948710198987287ITERATION : 41, loss : 0.02494870958354613ITERATION : 42, loss : 0.024948709134077162ITERATION : 43, loss : 0.024948708871137204ITERATION : 44, loss : 0.02494870867235131ITERATION : 45, loss : 0.024948708600586355ITERATION : 46, loss : 0.024948708529569107ITERATION : 47, loss : 0.024948708524023283ITERATION : 48, loss : 0.024948708524023283ITERATION : 49, loss : 0.024948708524023283ITERATION : 50, loss : 0.024948708524023283ITERATION : 51, loss : 0.024948708524023283ITERATION : 52, loss : 0.024948708524023283ITERATION : 53, loss : 0.024948708524023283ITERATION : 54, loss : 0.024948708524023283ITERATION : 55, loss : 0.024948708524023283ITERATION : 56, loss : 0.024948708524023283ITERATION : 57, loss : 0.024948708524023283ITERATION : 58, loss : 0.024948708524023283ITERATION : 59, loss : 0.024948708524023283ITERATION : 60, loss : 0.024948708524023283ITERATION : 61, loss : 0.024948708524023283ITERATION : 62, loss : 0.024948708524023283ITERATION : 63, loss : 0.024948708524023283ITERATION : 64, loss : 0.024948708524023283ITERATION : 65, loss : 0.024948708524023283ITERATION : 66, loss : 0.024948708524023283ITERATION : 67, loss : 0.024948708524023283ITERATION : 68, loss : 0.024948708524023283ITERATION : 69, loss : 0.024948708524023283ITERATION : 70, loss : 0.024948708524023283ITERATION : 71, loss : 0.024948708524023283ITERATION : 72, loss : 0.024948708524023283ITERATION : 73, loss : 0.024948708524023283ITERATION : 74, loss : 0.024948708524023283ITERATION : 75, loss : 0.024948708524023283ITERATION : 76, loss : 0.024948708524023283ITERATION : 77, loss : 0.024948708524023283ITERATION : 78, loss : 0.024948708524023283ITERATION : 79, loss : 0.024948708524023283ITERATION : 80, loss : 0.024948708524023283ITERATION : 81, loss : 0.024948708524023283ITERATION : 82, loss : 0.024948708524023283ITERATION : 83, loss : 0.024948708524023283ITERATION : 84, loss : 0.024948708524023283ITERATION : 85, loss : 0.024948708524023283ITERATION : 86, loss : 0.024948708524023283ITERATION : 87, loss : 0.024948708524023283ITERATION : 88, loss : 0.024948708524023283ITERATION : 89, loss : 0.024948708524023283ITERATION : 90, loss : 0.024948708524023283ITERATION : 91, loss : 0.024948708524023283ITERATION : 92, loss : 0.024948708524023283ITERATION : 93, loss : 0.024948708524023283ITERATION : 94, loss : 0.024948708524023283ITERATION : 95, loss : 0.024948708524023283ITERATION : 96, loss : 0.024948708524023283ITERATION : 97, loss : 0.024948708524023283ITERATION : 98, loss : 0.024948708524023283ITERATION : 99, loss : 0.024948708524023283ITERATION : 100, loss : 0.024948708524023283
ITERATION : 1, loss : 0.05686418485049813ITERATION : 2, loss : 0.04320871647660937ITERATION : 3, loss : 0.03965408121705385ITERATION : 4, loss : 0.0379685852543758ITERATION : 5, loss : 0.03699839355784923ITERATION : 6, loss : 0.03638001456171906ITERATION : 7, loss : 0.035964079279623334ITERATION : 8, loss : 0.035675727598786296ITERATION : 9, loss : 0.035638185468695474ITERATION : 10, loss : 0.035766799618151615ITERATION : 11, loss : 0.03586393984947733ITERATION : 12, loss : 0.03593620805727888ITERATION : 13, loss : 0.03598945565015454ITERATION : 14, loss : 0.03602843759883522ITERATION : 15, loss : 0.036056850954647904ITERATION : 16, loss : 0.03607749766402474ITERATION : 17, loss : 0.036092468202882526ITERATION : 18, loss : 0.03610330622163271ITERATION : 19, loss : 0.03611114372038467ITERATION : 20, loss : 0.036116806744145295ITERATION : 21, loss : 0.03612089622727336ITERATION : 22, loss : 0.036123848041096515ITERATION : 23, loss : 0.03612597803478724ITERATION : 24, loss : 0.036127514657822206ITERATION : 25, loss : 0.03612862302619426ITERATION : 26, loss : 0.036129422389695784ITERATION : 27, loss : 0.036129998848897ITERATION : 28, loss : 0.03613041452507479ITERATION : 29, loss : 0.03613071426964819ITERATION : 30, loss : 0.036130930379690326ITERATION : 31, loss : 0.03613108618310391ITERATION : 32, loss : 0.03613119853566621ITERATION : 33, loss : 0.03613127955084865ITERATION : 34, loss : 0.03613133793866777ITERATION : 35, loss : 0.03613138002808563ITERATION : 36, loss : 0.03613141039372164ITERATION : 37, loss : 0.03613143230273906ITERATION : 38, loss : 0.03613144811114509ITERATION : 39, loss : 0.03613145948950515ITERATION : 40, loss : 0.03613146767664022ITERATION : 41, loss : 0.036131473583158234ITERATION : 42, loss : 0.036131477820315394ITERATION : 43, loss : 0.03613148088559694ITERATION : 44, loss : 0.03613148306939254ITERATION : 45, loss : 0.03613148467382562ITERATION : 46, loss : 0.03613148579506774ITERATION : 47, loss : 0.03613148663887781ITERATION : 48, loss : 0.036131487203279654ITERATION : 49, loss : 0.03613148765619192ITERATION : 50, loss : 0.03613148793292048ITERATION : 51, loss : 0.03613148817349816ITERATION : 52, loss : 0.03613148830551926ITERATION : 53, loss : 0.036131488440166615ITERATION : 54, loss : 0.03613148846884721ITERATION : 55, loss : 0.03613148853905276ITERATION : 56, loss : 0.036131488542645265ITERATION : 57, loss : 0.03613148854353147ITERATION : 58, loss : 0.03613148854353147ITERATION : 59, loss : 0.03613148854353147ITERATION : 60, loss : 0.03613148854353147ITERATION : 61, loss : 0.03613148854353147ITERATION : 62, loss : 0.03613148854353147ITERATION : 63, loss : 0.03613148854353147ITERATION : 64, loss : 0.03613148854353147ITERATION : 65, loss : 0.03613148854353147ITERATION : 66, loss : 0.03613148854353147ITERATION : 67, loss : 0.03613148854353147ITERATION : 68, loss : 0.03613148854353147ITERATION : 69, loss : 0.03613148854353147ITERATION : 70, loss : 0.03613148854353147ITERATION : 71, loss : 0.03613148854353147ITERATION : 72, loss : 0.03613148854353147ITERATION : 73, loss : 0.03613148854353147ITERATION : 74, loss : 0.03613148854353147ITERATION : 75, loss : 0.03613148854353147ITERATION : 76, loss : 0.03613148854353147ITERATION : 77, loss : 0.03613148854353147ITERATION : 78, loss : 0.03613148854353147ITERATION : 79, loss : 0.03613148854353147ITERATION : 80, loss : 0.03613148854353147ITERATION : 81, loss : 0.03613148854353147ITERATION : 82, loss : 0.03613148854353147ITERATION : 83, loss : 0.03613148854353147ITERATION : 84, loss : 0.03613148854353147ITERATION : 85, loss : 0.03613148854353147ITERATION : 86, loss : 0.03613148854353147ITERATION : 87, loss : 0.03613148854353147ITERATION : 88, loss : 0.03613148854353147ITERATION : 89, loss : 0.03613148854353147ITERATION : 90, loss : 0.03613148854353147ITERATION : 91, loss : 0.03613148854353147ITERATION : 92, loss : 0.03613148854353147ITERATION : 93, loss : 0.03613148854353147ITERATION : 94, loss : 0.03613148854353147ITERATION : 95, loss : 0.03613148854353147ITERATION : 96, loss : 0.03613148854353147ITERATION : 97, loss : 0.03613148854353147ITERATION : 98, loss : 0.03613148854353147ITERATION : 99, loss : 0.03613148854353147ITERATION : 100, loss : 0.03613148854353147
ITERATION : 1, loss : 0.03571742737048587ITERATION : 2, loss : 0.022482810444341056ITERATION : 3, loss : 0.018076054096672164ITERATION : 4, loss : 0.016259406729492533ITERATION : 5, loss : 0.01541340233085121ITERATION : 6, loss : 0.0149818812659752ITERATION : 7, loss : 0.014744515979867003ITERATION : 8, loss : 0.01460556947125655ITERATION : 9, loss : 0.01452014721238208ITERATION : 10, loss : 0.014465657892922184ITERATION : 11, loss : 0.01442995695737336ITERATION : 12, loss : 0.014406116299315834ITERATION : 13, loss : 0.014389980329067227ITERATION : 14, loss : 0.014378953900178723ITERATION : 15, loss : 0.014371366696535381ITERATION : 16, loss : 0.014366119000530198ITERATION : 17, loss : 0.014362474999502552ITERATION : 18, loss : 0.014359936683904211ITERATION : 19, loss : 0.014358163869415894ITERATION : 20, loss : 0.014356922967236008ITERATION : 21, loss : 0.014356052695037844ITERATION : 22, loss : 0.014355441302995181ITERATION : 23, loss : 0.014355011110122079ITERATION : 24, loss : 0.014354707955975878ITERATION : 25, loss : 0.014354493998088611ITERATION : 26, loss : 0.01435434281844596ITERATION : 27, loss : 0.014354235831604363ITERATION : 28, loss : 0.014354160108734824ITERATION : 29, loss : 0.014354106426527855ITERATION : 30, loss : 0.014354068354390646ITERATION : 31, loss : 0.014354041294927159ITERATION : 32, loss : 0.014354022059012466ITERATION : 33, loss : 0.014354008361017705ITERATION : 34, loss : 0.014353998614901985ITERATION : 35, loss : 0.014353991646162498ITERATION : 36, loss : 0.014353986674224436ITERATION : 37, loss : 0.01435398313264356ITERATION : 38, loss : 0.014353980615671984ITERATION : 39, loss : 0.01435397881018147ITERATION : 40, loss : 0.014353977471171129ITERATION : 41, loss : 0.014353976560028176ITERATION : 42, loss : 0.014353975873151962ITERATION : 43, loss : 0.01435397542852842ITERATION : 44, loss : 0.014353975110071797ITERATION : 45, loss : 0.01435397485529144ITERATION : 46, loss : 0.014353974663299882ITERATION : 47, loss : 0.014353974556250282ITERATION : 48, loss : 0.014353974458475279ITERATION : 49, loss : 0.014353974419010573ITERATION : 50, loss : 0.014353974381521008ITERATION : 51, loss : 0.014353974370533422ITERATION : 52, loss : 0.014353974370932618ITERATION : 53, loss : 0.014353974370539474ITERATION : 54, loss : 0.014353974370539474ITERATION : 55, loss : 0.014353974370539474ITERATION : 56, loss : 0.014353974370539474ITERATION : 57, loss : 0.014353974370539474ITERATION : 58, loss : 0.014353974370539474ITERATION : 59, loss : 0.014353974370539474ITERATION : 60, loss : 0.014353974370539474ITERATION : 61, loss : 0.014353974370539474ITERATION : 62, loss : 0.014353974370539474ITERATION : 63, loss : 0.014353974370539474ITERATION : 64, loss : 0.014353974370539474ITERATION : 65, loss : 0.014353974370539474ITERATION : 66, loss : 0.014353974370539474ITERATION : 67, loss : 0.014353974370539474ITERATION : 68, loss : 0.014353974370539474ITERATION : 69, loss : 0.014353974370539474ITERATION : 70, loss : 0.014353974370539474ITERATION : 71, loss : 0.014353974370539474ITERATION : 72, loss : 0.014353974370539474ITERATION : 73, loss : 0.014353974370539474ITERATION : 74, loss : 0.014353974370539474ITERATION : 75, loss : 0.014353974370539474ITERATION : 76, loss : 0.014353974370539474ITERATION : 77, loss : 0.014353974370539474ITERATION : 78, loss : 0.014353974370539474ITERATION : 79, loss : 0.014353974370539474ITERATION : 80, loss : 0.014353974370539474ITERATION : 81, loss : 0.014353974370539474ITERATION : 82, loss : 0.014353974370539474ITERATION : 83, loss : 0.014353974370539474ITERATION : 84, loss : 0.014353974370539474ITERATION : 85, loss : 0.014353974370539474ITERATION : 86, loss : 0.014353974370539474ITERATION : 87, loss : 0.014353974370539474ITERATION : 88, loss : 0.014353974370539474ITERATION : 89, loss : 0.014353974370539474ITERATION : 90, loss : 0.014353974370539474ITERATION : 91, loss : 0.014353974370539474ITERATION : 92, loss : 0.014353974370539474ITERATION : 93, loss : 0.014353974370539474ITERATION : 94, loss : 0.014353974370539474ITERATION : 95, loss : 0.014353974370539474ITERATION : 96, loss : 0.014353974370539474ITERATION : 97, loss : 0.014353974370539474ITERATION : 98, loss : 0.014353974370539474ITERATION : 99, loss : 0.014353974370539474ITERATION : 100, loss : 0.014353974370539474
gradient norm in None layer : 0.0004959687088270635
gradient norm in None layer : 2.8523612844630544e-05
gradient norm in None layer : 3.906188150819969e-05
gradient norm in None layer : 0.00043582013103404517
gradient norm in None layer : 4.225263242201032e-05
gradient norm in None layer : 5.3067165370689045e-05
gradient norm in None layer : 0.00019858816279234366
gradient norm in None layer : 9.26299380783072e-06
gradient norm in None layer : 6.934880202729903e-06
gradient norm in None layer : 0.00019059183981905378
gradient norm in None layer : 8.174643336730949e-06
gradient norm in None layer : 6.302574135543216e-06
gradient norm in None layer : 6.501662441638421e-05
gradient norm in None layer : 2.266381329769048e-06
gradient norm in None layer : 1.4520263561448722e-06
gradient norm in None layer : 5.503587850881807e-05
gradient norm in None layer : 2.552836333783401e-06
gradient norm in None layer : 1.7058047623879616e-06
gradient norm in None layer : 7.063469902893179e-05
gradient norm in None layer : 7.958350320763652e-07
gradient norm in None layer : 0.00015671338436275937
gradient norm in None layer : 1.0910117622935615e-05
gradient norm in None layer : 7.88718960551981e-06
gradient norm in None layer : 0.00019890290803112958
gradient norm in None layer : 1.859353394065889e-05
gradient norm in None layer : 2.8402433058654177e-05
gradient norm in None layer : 0.000318415138065577
gradient norm in None layer : 2.159613928718234e-06
gradient norm in None layer : 0.000558927298766239
gradient norm in None layer : 4.814439929465765e-05
gradient norm in None layer : 5.99961237318772e-05
gradient norm in None layer : 0.0006793737233672948
gradient norm in None layer : 4.373084622915766e-05
gradient norm in None layer : 5.577259070542349e-05
gradient norm in None layer : 4.0650306368362034e-05
gradient norm in None layer : 2.345880042668511e-06
Total gradient norm: 0.0012184109729865612
invariance loss : 4.336021690303944, avg_den : 0.42279815673828125, density loss : 0.32279815673828127, mse loss : 0.022371306033026506, solver time : 119.83126735687256 sec , total loss : 0.027030125880068732, running loss : 0.050852344055575815
Epoch 0/10 , batch 16/12500 
ITERATION : 1, loss : 0.018106117578932816ITERATION : 2, loss : 0.026429168052116456ITERATION : 3, loss : 0.023253663703537954ITERATION : 4, loss : 0.021308449115078124ITERATION : 5, loss : 0.020171925267310736ITERATION : 6, loss : 0.019516829256120208ITERATION : 7, loss : 0.019139438709487328ITERATION : 8, loss : 0.01892072920561628ITERATION : 9, loss : 0.01879263716663256ITERATION : 10, loss : 0.018716538990283086ITERATION : 11, loss : 0.018670542102619914ITERATION : 12, loss : 0.018642195676416222ITERATION : 13, loss : 0.018624365377696683ITERATION : 14, loss : 0.018612917664506193ITERATION : 15, loss : 0.018605422939560894ITERATION : 16, loss : 0.018600427597496364ITERATION : 17, loss : 0.018597045522648466ITERATION : 18, loss : 0.01859472459411998ITERATION : 19, loss : 0.01859311410796321ITERATION : 20, loss : 0.018591986476140045ITERATION : 21, loss : 0.018591191336811152ITERATION : 22, loss : 0.018590627496272716ITERATION : 23, loss : 0.018590225801635683ITERATION : 24, loss : 0.018589938626344633ITERATION : 25, loss : 0.01858973278803848ITERATION : 26, loss : 0.01858958502477235ITERATION : 27, loss : 0.018589478781151986ITERATION : 28, loss : 0.018589402231432833ITERATION : 29, loss : 0.0185893470490818ITERATION : 30, loss : 0.018589307302203605ITERATION : 31, loss : 0.01858927853833871ITERATION : 32, loss : 0.01858925775493203ITERATION : 33, loss : 0.018589242834254446ITERATION : 34, loss : 0.01858923199198606ITERATION : 35, loss : 0.018589224155941875ITERATION : 36, loss : 0.01858921857282308ITERATION : 37, loss : 0.018589214480512484ITERATION : 38, loss : 0.018589211565730523ITERATION : 39, loss : 0.018589209429029595ITERATION : 40, loss : 0.018589207903092094ITERATION : 41, loss : 0.01858920677184893ITERATION : 42, loss : 0.01858920598778146ITERATION : 43, loss : 0.018589205479670256ITERATION : 44, loss : 0.018589204904208376ITERATION : 45, loss : 0.018589204579747866ITERATION : 46, loss : 0.018589204426494046ITERATION : 47, loss : 0.01858920424337086ITERATION : 48, loss : 0.018589204198764746ITERATION : 49, loss : 0.018589204102964812ITERATION : 50, loss : 0.018589204091347167ITERATION : 51, loss : 0.018589204091347167ITERATION : 52, loss : 0.018589204091347167ITERATION : 53, loss : 0.018589204091347167ITERATION : 54, loss : 0.018589204091347167ITERATION : 55, loss : 0.018589204091347167ITERATION : 56, loss : 0.018589204091347167ITERATION : 57, loss : 0.018589204091347167ITERATION : 58, loss : 0.018589204091347167ITERATION : 59, loss : 0.018589204091347167ITERATION : 60, loss : 0.018589204091347167ITERATION : 61, loss : 0.018589204091347167ITERATION : 62, loss : 0.018589204091347167ITERATION : 63, loss : 0.018589204091347167ITERATION : 64, loss : 0.018589204091347167ITERATION : 65, loss : 0.018589204091347167ITERATION : 66, loss : 0.018589204091347167ITERATION : 67, loss : 0.018589204091347167ITERATION : 68, loss : 0.018589204091347167ITERATION : 69, loss : 0.018589204091347167ITERATION : 70, loss : 0.018589204091347167ITERATION : 71, loss : 0.018589204091347167ITERATION : 72, loss : 0.018589204091347167ITERATION : 73, loss : 0.018589204091347167ITERATION : 74, loss : 0.018589204091347167ITERATION : 75, loss : 0.018589204091347167ITERATION : 76, loss : 0.018589204091347167ITERATION : 77, loss : 0.018589204091347167ITERATION : 78, loss : 0.018589204091347167ITERATION : 79, loss : 0.018589204091347167ITERATION : 80, loss : 0.018589204091347167ITERATION : 81, loss : 0.018589204091347167ITERATION : 82, loss : 0.018589204091347167ITERATION : 83, loss : 0.018589204091347167ITERATION : 84, loss : 0.018589204091347167ITERATION : 85, loss : 0.018589204091347167ITERATION : 86, loss : 0.018589204091347167ITERATION : 87, loss : 0.018589204091347167ITERATION : 88, loss : 0.018589204091347167ITERATION : 89, loss : 0.018589204091347167ITERATION : 90, loss : 0.018589204091347167ITERATION : 91, loss : 0.018589204091347167ITERATION : 92, loss : 0.018589204091347167ITERATION : 93, loss : 0.018589204091347167ITERATION : 94, loss : 0.018589204091347167ITERATION : 95, loss : 0.018589204091347167ITERATION : 96, loss : 0.018589204091347167ITERATION : 97, loss : 0.018589204091347167ITERATION : 98, loss : 0.018589204091347167ITERATION : 99, loss : 0.018589204091347167ITERATION : 100, loss : 0.018589204091347167
ITERATION : 1, loss : 0.02259571655433209ITERATION : 2, loss : 0.022634438077690385ITERATION : 3, loss : 0.025145346857008713ITERATION : 4, loss : 0.027508870108774723ITERATION : 5, loss : 0.029403451665756128ITERATION : 6, loss : 0.03085852199171318ITERATION : 7, loss : 0.031957099246001694ITERATION : 8, loss : 0.032779191893241494ITERATION : 9, loss : 0.03339107940661802ITERATION : 10, loss : 0.033831086388163345ITERATION : 11, loss : 0.0341559720157789ITERATION : 12, loss : 0.034395766418389886ITERATION : 13, loss : 0.03457261542871399ITERATION : 14, loss : 0.034702976409946254ITERATION : 15, loss : 0.034799041288463235ITERATION : 16, loss : 0.034869822509792356ITERATION : 17, loss : 0.03492197237688597ITERATION : 18, loss : 0.034960396444586594ITERATION : 19, loss : 0.034988709763461884ITERATION : 20, loss : 0.035009575384877596ITERATION : 21, loss : 0.03502495461504481ITERATION : 22, loss : 0.035036291806419685ITERATION : 23, loss : 0.0350446506087074ITERATION : 24, loss : 0.035050814626571926ITERATION : 25, loss : 0.03505536094479061ITERATION : 26, loss : 0.0350587146321988ITERATION : 27, loss : 0.03506118901507705ITERATION : 28, loss : 0.03506301489830186ITERATION : 29, loss : 0.035064362428439716ITERATION : 30, loss : 0.03506535714941885ITERATION : 31, loss : 0.03506609149828577ITERATION : 32, loss : 0.03506663370874412ITERATION : 33, loss : 0.035067034110594836ITERATION : 34, loss : 0.0350673298469874ITERATION : 35, loss : 0.035067548264490205ITERATION : 36, loss : 0.03506770962413369ITERATION : 37, loss : 0.035067828831886555ITERATION : 38, loss : 0.03506791690590381ITERATION : 39, loss : 0.03506798199561165ITERATION : 40, loss : 0.03506803009681334ITERATION : 41, loss : 0.035068065652484465ITERATION : 42, loss : 0.0350680919420364ITERATION : 43, loss : 0.03506811136747638ITERATION : 44, loss : 0.03506812572366854ITERATION : 45, loss : 0.03506813632070103ITERATION : 46, loss : 0.03506814416703529ITERATION : 47, loss : 0.03506814996131044ITERATION : 48, loss : 0.03506815424194526ITERATION : 49, loss : 0.035068157420004804ITERATION : 50, loss : 0.035068159766340444ITERATION : 51, loss : 0.03506816144647298ITERATION : 52, loss : 0.03506816272511874ITERATION : 53, loss : 0.03506816376456411ITERATION : 54, loss : 0.035068164435275766ITERATION : 55, loss : 0.03506816493094062ITERATION : 56, loss : 0.03506816532833858ITERATION : 57, loss : 0.035068165615728224ITERATION : 58, loss : 0.03506816575421385ITERATION : 59, loss : 0.03506816587145438ITERATION : 60, loss : 0.035068165992179746ITERATION : 61, loss : 0.035068166094380564ITERATION : 62, loss : 0.0350681662398339ITERATION : 63, loss : 0.0350681662398339ITERATION : 64, loss : 0.0350681662398339ITERATION : 65, loss : 0.0350681662398339ITERATION : 66, loss : 0.0350681662398339ITERATION : 67, loss : 0.0350681662398339ITERATION : 68, loss : 0.0350681662398339ITERATION : 69, loss : 0.0350681662398339ITERATION : 70, loss : 0.0350681662398339ITERATION : 71, loss : 0.0350681662398339ITERATION : 72, loss : 0.0350681662398339ITERATION : 73, loss : 0.0350681662398339ITERATION : 74, loss : 0.0350681662398339ITERATION : 75, loss : 0.0350681662398339ITERATION : 76, loss : 0.0350681662398339ITERATION : 77, loss : 0.0350681662398339ITERATION : 78, loss : 0.0350681662398339ITERATION : 79, loss : 0.0350681662398339ITERATION : 80, loss : 0.0350681662398339ITERATION : 81, loss : 0.0350681662398339ITERATION : 82, loss : 0.0350681662398339ITERATION : 83, loss : 0.0350681662398339ITERATION : 84, loss : 0.0350681662398339ITERATION : 85, loss : 0.0350681662398339ITERATION : 86, loss : 0.0350681662398339ITERATION : 87, loss : 0.0350681662398339ITERATION : 88, loss : 0.0350681662398339ITERATION : 89, loss : 0.0350681662398339ITERATION : 90, loss : 0.0350681662398339ITERATION : 91, loss : 0.0350681662398339ITERATION : 92, loss : 0.0350681662398339ITERATION : 93, loss : 0.0350681662398339ITERATION : 94, loss : 0.0350681662398339ITERATION : 95, loss : 0.0350681662398339ITERATION : 96, loss : 0.0350681662398339ITERATION : 97, loss : 0.0350681662398339ITERATION : 98, loss : 0.0350681662398339ITERATION : 99, loss : 0.0350681662398339ITERATION : 100, loss : 0.0350681662398339
ITERATION : 1, loss : 0.03584200248995994ITERATION : 2, loss : 0.02901212450452623ITERATION : 3, loss : 0.027182579856141382ITERATION : 4, loss : 0.026583539100559033ITERATION : 5, loss : 0.026372721968854772ITERATION : 6, loss : 0.026296907070418197ITERATION : 7, loss : 0.02626952943736731ITERATION : 8, loss : 0.026259595976265697ITERATION : 9, loss : 0.026255893289076047ITERATION : 10, loss : 0.026254386373398945ITERATION : 11, loss : 0.02625364608841584ITERATION : 12, loss : 0.0262531798286921ITERATION : 13, loss : 0.026252824700599336ITERATION : 14, loss : 0.026252529650361827ITERATION : 15, loss : 0.02625227897539285ITERATION : 16, loss : 0.026252067095328294ITERATION : 17, loss : 0.026251890083333985ITERATION : 18, loss : 0.026251744202259522ITERATION : 19, loss : 0.026251625378804454ITERATION : 20, loss : 0.02625152957012018ITERATION : 21, loss : 0.02625145294553424ITERATION : 22, loss : 0.0262513922243968ITERATION : 23, loss : 0.02625134427899608ITERATION : 24, loss : 0.02625130665303978ITERATION : 25, loss : 0.026251277272729155ITERATION : 26, loss : 0.026251254431686327ITERATION : 27, loss : 0.026251236712588867ITERATION : 28, loss : 0.026251223036369627ITERATION : 29, loss : 0.02625121251972799ITERATION : 30, loss : 0.026251204429755755ITERATION : 31, loss : 0.026251198208609976ITERATION : 32, loss : 0.02625119348175439ITERATION : 33, loss : 0.026251189907003678ITERATION : 34, loss : 0.026251187140413943ITERATION : 35, loss : 0.026251185029555216ITERATION : 36, loss : 0.02625118339247458ITERATION : 37, loss : 0.026251182155190764ITERATION : 38, loss : 0.026251181197497978ITERATION : 39, loss : 0.02625118048392698ITERATION : 40, loss : 0.0262511799253564ITERATION : 41, loss : 0.026251179518484952ITERATION : 42, loss : 0.026251179188768178ITERATION : 43, loss : 0.02625117896128265ITERATION : 44, loss : 0.026251178767252206ITERATION : 45, loss : 0.026251178644534124ITERATION : 46, loss : 0.026251178545630385ITERATION : 47, loss : 0.026251178455387363ITERATION : 48, loss : 0.0262511784016254ITERATION : 49, loss : 0.02625117836047634ITERATION : 50, loss : 0.02625117832242387ITERATION : 51, loss : 0.026251178295925198ITERATION : 52, loss : 0.02625117828553277ITERATION : 53, loss : 0.02625117828068009ITERATION : 54, loss : 0.02625117826016026ITERATION : 55, loss : 0.026251178262513202ITERATION : 56, loss : 0.026251178262513202ITERATION : 57, loss : 0.026251178262513202ITERATION : 58, loss : 0.026251178262513202ITERATION : 59, loss : 0.026251178262513202ITERATION : 60, loss : 0.026251178262513202ITERATION : 61, loss : 0.026251178262513202ITERATION : 62, loss : 0.026251178262513202ITERATION : 63, loss : 0.026251178262513202ITERATION : 64, loss : 0.026251178262513202ITERATION : 65, loss : 0.026251178262513202ITERATION : 66, loss : 0.026251178262513202ITERATION : 67, loss : 0.026251178262513202ITERATION : 68, loss : 0.026251178262513202ITERATION : 69, loss : 0.026251178262513202ITERATION : 70, loss : 0.026251178262513202ITERATION : 71, loss : 0.026251178262513202ITERATION : 72, loss : 0.026251178262513202ITERATION : 73, loss : 0.026251178262513202ITERATION : 74, loss : 0.026251178262513202ITERATION : 75, loss : 0.026251178262513202ITERATION : 76, loss : 0.026251178262513202ITERATION : 77, loss : 0.026251178262513202ITERATION : 78, loss : 0.026251178262513202ITERATION : 79, loss : 0.026251178262513202ITERATION : 80, loss : 0.026251178262513202ITERATION : 81, loss : 0.026251178262513202ITERATION : 82, loss : 0.026251178262513202ITERATION : 83, loss : 0.026251178262513202ITERATION : 84, loss : 0.026251178262513202ITERATION : 85, loss : 0.026251178262513202ITERATION : 86, loss : 0.026251178262513202ITERATION : 87, loss : 0.026251178262513202ITERATION : 88, loss : 0.026251178262513202ITERATION : 89, loss : 0.026251178262513202ITERATION : 90, loss : 0.026251178262513202ITERATION : 91, loss : 0.026251178262513202ITERATION : 92, loss : 0.026251178262513202ITERATION : 93, loss : 0.026251178262513202ITERATION : 94, loss : 0.026251178262513202ITERATION : 95, loss : 0.026251178262513202ITERATION : 96, loss : 0.026251178262513202ITERATION : 97, loss : 0.026251178262513202ITERATION : 98, loss : 0.026251178262513202ITERATION : 99, loss : 0.026251178262513202ITERATION : 100, loss : 0.026251178262513202
ITERATION : 1, loss : 0.07586864907290829ITERATION : 2, loss : 0.07252661451634647ITERATION : 3, loss : 0.07232014770176882ITERATION : 4, loss : 0.07245398402172132ITERATION : 5, loss : 0.07262837036083067ITERATION : 6, loss : 0.07280482548337305ITERATION : 7, loss : 0.0729528856634903ITERATION : 8, loss : 0.0730683678439188ITERATION : 9, loss : 0.07315518620248172ITERATION : 10, loss : 0.07321910335837109ITERATION : 11, loss : 0.07326556539463287ITERATION : 12, loss : 0.07329906844241729ITERATION : 13, loss : 0.07332310064417683ITERATION : 14, loss : 0.07334027917863524ITERATION : 15, loss : 0.07335252943532443ITERATION : 16, loss : 0.07336125072211468ITERATION : 17, loss : 0.07336745233050486ITERATION : 18, loss : 0.07337185851679874ITERATION : 19, loss : 0.07337498706213069ITERATION : 20, loss : 0.07337720727690004ITERATION : 21, loss : 0.07337878228302257ITERATION : 22, loss : 0.07337989922511905ITERATION : 23, loss : 0.07338069116385103ITERATION : 24, loss : 0.073381252374919ITERATION : 25, loss : 0.0733816500587221ITERATION : 26, loss : 0.07338193184957183ITERATION : 27, loss : 0.07338213145126894ITERATION : 28, loss : 0.07338227281773273ITERATION : 29, loss : 0.07338237288992014ITERATION : 30, loss : 0.07338244375191448ITERATION : 31, loss : 0.07338249393976333ITERATION : 32, loss : 0.07338252939837688ITERATION : 33, loss : 0.07338255444876943ITERATION : 34, loss : 0.07338257218742861ITERATION : 35, loss : 0.07338258478686116ITERATION : 36, loss : 0.07338259370486888ITERATION : 37, loss : 0.07338259989983606ITERATION : 38, loss : 0.07338260437242541ITERATION : 39, loss : 0.07338260751397192ITERATION : 40, loss : 0.07338260972311536ITERATION : 41, loss : 0.07338261127990739ITERATION : 42, loss : 0.07338261236157377ITERATION : 43, loss : 0.0733826131351503ITERATION : 44, loss : 0.07338261365767083ITERATION : 45, loss : 0.0733826140512917ITERATION : 46, loss : 0.07338261429470713ITERATION : 47, loss : 0.07338261449724945ITERATION : 48, loss : 0.07338261461044236ITERATION : 49, loss : 0.07338261470823562ITERATION : 50, loss : 0.07338261477249174ITERATION : 51, loss : 0.0733826148212435ITERATION : 52, loss : 0.0733826148415425ITERATION : 53, loss : 0.07338261485081402ITERATION : 54, loss : 0.07338261488604922ITERATION : 55, loss : 0.07338261488604922ITERATION : 56, loss : 0.07338261488604922ITERATION : 57, loss : 0.07338261488604922ITERATION : 58, loss : 0.07338261488604922ITERATION : 59, loss : 0.07338261488604922ITERATION : 60, loss : 0.07338261488604922ITERATION : 61, loss : 0.07338261488604922ITERATION : 62, loss : 0.07338261488604922ITERATION : 63, loss : 0.07338261488604922ITERATION : 64, loss : 0.07338261488604922ITERATION : 65, loss : 0.07338261488604922ITERATION : 66, loss : 0.07338261488604922ITERATION : 67, loss : 0.07338261488604922ITERATION : 68, loss : 0.07338261488604922ITERATION : 69, loss : 0.07338261488604922ITERATION : 70, loss : 0.07338261488604922ITERATION : 71, loss : 0.07338261488604922ITERATION : 72, loss : 0.07338261488604922ITERATION : 73, loss : 0.07338261488604922ITERATION : 74, loss : 0.07338261488604922ITERATION : 75, loss : 0.07338261488604922ITERATION : 76, loss : 0.07338261488604922ITERATION : 77, loss : 0.07338261488604922ITERATION : 78, loss : 0.07338261488604922ITERATION : 79, loss : 0.07338261488604922ITERATION : 80, loss : 0.07338261488604922ITERATION : 81, loss : 0.07338261488604922ITERATION : 82, loss : 0.07338261488604922ITERATION : 83, loss : 0.07338261488604922ITERATION : 84, loss : 0.07338261488604922ITERATION : 85, loss : 0.07338261488604922ITERATION : 86, loss : 0.07338261488604922ITERATION : 87, loss : 0.07338261488604922ITERATION : 88, loss : 0.07338261488604922ITERATION : 89, loss : 0.07338261488604922ITERATION : 90, loss : 0.07338261488604922ITERATION : 91, loss : 0.07338261488604922ITERATION : 92, loss : 0.07338261488604922ITERATION : 93, loss : 0.07338261488604922ITERATION : 94, loss : 0.07338261488604922ITERATION : 95, loss : 0.07338261488604922ITERATION : 96, loss : 0.07338261488604922ITERATION : 97, loss : 0.07338261488604922ITERATION : 98, loss : 0.07338261488604922ITERATION : 99, loss : 0.07338261488604922ITERATION : 100, loss : 0.07338261488604922
ITERATION : 1, loss : 0.029747466546189108ITERATION : 2, loss : 0.021835164198198725ITERATION : 3, loss : 0.01748350605751575ITERATION : 4, loss : 0.015326658360867657ITERATION : 5, loss : 0.014323197779518309ITERATION : 6, loss : 0.013914218335086602ITERATION : 7, loss : 0.013797719517028998ITERATION : 8, loss : 0.013812310124029462ITERATION : 9, loss : 0.013876096727434768ITERATION : 10, loss : 0.013950346643341046ITERATION : 11, loss : 0.014018653559597189ITERATION : 12, loss : 0.014075527790797624ITERATION : 13, loss : 0.014120422441126337ITERATION : 14, loss : 0.01415474863687038ITERATION : 15, loss : 0.014180467319674093ITERATION : 16, loss : 0.014199480326483849ITERATION : 17, loss : 0.014213409111234884ITERATION : 18, loss : 0.014223549756789078ITERATION : 19, loss : 0.01423090050558354ITERATION : 20, loss : 0.014236212762395289ITERATION : 21, loss : 0.014240043675712674ITERATION : 22, loss : 0.014242802176484846ITERATION : 23, loss : 0.014244786356024117ITERATION : 24, loss : 0.014246212518171705ITERATION : 25, loss : 0.014247237097216094ITERATION : 26, loss : 0.014247972895752649ITERATION : 27, loss : 0.014248501170571498ITERATION : 28, loss : 0.014248880408599061ITERATION : 29, loss : 0.014249152621323398ITERATION : 30, loss : 0.01424934800548589ITERATION : 31, loss : 0.01424948823505378ITERATION : 32, loss : 0.014249588881180598ITERATION : 33, loss : 0.014249661114584828ITERATION : 34, loss : 0.014249712961488359ITERATION : 35, loss : 0.014249750178337564ITERATION : 36, loss : 0.01424977689070119ITERATION : 37, loss : 0.01424979606433178ITERATION : 38, loss : 0.014249809824036624ITERATION : 39, loss : 0.014249819709401729ITERATION : 40, loss : 0.014249826805907535ITERATION : 41, loss : 0.014249831893319178ITERATION : 42, loss : 0.014249835546551314ITERATION : 43, loss : 0.01424983817144788ITERATION : 44, loss : 0.014249840056176879ITERATION : 45, loss : 0.014249841396097489ITERATION : 46, loss : 0.01424984236835509ITERATION : 47, loss : 0.014249843070372003ITERATION : 48, loss : 0.014249843570062594ITERATION : 49, loss : 0.014249843920198749ITERATION : 50, loss : 0.014249844155233714ITERATION : 51, loss : 0.014249844353148147ITERATION : 52, loss : 0.014249844481962478ITERATION : 53, loss : 0.014249844572279659ITERATION : 54, loss : 0.014249844648334719ITERATION : 55, loss : 0.014249844684189385ITERATION : 56, loss : 0.014249844723878928ITERATION : 57, loss : 0.01424984473296614ITERATION : 58, loss : 0.014249844743091754ITERATION : 59, loss : 0.014249844766896893ITERATION : 60, loss : 0.014249844767015624ITERATION : 61, loss : 0.014249844767015624ITERATION : 62, loss : 0.014249844767015624ITERATION : 63, loss : 0.014249844767015624ITERATION : 64, loss : 0.014249844767015624ITERATION : 65, loss : 0.014249844767015624ITERATION : 66, loss : 0.014249844767015624ITERATION : 67, loss : 0.014249844767015624ITERATION : 68, loss : 0.014249844767015624ITERATION : 69, loss : 0.014249844767015624ITERATION : 70, loss : 0.014249844767015624ITERATION : 71, loss : 0.014249844767015624ITERATION : 72, loss : 0.014249844767015624ITERATION : 73, loss : 0.014249844767015624ITERATION : 74, loss : 0.014249844767015624ITERATION : 75, loss : 0.014249844767015624ITERATION : 76, loss : 0.014249844767015624ITERATION : 77, loss : 0.014249844767015624ITERATION : 78, loss : 0.014249844767015624ITERATION : 79, loss : 0.014249844767015624ITERATION : 80, loss : 0.014249844767015624ITERATION : 81, loss : 0.014249844767015624ITERATION : 82, loss : 0.014249844767015624ITERATION : 83, loss : 0.014249844767015624ITERATION : 84, loss : 0.014249844767015624ITERATION : 85, loss : 0.014249844767015624ITERATION : 86, loss : 0.014249844767015624ITERATION : 87, loss : 0.014249844767015624ITERATION : 88, loss : 0.014249844767015624ITERATION : 89, loss : 0.014249844767015624ITERATION : 90, loss : 0.014249844767015624ITERATION : 91, loss : 0.014249844767015624ITERATION : 92, loss : 0.014249844767015624ITERATION : 93, loss : 0.014249844767015624ITERATION : 94, loss : 0.014249844767015624ITERATION : 95, loss : 0.014249844767015624ITERATION : 96, loss : 0.014249844767015624ITERATION : 97, loss : 0.014249844767015624ITERATION : 98, loss : 0.014249844767015624ITERATION : 99, loss : 0.014249844767015624ITERATION : 100, loss : 0.014249844767015624
ITERATION : 1, loss : 0.030727163297023987ITERATION : 2, loss : 0.02857969059219383ITERATION : 3, loss : 0.0268408064590771ITERATION : 4, loss : 0.02604364190333594ITERATION : 5, loss : 0.025601876649675635ITERATION : 6, loss : 0.025338714698907985ITERATION : 7, loss : 0.02517768883013432ITERATION : 8, loss : 0.025078685323829068ITERATION : 9, loss : 0.025018425176252003ITERATION : 10, loss : 0.024982629084378878ITERATION : 11, loss : 0.024962270077413255ITERATION : 12, loss : 0.02495155700061602ITERATION : 13, loss : 0.02494675758021089ITERATION : 14, loss : 0.024945472894638897ITERATION : 15, loss : 0.024946174421417076ITERATION : 16, loss : 0.024947903087326702ITERATION : 17, loss : 0.024950070745315772ITERATION : 18, loss : 0.02495232926898749ITERATION : 19, loss : 0.02495448374761818ITERATION : 20, loss : 0.02495643553041987ITERATION : 21, loss : 0.024958144801945352ITERATION : 22, loss : 0.024959606365169505ITERATION : 23, loss : 0.024960834250625737ITERATION : 24, loss : 0.0249618517728373ITERATION : 25, loss : 0.024962685987338626ITERATION : 26, loss : 0.024963363940842472ITERATION : 27, loss : 0.024963910999526065ITERATION : 28, loss : 0.024964349808683138ITERATION : 29, loss : 0.02496469990352043ITERATION : 30, loss : 0.024964978092245797ITERATION : 31, loss : 0.02496519831131989ITERATION : 32, loss : 0.02496537193127267ITERATION : 33, loss : 0.02496550852506968ITERATION : 34, loss : 0.02496561572808613ITERATION : 35, loss : 0.024965699756218825ITERATION : 36, loss : 0.02496576534604641ITERATION : 37, loss : 0.02496581650139511ITERATION : 38, loss : 0.02496585628534248ITERATION : 39, loss : 0.02496588726870318ITERATION : 40, loss : 0.024965911280482953ITERATION : 41, loss : 0.024965929921479783ITERATION : 42, loss : 0.024965944384496775ITERATION : 43, loss : 0.02496595556664482ITERATION : 44, loss : 0.02496596417540583ITERATION : 45, loss : 0.024965970855915093ITERATION : 46, loss : 0.024965976034512588ITERATION : 47, loss : 0.024965980040449406ITERATION : 48, loss : 0.024965983129407805ITERATION : 49, loss : 0.024965985505850524ITERATION : 50, loss : 0.024965987340875944ITERATION : 51, loss : 0.024965988757721568ITERATION : 52, loss : 0.02496598985729042ITERATION : 53, loss : 0.02496599069604747ITERATION : 54, loss : 0.024965991345864712ITERATION : 55, loss : 0.024965991838739106ITERATION : 56, loss : 0.024965992219518143ITERATION : 57, loss : 0.024965992488196385ITERATION : 58, loss : 0.024965992693493283ITERATION : 59, loss : 0.024965992844026914ITERATION : 60, loss : 0.024965992964065514ITERATION : 61, loss : 0.024965993061013554ITERATION : 62, loss : 0.02496599315737819ITERATION : 63, loss : 0.0249659932183228ITERATION : 64, loss : 0.024965993262262114ITERATION : 65, loss : 0.0249659932977312ITERATION : 66, loss : 0.024965993312232624ITERATION : 67, loss : 0.02496599333032956ITERATION : 68, loss : 0.024965993330428357ITERATION : 69, loss : 0.024965993330428357ITERATION : 70, loss : 0.024965993330428357ITERATION : 71, loss : 0.024965993330428357ITERATION : 72, loss : 0.024965993330428357ITERATION : 73, loss : 0.024965993330428357ITERATION : 74, loss : 0.024965993330428357ITERATION : 75, loss : 0.024965993330428357ITERATION : 76, loss : 0.024965993330428357ITERATION : 77, loss : 0.024965993330428357ITERATION : 78, loss : 0.024965993330428357ITERATION : 79, loss : 0.024965993330428357ITERATION : 80, loss : 0.024965993330428357ITERATION : 81, loss : 0.024965993330428357ITERATION : 82, loss : 0.024965993330428357ITERATION : 83, loss : 0.024965993330428357ITERATION : 84, loss : 0.024965993330428357ITERATION : 85, loss : 0.024965993330428357ITERATION : 86, loss : 0.024965993330428357ITERATION : 87, loss : 0.024965993330428357ITERATION : 88, loss : 0.024965993330428357ITERATION : 89, loss : 0.024965993330428357ITERATION : 90, loss : 0.024965993330428357ITERATION : 91, loss : 0.024965993330428357ITERATION : 92, loss : 0.024965993330428357ITERATION : 93, loss : 0.024965993330428357ITERATION : 94, loss : 0.024965993330428357ITERATION : 95, loss : 0.024965993330428357ITERATION : 96, loss : 0.024965993330428357ITERATION : 97, loss : 0.024965993330428357ITERATION : 98, loss : 0.024965993330428357ITERATION : 99, loss : 0.024965993330428357ITERATION : 100, loss : 0.024965993330428357
ITERATION : 1, loss : 0.03543784829771787ITERATION : 2, loss : 0.032344243376612625ITERATION : 3, loss : 0.027617787362222666ITERATION : 4, loss : 0.024832306309105764ITERATION : 5, loss : 0.02311359584098784ITERATION : 6, loss : 0.02202313223400262ITERATION : 7, loss : 0.021315815351064568ITERATION : 8, loss : 0.020848724951509547ITERATION : 9, loss : 0.020535866539724883ITERATION : 10, loss : 0.020324010287366745ITERATION : 11, loss : 0.02017936065453791ITERATION : 12, loss : 0.020079990382832688ITERATION : 13, loss : 0.02001141717247531ITERATION : 14, loss : 0.019963940324210096ITERATION : 15, loss : 0.019930990975181164ITERATION : 16, loss : 0.01990808423951936ITERATION : 17, loss : 0.019892139478116175ITERATION : 18, loss : 0.019881030793607903ITERATION : 19, loss : 0.019873286454010763ITERATION : 20, loss : 0.0198678850767731ITERATION : 21, loss : 0.019864116619237374ITERATION : 22, loss : 0.01986148685945262ITERATION : 23, loss : 0.0198596513950331ITERATION : 24, loss : 0.019858370211737916ITERATION : 25, loss : 0.019857475863366853ITERATION : 26, loss : 0.019856851519737064ITERATION : 27, loss : 0.019856415650457634ITERATION : 28, loss : 0.019856111413986383ITERATION : 29, loss : 0.01985589891663081ITERATION : 30, loss : 0.019855750643423906ITERATION : 31, loss : 0.019855647099698177ITERATION : 32, loss : 0.019855574778589343ITERATION : 33, loss : 0.01985552433939914ITERATION : 34, loss : 0.01985548907038884ITERATION : 35, loss : 0.019855464482073582ITERATION : 36, loss : 0.01985544734836826ITERATION : 37, loss : 0.019855435350947748ITERATION : 38, loss : 0.01985542699485413ITERATION : 39, loss : 0.019855421176740728ITERATION : 40, loss : 0.019855417213551614ITERATION : 41, loss : 0.01985541430644561ITERATION : 42, loss : 0.019855412317158032ITERATION : 43, loss : 0.019855410951514992ITERATION : 44, loss : 0.019855410013678916ITERATION : 45, loss : 0.019855409349754473ITERATION : 46, loss : 0.019855408903412895ITERATION : 47, loss : 0.019855408569753204ITERATION : 48, loss : 0.01985540835487603ITERATION : 49, loss : 0.019855408208088902ITERATION : 50, loss : 0.019855408130069405ITERATION : 51, loss : 0.019855408039734894ITERATION : 52, loss : 0.019855407995127076ITERATION : 53, loss : 0.019855407971015437ITERATION : 54, loss : 0.019855407960919256ITERATION : 55, loss : 0.019855407945996228ITERATION : 56, loss : 0.019855407930357453ITERATION : 57, loss : 0.019855407927815018ITERATION : 58, loss : 0.01985540792780878ITERATION : 59, loss : 0.01985540792780878ITERATION : 60, loss : 0.01985540792780878ITERATION : 61, loss : 0.01985540792780878ITERATION : 62, loss : 0.01985540792780878ITERATION : 63, loss : 0.01985540792780878ITERATION : 64, loss : 0.01985540792780878ITERATION : 65, loss : 0.01985540792780878ITERATION : 66, loss : 0.01985540792780878ITERATION : 67, loss : 0.01985540792780878ITERATION : 68, loss : 0.01985540792780878ITERATION : 69, loss : 0.01985540792780878ITERATION : 70, loss : 0.01985540792780878ITERATION : 71, loss : 0.01985540792780878ITERATION : 72, loss : 0.01985540792780878ITERATION : 73, loss : 0.01985540792780878ITERATION : 74, loss : 0.01985540792780878ITERATION : 75, loss : 0.01985540792780878ITERATION : 76, loss : 0.01985540792780878ITERATION : 77, loss : 0.01985540792780878ITERATION : 78, loss : 0.01985540792780878ITERATION : 79, loss : 0.01985540792780878ITERATION : 80, loss : 0.01985540792780878ITERATION : 81, loss : 0.01985540792780878ITERATION : 82, loss : 0.01985540792780878ITERATION : 83, loss : 0.01985540792780878ITERATION : 84, loss : 0.01985540792780878ITERATION : 85, loss : 0.01985540792780878ITERATION : 86, loss : 0.01985540792780878ITERATION : 87, loss : 0.01985540792780878ITERATION : 88, loss : 0.01985540792780878ITERATION : 89, loss : 0.01985540792780878ITERATION : 90, loss : 0.01985540792780878ITERATION : 91, loss : 0.01985540792780878ITERATION : 92, loss : 0.01985540792780878ITERATION : 93, loss : 0.01985540792780878ITERATION : 94, loss : 0.01985540792780878ITERATION : 95, loss : 0.01985540792780878ITERATION : 96, loss : 0.01985540792780878ITERATION : 97, loss : 0.01985540792780878ITERATION : 98, loss : 0.01985540792780878ITERATION : 99, loss : 0.01985540792780878ITERATION : 100, loss : 0.01985540792780878
ITERATION : 1, loss : 0.03399363362315355ITERATION : 2, loss : 0.03435891732922771ITERATION : 3, loss : 0.03439095714131617ITERATION : 4, loss : 0.03424580091097881ITERATION : 5, loss : 0.03414903987173704ITERATION : 6, loss : 0.03410384018401629ITERATION : 7, loss : 0.034086613884387634ITERATION : 8, loss : 0.03408195885397731ITERATION : 9, loss : 0.034082288493936075ITERATION : 10, loss : 0.034084252113935745ITERATION : 11, loss : 0.034086482786115944ITERATION : 12, loss : 0.03408848288978436ITERATION : 13, loss : 0.034090117906403594ITERATION : 14, loss : 0.034091394096602425ITERATION : 15, loss : 0.03409236459369084ITERATION : 16, loss : 0.03409309086221857ITERATION : 17, loss : 0.03409362871387274ITERATION : 18, loss : 0.034094024267741156ITERATION : 19, loss : 0.03409431378528099ITERATION : 20, loss : 0.03409452505556289ITERATION : 21, loss : 0.034094678576189946ITERATION : 22, loss : 0.03409478989255962ITERATION : 23, loss : 0.0340948705316674ITERATION : 24, loss : 0.03409492883794669ITERATION : 25, loss : 0.034094970921186164ITERATION : 26, loss : 0.034095001231049575ITERATION : 27, loss : 0.03409502304481216ITERATION : 28, loss : 0.03409503885234481ITERATION : 29, loss : 0.03409505012349039ITERATION : 30, loss : 0.03409505820985948ITERATION : 31, loss : 0.034095064049210914ITERATION : 32, loss : 0.03409506828052656ITERATION : 33, loss : 0.03409507132392705ITERATION : 34, loss : 0.03409507348929412ITERATION : 35, loss : 0.03409507507408821ITERATION : 36, loss : 0.03409507624712083ITERATION : 37, loss : 0.034095077068626314ITERATION : 38, loss : 0.03409507765332758ITERATION : 39, loss : 0.03409507803104049ITERATION : 40, loss : 0.034095078331983386ITERATION : 41, loss : 0.03409507851340256ITERATION : 42, loss : 0.034095078674943304ITERATION : 43, loss : 0.03409507877842562ITERATION : 44, loss : 0.034095078889289455ITERATION : 45, loss : 0.034095078919335574ITERATION : 46, loss : 0.03409507893858826ITERATION : 47, loss : 0.03409507899722937ITERATION : 48, loss : 0.034095078999357056ITERATION : 49, loss : 0.034095078999357056ITERATION : 50, loss : 0.034095078999357056ITERATION : 51, loss : 0.034095078999357056ITERATION : 52, loss : 0.034095078999357056ITERATION : 53, loss : 0.034095078999357056ITERATION : 54, loss : 0.034095078999357056ITERATION : 55, loss : 0.034095078999357056ITERATION : 56, loss : 0.034095078999357056ITERATION : 57, loss : 0.034095078999357056ITERATION : 58, loss : 0.034095078999357056ITERATION : 59, loss : 0.034095078999357056ITERATION : 60, loss : 0.034095078999357056ITERATION : 61, loss : 0.034095078999357056ITERATION : 62, loss : 0.034095078999357056ITERATION : 63, loss : 0.034095078999357056ITERATION : 64, loss : 0.034095078999357056ITERATION : 65, loss : 0.034095078999357056ITERATION : 66, loss : 0.034095078999357056ITERATION : 67, loss : 0.034095078999357056ITERATION : 68, loss : 0.034095078999357056ITERATION : 69, loss : 0.034095078999357056ITERATION : 70, loss : 0.034095078999357056ITERATION : 71, loss : 0.034095078999357056ITERATION : 72, loss : 0.034095078999357056ITERATION : 73, loss : 0.034095078999357056ITERATION : 74, loss : 0.034095078999357056ITERATION : 75, loss : 0.034095078999357056ITERATION : 76, loss : 0.034095078999357056ITERATION : 77, loss : 0.034095078999357056ITERATION : 78, loss : 0.034095078999357056ITERATION : 79, loss : 0.034095078999357056ITERATION : 80, loss : 0.034095078999357056ITERATION : 81, loss : 0.034095078999357056ITERATION : 82, loss : 0.034095078999357056ITERATION : 83, loss : 0.034095078999357056ITERATION : 84, loss : 0.034095078999357056ITERATION : 85, loss : 0.034095078999357056ITERATION : 86, loss : 0.034095078999357056ITERATION : 87, loss : 0.034095078999357056ITERATION : 88, loss : 0.034095078999357056ITERATION : 89, loss : 0.034095078999357056ITERATION : 90, loss : 0.034095078999357056ITERATION : 91, loss : 0.034095078999357056ITERATION : 92, loss : 0.034095078999357056ITERATION : 93, loss : 0.034095078999357056ITERATION : 94, loss : 0.034095078999357056ITERATION : 95, loss : 0.034095078999357056ITERATION : 96, loss : 0.034095078999357056ITERATION : 97, loss : 0.034095078999357056ITERATION : 98, loss : 0.034095078999357056ITERATION : 99, loss : 0.034095078999357056ITERATION : 100, loss : 0.034095078999357056
gradient norm in None layer : 0.0006348350150268801
gradient norm in None layer : 3.38076814630236e-05
gradient norm in None layer : 3.698217752864953e-05
gradient norm in None layer : 0.0004986677763862478
gradient norm in None layer : 5.1511434288091466e-05
gradient norm in None layer : 5.7826655744074056e-05
gradient norm in None layer : 0.00023733664943383962
gradient norm in None layer : 1.1120329315043407e-05
gradient norm in None layer : 6.817627500029235e-06
gradient norm in None layer : 0.00020076777298606646
gradient norm in None layer : 9.472627753322701e-06
gradient norm in None layer : 5.919160879342357e-06
gradient norm in None layer : 7.622818917247207e-05
gradient norm in None layer : 2.5256964205439556e-06
gradient norm in None layer : 1.5806584294140132e-06
gradient norm in None layer : 6.367002299428857e-05
gradient norm in None layer : 2.670576567105247e-06
gradient norm in None layer : 1.914653141592998e-06
gradient norm in None layer : 8.499317667991585e-05
gradient norm in None layer : 1.0425757919123044e-06
gradient norm in None layer : 0.0001696854444236636
gradient norm in None layer : 1.030232802606726e-05
gradient norm in None layer : 6.986455519563667e-06
gradient norm in None layer : 0.00019008310018247865
gradient norm in None layer : 2.141901893455612e-05
gradient norm in None layer : 2.843567264710027e-05
gradient norm in None layer : 0.00036478247450659354
gradient norm in None layer : 2.9982098405350085e-06
gradient norm in None layer : 0.0006767222875548247
gradient norm in None layer : 5.989483736208814e-05
gradient norm in None layer : 6.399717302691165e-05
gradient norm in None layer : 0.0008539144233164325
gradient norm in None layer : 6.395644316785943e-05
gradient norm in None layer : 7.523566784540663e-05
gradient norm in None layer : 5.657781722050667e-05
gradient norm in None layer : 6.941008173136003e-06
Total gradient norm: 0.0014769772565728664
invariance loss : 4.33526850582316, avg_den : 0.42554473876953125, density loss : 0.32554473876953127, mse loss : 0.03080718606304416, solver time : 142.47687482833862 sec , total loss : 0.035467999307636854, running loss : 0.04989082250882963
Epoch 0/10 , batch 17/12500 
ITERATION : 1, loss : 0.03990745350952235ITERATION : 2, loss : 0.03563765992677322ITERATION : 3, loss : 0.03553590181280976ITERATION : 4, loss : 0.036475698038182014ITERATION : 5, loss : 0.037557713579934114ITERATION : 6, loss : 0.03850305809250073ITERATION : 7, loss : 0.0392509541581976ITERATION : 8, loss : 0.03981485661716192ITERATION : 9, loss : 0.04022880998169198ITERATION : 10, loss : 0.04052786241041831ITERATION : 11, loss : 0.04074176034627818ITERATION : 12, loss : 0.04089377747443307ITERATION : 13, loss : 0.04100136969183331ITERATION : 14, loss : 0.041077313399816136ITERATION : 15, loss : 0.04113082280085149ITERATION : 16, loss : 0.04116848078348036ITERATION : 17, loss : 0.041194962701152193ITERATION : 18, loss : 0.04121357572581085ITERATION : 19, loss : 0.0412266535836686ITERATION : 20, loss : 0.04123584017213245ITERATION : 21, loss : 0.041242292394107455ITERATION : 22, loss : 0.041246823594465534ITERATION : 23, loss : 0.04125000552219426ITERATION : 24, loss : 0.04125223977067011ITERATION : 25, loss : 0.041253808420752235ITERATION : 26, loss : 0.04125490987778413ITERATION : 27, loss : 0.041255683225588306ITERATION : 28, loss : 0.041256226155228594ITERATION : 29, loss : 0.04125660725124192ITERATION : 30, loss : 0.041256874726069204ITERATION : 31, loss : 0.041257062496520995ITERATION : 32, loss : 0.04125719435360883ITERATION : 33, loss : 0.04125728688556885ITERATION : 34, loss : 0.04125735182638258ITERATION : 35, loss : 0.04125739736766952ITERATION : 36, loss : 0.04125742935349058ITERATION : 37, loss : 0.041257451808844ITERATION : 38, loss : 0.04125746757945172ITERATION : 39, loss : 0.0412574786627899ITERATION : 40, loss : 0.041257486441981676ITERATION : 41, loss : 0.04125749183399223ITERATION : 42, loss : 0.04125749564617008ITERATION : 43, loss : 0.041257498284179836ITERATION : 44, loss : 0.04125750015247394ITERATION : 45, loss : 0.04125750142956137ITERATION : 46, loss : 0.04125750228259922ITERATION : 47, loss : 0.041257502916654386ITERATION : 48, loss : 0.04125750327492929ITERATION : 49, loss : 0.041257503603011024ITERATION : 50, loss : 0.04125750376354434ITERATION : 51, loss : 0.041257503926182104ITERATION : 52, loss : 0.04125750402408223ITERATION : 53, loss : 0.04125750410464212ITERATION : 54, loss : 0.04125750410483777ITERATION : 55, loss : 0.04125750410483777ITERATION : 56, loss : 0.04125750410483777ITERATION : 57, loss : 0.04125750410483777ITERATION : 58, loss : 0.04125750410483777ITERATION : 59, loss : 0.04125750410483777ITERATION : 60, loss : 0.04125750410483777ITERATION : 61, loss : 0.04125750410483777ITERATION : 62, loss : 0.04125750410483777ITERATION : 63, loss : 0.04125750410483777ITERATION : 64, loss : 0.04125750410483777ITERATION : 65, loss : 0.04125750410483777ITERATION : 66, loss : 0.04125750410483777ITERATION : 67, loss : 0.04125750410483777ITERATION : 68, loss : 0.04125750410483777ITERATION : 69, loss : 0.04125750410483777ITERATION : 70, loss : 0.04125750410483777ITERATION : 71, loss : 0.04125750410483777ITERATION : 72, loss : 0.04125750410483777ITERATION : 73, loss : 0.04125750410483777ITERATION : 74, loss : 0.04125750410483777ITERATION : 75, loss : 0.04125750410483777ITERATION : 76, loss : 0.04125750410483777ITERATION : 77, loss : 0.04125750410483777ITERATION : 78, loss : 0.04125750410483777ITERATION : 79, loss : 0.04125750410483777ITERATION : 80, loss : 0.04125750410483777ITERATION : 81, loss : 0.04125750410483777ITERATION : 82, loss : 0.04125750410483777ITERATION : 83, loss : 0.04125750410483777ITERATION : 84, loss : 0.04125750410483777ITERATION : 85, loss : 0.04125750410483777ITERATION : 86, loss : 0.04125750410483777ITERATION : 87, loss : 0.04125750410483777ITERATION : 88, loss : 0.04125750410483777ITERATION : 89, loss : 0.04125750410483777ITERATION : 90, loss : 0.04125750410483777ITERATION : 91, loss : 0.04125750410483777ITERATION : 92, loss : 0.04125750410483777ITERATION : 93, loss : 0.04125750410483777ITERATION : 94, loss : 0.04125750410483777ITERATION : 95, loss : 0.04125750410483777ITERATION : 96, loss : 0.04125750410483777ITERATION : 97, loss : 0.04125750410483777ITERATION : 98, loss : 0.04125750410483777ITERATION : 99, loss : 0.04125750410483777ITERATION : 100, loss : 0.04125750410483777
ITERATION : 1, loss : 0.03788516207616048ITERATION : 2, loss : 0.033118467673031816ITERATION : 3, loss : 0.03137660585802548ITERATION : 4, loss : 0.030560888995417784ITERATION : 5, loss : 0.030166591494876924ITERATION : 6, loss : 0.02997398782258097ITERATION : 7, loss : 0.02987985008161746ITERATION : 8, loss : 0.02983437996772912ITERATION : 9, loss : 0.029813111717871977ITERATION : 10, loss : 0.029803860264909308ITERATION : 11, loss : 0.02980049934884818ITERATION : 12, loss : 0.029799934720003304ITERATION : 13, loss : 0.029800606961082665ITERATION : 14, loss : 0.02980174009341364ITERATION : 15, loss : 0.02980295963791819ITERATION : 16, loss : 0.029804096467845376ITERATION : 17, loss : 0.02980508538084836ITERATION : 18, loss : 0.029805912079098502ITERATION : 19, loss : 0.029806585887189388ITERATION : 20, loss : 0.029807125696119156ITERATION : 21, loss : 0.029807552761288773ITERATION : 22, loss : 0.029807887501033796ITERATION : 23, loss : 0.029808147917015634ITERATION : 24, loss : 0.029808349347889427ITERATION : 25, loss : 0.029808504396981045ITERATION : 26, loss : 0.02980862325815154ITERATION : 27, loss : 0.029808714085954438ITERATION : 28, loss : 0.029808783271818986ITERATION : 29, loss : 0.029808835836146495ITERATION : 30, loss : 0.029808875669583464ITERATION : 31, loss : 0.029808905810637516ITERATION : 32, loss : 0.029808928557389574ITERATION : 33, loss : 0.02980894570214508ITERATION : 34, loss : 0.029808958614752867ITERATION : 35, loss : 0.029808968314967123ITERATION : 36, loss : 0.029808975606452362ITERATION : 37, loss : 0.029808981081701914ITERATION : 38, loss : 0.029808985171779472ITERATION : 39, loss : 0.02980898824107054ITERATION : 40, loss : 0.029808990524422728ITERATION : 41, loss : 0.029808992237926837ITERATION : 42, loss : 0.029808993513677388ITERATION : 43, loss : 0.02980899446612906ITERATION : 44, loss : 0.02980899516810342ITERATION : 45, loss : 0.02980899570899348ITERATION : 46, loss : 0.029808996090898248ITERATION : 47, loss : 0.029808996391188582ITERATION : 48, loss : 0.029808996613149993ITERATION : 49, loss : 0.02980899676132035ITERATION : 50, loss : 0.029808996876081257ITERATION : 51, loss : 0.02980899698081079ITERATION : 52, loss : 0.029808997037315633ITERATION : 53, loss : 0.02980899709259667ITERATION : 54, loss : 0.029808997117371977ITERATION : 55, loss : 0.029808997152205516ITERATION : 56, loss : 0.029808997167368713ITERATION : 57, loss : 0.02980899718269443ITERATION : 58, loss : 0.02980899718020016ITERATION : 59, loss : 0.029808997180631874ITERATION : 60, loss : 0.029808997180631874ITERATION : 61, loss : 0.029808997180631874ITERATION : 62, loss : 0.029808997180631874ITERATION : 63, loss : 0.029808997180631874ITERATION : 64, loss : 0.029808997180631874ITERATION : 65, loss : 0.029808997180631874ITERATION : 66, loss : 0.029808997180631874ITERATION : 67, loss : 0.029808997180631874ITERATION : 68, loss : 0.029808997180631874ITERATION : 69, loss : 0.029808997180631874ITERATION : 70, loss : 0.029808997180631874ITERATION : 71, loss : 0.029808997180631874ITERATION : 72, loss : 0.029808997180631874ITERATION : 73, loss : 0.029808997180631874ITERATION : 74, loss : 0.029808997180631874ITERATION : 75, loss : 0.029808997180631874ITERATION : 76, loss : 0.029808997180631874ITERATION : 77, loss : 0.029808997180631874ITERATION : 78, loss : 0.029808997180631874ITERATION : 79, loss : 0.029808997180631874ITERATION : 80, loss : 0.029808997180631874ITERATION : 81, loss : 0.029808997180631874ITERATION : 82, loss : 0.029808997180631874ITERATION : 83, loss : 0.029808997180631874ITERATION : 84, loss : 0.029808997180631874ITERATION : 85, loss : 0.029808997180631874ITERATION : 86, loss : 0.029808997180631874ITERATION : 87, loss : 0.029808997180631874ITERATION : 88, loss : 0.029808997180631874ITERATION : 89, loss : 0.029808997180631874ITERATION : 90, loss : 0.029808997180631874ITERATION : 91, loss : 0.029808997180631874ITERATION : 92, loss : 0.029808997180631874ITERATION : 93, loss : 0.029808997180631874ITERATION : 94, loss : 0.029808997180631874ITERATION : 95, loss : 0.029808997180631874ITERATION : 96, loss : 0.029808997180631874ITERATION : 97, loss : 0.029808997180631874ITERATION : 98, loss : 0.029808997180631874ITERATION : 99, loss : 0.029808997180631874ITERATION : 100, loss : 0.029808997180631874
ITERATION : 1, loss : 0.041006206173361495ITERATION : 2, loss : 0.02994396020791318ITERATION : 3, loss : 0.02633121299233969ITERATION : 4, loss : 0.024832526857906614ITERATION : 5, loss : 0.024119892447783703ITERATION : 6, loss : 0.02374848219060877ITERATION : 7, loss : 0.023541601665246183ITERATION : 8, loss : 0.023420563469851947ITERATION : 9, loss : 0.023347177672193766ITERATION : 10, loss : 0.023301562637064618ITERATION : 11, loss : 0.02327274098652816ITERATION : 12, loss : 0.023254351039294886ITERATION : 13, loss : 0.023242562123841376ITERATION : 14, loss : 0.023235000844322933ITERATION : 15, loss : 0.023230164841404027ITERATION : 16, loss : 0.02322709077883605ITERATION : 17, loss : 0.02322515564863852ITERATION : 18, loss : 0.023223953589927876ITERATION : 19, loss : 0.023223220810238258ITERATION : 20, loss : 0.023222786166755917ITERATION : 21, loss : 0.02322253800870353ITERATION : 22, loss : 0.023222405050039377ITERATION : 23, loss : 0.023222341385790636ITERATION : 24, loss : 0.02322231814184942ITERATION : 25, loss : 0.023222317523645553ITERATION : 26, loss : 0.023222328243499832ITERATION : 27, loss : 0.02322234393236211ITERATION : 28, loss : 0.023222360991203228ITERATION : 29, loss : 0.02322237750157079ITERATION : 30, loss : 0.02322239201886233ITERATION : 31, loss : 0.023222404268121832ITERATION : 32, loss : 0.02322241479269063ITERATION : 33, loss : 0.023222423286088225ITERATION : 34, loss : 0.02322243009160859ITERATION : 35, loss : 0.023222435452797213ITERATION : 36, loss : 0.023222439689229057ITERATION : 37, loss : 0.023222442938876542ITERATION : 38, loss : 0.023222445370261545ITERATION : 39, loss : 0.02322244728104217ITERATION : 40, loss : 0.023222448795257474ITERATION : 41, loss : 0.023222449977802245ITERATION : 42, loss : 0.023222450882772076ITERATION : 43, loss : 0.023222451601863314ITERATION : 44, loss : 0.02322245220527136ITERATION : 45, loss : 0.023222452615388384ITERATION : 46, loss : 0.023222452902764024ITERATION : 47, loss : 0.023222453102948636ITERATION : 48, loss : 0.023222453247254138ITERATION : 49, loss : 0.023222453386889143ITERATION : 50, loss : 0.02322245344926708ITERATION : 51, loss : 0.023222453502080746ITERATION : 52, loss : 0.023222453537061646ITERATION : 53, loss : 0.02322245357467871ITERATION : 54, loss : 0.023222453590261653ITERATION : 55, loss : 0.0232224535907945ITERATION : 56, loss : 0.0232224535907945ITERATION : 57, loss : 0.0232224535907945ITERATION : 58, loss : 0.0232224535907945ITERATION : 59, loss : 0.0232224535907945ITERATION : 60, loss : 0.0232224535907945ITERATION : 61, loss : 0.0232224535907945ITERATION : 62, loss : 0.0232224535907945ITERATION : 63, loss : 0.0232224535907945ITERATION : 64, loss : 0.0232224535907945ITERATION : 65, loss : 0.0232224535907945ITERATION : 66, loss : 0.0232224535907945ITERATION : 67, loss : 0.0232224535907945ITERATION : 68, loss : 0.0232224535907945ITERATION : 69, loss : 0.0232224535907945ITERATION : 70, loss : 0.0232224535907945ITERATION : 71, loss : 0.0232224535907945ITERATION : 72, loss : 0.0232224535907945ITERATION : 73, loss : 0.0232224535907945ITERATION : 74, loss : 0.0232224535907945ITERATION : 75, loss : 0.0232224535907945ITERATION : 76, loss : 0.0232224535907945ITERATION : 77, loss : 0.0232224535907945ITERATION : 78, loss : 0.0232224535907945ITERATION : 79, loss : 0.0232224535907945ITERATION : 80, loss : 0.0232224535907945ITERATION : 81, loss : 0.0232224535907945ITERATION : 82, loss : 0.0232224535907945ITERATION : 83, loss : 0.0232224535907945ITERATION : 84, loss : 0.0232224535907945ITERATION : 85, loss : 0.0232224535907945ITERATION : 86, loss : 0.0232224535907945ITERATION : 87, loss : 0.0232224535907945ITERATION : 88, loss : 0.0232224535907945ITERATION : 89, loss : 0.0232224535907945ITERATION : 90, loss : 0.0232224535907945ITERATION : 91, loss : 0.0232224535907945ITERATION : 92, loss : 0.0232224535907945ITERATION : 93, loss : 0.0232224535907945ITERATION : 94, loss : 0.0232224535907945ITERATION : 95, loss : 0.0232224535907945ITERATION : 96, loss : 0.0232224535907945ITERATION : 97, loss : 0.0232224535907945ITERATION : 98, loss : 0.0232224535907945ITERATION : 99, loss : 0.0232224535907945ITERATION : 100, loss : 0.0232224535907945
ITERATION : 1, loss : 0.04274504133645642ITERATION : 2, loss : 0.0280688287445388ITERATION : 3, loss : 0.023102386012164366ITERATION : 4, loss : 0.02066737451709255ITERATION : 5, loss : 0.019467603874849786ITERATION : 6, loss : 0.018819300904937677ITERATION : 7, loss : 0.01844343884720027ITERATION : 8, loss : 0.018213870307958287ITERATION : 9, loss : 0.01806836605199197ITERATION : 10, loss : 0.017973766273202383ITERATION : 11, loss : 0.017911200586730874ITERATION : 12, loss : 0.017869348364337862ITERATION : 13, loss : 0.017841140971111138ITERATION : 14, loss : 0.017822035303207494ITERATION : 15, loss : 0.0178090519998976ITERATION : 16, loss : 0.01780020990473402ITERATION : 17, loss : 0.01779417936277994ITERATION : 18, loss : 0.017790062299379125ITERATION : 19, loss : 0.01778724982261953ITERATION : 20, loss : 0.017785327680491233ITERATION : 21, loss : 0.017784013589705022ITERATION : 22, loss : 0.0177831150039892ITERATION : 23, loss : 0.017782500500561627ITERATION : 24, loss : 0.01778208022937946ITERATION : 25, loss : 0.01778179276931492ITERATION : 26, loss : 0.017781596158963145ITERATION : 27, loss : 0.017781461652998612ITERATION : 28, loss : 0.017781369644623273ITERATION : 29, loss : 0.01778130670678545ITERATION : 30, loss : 0.017781263658388977ITERATION : 31, loss : 0.01778123419803633ITERATION : 32, loss : 0.017781214019153814ITERATION : 33, loss : 0.017781200242699104ITERATION : 34, loss : 0.017781190805680205ITERATION : 35, loss : 0.017781184357593453ITERATION : 36, loss : 0.01778117994997383ITERATION : 37, loss : 0.01778117692567311ITERATION : 38, loss : 0.017781174866949256ITERATION : 39, loss : 0.01778117345797045ITERATION : 40, loss : 0.017781172499410774ITERATION : 41, loss : 0.017781171840660956ITERATION : 42, loss : 0.01778117141257262ITERATION : 43, loss : 0.017781171092938936ITERATION : 44, loss : 0.01778117087779937ITERATION : 45, loss : 0.017781170742512134ITERATION : 46, loss : 0.01778117064448147ITERATION : 47, loss : 0.017781170573395975ITERATION : 48, loss : 0.017781170547346698ITERATION : 49, loss : 0.017781170501370784ITERATION : 50, loss : 0.017781170500276944ITERATION : 51, loss : 0.017781170500276944ITERATION : 52, loss : 0.017781170500276944ITERATION : 53, loss : 0.017781170500276944ITERATION : 54, loss : 0.017781170500276944ITERATION : 55, loss : 0.017781170500276944ITERATION : 56, loss : 0.017781170500276944ITERATION : 57, loss : 0.017781170500276944ITERATION : 58, loss : 0.017781170500276944ITERATION : 59, loss : 0.017781170500276944ITERATION : 60, loss : 0.017781170500276944ITERATION : 61, loss : 0.017781170500276944ITERATION : 62, loss : 0.017781170500276944ITERATION : 63, loss : 0.017781170500276944ITERATION : 64, loss : 0.017781170500276944ITERATION : 65, loss : 0.017781170500276944ITERATION : 66, loss : 0.017781170500276944ITERATION : 67, loss : 0.017781170500276944ITERATION : 68, loss : 0.017781170500276944ITERATION : 69, loss : 0.017781170500276944ITERATION : 70, loss : 0.017781170500276944ITERATION : 71, loss : 0.017781170500276944ITERATION : 72, loss : 0.017781170500276944ITERATION : 73, loss : 0.017781170500276944ITERATION : 74, loss : 0.017781170500276944ITERATION : 75, loss : 0.017781170500276944ITERATION : 76, loss : 0.017781170500276944ITERATION : 77, loss : 0.017781170500276944ITERATION : 78, loss : 0.017781170500276944ITERATION : 79, loss : 0.017781170500276944ITERATION : 80, loss : 0.017781170500276944ITERATION : 81, loss : 0.017781170500276944ITERATION : 82, loss : 0.017781170500276944ITERATION : 83, loss : 0.017781170500276944ITERATION : 84, loss : 0.017781170500276944ITERATION : 85, loss : 0.017781170500276944ITERATION : 86, loss : 0.017781170500276944ITERATION : 87, loss : 0.017781170500276944ITERATION : 88, loss : 0.017781170500276944ITERATION : 89, loss : 0.017781170500276944ITERATION : 90, loss : 0.017781170500276944ITERATION : 91, loss : 0.017781170500276944ITERATION : 92, loss : 0.017781170500276944ITERATION : 93, loss : 0.017781170500276944ITERATION : 94, loss : 0.017781170500276944ITERATION : 95, loss : 0.017781170500276944ITERATION : 96, loss : 0.017781170500276944ITERATION : 97, loss : 0.017781170500276944ITERATION : 98, loss : 0.017781170500276944ITERATION : 99, loss : 0.017781170500276944ITERATION : 100, loss : 0.017781170500276944
ITERATION : 1, loss : 0.04664655152573206ITERATION : 2, loss : 0.04035195133586118ITERATION : 3, loss : 0.03632644470613136ITERATION : 4, loss : 0.03382786494729435ITERATION : 5, loss : 0.03271583513498712ITERATION : 6, loss : 0.03194351430704644ITERATION : 7, loss : 0.031402228656840336ITERATION : 8, loss : 0.03101981107930439ITERATION : 9, loss : 0.030875491746963ITERATION : 10, loss : 0.03086957468784507ITERATION : 11, loss : 0.03086627549620173ITERATION : 12, loss : 0.030864529607964243ITERATION : 13, loss : 0.030863703552773875ITERATION : 14, loss : 0.030863410581348723ITERATION : 15, loss : 0.03086341299594127ITERATION : 16, loss : 0.030863565956482014ITERATION : 17, loss : 0.030863783259963293ITERATION : 18, loss : 0.03086401551907376ITERATION : 19, loss : 0.030864236182076864ITERATION : 20, loss : 0.030864432705989604ITERATION : 21, loss : 0.030864600607326723ITERATION : 22, loss : 0.030864740031897767ITERATION : 23, loss : 0.030864853357773482ITERATION : 24, loss : 0.030864944007861866ITERATION : 25, loss : 0.03086501561169243ITERATION : 26, loss : 0.030865071540254422ITERATION : 27, loss : 0.030865114912371434ITERATION : 28, loss : 0.03086514828554948ITERATION : 29, loss : 0.03086517385699423ITERATION : 30, loss : 0.03086519328776197ITERATION : 31, loss : 0.030865208009951933ITERATION : 32, loss : 0.03086521911788353ITERATION : 33, loss : 0.03086522748061687ITERATION : 34, loss : 0.030865233747992375ITERATION : 35, loss : 0.030865238420527668ITERATION : 36, loss : 0.030865241914486894ITERATION : 37, loss : 0.030865244495954482ITERATION : 38, loss : 0.030865246418839046ITERATION : 39, loss : 0.03086524784739576ITERATION : 40, loss : 0.030865248910700922ITERATION : 41, loss : 0.030865249703998777ITERATION : 42, loss : 0.03086525029425745ITERATION : 43, loss : 0.030865250728186282ITERATION : 44, loss : 0.030865251056196488ITERATION : 45, loss : 0.03086525128895891ITERATION : 46, loss : 0.030865251470848364ITERATION : 47, loss : 0.03086525159318023ITERATION : 48, loss : 0.030865251687837648ITERATION : 49, loss : 0.030865251754409174ITERATION : 50, loss : 0.030865251804008106ITERATION : 51, loss : 0.030865251848596568ITERATION : 52, loss : 0.030865251867346184ITERATION : 53, loss : 0.030865251892273ITERATION : 54, loss : 0.030865251915905424ITERATION : 55, loss : 0.030865251924140757ITERATION : 56, loss : 0.030865251925560055ITERATION : 57, loss : 0.030865251925560055ITERATION : 58, loss : 0.030865251925560055ITERATION : 59, loss : 0.030865251925560055ITERATION : 60, loss : 0.030865251925560055ITERATION : 61, loss : 0.030865251925560055ITERATION : 62, loss : 0.030865251925560055ITERATION : 63, loss : 0.030865251925560055ITERATION : 64, loss : 0.030865251925560055ITERATION : 65, loss : 0.030865251925560055ITERATION : 66, loss : 0.030865251925560055ITERATION : 67, loss : 0.030865251925560055ITERATION : 68, loss : 0.030865251925560055ITERATION : 69, loss : 0.030865251925560055ITERATION : 70, loss : 0.030865251925560055ITERATION : 71, loss : 0.030865251925560055ITERATION : 72, loss : 0.030865251925560055ITERATION : 73, loss : 0.030865251925560055ITERATION : 74, loss : 0.030865251925560055ITERATION : 75, loss : 0.030865251925560055ITERATION : 76, loss : 0.030865251925560055ITERATION : 77, loss : 0.030865251925560055ITERATION : 78, loss : 0.030865251925560055ITERATION : 79, loss : 0.030865251925560055ITERATION : 80, loss : 0.030865251925560055ITERATION : 81, loss : 0.030865251925560055ITERATION : 82, loss : 0.030865251925560055ITERATION : 83, loss : 0.030865251925560055ITERATION : 84, loss : 0.030865251925560055ITERATION : 85, loss : 0.030865251925560055ITERATION : 86, loss : 0.030865251925560055ITERATION : 87, loss : 0.030865251925560055ITERATION : 88, loss : 0.030865251925560055ITERATION : 89, loss : 0.030865251925560055ITERATION : 90, loss : 0.030865251925560055ITERATION : 91, loss : 0.030865251925560055ITERATION : 92, loss : 0.030865251925560055ITERATION : 93, loss : 0.030865251925560055ITERATION : 94, loss : 0.030865251925560055ITERATION : 95, loss : 0.030865251925560055ITERATION : 96, loss : 0.030865251925560055ITERATION : 97, loss : 0.030865251925560055ITERATION : 98, loss : 0.030865251925560055ITERATION : 99, loss : 0.030865251925560055ITERATION : 100, loss : 0.030865251925560055
ITERATION : 1, loss : 0.04437787057320894ITERATION : 2, loss : 0.0500486536804803ITERATION : 3, loss : 0.05299701236829309ITERATION : 4, loss : 0.054270954415495135ITERATION : 5, loss : 0.05476727222841146ITERATION : 6, loss : 0.05491084468043939ITERATION : 7, loss : 0.05489947085459088ITERATION : 8, loss : 0.05482586687848858ITERATION : 9, loss : 0.054733410373430186ITERATION : 10, loss : 0.05464185771500976ITERATION : 11, loss : 0.05455949538050637ITERATION : 12, loss : 0.05448904017413658ITERATION : 13, loss : 0.05443057139570853ITERATION : 14, loss : 0.05438300720837401ITERATION : 15, loss : 0.05434484914011027ITERATION : 16, loss : 0.05431454758083658ITERATION : 17, loss : 0.05429067069213839ITERATION : 18, loss : 0.054271969770206326ITERATION : 19, loss : 0.054257393922444365ITERATION : 20, loss : 0.054246078374961385ITERATION : 21, loss : 0.05423732315755733ITERATION : 22, loss : 0.05423056775277113ITERATION : 23, loss : 0.05422536813888914ITERATION : 24, loss : 0.05422137451697444ITERATION : 25, loss : 0.0542183129719888ITERATION : 26, loss : 0.05421596963646902ITERATION : 27, loss : 0.054214178849143944ITERATION : 28, loss : 0.05421281200650158ITERATION : 29, loss : 0.054211770068507276ITERATION : 30, loss : 0.05421097664113798ITERATION : 31, loss : 0.054210373165366726ITERATION : 32, loss : 0.05420991456867472ITERATION : 33, loss : 0.054209566287713826ITERATION : 34, loss : 0.05420930201191836ITERATION : 35, loss : 0.05420910164218898ITERATION : 36, loss : 0.05420894980833164ITERATION : 37, loss : 0.05420883476472202ITERATION : 38, loss : 0.0542087478325359ITERATION : 39, loss : 0.05420868205075518ITERATION : 40, loss : 0.05420863224872101ITERATION : 41, loss : 0.05420859468206244ITERATION : 42, loss : 0.05420856632646755ITERATION : 43, loss : 0.05420854491940657ITERATION : 44, loss : 0.05420852878732971ITERATION : 45, loss : 0.05420851656027364ITERATION : 46, loss : 0.054208507356948175ITERATION : 47, loss : 0.0542085004188295ITERATION : 48, loss : 0.054208495279981304ITERATION : 49, loss : 0.054208491327418964ITERATION : 50, loss : 0.054208488336930094ITERATION : 51, loss : 0.05420848613980665ITERATION : 52, loss : 0.05420848449971277ITERATION : 53, loss : 0.054208483199519895ITERATION : 54, loss : 0.05420848229274538ITERATION : 55, loss : 0.05420848156892557ITERATION : 56, loss : 0.054208481006234224ITERATION : 57, loss : 0.05420848064214419ITERATION : 58, loss : 0.054208480333943536ITERATION : 59, loss : 0.05420847996001979ITERATION : 60, loss : 0.054208479902956984ITERATION : 61, loss : 0.0542084798568794ITERATION : 62, loss : 0.054208479758088506ITERATION : 63, loss : 0.054208479756784ITERATION : 64, loss : 0.054208479756784ITERATION : 65, loss : 0.054208479756784ITERATION : 66, loss : 0.054208479756784ITERATION : 67, loss : 0.054208479756784ITERATION : 68, loss : 0.054208479756784ITERATION : 69, loss : 0.054208479756784ITERATION : 70, loss : 0.054208479756784ITERATION : 71, loss : 0.054208479756784ITERATION : 72, loss : 0.054208479756784ITERATION : 73, loss : 0.054208479756784ITERATION : 74, loss : 0.054208479756784ITERATION : 75, loss : 0.054208479756784ITERATION : 76, loss : 0.054208479756784ITERATION : 77, loss : 0.054208479756784ITERATION : 78, loss : 0.054208479756784ITERATION : 79, loss : 0.054208479756784ITERATION : 80, loss : 0.054208479756784ITERATION : 81, loss : 0.054208479756784ITERATION : 82, loss : 0.054208479756784ITERATION : 83, loss : 0.054208479756784ITERATION : 84, loss : 0.054208479756784ITERATION : 85, loss : 0.054208479756784ITERATION : 86, loss : 0.054208479756784ITERATION : 87, loss : 0.054208479756784ITERATION : 88, loss : 0.054208479756784ITERATION : 89, loss : 0.054208479756784ITERATION : 90, loss : 0.054208479756784ITERATION : 91, loss : 0.054208479756784ITERATION : 92, loss : 0.054208479756784ITERATION : 93, loss : 0.054208479756784ITERATION : 94, loss : 0.054208479756784ITERATION : 95, loss : 0.054208479756784ITERATION : 96, loss : 0.054208479756784ITERATION : 97, loss : 0.054208479756784ITERATION : 98, loss : 0.054208479756784ITERATION : 99, loss : 0.054208479756784ITERATION : 100, loss : 0.054208479756784
ITERATION : 1, loss : 0.0542566550256317ITERATION : 2, loss : 0.052749839110323146ITERATION : 3, loss : 0.05168929432092881ITERATION : 4, loss : 0.050543098352926134ITERATION : 5, loss : 0.049478098670009206ITERATION : 6, loss : 0.04861294586512992ITERATION : 7, loss : 0.04795304727562978ITERATION : 8, loss : 0.04746457938297676ITERATION : 9, loss : 0.04710841797450549ITERATION : 10, loss : 0.04685080468271275ITERATION : 11, loss : 0.046665317695979674ITERATION : 12, loss : 0.04653212740225195ITERATION : 13, loss : 0.046436654161590936ITERATION : 14, loss : 0.04636829587451398ITERATION : 15, loss : 0.046319389580480486ITERATION : 16, loss : 0.04628441877573023ITERATION : 17, loss : 0.04625942150213988ITERATION : 18, loss : 0.04624155792370189ITERATION : 19, loss : 0.0462287939759352ITERATION : 20, loss : 0.04621967442266735ITERATION : 21, loss : 0.046213158752502065ITERATION : 22, loss : 0.04620850332465838ITERATION : 23, loss : 0.046205176669375646ITERATION : 24, loss : 0.04620279936421847ITERATION : 25, loss : 0.046201100154789156ITERATION : 26, loss : 0.046199885457100645ITERATION : 27, loss : 0.04619901713956444ITERATION : 28, loss : 0.04619839615231809ITERATION : 29, loss : 0.0461979520862125ITERATION : 30, loss : 0.04619763442565008ITERATION : 31, loss : 0.04619740705122964ITERATION : 32, loss : 0.04619724433767772ITERATION : 33, loss : 0.04619712788239462ITERATION : 34, loss : 0.04619704444127584ITERATION : 35, loss : 0.04619698471575132ITERATION : 36, loss : 0.04619694197727167ITERATION : 37, loss : 0.04619691132066034ITERATION : 38, loss : 0.04619688931282391ITERATION : 39, loss : 0.04619687356923171ITERATION : 40, loss : 0.046196862270420566ITERATION : 41, loss : 0.04619685416796961ITERATION : 42, loss : 0.04619684844955923ITERATION : 43, loss : 0.04619684412408335ITERATION : 44, loss : 0.0461968411010141ITERATION : 45, loss : 0.046196838879729266ITERATION : 46, loss : 0.04619683739246511ITERATION : 47, loss : 0.04619683624881539ITERATION : 48, loss : 0.04619683543471918ITERATION : 49, loss : 0.046196834950023684ITERATION : 50, loss : 0.046196834535743955ITERATION : 51, loss : 0.04619683425172716ITERATION : 52, loss : 0.04619683402851421ITERATION : 53, loss : 0.04619683402519342ITERATION : 54, loss : 0.04619683402519342ITERATION : 55, loss : 0.04619683402519342ITERATION : 56, loss : 0.04619683402519342ITERATION : 57, loss : 0.04619683402519342ITERATION : 58, loss : 0.04619683402519342ITERATION : 59, loss : 0.04619683402519342ITERATION : 60, loss : 0.04619683402519342ITERATION : 61, loss : 0.04619683402519342ITERATION : 62, loss : 0.04619683402519342ITERATION : 63, loss : 0.04619683402519342ITERATION : 64, loss : 0.04619683402519342ITERATION : 65, loss : 0.04619683402519342ITERATION : 66, loss : 0.04619683402519342ITERATION : 67, loss : 0.04619683402519342ITERATION : 68, loss : 0.04619683402519342ITERATION : 69, loss : 0.04619683402519342ITERATION : 70, loss : 0.04619683402519342ITERATION : 71, loss : 0.04619683402519342ITERATION : 72, loss : 0.04619683402519342ITERATION : 73, loss : 0.04619683402519342ITERATION : 74, loss : 0.04619683402519342ITERATION : 75, loss : 0.04619683402519342ITERATION : 76, loss : 0.04619683402519342ITERATION : 77, loss : 0.04619683402519342ITERATION : 78, loss : 0.04619683402519342ITERATION : 79, loss : 0.04619683402519342ITERATION : 80, loss : 0.04619683402519342ITERATION : 81, loss : 0.04619683402519342ITERATION : 82, loss : 0.04619683402519342ITERATION : 83, loss : 0.04619683402519342ITERATION : 84, loss : 0.04619683402519342ITERATION : 85, loss : 0.04619683402519342ITERATION : 86, loss : 0.04619683402519342ITERATION : 87, loss : 0.04619683402519342ITERATION : 88, loss : 0.04619683402519342ITERATION : 89, loss : 0.04619683402519342ITERATION : 90, loss : 0.04619683402519342ITERATION : 91, loss : 0.04619683402519342ITERATION : 92, loss : 0.04619683402519342ITERATION : 93, loss : 0.04619683402519342ITERATION : 94, loss : 0.04619683402519342ITERATION : 95, loss : 0.04619683402519342ITERATION : 96, loss : 0.04619683402519342ITERATION : 97, loss : 0.04619683402519342ITERATION : 98, loss : 0.04619683402519342ITERATION : 99, loss : 0.04619683402519342ITERATION : 100, loss : 0.04619683402519342
ITERATION : 1, loss : 0.03302262896118449ITERATION : 2, loss : 0.0266973511145158ITERATION : 3, loss : 0.02481023774312587ITERATION : 4, loss : 0.024012055678240114ITERATION : 5, loss : 0.02353318478445636ITERATION : 6, loss : 0.023198996809507065ITERATION : 7, loss : 0.022955128943888292ITERATION : 8, loss : 0.0227747614092162ITERATION : 9, loss : 0.02264061843064582ITERATION : 10, loss : 0.022540516593797362ITERATION : 11, loss : 0.02246562687372968ITERATION : 12, loss : 0.022409485303257582ITERATION : 13, loss : 0.022367330466226805ITERATION : 14, loss : 0.02233563791540128ITERATION : 15, loss : 0.022311788141168493ITERATION : 16, loss : 0.022293827476713313ITERATION : 17, loss : 0.022280294676965313ITERATION : 18, loss : 0.022270094213383794ITERATION : 19, loss : 0.022262403454328294ITERATION : 20, loss : 0.022256603929355887ITERATION : 21, loss : 0.022252230022864448ITERATION : 22, loss : 0.022248931087460493ITERATION : 23, loss : 0.022246442875623273ITERATION : 24, loss : 0.022244566213370263ITERATION : 25, loss : 0.022243150738647615ITERATION : 26, loss : 0.022242083136243283ITERATION : 27, loss : 0.02224127796020195ITERATION : 28, loss : 0.022240670732553883ITERATION : 29, loss : 0.022240212816665092ITERATION : 30, loss : 0.022239867511899984ITERATION : 31, loss : 0.022239607104856143ITERATION : 32, loss : 0.02223941076545249ITERATION : 33, loss : 0.022239262728187006ITERATION : 34, loss : 0.022239151151352513ITERATION : 35, loss : 0.022239067022825697ITERATION : 36, loss : 0.022239003584868026ITERATION : 37, loss : 0.02223895578005383ITERATION : 38, loss : 0.022238919758609355ITERATION : 39, loss : 0.022238892594024ITERATION : 40, loss : 0.02223887213368634ITERATION : 41, loss : 0.02223885671123665ITERATION : 42, loss : 0.02223884507620337ITERATION : 43, loss : 0.022238836314326393ITERATION : 44, loss : 0.02223882969560311ITERATION : 45, loss : 0.022238824725082602ITERATION : 46, loss : 0.02223882095802128ITERATION : 47, loss : 0.02223881810220901ITERATION : 48, loss : 0.022238815962577433ITERATION : 49, loss : 0.022238814372437885ITERATION : 50, loss : 0.02223881319865544ITERATION : 51, loss : 0.022238812272094507ITERATION : 52, loss : 0.0222388116041464ITERATION : 53, loss : 0.022238811077964736ITERATION : 54, loss : 0.022238810702855685ITERATION : 55, loss : 0.022238810356395945ITERATION : 56, loss : 0.022238810182313055ITERATION : 57, loss : 0.022238810057672045ITERATION : 58, loss : 0.02223880994939624ITERATION : 59, loss : 0.02223880985988963ITERATION : 60, loss : 0.022238809777780064ITERATION : 61, loss : 0.02223880971350036ITERATION : 62, loss : 0.022238809678106833ITERATION : 63, loss : 0.02223880967511694ITERATION : 64, loss : 0.02223880967511694ITERATION : 65, loss : 0.02223880967511694ITERATION : 66, loss : 0.02223880967511694ITERATION : 67, loss : 0.02223880967511694ITERATION : 68, loss : 0.02223880967511694ITERATION : 69, loss : 0.02223880967511694ITERATION : 70, loss : 0.02223880967511694ITERATION : 71, loss : 0.02223880967511694ITERATION : 72, loss : 0.02223880967511694ITERATION : 73, loss : 0.02223880967511694ITERATION : 74, loss : 0.02223880967511694ITERATION : 75, loss : 0.02223880967511694ITERATION : 76, loss : 0.02223880967511694ITERATION : 77, loss : 0.02223880967511694ITERATION : 78, loss : 0.02223880967511694ITERATION : 79, loss : 0.02223880967511694ITERATION : 80, loss : 0.02223880967511694ITERATION : 81, loss : 0.02223880967511694ITERATION : 82, loss : 0.02223880967511694ITERATION : 83, loss : 0.02223880967511694ITERATION : 84, loss : 0.02223880967511694ITERATION : 85, loss : 0.02223880967511694ITERATION : 86, loss : 0.02223880967511694ITERATION : 87, loss : 0.02223880967511694ITERATION : 88, loss : 0.02223880967511694ITERATION : 89, loss : 0.02223880967511694ITERATION : 90, loss : 0.02223880967511694ITERATION : 91, loss : 0.02223880967511694ITERATION : 92, loss : 0.02223880967511694ITERATION : 93, loss : 0.02223880967511694ITERATION : 94, loss : 0.02223880967511694ITERATION : 95, loss : 0.02223880967511694ITERATION : 96, loss : 0.02223880967511694ITERATION : 97, loss : 0.02223880967511694ITERATION : 98, loss : 0.02223880967511694ITERATION : 99, loss : 0.02223880967511694ITERATION : 100, loss : 0.02223880967511694
gradient norm in None layer : 0.0005065981222365997
gradient norm in None layer : 2.774259534959301e-05
gradient norm in None layer : 3.6554275643395644e-05
gradient norm in None layer : 0.00040446204898777804
gradient norm in None layer : 4.3665251333561035e-05
gradient norm in None layer : 5.433588278966336e-05
gradient norm in None layer : 0.00017454234461858383
gradient norm in None layer : 8.666678247350405e-06
gradient norm in None layer : 6.454509713517161e-06
gradient norm in None layer : 0.00017296417629973607
gradient norm in None layer : 8.269711447891117e-06
gradient norm in None layer : 6.464865411052354e-06
gradient norm in None layer : 6.48754827513351e-05
gradient norm in None layer : 2.262719024944072e-06
gradient norm in None layer : 1.632230483252946e-06
gradient norm in None layer : 5.644634354046856e-05
gradient norm in None layer : 2.709374482503619e-06
gradient norm in None layer : 1.8505723336059865e-06
gradient norm in None layer : 7.596880866075666e-05
gradient norm in None layer : 9.269490832458067e-07
gradient norm in None layer : 0.00015838995302090237
gradient norm in None layer : 1.0968352376173573e-05
gradient norm in None layer : 8.160862927551623e-06
gradient norm in None layer : 0.00020044757806173292
gradient norm in None layer : 1.964641966219745e-05
gradient norm in None layer : 2.4577697458485834e-05
gradient norm in None layer : 0.0003364645865302745
gradient norm in None layer : 2.6234613000922772e-06
gradient norm in None layer : 0.0005816336386763253
gradient norm in None layer : 4.678352768856281e-05
gradient norm in None layer : 5.790526491652258e-05
gradient norm in None layer : 0.0007083256121960417
gradient norm in None layer : 6.057140680908226e-05
gradient norm in None layer : 8.152133069870386e-05
gradient norm in None layer : 5.744579805136646e-05
gradient norm in None layer : 1.1489694769802096e-05
Total gradient norm: 0.001240951444626856
invariance loss : 4.267590418693412, avg_den : 0.42104339599609375, density loss : 0.32104339599609377, mse loss : 0.03319743759489944, solver time : 130.88494658470154 sec , total loss : 0.03778607140958895, running loss : 0.04917877832652136
Epoch 0/10 , batch 18/12500 
ITERATION : 1, loss : 0.03980127268916352ITERATION : 2, loss : 0.031783744101434884ITERATION : 3, loss : 0.02739580211636247ITERATION : 4, loss : 0.024681006264280023ITERATION : 5, loss : 0.022947484506552002ITERATION : 6, loss : 0.021818942599769423ITERATION : 7, loss : 0.02107264122932083ITERATION : 8, loss : 0.020572510142572574ITERATION : 9, loss : 0.02023363023356404ITERATION : 10, loss : 0.02000195728127281ITERATION : 11, loss : 0.019842458149348534ITERATION : 12, loss : 0.01973204649461007ITERATION : 13, loss : 0.019655292832062573ITERATION : 14, loss : 0.019601764450478173ITERATION : 15, loss : 0.019564341309395078ITERATION : 16, loss : 0.019538128248218826ITERATION : 17, loss : 0.019519740519084776ITERATION : 18, loss : 0.019506827471332325ITERATION : 19, loss : 0.019497751070108583ITERATION : 20, loss : 0.019491366881105896ITERATION : 21, loss : 0.01948687380124467ITERATION : 22, loss : 0.01948371014910662ITERATION : 23, loss : 0.019481481652422056ITERATION : 24, loss : 0.01947991137168432ITERATION : 25, loss : 0.019478804470686263ITERATION : 26, loss : 0.019478024067537233ITERATION : 27, loss : 0.019477473765415348ITERATION : 28, loss : 0.019477085514554755ITERATION : 29, loss : 0.019476811516498053ITERATION : 30, loss : 0.019476618168782735ITERATION : 31, loss : 0.01947648168173664ITERATION : 32, loss : 0.019476385283759248ITERATION : 33, loss : 0.019476317216809737ITERATION : 34, loss : 0.019476269151879815ITERATION : 35, loss : 0.01947623517007009ITERATION : 36, loss : 0.019476211148349385ITERATION : 37, loss : 0.01947619417498826ITERATION : 38, loss : 0.019476182173020155ITERATION : 39, loss : 0.019476173690901193ITERATION : 40, loss : 0.019476167676374976ITERATION : 41, loss : 0.019476163412161122ITERATION : 42, loss : 0.01947616045451298ITERATION : 43, loss : 0.019476158321581134ITERATION : 44, loss : 0.019476156847897042ITERATION : 45, loss : 0.019476155838965583ITERATION : 46, loss : 0.01947615506617684ITERATION : 47, loss : 0.019476154531855156ITERATION : 48, loss : 0.019476154166430126ITERATION : 49, loss : 0.01947615385005882ITERATION : 50, loss : 0.019476153682539403ITERATION : 51, loss : 0.01947615361382069ITERATION : 52, loss : 0.019476153316069167ITERATION : 53, loss : 0.019476153316069167ITERATION : 54, loss : 0.019476153316069167ITERATION : 55, loss : 0.019476153316069167ITERATION : 56, loss : 0.019476153316069167ITERATION : 57, loss : 0.019476153316069167ITERATION : 58, loss : 0.019476153316069167ITERATION : 59, loss : 0.019476153316069167ITERATION : 60, loss : 0.019476153316069167ITERATION : 61, loss : 0.019476153316069167ITERATION : 62, loss : 0.019476153316069167ITERATION : 63, loss : 0.019476153316069167ITERATION : 64, loss : 0.019476153316069167ITERATION : 65, loss : 0.019476153316069167ITERATION : 66, loss : 0.019476153316069167ITERATION : 67, loss : 0.019476153316069167ITERATION : 68, loss : 0.019476153316069167ITERATION : 69, loss : 0.019476153316069167ITERATION : 70, loss : 0.019476153316069167ITERATION : 71, loss : 0.019476153316069167ITERATION : 72, loss : 0.019476153316069167ITERATION : 73, loss : 0.019476153316069167ITERATION : 74, loss : 0.019476153316069167ITERATION : 75, loss : 0.019476153316069167ITERATION : 76, loss : 0.019476153316069167ITERATION : 77, loss : 0.019476153316069167ITERATION : 78, loss : 0.019476153316069167ITERATION : 79, loss : 0.019476153316069167ITERATION : 80, loss : 0.019476153316069167ITERATION : 81, loss : 0.019476153316069167ITERATION : 82, loss : 0.019476153316069167ITERATION : 83, loss : 0.019476153316069167ITERATION : 84, loss : 0.019476153316069167ITERATION : 85, loss : 0.019476153316069167ITERATION : 86, loss : 0.019476153316069167ITERATION : 87, loss : 0.019476153316069167ITERATION : 88, loss : 0.019476153316069167ITERATION : 89, loss : 0.019476153316069167ITERATION : 90, loss : 0.019476153316069167ITERATION : 91, loss : 0.019476153316069167ITERATION : 92, loss : 0.019476153316069167ITERATION : 93, loss : 0.019476153316069167ITERATION : 94, loss : 0.019476153316069167ITERATION : 95, loss : 0.019476153316069167ITERATION : 96, loss : 0.019476153316069167ITERATION : 97, loss : 0.019476153316069167ITERATION : 98, loss : 0.019476153316069167ITERATION : 99, loss : 0.019476153316069167ITERATION : 100, loss : 0.019476153316069167
ITERATION : 1, loss : 0.056194542153009626ITERATION : 2, loss : 0.042407669271805885ITERATION : 3, loss : 0.03434702104134423ITERATION : 4, loss : 0.02945013535990767ITERATION : 5, loss : 0.026329518241675058ITERATION : 6, loss : 0.02427586607152618ITERATION : 7, loss : 0.022895362469535246ITERATION : 8, loss : 0.02195373107507056ITERATION : 9, loss : 0.02130481937843818ITERATION : 10, loss : 0.020854349899599237ITERATION : 11, loss : 0.020540003993617408ITERATION : 12, loss : 0.02031983359469201ITERATION : 13, loss : 0.020165218907658145ITERATION : 14, loss : 0.02005643899763854ITERATION : 15, loss : 0.019979805948955157ITERATION : 16, loss : 0.019925769589713017ITERATION : 17, loss : 0.01988764183824509ITERATION : 18, loss : 0.019860726503920283ITERATION : 19, loss : 0.019841719848235087ITERATION : 20, loss : 0.01982829470526909ITERATION : 21, loss : 0.019818810308209873ITERATION : 22, loss : 0.019812108874058022ITERATION : 23, loss : 0.01980737337198321ITERATION : 24, loss : 0.019804026643226752ITERATION : 25, loss : 0.019801661344037623ITERATION : 26, loss : 0.019799989537044146ITERATION : 27, loss : 0.01979880773388001ITERATION : 28, loss : 0.01979797235568495ITERATION : 29, loss : 0.019797381835273457ITERATION : 30, loss : 0.01979696422412829ITERATION : 31, loss : 0.01979666899734131ITERATION : 32, loss : 0.019796460190879658ITERATION : 33, loss : 0.019796312562639758ITERATION : 34, loss : 0.019796208160676846ITERATION : 35, loss : 0.01979613435270821ITERATION : 36, loss : 0.01979608217902299ITERATION : 37, loss : 0.01979604533403412ITERATION : 38, loss : 0.01979601926241682ITERATION : 39, loss : 0.019796000921279117ITERATION : 40, loss : 0.019795987872611644ITERATION : 41, loss : 0.019795978719019343ITERATION : 42, loss : 0.019795972075117026ITERATION : 43, loss : 0.01979596754858471ITERATION : 44, loss : 0.01979596435944607ITERATION : 45, loss : 0.01979596207034853ITERATION : 46, loss : 0.019795960380276ITERATION : 47, loss : 0.019795959278650327ITERATION : 48, loss : 0.019795958496000472ITERATION : 49, loss : 0.019795958059807064ITERATION : 50, loss : 0.01979595773732529ITERATION : 51, loss : 0.01979595743705138ITERATION : 52, loss : 0.01979595717106003ITERATION : 53, loss : 0.019795957070921376ITERATION : 54, loss : 0.019795956928012278ITERATION : 55, loss : 0.01979595689920683ITERATION : 56, loss : 0.019795956898565364ITERATION : 57, loss : 0.019795956898565364ITERATION : 58, loss : 0.019795956898565364ITERATION : 59, loss : 0.019795956898565364ITERATION : 60, loss : 0.019795956898565364ITERATION : 61, loss : 0.019795956898565364ITERATION : 62, loss : 0.019795956898565364ITERATION : 63, loss : 0.019795956898565364ITERATION : 64, loss : 0.019795956898565364ITERATION : 65, loss : 0.019795956898565364ITERATION : 66, loss : 0.019795956898565364ITERATION : 67, loss : 0.019795956898565364ITERATION : 68, loss : 0.019795956898565364ITERATION : 69, loss : 0.019795956898565364ITERATION : 70, loss : 0.019795956898565364ITERATION : 71, loss : 0.019795956898565364ITERATION : 72, loss : 0.019795956898565364ITERATION : 73, loss : 0.019795956898565364ITERATION : 74, loss : 0.019795956898565364ITERATION : 75, loss : 0.019795956898565364ITERATION : 76, loss : 0.019795956898565364ITERATION : 77, loss : 0.019795956898565364ITERATION : 78, loss : 0.019795956898565364ITERATION : 79, loss : 0.019795956898565364ITERATION : 80, loss : 0.019795956898565364ITERATION : 81, loss : 0.019795956898565364ITERATION : 82, loss : 0.019795956898565364ITERATION : 83, loss : 0.019795956898565364ITERATION : 84, loss : 0.019795956898565364ITERATION : 85, loss : 0.019795956898565364ITERATION : 86, loss : 0.019795956898565364ITERATION : 87, loss : 0.019795956898565364ITERATION : 88, loss : 0.019795956898565364ITERATION : 89, loss : 0.019795956898565364ITERATION : 90, loss : 0.019795956898565364ITERATION : 91, loss : 0.019795956898565364ITERATION : 92, loss : 0.019795956898565364ITERATION : 93, loss : 0.019795956898565364ITERATION : 94, loss : 0.019795956898565364ITERATION : 95, loss : 0.019795956898565364ITERATION : 96, loss : 0.019795956898565364ITERATION : 97, loss : 0.019795956898565364ITERATION : 98, loss : 0.019795956898565364ITERATION : 99, loss : 0.019795956898565364ITERATION : 100, loss : 0.019795956898565364
ITERATION : 1, loss : 0.022658875930559316ITERATION : 2, loss : 0.019544631974079903ITERATION : 3, loss : 0.019910978871244985ITERATION : 4, loss : 0.01869340408659442ITERATION : 5, loss : 0.018078888210983995ITERATION : 6, loss : 0.017777664426902224ITERATION : 7, loss : 0.017635987982431257ITERATION : 8, loss : 0.017573623540382638ITERATION : 9, loss : 0.017549542489747792ITERATION : 10, loss : 0.017543133566409106ITERATION : 11, loss : 0.017544256438708602ITERATION : 12, loss : 0.017548074799651724ITERATION : 13, loss : 0.01755240012162813ITERATION : 14, loss : 0.017556336751002505ITERATION : 15, loss : 0.01755959781647081ITERATION : 16, loss : 0.017562165279711793ITERATION : 17, loss : 0.017564125360488094ITERATION : 18, loss : 0.017565592245994403ITERATION : 19, loss : 0.01756667542038719ITERATION : 20, loss : 0.017567467833350295ITERATION : 21, loss : 0.01756804380751263ITERATION : 22, loss : 0.017568460474364972ITERATION : 23, loss : 0.017568760964006364ITERATION : 24, loss : 0.017568977132882267ITERATION : 25, loss : 0.017569132403431838ITERATION : 26, loss : 0.017569243808648896ITERATION : 27, loss : 0.017569323653721273ITERATION : 28, loss : 0.017569380889226334ITERATION : 29, loss : 0.017569421871166065ITERATION : 30, loss : 0.017569451243126886ITERATION : 31, loss : 0.01756947225384775ITERATION : 32, loss : 0.01756948731242405ITERATION : 33, loss : 0.017569498115262653ITERATION : 34, loss : 0.017569505863339142ITERATION : 35, loss : 0.017569511424747ITERATION : 36, loss : 0.017569515403772176ITERATION : 37, loss : 0.017569518259804406ITERATION : 38, loss : 0.01756952030251742ITERATION : 39, loss : 0.017569521764670134ITERATION : 40, loss : 0.017569522794967332ITERATION : 41, loss : 0.017569523543614886ITERATION : 42, loss : 0.01756952406400237ITERATION : 43, loss : 0.01756952445079529ITERATION : 44, loss : 0.017569524711788697ITERATION : 45, loss : 0.017569524914323523ITERATION : 46, loss : 0.01756952504393731ITERATION : 47, loss : 0.017569525151484536ITERATION : 48, loss : 0.017569525221647717ITERATION : 49, loss : 0.017569525270633178ITERATION : 50, loss : 0.01756952530082896ITERATION : 51, loss : 0.017569525336953908ITERATION : 52, loss : 0.01756952535057836ITERATION : 53, loss : 0.017569525370804313ITERATION : 54, loss : 0.017569525374463508ITERATION : 55, loss : 0.017569525385982512ITERATION : 56, loss : 0.017569525384691576ITERATION : 57, loss : 0.017569525393913373ITERATION : 58, loss : 0.017569525389390963ITERATION : 59, loss : 0.017569525397856774ITERATION : 60, loss : 0.017569525392695948ITERATION : 61, loss : 0.01756952539287637ITERATION : 62, loss : 0.01756952539287637ITERATION : 63, loss : 0.01756952539287637ITERATION : 64, loss : 0.01756952539287637ITERATION : 65, loss : 0.01756952539287637ITERATION : 66, loss : 0.01756952539287637ITERATION : 67, loss : 0.01756952539287637ITERATION : 68, loss : 0.01756952539287637ITERATION : 69, loss : 0.01756952539287637ITERATION : 70, loss : 0.01756952539287637ITERATION : 71, loss : 0.01756952539287637ITERATION : 72, loss : 0.01756952539287637ITERATION : 73, loss : 0.01756952539287637ITERATION : 74, loss : 0.01756952539287637ITERATION : 75, loss : 0.01756952539287637ITERATION : 76, loss : 0.01756952539287637ITERATION : 77, loss : 0.01756952539287637ITERATION : 78, loss : 0.01756952539287637ITERATION : 79, loss : 0.01756952539287637ITERATION : 80, loss : 0.01756952539287637ITERATION : 81, loss : 0.01756952539287637ITERATION : 82, loss : 0.01756952539287637ITERATION : 83, loss : 0.01756952539287637ITERATION : 84, loss : 0.01756952539287637ITERATION : 85, loss : 0.01756952539287637ITERATION : 86, loss : 0.01756952539287637ITERATION : 87, loss : 0.01756952539287637ITERATION : 88, loss : 0.01756952539287637ITERATION : 89, loss : 0.01756952539287637ITERATION : 90, loss : 0.01756952539287637ITERATION : 91, loss : 0.01756952539287637ITERATION : 92, loss : 0.01756952539287637ITERATION : 93, loss : 0.01756952539287637ITERATION : 94, loss : 0.01756952539287637ITERATION : 95, loss : 0.01756952539287637ITERATION : 96, loss : 0.01756952539287637ITERATION : 97, loss : 0.01756952539287637ITERATION : 98, loss : 0.01756952539287637ITERATION : 99, loss : 0.01756952539287637ITERATION : 100, loss : 0.01756952539287637
ITERATION : 1, loss : 0.030714354245495932ITERATION : 2, loss : 0.024235518706908536ITERATION : 3, loss : 0.021623058177533166ITERATION : 4, loss : 0.021406606099787465ITERATION : 5, loss : 0.021463742369707058ITERATION : 6, loss : 0.02158169323671279ITERATION : 7, loss : 0.021695623534622725ITERATION : 8, loss : 0.021788976451779598ITERATION : 9, loss : 0.021860944718220574ITERATION : 10, loss : 0.021915044426582042ITERATION : 11, loss : 0.021955291277012828ITERATION : 12, loss : 0.021985110817744874ITERATION : 13, loss : 0.02200717046059702ITERATION : 14, loss : 0.022023477465419553ITERATION : 15, loss : 0.02203552420915796ITERATION : 16, loss : 0.022044416686307532ITERATION : 17, loss : 0.022050974438224215ITERATION : 18, loss : 0.02205580503867028ITERATION : 19, loss : 0.022059359119194828ITERATION : 20, loss : 0.022061970575342652ITERATION : 21, loss : 0.022063887139633608ITERATION : 22, loss : 0.02206529201895512ITERATION : 23, loss : 0.022066320698346267ITERATION : 24, loss : 0.022067073085420132ITERATION : 25, loss : 0.02206762286068913ITERATION : 26, loss : 0.022068024187614776ITERATION : 27, loss : 0.02206831694580183ITERATION : 28, loss : 0.0220685303394967ITERATION : 29, loss : 0.022068685750070885ITERATION : 30, loss : 0.02206879886305638ITERATION : 31, loss : 0.022068881159799995ITERATION : 32, loss : 0.02206894102817375ITERATION : 33, loss : 0.022068984436648304ITERATION : 34, loss : 0.022069016039961583ITERATION : 35, loss : 0.022069038948097357ITERATION : 36, loss : 0.022069055600169443ITERATION : 37, loss : 0.022069067698899474ITERATION : 38, loss : 0.022069076492713998ITERATION : 39, loss : 0.02206908286333007ITERATION : 40, loss : 0.02206908748498195ITERATION : 41, loss : 0.022069090838982786ITERATION : 42, loss : 0.02206909324477873ITERATION : 43, loss : 0.022069095009394437ITERATION : 44, loss : 0.022069096290124667ITERATION : 45, loss : 0.022069097193747073ITERATION : 46, loss : 0.022069097882141547ITERATION : 47, loss : 0.022069098375528808ITERATION : 48, loss : 0.022069098708330587ITERATION : 49, loss : 0.022069098967860766ITERATION : 50, loss : 0.022069099164409203ITERATION : 51, loss : 0.02206909926857766ITERATION : 52, loss : 0.02206909933905359ITERATION : 53, loss : 0.022069099436483244ITERATION : 54, loss : 0.0220690994814623ITERATION : 55, loss : 0.02206909948131398ITERATION : 56, loss : 0.02206909948131398ITERATION : 57, loss : 0.02206909948131398ITERATION : 58, loss : 0.02206909948131398ITERATION : 59, loss : 0.02206909948131398ITERATION : 60, loss : 0.02206909948131398ITERATION : 61, loss : 0.02206909948131398ITERATION : 62, loss : 0.02206909948131398ITERATION : 63, loss : 0.02206909948131398ITERATION : 64, loss : 0.02206909948131398ITERATION : 65, loss : 0.02206909948131398ITERATION : 66, loss : 0.02206909948131398ITERATION : 67, loss : 0.02206909948131398ITERATION : 68, loss : 0.02206909948131398ITERATION : 69, loss : 0.02206909948131398ITERATION : 70, loss : 0.02206909948131398ITERATION : 71, loss : 0.02206909948131398ITERATION : 72, loss : 0.02206909948131398ITERATION : 73, loss : 0.02206909948131398ITERATION : 74, loss : 0.02206909948131398ITERATION : 75, loss : 0.02206909948131398ITERATION : 76, loss : 0.02206909948131398ITERATION : 77, loss : 0.02206909948131398ITERATION : 78, loss : 0.02206909948131398ITERATION : 79, loss : 0.02206909948131398ITERATION : 80, loss : 0.02206909948131398ITERATION : 81, loss : 0.02206909948131398ITERATION : 82, loss : 0.02206909948131398ITERATION : 83, loss : 0.02206909948131398ITERATION : 84, loss : 0.02206909948131398ITERATION : 85, loss : 0.02206909948131398ITERATION : 86, loss : 0.02206909948131398ITERATION : 87, loss : 0.02206909948131398ITERATION : 88, loss : 0.02206909948131398ITERATION : 89, loss : 0.02206909948131398ITERATION : 90, loss : 0.02206909948131398ITERATION : 91, loss : 0.02206909948131398ITERATION : 92, loss : 0.02206909948131398ITERATION : 93, loss : 0.02206909948131398ITERATION : 94, loss : 0.02206909948131398ITERATION : 95, loss : 0.02206909948131398ITERATION : 96, loss : 0.02206909948131398ITERATION : 97, loss : 0.02206909948131398ITERATION : 98, loss : 0.02206909948131398ITERATION : 99, loss : 0.02206909948131398ITERATION : 100, loss : 0.02206909948131398
ITERATION : 1, loss : 0.01458215861898078ITERATION : 2, loss : 0.011378856477851071ITERATION : 3, loss : 0.010572656974175023ITERATION : 4, loss : 0.010264752393128435ITERATION : 5, loss : 0.010125408385793525ITERATION : 6, loss : 0.010055504085288543ITERATION : 7, loss : 0.010017784378951485ITERATION : 8, loss : 0.009996260428496592ITERATION : 9, loss : 0.009983419818082356ITERATION : 10, loss : 0.009975481112696818ITERATION : 11, loss : 0.009970431200207954ITERATION : 12, loss : 0.009967146149283726ITERATION : 13, loss : 0.009964971964847087ITERATION : 14, loss : 0.009963514088239964ITERATION : 15, loss : 0.00996252701674927ITERATION : 16, loss : 0.009961853967789465ITERATION : 17, loss : 0.009961392694742579ITERATION : 18, loss : 0.0099610754262221ITERATION : 19, loss : 0.00996085666243165ITERATION : 20, loss : 0.009960705555009736ITERATION : 21, loss : 0.00996060106624165ITERATION : 22, loss : 0.009960528755519348ITERATION : 23, loss : 0.009960478696996185ITERATION : 24, loss : 0.009960444029062952ITERATION : 25, loss : 0.009960420028742586ITERATION : 26, loss : 0.00996040340206797ITERATION : 27, loss : 0.009960391888422971ITERATION : 28, loss : 0.009960383920744454ITERATION : 29, loss : 0.009960378406537917ITERATION : 30, loss : 0.009960374587361698ITERATION : 31, loss : 0.00996037194693032ITERATION : 32, loss : 0.00996037011661788ITERATION : 33, loss : 0.00996036885262587ITERATION : 34, loss : 0.00996036798104446ITERATION : 35, loss : 0.009960367382327627ITERATION : 36, loss : 0.009960366971434329ITERATION : 37, loss : 0.009960366689519306ITERATION : 38, loss : 0.009960366496966723ITERATION : 39, loss : 0.009960366364204715ITERATION : 40, loss : 0.009960366274202947ITERATION : 41, loss : 0.009960366212057082ITERATION : 42, loss : 0.009960366170306531ITERATION : 43, loss : 0.009960366141251203ITERATION : 44, loss : 0.009960366123972691ITERATION : 45, loss : 0.009960366108700543ITERATION : 46, loss : 0.009960366101254088ITERATION : 47, loss : 0.009960366093908808ITERATION : 48, loss : 0.00996036609232399ITERATION : 49, loss : 0.009960366088886403ITERATION : 50, loss : 0.009960366087735335ITERATION : 51, loss : 0.009960366085300612ITERATION : 52, loss : 0.009960366085300612ITERATION : 53, loss : 0.009960366085300612ITERATION : 54, loss : 0.009960366085300612ITERATION : 55, loss : 0.009960366085300612ITERATION : 56, loss : 0.009960366085300612ITERATION : 57, loss : 0.009960366085300612ITERATION : 58, loss : 0.009960366085300612ITERATION : 59, loss : 0.009960366085300612ITERATION : 60, loss : 0.009960366085300612ITERATION : 61, loss : 0.009960366085300612ITERATION : 62, loss : 0.009960366085300612ITERATION : 63, loss : 0.009960366085300612ITERATION : 64, loss : 0.009960366085300612ITERATION : 65, loss : 0.009960366085300612ITERATION : 66, loss : 0.009960366085300612ITERATION : 67, loss : 0.009960366085300612ITERATION : 68, loss : 0.009960366085300612ITERATION : 69, loss : 0.009960366085300612ITERATION : 70, loss : 0.009960366085300612ITERATION : 71, loss : 0.009960366085300612ITERATION : 72, loss : 0.009960366085300612ITERATION : 73, loss : 0.009960366085300612ITERATION : 74, loss : 0.009960366085300612ITERATION : 75, loss : 0.009960366085300612ITERATION : 76, loss : 0.009960366085300612ITERATION : 77, loss : 0.009960366085300612ITERATION : 78, loss : 0.009960366085300612ITERATION : 79, loss : 0.009960366085300612ITERATION : 80, loss : 0.009960366085300612ITERATION : 81, loss : 0.009960366085300612ITERATION : 82, loss : 0.009960366085300612ITERATION : 83, loss : 0.009960366085300612ITERATION : 84, loss : 0.009960366085300612ITERATION : 85, loss : 0.009960366085300612ITERATION : 86, loss : 0.009960366085300612ITERATION : 87, loss : 0.009960366085300612ITERATION : 88, loss : 0.009960366085300612ITERATION : 89, loss : 0.009960366085300612ITERATION : 90, loss : 0.009960366085300612ITERATION : 91, loss : 0.009960366085300612ITERATION : 92, loss : 0.009960366085300612ITERATION : 93, loss : 0.009960366085300612ITERATION : 94, loss : 0.009960366085300612ITERATION : 95, loss : 0.009960366085300612ITERATION : 96, loss : 0.009960366085300612ITERATION : 97, loss : 0.009960366085300612ITERATION : 98, loss : 0.009960366085300612ITERATION : 99, loss : 0.009960366085300612ITERATION : 100, loss : 0.009960366085300612
ITERATION : 1, loss : 0.02162021225378476ITERATION : 2, loss : 0.019958692768545266ITERATION : 3, loss : 0.021001819931259307ITERATION : 4, loss : 0.021255491695262634ITERATION : 5, loss : 0.021218316875429424ITERATION : 6, loss : 0.02131642624881978ITERATION : 7, loss : 0.021450486327827638ITERATION : 8, loss : 0.021406116944400224ITERATION : 9, loss : 0.02133070583357217ITERATION : 10, loss : 0.021279848615937694ITERATION : 11, loss : 0.021247654014073525ITERATION : 12, loss : 0.021228445824494038ITERATION : 13, loss : 0.02121777497083632ITERATION : 14, loss : 0.02121246205932956ITERATION : 15, loss : 0.021210358942871967ITERATION : 16, loss : 0.021210071473371037ITERATION : 17, loss : 0.021210727464471765ITERATION : 18, loss : 0.021211801821791484ITERATION : 19, loss : 0.02121299250511312ITERATION : 20, loss : 0.021214138086433007ITERATION : 21, loss : 0.02121516120752946ITERATION : 22, loss : 0.02121603425934075ITERATION : 23, loss : 0.021216756465130205ITERATION : 24, loss : 0.021217340495501016ITERATION : 25, loss : 0.0212178050525453ITERATION : 26, loss : 0.02121816941983398ITERATION : 27, loss : 0.021218452438407767ITERATION : 28, loss : 0.021218670383913226ITERATION : 29, loss : 0.021218836781691622ITERATION : 30, loss : 0.021218963251516502ITERATION : 31, loss : 0.021219058631400167ITERATION : 32, loss : 0.021219130549199865ITERATION : 33, loss : 0.021219184513013117ITERATION : 34, loss : 0.02121922460552068ITERATION : 35, loss : 0.021219254615653212ITERATION : 36, loss : 0.021219276913928796ITERATION : 37, loss : 0.02121929328372877ITERATION : 38, loss : 0.021219305462813445ITERATION : 39, loss : 0.021219314362564187ITERATION : 40, loss : 0.02121932095344566ITERATION : 41, loss : 0.021219325842398995ITERATION : 42, loss : 0.021219329500787424ITERATION : 43, loss : 0.021219332146451088ITERATION : 44, loss : 0.02121933412238542ITERATION : 45, loss : 0.021219335531161888ITERATION : 46, loss : 0.021219336623212126ITERATION : 47, loss : 0.02121933729716535ITERATION : 48, loss : 0.02121933787610843ITERATION : 49, loss : 0.021219338229422142ITERATION : 50, loss : 0.021219338563937463ITERATION : 51, loss : 0.021219338782567966ITERATION : 52, loss : 0.02121933895749975ITERATION : 53, loss : 0.02121933904205122ITERATION : 54, loss : 0.02121933907325109ITERATION : 55, loss : 0.02121933907325109ITERATION : 56, loss : 0.02121933907325109ITERATION : 57, loss : 0.02121933907325109ITERATION : 58, loss : 0.02121933907325109ITERATION : 59, loss : 0.02121933907325109ITERATION : 60, loss : 0.02121933907325109ITERATION : 61, loss : 0.02121933907325109ITERATION : 62, loss : 0.02121933907325109ITERATION : 63, loss : 0.02121933907325109ITERATION : 64, loss : 0.02121933907325109ITERATION : 65, loss : 0.02121933907325109ITERATION : 66, loss : 0.02121933907325109ITERATION : 67, loss : 0.02121933907325109ITERATION : 68, loss : 0.02121933907325109ITERATION : 69, loss : 0.02121933907325109ITERATION : 70, loss : 0.02121933907325109ITERATION : 71, loss : 0.02121933907325109ITERATION : 72, loss : 0.02121933907325109ITERATION : 73, loss : 0.02121933907325109ITERATION : 74, loss : 0.02121933907325109ITERATION : 75, loss : 0.02121933907325109ITERATION : 76, loss : 0.02121933907325109ITERATION : 77, loss : 0.02121933907325109ITERATION : 78, loss : 0.02121933907325109ITERATION : 79, loss : 0.02121933907325109ITERATION : 80, loss : 0.02121933907325109ITERATION : 81, loss : 0.02121933907325109ITERATION : 82, loss : 0.02121933907325109ITERATION : 83, loss : 0.02121933907325109ITERATION : 84, loss : 0.02121933907325109ITERATION : 85, loss : 0.02121933907325109ITERATION : 86, loss : 0.02121933907325109ITERATION : 87, loss : 0.02121933907325109ITERATION : 88, loss : 0.02121933907325109ITERATION : 89, loss : 0.02121933907325109ITERATION : 90, loss : 0.02121933907325109ITERATION : 91, loss : 0.02121933907325109ITERATION : 92, loss : 0.02121933907325109ITERATION : 93, loss : 0.02121933907325109ITERATION : 94, loss : 0.02121933907325109ITERATION : 95, loss : 0.02121933907325109ITERATION : 96, loss : 0.02121933907325109ITERATION : 97, loss : 0.02121933907325109ITERATION : 98, loss : 0.02121933907325109ITERATION : 99, loss : 0.02121933907325109ITERATION : 100, loss : 0.02121933907325109
ITERATION : 1, loss : 0.03395551546344383ITERATION : 2, loss : 0.029086281469703323ITERATION : 3, loss : 0.028214621106445754ITERATION : 4, loss : 0.02797010718274344ITERATION : 5, loss : 0.027900419112053508ITERATION : 6, loss : 0.027896831472616665ITERATION : 7, loss : 0.02791832002957038ITERATION : 8, loss : 0.027946605784343218ITERATION : 9, loss : 0.02797379063144447ITERATION : 10, loss : 0.027996983089631996ITERATION : 11, loss : 0.028015603857160124ITERATION : 12, loss : 0.028030036232892155ITERATION : 13, loss : 0.028040979318889377ITERATION : 14, loss : 0.02804915905690186ITERATION : 15, loss : 0.028055215208645918ITERATION : 16, loss : 0.028059669949368347ITERATION : 17, loss : 0.02806293207767157ITERATION : 18, loss : 0.028065313393334314ITERATION : 19, loss : 0.02806704790590057ITERATION : 20, loss : 0.028068309257165063ITERATION : 21, loss : 0.028069225531129727ITERATION : 22, loss : 0.028069890569624306ITERATION : 23, loss : 0.0280703729862915ITERATION : 24, loss : 0.028070722751165764ITERATION : 25, loss : 0.028070976253001523ITERATION : 26, loss : 0.02807115995755853ITERATION : 27, loss : 0.02807129304036779ITERATION : 28, loss : 0.028071389454950274ITERATION : 29, loss : 0.02807145927029409ITERATION : 30, loss : 0.02807150982282304ITERATION : 31, loss : 0.028071546420498302ITERATION : 32, loss : 0.028071572926608855ITERATION : 33, loss : 0.028071592075861903ITERATION : 34, loss : 0.028071605953592888ITERATION : 35, loss : 0.02807161600373556ITERATION : 36, loss : 0.028071623279420976ITERATION : 37, loss : 0.02807162852049819ITERATION : 38, loss : 0.028071632341757018ITERATION : 39, loss : 0.028071635076701944ITERATION : 40, loss : 0.028071637098473413ITERATION : 41, loss : 0.028071638518575463ITERATION : 42, loss : 0.028071639594580932ITERATION : 43, loss : 0.028071640323714824ITERATION : 44, loss : 0.02807164089777423ITERATION : 45, loss : 0.028071641265706816ITERATION : 46, loss : 0.028071641578177996ITERATION : 47, loss : 0.0280716417597286ITERATION : 48, loss : 0.028071641934795805ITERATION : 49, loss : 0.02807164202470932ITERATION : 50, loss : 0.028071642101103782ITERATION : 51, loss : 0.028071642154167794ITERATION : 52, loss : 0.028071642228414885ITERATION : 53, loss : 0.028071642234973947ITERATION : 54, loss : 0.028071642268911907ITERATION : 55, loss : 0.02807164226642608ITERATION : 56, loss : 0.028071642266941615ITERATION : 57, loss : 0.028071642266941615ITERATION : 58, loss : 0.028071642266941615ITERATION : 59, loss : 0.028071642266941615ITERATION : 60, loss : 0.028071642266941615ITERATION : 61, loss : 0.028071642266941615ITERATION : 62, loss : 0.028071642266941615ITERATION : 63, loss : 0.028071642266941615ITERATION : 64, loss : 0.028071642266941615ITERATION : 65, loss : 0.028071642266941615ITERATION : 66, loss : 0.028071642266941615ITERATION : 67, loss : 0.028071642266941615ITERATION : 68, loss : 0.028071642266941615ITERATION : 69, loss : 0.028071642266941615ITERATION : 70, loss : 0.028071642266941615ITERATION : 71, loss : 0.028071642266941615ITERATION : 72, loss : 0.028071642266941615ITERATION : 73, loss : 0.028071642266941615ITERATION : 74, loss : 0.028071642266941615ITERATION : 75, loss : 0.028071642266941615ITERATION : 76, loss : 0.028071642266941615ITERATION : 77, loss : 0.028071642266941615ITERATION : 78, loss : 0.028071642266941615ITERATION : 79, loss : 0.028071642266941615ITERATION : 80, loss : 0.028071642266941615ITERATION : 81, loss : 0.028071642266941615ITERATION : 82, loss : 0.028071642266941615ITERATION : 83, loss : 0.028071642266941615ITERATION : 84, loss : 0.028071642266941615ITERATION : 85, loss : 0.028071642266941615ITERATION : 86, loss : 0.028071642266941615ITERATION : 87, loss : 0.028071642266941615ITERATION : 88, loss : 0.028071642266941615ITERATION : 89, loss : 0.028071642266941615ITERATION : 90, loss : 0.028071642266941615ITERATION : 91, loss : 0.028071642266941615ITERATION : 92, loss : 0.028071642266941615ITERATION : 93, loss : 0.028071642266941615ITERATION : 94, loss : 0.028071642266941615ITERATION : 95, loss : 0.028071642266941615ITERATION : 96, loss : 0.028071642266941615ITERATION : 97, loss : 0.028071642266941615ITERATION : 98, loss : 0.028071642266941615ITERATION : 99, loss : 0.028071642266941615ITERATION : 100, loss : 0.028071642266941615
ITERATION : 1, loss : 0.022233902560324204ITERATION : 2, loss : 0.01576818471264778ITERATION : 3, loss : 0.013425489226441295ITERATION : 4, loss : 0.012444172794108251ITERATION : 5, loss : 0.01197347701174332ITERATION : 6, loss : 0.011719885149213545ITERATION : 7, loss : 0.01157061241919011ITERATION : 8, loss : 0.011477264259407398ITERATION : 9, loss : 0.011416613157358575ITERATION : 10, loss : 0.01137628568932397ITERATION : 11, loss : 0.011349102746278667ITERATION : 12, loss : 0.011330631877202194ITERATION : 13, loss : 0.011318020959649407ITERATION : 14, loss : 0.011309386485433084ITERATION : 15, loss : 0.011303464485832025ITERATION : 16, loss : 0.011299398808893624ITERATION : 17, loss : 0.011296605793038237ITERATION : 18, loss : 0.011294686470306943ITERATION : 19, loss : 0.01129336732455562ITERATION : 20, loss : 0.011292460582310045ITERATION : 21, loss : 0.011291837289181593ITERATION : 22, loss : 0.011291408884967742ITERATION : 23, loss : 0.011291114424472327ITERATION : 24, loss : 0.01129091203547937ITERATION : 25, loss : 0.01129077299148645ITERATION : 26, loss : 0.011290677447502841ITERATION : 27, loss : 0.011290611790164036ITERATION : 28, loss : 0.011290566674279408ITERATION : 29, loss : 0.011290535717937424ITERATION : 30, loss : 0.011290514434487578ITERATION : 31, loss : 0.011290499830510826ITERATION : 32, loss : 0.01129048975136686ITERATION : 33, loss : 0.011290482849042799ITERATION : 34, loss : 0.011290478146881387ITERATION : 35, loss : 0.011290474905834968ITERATION : 36, loss : 0.011290472715777896ITERATION : 37, loss : 0.011290471195756602ITERATION : 38, loss : 0.011290470199319587ITERATION : 39, loss : 0.011290469487566436ITERATION : 40, loss : 0.011290469001296391ITERATION : 41, loss : 0.011290468662165494ITERATION : 42, loss : 0.01129046844700107ITERATION : 43, loss : 0.01129046829392688ITERATION : 44, loss : 0.011290468219223727ITERATION : 45, loss : 0.011290468117018773ITERATION : 46, loss : 0.011290468085522799ITERATION : 47, loss : 0.011290468036477099ITERATION : 48, loss : 0.011290468052236744ITERATION : 49, loss : 0.011290468031838502ITERATION : 50, loss : 0.011290468031838502ITERATION : 51, loss : 0.011290468031838502ITERATION : 52, loss : 0.011290468031838502ITERATION : 53, loss : 0.011290468031838502ITERATION : 54, loss : 0.011290468031838502ITERATION : 55, loss : 0.011290468031838502ITERATION : 56, loss : 0.011290468031838502ITERATION : 57, loss : 0.011290468031838502ITERATION : 58, loss : 0.011290468031838502ITERATION : 59, loss : 0.011290468031838502ITERATION : 60, loss : 0.011290468031838502ITERATION : 61, loss : 0.011290468031838502ITERATION : 62, loss : 0.011290468031838502ITERATION : 63, loss : 0.011290468031838502ITERATION : 64, loss : 0.011290468031838502ITERATION : 65, loss : 0.011290468031838502ITERATION : 66, loss : 0.011290468031838502ITERATION : 67, loss : 0.011290468031838502ITERATION : 68, loss : 0.011290468031838502ITERATION : 69, loss : 0.011290468031838502ITERATION : 70, loss : 0.011290468031838502ITERATION : 71, loss : 0.011290468031838502ITERATION : 72, loss : 0.011290468031838502ITERATION : 73, loss : 0.011290468031838502ITERATION : 74, loss : 0.011290468031838502ITERATION : 75, loss : 0.011290468031838502ITERATION : 76, loss : 0.011290468031838502ITERATION : 77, loss : 0.011290468031838502ITERATION : 78, loss : 0.011290468031838502ITERATION : 79, loss : 0.011290468031838502ITERATION : 80, loss : 0.011290468031838502ITERATION : 81, loss : 0.011290468031838502ITERATION : 82, loss : 0.011290468031838502ITERATION : 83, loss : 0.011290468031838502ITERATION : 84, loss : 0.011290468031838502ITERATION : 85, loss : 0.011290468031838502ITERATION : 86, loss : 0.011290468031838502ITERATION : 87, loss : 0.011290468031838502ITERATION : 88, loss : 0.011290468031838502ITERATION : 89, loss : 0.011290468031838502ITERATION : 90, loss : 0.011290468031838502ITERATION : 91, loss : 0.011290468031838502ITERATION : 92, loss : 0.011290468031838502ITERATION : 93, loss : 0.011290468031838502ITERATION : 94, loss : 0.011290468031838502ITERATION : 95, loss : 0.011290468031838502ITERATION : 96, loss : 0.011290468031838502ITERATION : 97, loss : 0.011290468031838502ITERATION : 98, loss : 0.011290468031838502ITERATION : 99, loss : 0.011290468031838502ITERATION : 100, loss : 0.011290468031838502
gradient norm in None layer : 0.001747138234406765
gradient norm in None layer : 9.04462570480549e-05
gradient norm in None layer : 0.00010235253052188894
gradient norm in None layer : 0.0012954908978023581
gradient norm in None layer : 9.320753987230542e-05
gradient norm in None layer : 0.0001073809044375119
gradient norm in None layer : 0.0006379037870283658
gradient norm in None layer : 2.165625717247714e-05
gradient norm in None layer : 2.3874923553373265e-05
gradient norm in None layer : 0.0004382565726414537
gradient norm in None layer : 2.1040843837411935e-05
gradient norm in None layer : 2.0950466856166177e-05
gradient norm in None layer : 0.00017247879101660657
gradient norm in None layer : 6.305580235430625e-06
gradient norm in None layer : 5.4583848982333925e-06
gradient norm in None layer : 0.00014936198999839414
gradient norm in None layer : 5.8116383660820945e-06
gradient norm in None layer : 6.0447625488312445e-06
gradient norm in None layer : 0.00019483847280326238
gradient norm in None layer : 1.1068142767822443e-06
gradient norm in None layer : 0.0004030693681629379
gradient norm in None layer : 2.6611523305135624e-05
gradient norm in None layer : 2.281360296895263e-05
gradient norm in None layer : 0.000414902940444101
gradient norm in None layer : 3.8753060131265506e-05
gradient norm in None layer : 3.500917363981317e-05
gradient norm in None layer : 0.0005967808467255124
gradient norm in None layer : 3.0476390375277654e-06
gradient norm in None layer : 0.0011075209541244184
gradient norm in None layer : 8.322523942491329e-05
gradient norm in None layer : 9.254971694556974e-05
gradient norm in None layer : 0.0013061136690214028
gradient norm in None layer : 0.00010883662015443755
gradient norm in None layer : 0.0001287740017136465
gradient norm in None layer : 9.359477566908515e-05
gradient norm in None layer : 1.79261529261764e-05
Total gradient norm: 0.003023407742765247
invariance loss : 4.577100180596504, avg_den : 0.416412353515625, density loss : 0.316412353515625, mse loss : 0.018681568818269587, solver time : 124.86955618858337 sec , total loss : 0.023575081352381715, running loss : 0.04775635071684693
Epoch 0/10 , batch 19/12500 
ITERATION : 1, loss : 0.0268756189386885ITERATION : 2, loss : 0.04030651107881265ITERATION : 3, loss : 0.043113799668350404ITERATION : 4, loss : 0.03948301190544969ITERATION : 5, loss : 0.03708612249803324ITERATION : 6, loss : 0.03544063899088373ITERATION : 7, loss : 0.03428860793114258ITERATION : 8, loss : 0.03347507425982815ITERATION : 9, loss : 0.03289884807580819ITERATION : 10, loss : 0.032490578975686676ITERATION : 11, loss : 0.032201577009411585ITERATION : 12, loss : 0.0319972943390045ITERATION : 13, loss : 0.031853127659081125ITERATION : 14, loss : 0.0317515513053322ITERATION : 15, loss : 0.03168009518183579ITERATION : 16, loss : 0.031629902613511555ITERATION : 17, loss : 0.031594696131789904ITERATION : 18, loss : 0.031570034301820815ITERATION : 19, loss : 0.03155278138661936ITERATION : 20, loss : 0.03154072689543233ITERATION : 21, loss : 0.03153231497684465ITERATION : 22, loss : 0.03152645254060789ITERATION : 23, loss : 0.03152237210654037ITERATION : 24, loss : 0.03151953560068714ITERATION : 25, loss : 0.031517566500509964ITERATION : 26, loss : 0.031516201600613164ITERATION : 27, loss : 0.03151525710556101ITERATION : 28, loss : 0.03151460448227601ITERATION : 29, loss : 0.031514154205142406ITERATION : 30, loss : 0.03151384454445614ITERATION : 31, loss : 0.031513631770749874ITERATION : 32, loss : 0.03151348586282041ITERATION : 33, loss : 0.031513386244018934ITERATION : 34, loss : 0.031513318465843425ITERATION : 35, loss : 0.03151327244625724ITERATION : 36, loss : 0.031513241248937854ITERATION : 37, loss : 0.03151322030184256ITERATION : 38, loss : 0.03151320620528506ITERATION : 39, loss : 0.03151319685372457ITERATION : 40, loss : 0.031513190608100275ITERATION : 41, loss : 0.03151318652665269ITERATION : 42, loss : 0.03151318369008364ITERATION : 43, loss : 0.03151318192895374ITERATION : 44, loss : 0.031513180863806695ITERATION : 45, loss : 0.03151318027896013ITERATION : 46, loss : 0.031513179934408754ITERATION : 47, loss : 0.03151317980800272ITERATION : 48, loss : 0.03151317971961983ITERATION : 49, loss : 0.03151317974357243ITERATION : 50, loss : 0.031513179739307076ITERATION : 51, loss : 0.031513179804012775ITERATION : 52, loss : 0.031513179813416295ITERATION : 53, loss : 0.03151317986067624ITERATION : 54, loss : 0.031513179896106455ITERATION : 55, loss : 0.0315131799333807ITERATION : 56, loss : 0.031513179943194705ITERATION : 57, loss : 0.03151317995435197ITERATION : 58, loss : 0.03151317996103379ITERATION : 59, loss : 0.03151317996486311ITERATION : 60, loss : 0.03151317996486311ITERATION : 61, loss : 0.03151317996486311ITERATION : 62, loss : 0.03151317996486311ITERATION : 63, loss : 0.03151317996486311ITERATION : 64, loss : 0.03151317996486311ITERATION : 65, loss : 0.03151317996486311ITERATION : 66, loss : 0.03151317996486311ITERATION : 67, loss : 0.03151317996486311ITERATION : 68, loss : 0.03151317996486311ITERATION : 69, loss : 0.03151317996486311ITERATION : 70, loss : 0.03151317996486311ITERATION : 71, loss : 0.03151317996486311ITERATION : 72, loss : 0.03151317996486311ITERATION : 73, loss : 0.03151317996486311ITERATION : 74, loss : 0.03151317996486311ITERATION : 75, loss : 0.03151317996486311ITERATION : 76, loss : 0.03151317996486311ITERATION : 77, loss : 0.03151317996486311ITERATION : 78, loss : 0.03151317996486311ITERATION : 79, loss : 0.03151317996486311ITERATION : 80, loss : 0.03151317996486311ITERATION : 81, loss : 0.03151317996486311ITERATION : 82, loss : 0.03151317996486311ITERATION : 83, loss : 0.03151317996486311ITERATION : 84, loss : 0.03151317996486311ITERATION : 85, loss : 0.03151317996486311ITERATION : 86, loss : 0.03151317996486311ITERATION : 87, loss : 0.03151317996486311ITERATION : 88, loss : 0.03151317996486311ITERATION : 89, loss : 0.03151317996486311ITERATION : 90, loss : 0.03151317996486311ITERATION : 91, loss : 0.03151317996486311ITERATION : 92, loss : 0.03151317996486311ITERATION : 93, loss : 0.03151317996486311ITERATION : 94, loss : 0.03151317996486311ITERATION : 95, loss : 0.03151317996486311ITERATION : 96, loss : 0.03151317996486311ITERATION : 97, loss : 0.03151317996486311ITERATION : 98, loss : 0.03151317996486311ITERATION : 99, loss : 0.03151317996486311ITERATION : 100, loss : 0.03151317996486311
ITERATION : 1, loss : 0.04735455000405777ITERATION : 2, loss : 0.04090437276930665ITERATION : 3, loss : 0.038554239603012026ITERATION : 4, loss : 0.0373921028089392ITERATION : 5, loss : 0.036741546187297164ITERATION : 6, loss : 0.03635522736902395ITERATION : 7, loss : 0.03611732915803087ITERATION : 8, loss : 0.03596669441710674ITERATION : 9, loss : 0.03586901158337702ITERATION : 10, loss : 0.03580430986403193ITERATION : 11, loss : 0.0357606401505839ITERATION : 12, loss : 0.03573067832722755ITERATION : 13, loss : 0.035709832172485106ITERATION : 14, loss : 0.03569515855588223ITERATION : 15, loss : 0.03568473141869163ITERATION : 16, loss : 0.035677265474456446ITERATION : 17, loss : 0.03567188776366605ITERATION : 18, loss : 0.035667996244579635ITERATION : 19, loss : 0.03566517007400809ITERATION : 20, loss : 0.03566311202439611ITERATION : 21, loss : 0.035661610167383194ITERATION : 22, loss : 0.03566051250241205ITERATION : 23, loss : 0.03565970926519946ITERATION : 24, loss : 0.035659120983485545ITERATION : 25, loss : 0.03565868983351132ITERATION : 26, loss : 0.03565837364582138ITERATION : 27, loss : 0.0356581417197858ITERATION : 28, loss : 0.03565797154264676ITERATION : 29, loss : 0.03565784663179533ITERATION : 30, loss : 0.03565775494133513ITERATION : 31, loss : 0.03565768761945872ITERATION : 32, loss : 0.035657638189915505ITERATION : 33, loss : 0.03565760189548465ITERATION : 34, loss : 0.03565757523408567ITERATION : 35, loss : 0.03565755565705928ITERATION : 36, loss : 0.03565754127312853ITERATION : 37, loss : 0.035657530711969115ITERATION : 38, loss : 0.03565752294895614ITERATION : 39, loss : 0.03565751725733847ITERATION : 40, loss : 0.035657513071590735ITERATION : 41, loss : 0.035657510001984576ITERATION : 42, loss : 0.03565750774774287ITERATION : 43, loss : 0.03565750609605098ITERATION : 44, loss : 0.03565750487929362ITERATION : 45, loss : 0.03565750399176448ITERATION : 46, loss : 0.035657503342954436ITERATION : 47, loss : 0.03565750286010058ITERATION : 48, loss : 0.03565750250788012ITERATION : 49, loss : 0.03565750226058113ITERATION : 50, loss : 0.03565750207450062ITERATION : 51, loss : 0.03565750194499365ITERATION : 52, loss : 0.03565750184027924ITERATION : 53, loss : 0.03565750176695277ITERATION : 54, loss : 0.0356575017054953ITERATION : 55, loss : 0.03565750166021753ITERATION : 56, loss : 0.03565750163373042ITERATION : 57, loss : 0.03565750161899646ITERATION : 58, loss : 0.0356575015923936ITERATION : 59, loss : 0.03565750159256891ITERATION : 60, loss : 0.03565750159256891ITERATION : 61, loss : 0.03565750159256891ITERATION : 62, loss : 0.03565750159256891ITERATION : 63, loss : 0.03565750159256891ITERATION : 64, loss : 0.03565750159256891ITERATION : 65, loss : 0.03565750159256891ITERATION : 66, loss : 0.03565750159256891ITERATION : 67, loss : 0.03565750159256891ITERATION : 68, loss : 0.03565750159256891ITERATION : 69, loss : 0.03565750159256891ITERATION : 70, loss : 0.03565750159256891ITERATION : 71, loss : 0.03565750159256891ITERATION : 72, loss : 0.03565750159256891ITERATION : 73, loss : 0.03565750159256891ITERATION : 74, loss : 0.03565750159256891ITERATION : 75, loss : 0.03565750159256891ITERATION : 76, loss : 0.03565750159256891ITERATION : 77, loss : 0.03565750159256891ITERATION : 78, loss : 0.03565750159256891ITERATION : 79, loss : 0.03565750159256891ITERATION : 80, loss : 0.03565750159256891ITERATION : 81, loss : 0.03565750159256891ITERATION : 82, loss : 0.03565750159256891ITERATION : 83, loss : 0.03565750159256891ITERATION : 84, loss : 0.03565750159256891ITERATION : 85, loss : 0.03565750159256891ITERATION : 86, loss : 0.03565750159256891ITERATION : 87, loss : 0.03565750159256891ITERATION : 88, loss : 0.03565750159256891ITERATION : 89, loss : 0.03565750159256891ITERATION : 90, loss : 0.03565750159256891ITERATION : 91, loss : 0.03565750159256891ITERATION : 92, loss : 0.03565750159256891ITERATION : 93, loss : 0.03565750159256891ITERATION : 94, loss : 0.03565750159256891ITERATION : 95, loss : 0.03565750159256891ITERATION : 96, loss : 0.03565750159256891ITERATION : 97, loss : 0.03565750159256891ITERATION : 98, loss : 0.03565750159256891ITERATION : 99, loss : 0.03565750159256891ITERATION : 100, loss : 0.03565750159256891
ITERATION : 1, loss : 0.03299371798312521ITERATION : 2, loss : 0.02827007846712601ITERATION : 3, loss : 0.026743227478243717ITERATION : 4, loss : 0.025395200794543177ITERATION : 5, loss : 0.02456632693413012ITERATION : 6, loss : 0.024027906222487495ITERATION : 7, loss : 0.02366075457648664ITERATION : 8, loss : 0.02340328215504682ITERATION : 9, loss : 0.02321967598033296ITERATION : 10, loss : 0.023087387916358167ITERATION : 11, loss : 0.02299144976096378ITERATION : 12, loss : 0.022921576296129377ITERATION : 13, loss : 0.0228705408024811ITERATION : 14, loss : 0.022833191476718643ITERATION : 15, loss : 0.022805820665105565ITERATION : 16, loss : 0.022785742859810037ITERATION : 17, loss : 0.022771004563288434ITERATION : 18, loss : 0.022760180261067395ITERATION : 19, loss : 0.022752227613910575ITERATION : 20, loss : 0.02274638318861127ITERATION : 21, loss : 0.022742087228496227ITERATION : 22, loss : 0.022738929003711664ITERATION : 23, loss : 0.02273660694112411ITERATION : 24, loss : 0.022734899520844953ITERATION : 25, loss : 0.022733643974984105ITERATION : 26, loss : 0.02273272066067511ITERATION : 27, loss : 0.022732041639557072ITERATION : 28, loss : 0.02273154229154264ITERATION : 29, loss : 0.02273117503582057ITERATION : 30, loss : 0.022730904938216525ITERATION : 31, loss : 0.022730706269712356ITERATION : 32, loss : 0.022730560146725382ITERATION : 33, loss : 0.022730452645986395ITERATION : 34, loss : 0.02273037362305322ITERATION : 35, loss : 0.022730315498568132ITERATION : 36, loss : 0.022730272761216513ITERATION : 37, loss : 0.022730241353726664ITERATION : 38, loss : 0.022730218241719648ITERATION : 39, loss : 0.022730201274462833ITERATION : 40, loss : 0.022730188765809803ITERATION : 41, loss : 0.022730179576752704ITERATION : 42, loss : 0.022730172804753267ITERATION : 43, loss : 0.022730167829755725ITERATION : 44, loss : 0.022730164175530008ITERATION : 45, loss : 0.022730161496023964ITERATION : 46, loss : 0.022730159520768466ITERATION : 47, loss : 0.022730158008992443ITERATION : 48, loss : 0.02273015693924212ITERATION : 49, loss : 0.02273015613496271ITERATION : 50, loss : 0.022730155562492617ITERATION : 51, loss : 0.022730155188725527ITERATION : 52, loss : 0.02273015482591026ITERATION : 53, loss : 0.022730154606239155ITERATION : 54, loss : 0.022730154489076646ITERATION : 55, loss : 0.022730154357722174ITERATION : 56, loss : 0.022730154266783757ITERATION : 57, loss : 0.022730154229050007ITERATION : 58, loss : 0.02273015422823814ITERATION : 59, loss : 0.02273015422823814ITERATION : 60, loss : 0.02273015422823814ITERATION : 61, loss : 0.02273015422823814ITERATION : 62, loss : 0.02273015422823814ITERATION : 63, loss : 0.02273015422823814ITERATION : 64, loss : 0.02273015422823814ITERATION : 65, loss : 0.02273015422823814ITERATION : 66, loss : 0.02273015422823814ITERATION : 67, loss : 0.02273015422823814ITERATION : 68, loss : 0.02273015422823814ITERATION : 69, loss : 0.02273015422823814ITERATION : 70, loss : 0.02273015422823814ITERATION : 71, loss : 0.02273015422823814ITERATION : 72, loss : 0.02273015422823814ITERATION : 73, loss : 0.02273015422823814ITERATION : 74, loss : 0.02273015422823814ITERATION : 75, loss : 0.02273015422823814ITERATION : 76, loss : 0.02273015422823814ITERATION : 77, loss : 0.02273015422823814ITERATION : 78, loss : 0.02273015422823814ITERATION : 79, loss : 0.02273015422823814ITERATION : 80, loss : 0.02273015422823814ITERATION : 81, loss : 0.02273015422823814ITERATION : 82, loss : 0.02273015422823814ITERATION : 83, loss : 0.02273015422823814ITERATION : 84, loss : 0.02273015422823814ITERATION : 85, loss : 0.02273015422823814ITERATION : 86, loss : 0.02273015422823814ITERATION : 87, loss : 0.02273015422823814ITERATION : 88, loss : 0.02273015422823814ITERATION : 89, loss : 0.02273015422823814ITERATION : 90, loss : 0.02273015422823814ITERATION : 91, loss : 0.02273015422823814ITERATION : 92, loss : 0.02273015422823814ITERATION : 93, loss : 0.02273015422823814ITERATION : 94, loss : 0.02273015422823814ITERATION : 95, loss : 0.02273015422823814ITERATION : 96, loss : 0.02273015422823814ITERATION : 97, loss : 0.02273015422823814ITERATION : 98, loss : 0.02273015422823814ITERATION : 99, loss : 0.02273015422823814ITERATION : 100, loss : 0.02273015422823814
ITERATION : 1, loss : 0.029990625673896938ITERATION : 2, loss : 0.023134726235155258ITERATION : 3, loss : 0.020689185822305913ITERATION : 4, loss : 0.02112981068905802ITERATION : 5, loss : 0.02244809177936223ITERATION : 6, loss : 0.022786865812466032ITERATION : 7, loss : 0.022064328313162237ITERATION : 8, loss : 0.02163356039045308ITERATION : 9, loss : 0.021366071848738746ITERATION : 10, loss : 0.021195002416598285ITERATION : 11, loss : 0.021083334013228695ITERATION : 12, loss : 0.02100942417767785ITERATION : 13, loss : 0.020960051985113342ITERATION : 14, loss : 0.02092686936278527ITERATION : 15, loss : 0.020904477963593737ITERATION : 16, loss : 0.02088932869983147ITERATION : 17, loss : 0.020879061702562013ITERATION : 18, loss : 0.020872095822247565ITERATION : 19, loss : 0.020867366310925672ITERATION : 20, loss : 0.020864153756381124ITERATION : 21, loss : 0.02086197111167196ITERATION : 22, loss : 0.020860487882432974ITERATION : 23, loss : 0.020859479939426103ITERATION : 24, loss : 0.020858794903016308ITERATION : 25, loss : 0.020858329364693725ITERATION : 26, loss : 0.020858012996552926ITERATION : 27, loss : 0.0208577980039425ITERATION : 28, loss : 0.020857651895506195ITERATION : 29, loss : 0.020857552634465305ITERATION : 30, loss : 0.02085748518514815ITERATION : 31, loss : 0.020857439354500046ITERATION : 32, loss : 0.020857408219056107ITERATION : 33, loss : 0.020857387070323283ITERATION : 34, loss : 0.02085737268845642ITERATION : 35, loss : 0.02085736292776498ITERATION : 36, loss : 0.020857356287972622ITERATION : 37, loss : 0.020857351777888077ITERATION : 38, loss : 0.02085734871243535ITERATION : 39, loss : 0.02085734663635293ITERATION : 40, loss : 0.020857345220740616ITERATION : 41, loss : 0.02085734427127745ITERATION : 42, loss : 0.02085734361443879ITERATION : 43, loss : 0.02085734318638289ITERATION : 44, loss : 0.02085734288357823ITERATION : 45, loss : 0.020857342710678684ITERATION : 46, loss : 0.02085734257195468ITERATION : 47, loss : 0.020857342483973695ITERATION : 48, loss : 0.020857342409648847ITERATION : 49, loss : 0.0208573423714468ITERATION : 50, loss : 0.020857342363999824ITERATION : 51, loss : 0.020857342327219603ITERATION : 52, loss : 0.02085734232736794ITERATION : 53, loss : 0.02085734232736794ITERATION : 54, loss : 0.02085734232736794ITERATION : 55, loss : 0.02085734232736794ITERATION : 56, loss : 0.02085734232736794ITERATION : 57, loss : 0.02085734232736794ITERATION : 58, loss : 0.02085734232736794ITERATION : 59, loss : 0.02085734232736794ITERATION : 60, loss : 0.02085734232736794ITERATION : 61, loss : 0.02085734232736794ITERATION : 62, loss : 0.02085734232736794ITERATION : 63, loss : 0.02085734232736794ITERATION : 64, loss : 0.02085734232736794ITERATION : 65, loss : 0.02085734232736794ITERATION : 66, loss : 0.02085734232736794ITERATION : 67, loss : 0.02085734232736794ITERATION : 68, loss : 0.02085734232736794ITERATION : 69, loss : 0.02085734232736794ITERATION : 70, loss : 0.02085734232736794ITERATION : 71, loss : 0.02085734232736794ITERATION : 72, loss : 0.02085734232736794ITERATION : 73, loss : 0.02085734232736794ITERATION : 74, loss : 0.02085734232736794ITERATION : 75, loss : 0.02085734232736794ITERATION : 76, loss : 0.02085734232736794ITERATION : 77, loss : 0.02085734232736794ITERATION : 78, loss : 0.02085734232736794ITERATION : 79, loss : 0.02085734232736794ITERATION : 80, loss : 0.02085734232736794ITERATION : 81, loss : 0.02085734232736794ITERATION : 82, loss : 0.02085734232736794ITERATION : 83, loss : 0.02085734232736794ITERATION : 84, loss : 0.02085734232736794ITERATION : 85, loss : 0.02085734232736794ITERATION : 86, loss : 0.02085734232736794ITERATION : 87, loss : 0.02085734232736794ITERATION : 88, loss : 0.02085734232736794ITERATION : 89, loss : 0.02085734232736794ITERATION : 90, loss : 0.02085734232736794ITERATION : 91, loss : 0.02085734232736794ITERATION : 92, loss : 0.02085734232736794ITERATION : 93, loss : 0.02085734232736794ITERATION : 94, loss : 0.02085734232736794ITERATION : 95, loss : 0.02085734232736794ITERATION : 96, loss : 0.02085734232736794ITERATION : 97, loss : 0.02085734232736794ITERATION : 98, loss : 0.02085734232736794ITERATION : 99, loss : 0.02085734232736794ITERATION : 100, loss : 0.02085734232736794
ITERATION : 1, loss : 0.06464726205193579ITERATION : 2, loss : 0.03416955389594725ITERATION : 3, loss : 0.024560409029843945ITERATION : 4, loss : 0.02181513548921703ITERATION : 5, loss : 0.02102561020062533ITERATION : 6, loss : 0.021048559246970893ITERATION : 7, loss : 0.02113620931984795ITERATION : 8, loss : 0.021225525962781606ITERATION : 9, loss : 0.021299124564643012ITERATION : 10, loss : 0.02135504051949996ITERATION : 11, loss : 0.021395884552198173ITERATION : 12, loss : 0.021425073999113437ITERATION : 13, loss : 0.021445650180781532ITERATION : 14, loss : 0.021460013741913475ITERATION : 15, loss : 0.02146996151620955ITERATION : 16, loss : 0.021476801590594233ITERATION : 17, loss : 0.021481471086362797ITERATION : 18, loss : 0.021484634412224528ITERATION : 19, loss : 0.021486758798847067ITERATION : 20, loss : 0.021488171143500958ITERATION : 21, loss : 0.02148909869811118ITERATION : 22, loss : 0.021489698675906208ITERATION : 23, loss : 0.021490079196438167ITERATION : 24, loss : 0.021490314217280627ITERATION : 25, loss : 0.02149045398885826ITERATION : 26, loss : 0.02149053235326328ITERATION : 27, loss : 0.021490572009447734ITERATION : 28, loss : 0.02149058789102855ITERATION : 29, loss : 0.02149058976374969ITERATION : 30, loss : 0.02149058393628748ITERATION : 31, loss : 0.02149057429152605ITERATION : 32, loss : 0.021490563304674914ITERATION : 33, loss : 0.021490552217984907ITERATION : 34, loss : 0.021490541989855217ITERATION : 35, loss : 0.02149053289954709ITERATION : 36, loss : 0.021490525084687264ITERATION : 37, loss : 0.0214905184146161ITERATION : 38, loss : 0.02149051289238289ITERATION : 39, loss : 0.021490508342766404ITERATION : 40, loss : 0.02149050467157434ITERATION : 41, loss : 0.021490501721293912ITERATION : 42, loss : 0.021490499333140064ITERATION : 43, loss : 0.021490497498079627ITERATION : 44, loss : 0.021490496023496754ITERATION : 45, loss : 0.02149049485094357ITERATION : 46, loss : 0.021490493902668516ITERATION : 47, loss : 0.021490493167607042ITERATION : 48, loss : 0.021490492570524558ITERATION : 49, loss : 0.021490492088834086ITERATION : 50, loss : 0.021490491743149643ITERATION : 51, loss : 0.021490491470361712ITERATION : 52, loss : 0.02149049126189986ITERATION : 53, loss : 0.021490491089383466ITERATION : 54, loss : 0.021490490981431205ITERATION : 55, loss : 0.021490490878382076ITERATION : 56, loss : 0.021490490821565598ITERATION : 57, loss : 0.021490490749288365ITERATION : 58, loss : 0.021490490718563734ITERATION : 59, loss : 0.02149049069018382ITERATION : 60, loss : 0.021490490643741134ITERATION : 61, loss : 0.021490490653665244ITERATION : 62, loss : 0.021490490632806197ITERATION : 63, loss : 0.021490490632784708ITERATION : 64, loss : 0.021490490632784708ITERATION : 65, loss : 0.021490490632784708ITERATION : 66, loss : 0.021490490632784708ITERATION : 67, loss : 0.021490490632784708ITERATION : 68, loss : 0.021490490632784708ITERATION : 69, loss : 0.021490490632784708ITERATION : 70, loss : 0.021490490632784708ITERATION : 71, loss : 0.021490490632784708ITERATION : 72, loss : 0.021490490632784708ITERATION : 73, loss : 0.021490490632784708ITERATION : 74, loss : 0.021490490632784708ITERATION : 75, loss : 0.021490490632784708ITERATION : 76, loss : 0.021490490632784708ITERATION : 77, loss : 0.021490490632784708ITERATION : 78, loss : 0.021490490632784708ITERATION : 79, loss : 0.021490490632784708ITERATION : 80, loss : 0.021490490632784708ITERATION : 81, loss : 0.021490490632784708ITERATION : 82, loss : 0.021490490632784708ITERATION : 83, loss : 0.021490490632784708ITERATION : 84, loss : 0.021490490632784708ITERATION : 85, loss : 0.021490490632784708ITERATION : 86, loss : 0.021490490632784708ITERATION : 87, loss : 0.021490490632784708ITERATION : 88, loss : 0.021490490632784708ITERATION : 89, loss : 0.021490490632784708ITERATION : 90, loss : 0.021490490632784708ITERATION : 91, loss : 0.021490490632784708ITERATION : 92, loss : 0.021490490632784708ITERATION : 93, loss : 0.021490490632784708ITERATION : 94, loss : 0.021490490632784708ITERATION : 95, loss : 0.021490490632784708ITERATION : 96, loss : 0.021490490632784708ITERATION : 97, loss : 0.021490490632784708ITERATION : 98, loss : 0.021490490632784708ITERATION : 99, loss : 0.021490490632784708ITERATION : 100, loss : 0.021490490632784708
ITERATION : 1, loss : 0.021297478546366187ITERATION : 2, loss : 0.02158344895561682ITERATION : 3, loss : 0.023089575913945382ITERATION : 4, loss : 0.02343876683897758ITERATION : 5, loss : 0.023262219276348833ITERATION : 6, loss : 0.022977116952638942ITERATION : 7, loss : 0.022723527064037168ITERATION : 8, loss : 0.02252935458813338ITERATION : 9, loss : 0.022388768722910967ITERATION : 10, loss : 0.022289269957977177ITERATION : 11, loss : 0.02221947396252567ITERATION : 12, loss : 0.022170652198652316ITERATION : 13, loss : 0.022136510802280584ITERATION : 14, loss : 0.022112619630018957ITERATION : 15, loss : 0.022095887901423597ITERATION : 16, loss : 0.02208416316720804ITERATION : 17, loss : 0.022075945387709208ITERATION : 18, loss : 0.022070186588653628ITERATION : 19, loss : 0.022066153675368218ITERATION : 20, loss : 0.0220633323882626ITERATION : 21, loss : 0.022061361649484433ITERATION : 22, loss : 0.022059987568805266ITERATION : 23, loss : 0.022059031763152912ITERATION : 24, loss : 0.022058368964165086ITERATION : 25, loss : 0.022057910844442134ITERATION : 26, loss : 0.02205759541851417ITERATION : 27, loss : 0.022057379197881723ITERATION : 28, loss : 0.022057231980701444ITERATION : 29, loss : 0.022057132340561184ITERATION : 30, loss : 0.022057065454894734ITERATION : 31, loss : 0.02205702087659712ITERATION : 32, loss : 0.022056991689383635ITERATION : 33, loss : 0.022056972784198436ITERATION : 34, loss : 0.022056960859646285ITERATION : 35, loss : 0.022056953499439622ITERATION : 36, loss : 0.022056949228017746ITERATION : 37, loss : 0.022056946831082582ITERATION : 38, loss : 0.022056945558024933ITERATION : 39, loss : 0.02205694516536665ITERATION : 40, loss : 0.022056945088538817ITERATION : 41, loss : 0.022056945319760508ITERATION : 42, loss : 0.022056945687595888ITERATION : 43, loss : 0.022056946140493444ITERATION : 44, loss : 0.022056946517562494ITERATION : 45, loss : 0.022056946895766805ITERATION : 46, loss : 0.02205694725011107ITERATION : 47, loss : 0.022056947550808857ITERATION : 48, loss : 0.02205694783252316ITERATION : 49, loss : 0.022056948041882334ITERATION : 50, loss : 0.022056948239706033ITERATION : 51, loss : 0.022056948375332144ITERATION : 52, loss : 0.02205694851710352ITERATION : 53, loss : 0.02205694861205874ITERATION : 54, loss : 0.022056948690262236ITERATION : 55, loss : 0.022056948812596396ITERATION : 56, loss : 0.022056948864226506ITERATION : 57, loss : 0.022056948868351762ITERATION : 58, loss : 0.022056948878428285ITERATION : 59, loss : 0.02205694888239839ITERATION : 60, loss : 0.022056948882997068ITERATION : 61, loss : 0.022056948882997068ITERATION : 62, loss : 0.022056948882997068ITERATION : 63, loss : 0.022056948882997068ITERATION : 64, loss : 0.022056948882997068ITERATION : 65, loss : 0.022056948882997068ITERATION : 66, loss : 0.022056948882997068ITERATION : 67, loss : 0.022056948882997068ITERATION : 68, loss : 0.022056948882997068ITERATION : 69, loss : 0.022056948882997068ITERATION : 70, loss : 0.022056948882997068ITERATION : 71, loss : 0.022056948882997068ITERATION : 72, loss : 0.022056948882997068ITERATION : 73, loss : 0.022056948882997068ITERATION : 74, loss : 0.022056948882997068ITERATION : 75, loss : 0.022056948882997068ITERATION : 76, loss : 0.022056948882997068ITERATION : 77, loss : 0.022056948882997068ITERATION : 78, loss : 0.022056948882997068ITERATION : 79, loss : 0.022056948882997068ITERATION : 80, loss : 0.022056948882997068ITERATION : 81, loss : 0.022056948882997068ITERATION : 82, loss : 0.022056948882997068ITERATION : 83, loss : 0.022056948882997068ITERATION : 84, loss : 0.022056948882997068ITERATION : 85, loss : 0.022056948882997068ITERATION : 86, loss : 0.022056948882997068ITERATION : 87, loss : 0.022056948882997068ITERATION : 88, loss : 0.022056948882997068ITERATION : 89, loss : 0.022056948882997068ITERATION : 90, loss : 0.022056948882997068ITERATION : 91, loss : 0.022056948882997068ITERATION : 92, loss : 0.022056948882997068ITERATION : 93, loss : 0.022056948882997068ITERATION : 94, loss : 0.022056948882997068ITERATION : 95, loss : 0.022056948882997068ITERATION : 96, loss : 0.022056948882997068ITERATION : 97, loss : 0.022056948882997068ITERATION : 98, loss : 0.022056948882997068ITERATION : 99, loss : 0.022056948882997068ITERATION : 100, loss : 0.022056948882997068
ITERATION : 1, loss : 0.021011474142505645ITERATION : 2, loss : 0.021550267018386766ITERATION : 3, loss : 0.021003051686712147ITERATION : 4, loss : 0.02077993220056935ITERATION : 5, loss : 0.0207002263657663ITERATION : 6, loss : 0.02068044146253425ITERATION : 7, loss : 0.020684006995855337ITERATION : 8, loss : 0.020695271594922723ITERATION : 9, loss : 0.020707776933770298ITERATION : 10, loss : 0.02071907778816128ITERATION : 11, loss : 0.020728453044967333ITERATION : 12, loss : 0.020735890504771506ITERATION : 13, loss : 0.020741636498423012ITERATION : 14, loss : 0.02074600089169748ITERATION : 15, loss : 0.020749278211026723ITERATION : 16, loss : 0.02075171949808089ITERATION : 17, loss : 0.020753527241673557ITERATION : 18, loss : 0.020754859799337052ITERATION : 19, loss : 0.020755838684896964ITERATION : 20, loss : 0.020756555883230554ITERATION : 21, loss : 0.020757080145207808ITERATION : 22, loss : 0.020757462632013484ITERATION : 23, loss : 0.020757741176642183ITERATION : 24, loss : 0.020757943869427354ITERATION : 25, loss : 0.02075809119747158ITERATION : 26, loss : 0.020758198070108584ITERATION : 27, loss : 0.020758275541620814ITERATION : 28, loss : 0.020758331665911036ITERATION : 29, loss : 0.020758372283230537ITERATION : 30, loss : 0.02075840169562344ITERATION : 31, loss : 0.020758422867595366ITERATION : 32, loss : 0.02075843816315301ITERATION : 33, loss : 0.020758449270029188ITERATION : 34, loss : 0.020758457274249416ITERATION : 35, loss : 0.02075846298284108ITERATION : 36, loss : 0.02075846709887208ITERATION : 37, loss : 0.020758470107806844ITERATION : 38, loss : 0.020758472289033443ITERATION : 39, loss : 0.020758473905319295ITERATION : 40, loss : 0.02075847502957929ITERATION : 41, loss : 0.02075847578391013ITERATION : 42, loss : 0.020758476436761975ITERATION : 43, loss : 0.020758476868765682ITERATION : 44, loss : 0.020758477140396768ITERATION : 45, loss : 0.020758477307833978ITERATION : 46, loss : 0.020758477460080027ITERATION : 47, loss : 0.02075847758748807ITERATION : 48, loss : 0.020758477631401623ITERATION : 49, loss : 0.020758477662036482ITERATION : 50, loss : 0.020758477719597525ITERATION : 51, loss : 0.020758477719692033ITERATION : 52, loss : 0.020758477719692033ITERATION : 53, loss : 0.020758477719692033ITERATION : 54, loss : 0.020758477719692033ITERATION : 55, loss : 0.020758477719692033ITERATION : 56, loss : 0.020758477719692033ITERATION : 57, loss : 0.020758477719692033ITERATION : 58, loss : 0.020758477719692033ITERATION : 59, loss : 0.020758477719692033ITERATION : 60, loss : 0.020758477719692033ITERATION : 61, loss : 0.020758477719692033ITERATION : 62, loss : 0.020758477719692033ITERATION : 63, loss : 0.020758477719692033ITERATION : 64, loss : 0.020758477719692033ITERATION : 65, loss : 0.020758477719692033ITERATION : 66, loss : 0.020758477719692033ITERATION : 67, loss : 0.020758477719692033ITERATION : 68, loss : 0.020758477719692033ITERATION : 69, loss : 0.020758477719692033ITERATION : 70, loss : 0.020758477719692033ITERATION : 71, loss : 0.020758477719692033ITERATION : 72, loss : 0.020758477719692033ITERATION : 73, loss : 0.020758477719692033ITERATION : 74, loss : 0.020758477719692033ITERATION : 75, loss : 0.020758477719692033ITERATION : 76, loss : 0.020758477719692033ITERATION : 77, loss : 0.020758477719692033ITERATION : 78, loss : 0.020758477719692033ITERATION : 79, loss : 0.020758477719692033ITERATION : 80, loss : 0.020758477719692033ITERATION : 81, loss : 0.020758477719692033ITERATION : 82, loss : 0.020758477719692033ITERATION : 83, loss : 0.020758477719692033ITERATION : 84, loss : 0.020758477719692033ITERATION : 85, loss : 0.020758477719692033ITERATION : 86, loss : 0.020758477719692033ITERATION : 87, loss : 0.020758477719692033ITERATION : 88, loss : 0.020758477719692033ITERATION : 89, loss : 0.020758477719692033ITERATION : 90, loss : 0.020758477719692033ITERATION : 91, loss : 0.020758477719692033ITERATION : 92, loss : 0.020758477719692033ITERATION : 93, loss : 0.020758477719692033ITERATION : 94, loss : 0.020758477719692033ITERATION : 95, loss : 0.020758477719692033ITERATION : 96, loss : 0.020758477719692033ITERATION : 97, loss : 0.020758477719692033ITERATION : 98, loss : 0.020758477719692033ITERATION : 99, loss : 0.020758477719692033ITERATION : 100, loss : 0.020758477719692033
ITERATION : 1, loss : 0.027996300658809627ITERATION : 2, loss : 0.017791139454511936ITERATION : 3, loss : 0.015111510535261252ITERATION : 4, loss : 0.014041543863078865ITERATION : 5, loss : 0.013791827266533ITERATION : 6, loss : 0.013850522090832738ITERATION : 7, loss : 0.013859079836687868ITERATION : 8, loss : 0.013894114782430612ITERATION : 9, loss : 0.013954876065097911ITERATION : 10, loss : 0.014026072303314917ITERATION : 11, loss : 0.014099165423603604ITERATION : 12, loss : 0.01416940265224543ITERATION : 13, loss : 0.014234269114195832ITERATION : 14, loss : 0.014292610533008142ITERATION : 15, loss : 0.014344102575891728ITERATION : 16, loss : 0.014388915553785749ITERATION : 17, loss : 0.014427498460079466ITERATION : 18, loss : 0.014460439107575182ITERATION : 19, loss : 0.014488375767955213ITERATION : 20, loss : 0.014511942764563268ITERATION : 21, loss : 0.014531738508527767ITERATION : 22, loss : 0.014548308833472627ITERATION : 23, loss : 0.014562140265383886ITERATION : 24, loss : 0.014573659029378681ITERATION : 25, loss : 0.014583233856247565ITERATION : 26, loss : 0.014591180660450278ITERATION : 27, loss : 0.014597767989258907ITERATION : 28, loss : 0.014603222849942893ITERATION : 29, loss : 0.014607736103227437ITERATION : 30, loss : 0.014611467776772651ITERATION : 31, loss : 0.01461455143431532ITERATION : 32, loss : 0.014617098432746133ITERATION : 33, loss : 0.014619201370479645ITERATION : 34, loss : 0.01462093714219857ITERATION : 35, loss : 0.014622369435677124ITERATION : 36, loss : 0.014623551141451844ITERATION : 37, loss : 0.014624525879560127ITERATION : 38, loss : 0.014625329730103847ITERATION : 39, loss : 0.014625992710240109ITERATION : 40, loss : 0.014626539364730852ITERATION : 41, loss : 0.014626990067199669ITERATION : 42, loss : 0.014627361662174383ITERATION : 43, loss : 0.014627668008110974ITERATION : 44, loss : 0.014627920633981538ITERATION : 45, loss : 0.014628128812287018ITERATION : 46, loss : 0.014628300472824879ITERATION : 47, loss : 0.01462844199921842ITERATION : 48, loss : 0.014628558651200559ITERATION : 49, loss : 0.014628654799144237ITERATION : 50, loss : 0.014628733956390642ITERATION : 51, loss : 0.014628799227092312ITERATION : 52, loss : 0.014628853047991802ITERATION : 53, loss : 0.014628897353069357ITERATION : 54, loss : 0.014628933963681168ITERATION : 55, loss : 0.014628964053930538ITERATION : 56, loss : 0.014628988904380403ITERATION : 57, loss : 0.01462900934490544ITERATION : 58, loss : 0.014629026265437698ITERATION : 59, loss : 0.014629040206741934ITERATION : 60, loss : 0.01462905158727328ITERATION : 61, loss : 0.014629060970305762ITERATION : 62, loss : 0.01462906880329919ITERATION : 63, loss : 0.014629075200924211ITERATION : 64, loss : 0.014629080486129944ITERATION : 65, loss : 0.014629084918431418ITERATION : 66, loss : 0.014629088522933301ITERATION : 67, loss : 0.014629091401747228ITERATION : 68, loss : 0.014629093858264788ITERATION : 69, loss : 0.014629095859158482ITERATION : 70, loss : 0.01462909750813707ITERATION : 71, loss : 0.014629098893268725ITERATION : 72, loss : 0.014629100012056159ITERATION : 73, loss : 0.01462910096291166ITERATION : 74, loss : 0.014629101681018384ITERATION : 75, loss : 0.014629102339303845ITERATION : 76, loss : 0.014629102803265825ITERATION : 77, loss : 0.014629103191017612ITERATION : 78, loss : 0.014629103546244401ITERATION : 79, loss : 0.014629103759926545ITERATION : 80, loss : 0.014629103969591188ITERATION : 81, loss : 0.014629104141024305ITERATION : 82, loss : 0.014629104390723435ITERATION : 83, loss : 0.01462910459671784ITERATION : 84, loss : 0.014629104651633683ITERATION : 85, loss : 0.014629104690164386ITERATION : 86, loss : 0.014629104734934466ITERATION : 87, loss : 0.014629104769463776ITERATION : 88, loss : 0.014629104804821406ITERATION : 89, loss : 0.01462910483812745ITERATION : 90, loss : 0.014629104855077044ITERATION : 91, loss : 0.014629104861341958ITERATION : 92, loss : 0.014629104861341958ITERATION : 93, loss : 0.014629104861341958ITERATION : 94, loss : 0.014629104861341958ITERATION : 95, loss : 0.014629104861341958ITERATION : 96, loss : 0.014629104861341958ITERATION : 97, loss : 0.014629104861341958ITERATION : 98, loss : 0.014629104861341958ITERATION : 99, loss : 0.014629104861341958ITERATION : 100, loss : 0.014629104861341958
gradient norm in None layer : 0.0006734861407326107
gradient norm in None layer : 2.243974835412514e-05
gradient norm in None layer : 1.6630165190745243e-05
gradient norm in None layer : 0.0003733995590118447
gradient norm in None layer : 2.9373087023694635e-05
gradient norm in None layer : 2.6557101315465495e-05
gradient norm in None layer : 0.00014544335402333204
gradient norm in None layer : 6.233134109719405e-06
gradient norm in None layer : 4.990672503294427e-06
gradient norm in None layer : 0.00013654532391018263
gradient norm in None layer : 5.922771527353681e-06
gradient norm in None layer : 4.559926565290766e-06
gradient norm in None layer : 5.054258205976301e-05
gradient norm in None layer : 1.7397654557048568e-06
gradient norm in None layer : 1.1245944755785343e-06
gradient norm in None layer : 4.553603231563194e-05
gradient norm in None layer : 1.8474130183099386e-06
gradient norm in None layer : 1.3573277864906865e-06
gradient norm in None layer : 6.179778234475566e-05
gradient norm in None layer : 9.413024480491987e-07
gradient norm in None layer : 0.00012004988771933065
gradient norm in None layer : 6.674571169960004e-06
gradient norm in None layer : 4.943736868685599e-06
gradient norm in None layer : 0.0001320974797411954
gradient norm in None layer : 1.0388126525958994e-05
gradient norm in None layer : 1.1243949815863105e-05
gradient norm in None layer : 0.00018717948890755072
gradient norm in None layer : 3.8024810157601365e-06
gradient norm in None layer : 0.0003777931219412712
gradient norm in None layer : 2.919833071432104e-05
gradient norm in None layer : 2.859938231374069e-05
gradient norm in None layer : 0.00041972174262231766
gradient norm in None layer : 3.828796952145181e-05
gradient norm in None layer : 4.3988589311969754e-05
gradient norm in None layer : 3.3207952566708735e-05
gradient norm in None layer : 5.770904412912238e-06
Total gradient norm: 0.0010178662187605033
invariance loss : 4.486910543477412, avg_den : 0.42050933837890625, density loss : 0.32050933837890627, mse loss : 0.023711650026231733, solver time : 150.69438290596008 sec , total loss : 0.02851906990808805, running loss : 0.04674386225322805
Epoch 0/10 , batch 20/12500 
ITERATION : 1, loss : 0.07143967149616448ITERATION : 2, loss : 0.05470392561127059ITERATION : 3, loss : 0.04219288142577711ITERATION : 4, loss : 0.0354838159753775ITERATION : 5, loss : 0.03159700013626222ITERATION : 6, loss : 0.029213233795202898ITERATION : 7, loss : 0.027692850104642938ITERATION : 8, loss : 0.026697385723599337ITERATION : 9, loss : 0.02603419853428824ITERATION : 10, loss : 0.025587271540267405ITERATION : 11, loss : 0.025283774891560584ITERATION : 12, loss : 0.025076622981705ITERATION : 13, loss : 0.024934744544961203ITERATION : 14, loss : 0.024837346035418047ITERATION : 15, loss : 0.024770377019148502ITERATION : 16, loss : 0.02472428101974672ITERATION : 17, loss : 0.02469252891465611ITERATION : 18, loss : 0.024670646244404723ITERATION : 19, loss : 0.024655560039434768ITERATION : 20, loss : 0.024645156859641366ITERATION : 21, loss : 0.02463798192967564ITERATION : 22, loss : 0.02463303289304291ITERATION : 23, loss : 0.024629618904514317ITERATION : 24, loss : 0.02462726373038565ITERATION : 25, loss : 0.02462563890735927ITERATION : 26, loss : 0.02462451795600427ITERATION : 27, loss : 0.02462374458109708ITERATION : 28, loss : 0.024623211028262863ITERATION : 29, loss : 0.024622842908722548ITERATION : 30, loss : 0.024622588933219832ITERATION : 31, loss : 0.024622413727426587ITERATION : 32, loss : 0.024622292798147326ITERATION : 33, loss : 0.024622209406725267ITERATION : 34, loss : 0.024622151827131743ITERATION : 35, loss : 0.024622112146763093ITERATION : 36, loss : 0.024622084724548805ITERATION : 37, loss : 0.0246220658421268ITERATION : 38, loss : 0.024622052773802514ITERATION : 39, loss : 0.02462204379013258ITERATION : 40, loss : 0.02462203758074667ITERATION : 41, loss : 0.02462203334180279ITERATION : 42, loss : 0.024622030375250008ITERATION : 43, loss : 0.02462202838030295ITERATION : 44, loss : 0.024622026942970974ITERATION : 45, loss : 0.024622025973784076ITERATION : 46, loss : 0.02462202528002505ITERATION : 47, loss : 0.024622024847857246ITERATION : 48, loss : 0.024622024510732417ITERATION : 49, loss : 0.024622024333954322ITERATION : 50, loss : 0.024622024208432312ITERATION : 51, loss : 0.024622024202817706ITERATION : 52, loss : 0.0246220241430503ITERATION : 53, loss : 0.024622024141421894ITERATION : 54, loss : 0.024622024141421894ITERATION : 55, loss : 0.024622024141421894ITERATION : 56, loss : 0.024622024141421894ITERATION : 57, loss : 0.024622024141421894ITERATION : 58, loss : 0.024622024141421894ITERATION : 59, loss : 0.024622024141421894ITERATION : 60, loss : 0.024622024141421894ITERATION : 61, loss : 0.024622024141421894ITERATION : 62, loss : 0.024622024141421894ITERATION : 63, loss : 0.024622024141421894ITERATION : 64, loss : 0.024622024141421894ITERATION : 65, loss : 0.024622024141421894ITERATION : 66, loss : 0.024622024141421894ITERATION : 67, loss : 0.024622024141421894ITERATION : 68, loss : 0.024622024141421894ITERATION : 69, loss : 0.024622024141421894ITERATION : 70, loss : 0.024622024141421894ITERATION : 71, loss : 0.024622024141421894ITERATION : 72, loss : 0.024622024141421894ITERATION : 73, loss : 0.024622024141421894ITERATION : 74, loss : 0.024622024141421894ITERATION : 75, loss : 0.024622024141421894ITERATION : 76, loss : 0.024622024141421894ITERATION : 77, loss : 0.024622024141421894ITERATION : 78, loss : 0.024622024141421894ITERATION : 79, loss : 0.024622024141421894ITERATION : 80, loss : 0.024622024141421894ITERATION : 81, loss : 0.024622024141421894ITERATION : 82, loss : 0.024622024141421894ITERATION : 83, loss : 0.024622024141421894ITERATION : 84, loss : 0.024622024141421894ITERATION : 85, loss : 0.024622024141421894ITERATION : 86, loss : 0.024622024141421894ITERATION : 87, loss : 0.024622024141421894ITERATION : 88, loss : 0.024622024141421894ITERATION : 89, loss : 0.024622024141421894ITERATION : 90, loss : 0.024622024141421894ITERATION : 91, loss : 0.024622024141421894ITERATION : 92, loss : 0.024622024141421894ITERATION : 93, loss : 0.024622024141421894ITERATION : 94, loss : 0.024622024141421894ITERATION : 95, loss : 0.024622024141421894ITERATION : 96, loss : 0.024622024141421894ITERATION : 97, loss : 0.024622024141421894ITERATION : 98, loss : 0.024622024141421894ITERATION : 99, loss : 0.024622024141421894ITERATION : 100, loss : 0.024622024141421894
ITERATION : 1, loss : 0.015421888787094315ITERATION : 2, loss : 0.01635517720385307ITERATION : 3, loss : 0.016410395911740514ITERATION : 4, loss : 0.016287060782225ITERATION : 5, loss : 0.016168445254435268ITERATION : 6, loss : 0.01608428521641742ITERATION : 7, loss : 0.01600658731271004ITERATION : 8, loss : 0.015927809049353533ITERATION : 9, loss : 0.015874166823224314ITERATION : 10, loss : 0.015837165090541814ITERATION : 11, loss : 0.015811384147914793ITERATION : 12, loss : 0.015793285684076508ITERATION : 13, loss : 0.015780510522245057ITERATION : 14, loss : 0.015771457558365974ITERATION : 15, loss : 0.01576502457010089ITERATION : 16, loss : 0.015760444385215122ITERATION : 17, loss : 0.015757179273695028ITERATION : 18, loss : 0.015754849459733992ITERATION : 19, loss : 0.015753185957364347ITERATION : 20, loss : 0.015751997596453727ITERATION : 21, loss : 0.015751148644248283ITERATION : 22, loss : 0.015750541989190223ITERATION : 23, loss : 0.015750108373405194ITERATION : 24, loss : 0.015749798411706706ITERATION : 25, loss : 0.015749576905844005ITERATION : 26, loss : 0.015749418469517742ITERATION : 27, loss : 0.015749305265227168ITERATION : 28, loss : 0.015749224357032226ITERATION : 29, loss : 0.015749166468709736ITERATION : 30, loss : 0.015749125116984265ITERATION : 31, loss : 0.015749095602419393ITERATION : 32, loss : 0.01574907449029585ITERATION : 33, loss : 0.015749059369033914ITERATION : 34, loss : 0.015749048605339863ITERATION : 35, loss : 0.01574904088493207ITERATION : 36, loss : 0.015749035347936115ITERATION : 37, loss : 0.015749031378588556ITERATION : 38, loss : 0.015749028531170644ITERATION : 39, loss : 0.015749026491378545ITERATION : 40, loss : 0.015749025036247827ITERATION : 41, loss : 0.015749023990495083ITERATION : 42, loss : 0.015749023240359875ITERATION : 43, loss : 0.015749022716631977ITERATION : 44, loss : 0.01574902232006755ITERATION : 45, loss : 0.01574902204491269ITERATION : 46, loss : 0.01574902184436261ITERATION : 47, loss : 0.015749021693418973ITERATION : 48, loss : 0.015749021600041428ITERATION : 49, loss : 0.0157490215344629ITERATION : 50, loss : 0.015749021501016047ITERATION : 51, loss : 0.01574902144725113ITERATION : 52, loss : 0.015749021431991025ITERATION : 53, loss : 0.01574902140273976ITERATION : 54, loss : 0.01574902140292285ITERATION : 55, loss : 0.01574902140292285ITERATION : 56, loss : 0.01574902140292285ITERATION : 57, loss : 0.01574902140292285ITERATION : 58, loss : 0.01574902140292285ITERATION : 59, loss : 0.01574902140292285ITERATION : 60, loss : 0.01574902140292285ITERATION : 61, loss : 0.01574902140292285ITERATION : 62, loss : 0.01574902140292285ITERATION : 63, loss : 0.01574902140292285ITERATION : 64, loss : 0.01574902140292285ITERATION : 65, loss : 0.01574902140292285ITERATION : 66, loss : 0.01574902140292285ITERATION : 67, loss : 0.01574902140292285ITERATION : 68, loss : 0.01574902140292285ITERATION : 69, loss : 0.01574902140292285ITERATION : 70, loss : 0.01574902140292285ITERATION : 71, loss : 0.01574902140292285ITERATION : 72, loss : 0.01574902140292285ITERATION : 73, loss : 0.01574902140292285ITERATION : 74, loss : 0.01574902140292285ITERATION : 75, loss : 0.01574902140292285ITERATION : 76, loss : 0.01574902140292285ITERATION : 77, loss : 0.01574902140292285ITERATION : 78, loss : 0.01574902140292285ITERATION : 79, loss : 0.01574902140292285ITERATION : 80, loss : 0.01574902140292285ITERATION : 81, loss : 0.01574902140292285ITERATION : 82, loss : 0.01574902140292285ITERATION : 83, loss : 0.01574902140292285ITERATION : 84, loss : 0.01574902140292285ITERATION : 85, loss : 0.01574902140292285ITERATION : 86, loss : 0.01574902140292285ITERATION : 87, loss : 0.01574902140292285ITERATION : 88, loss : 0.01574902140292285ITERATION : 89, loss : 0.01574902140292285ITERATION : 90, loss : 0.01574902140292285ITERATION : 91, loss : 0.01574902140292285ITERATION : 92, loss : 0.01574902140292285ITERATION : 93, loss : 0.01574902140292285ITERATION : 94, loss : 0.01574902140292285ITERATION : 95, loss : 0.01574902140292285ITERATION : 96, loss : 0.01574902140292285ITERATION : 97, loss : 0.01574902140292285ITERATION : 98, loss : 0.01574902140292285ITERATION : 99, loss : 0.01574902140292285ITERATION : 100, loss : 0.01574902140292285
ITERATION : 1, loss : 0.030062900702057718ITERATION : 2, loss : 0.027257334876153956ITERATION : 3, loss : 0.024261158432980233ITERATION : 4, loss : 0.022263459394325516ITERATION : 5, loss : 0.02095398318204951ITERATION : 6, loss : 0.020103285279768113ITERATION : 7, loss : 0.019595594907164365ITERATION : 8, loss : 0.01924955185694109ITERATION : 9, loss : 0.019011765411546498ITERATION : 10, loss : 0.018847285553028975ITERATION : 11, loss : 0.018732929771206996ITERATION : 12, loss : 0.01865311450811868ITERATION : 13, loss : 0.018597242973149568ITERATION : 14, loss : 0.01855804337893125ITERATION : 15, loss : 0.018530490999180648ITERATION : 16, loss : 0.018511095739013057ITERATION : 17, loss : 0.018497424591548975ITERATION : 18, loss : 0.018487776419916318ITERATION : 19, loss : 0.018480959696635924ITERATION : 20, loss : 0.018476137971781017ITERATION : 21, loss : 0.01847272370744023ITERATION : 22, loss : 0.018470303264524444ITERATION : 23, loss : 0.01846858546911269ITERATION : 24, loss : 0.01846736499416101ITERATION : 25, loss : 0.018466496887831398ITERATION : 26, loss : 0.01846587867960021ITERATION : 27, loss : 0.01846543796805987ITERATION : 28, loss : 0.018465123421911634ITERATION : 29, loss : 0.01846489865908006ITERATION : 30, loss : 0.018464737879387193ITERATION : 31, loss : 0.018464622750109974ITERATION : 32, loss : 0.018464540184475697ITERATION : 33, loss : 0.018464480930170033ITERATION : 34, loss : 0.018464438346825936ITERATION : 35, loss : 0.018464407720776115ITERATION : 36, loss : 0.01846438566153077ITERATION : 37, loss : 0.0184643697636041ITERATION : 38, loss : 0.01846435828099167ITERATION : 39, loss : 0.018464350037565913ITERATION : 40, loss : 0.018464344033409175ITERATION : 41, loss : 0.018464339724865234ITERATION : 42, loss : 0.01846433660494247ITERATION : 43, loss : 0.018464334345701303ITERATION : 44, loss : 0.018464332722579035ITERATION : 45, loss : 0.018464331587826288ITERATION : 46, loss : 0.01846433070285615ITERATION : 47, loss : 0.01846433012886841ITERATION : 48, loss : 0.01846432969607918ITERATION : 49, loss : 0.018464329377941038ITERATION : 50, loss : 0.018464329153170818ITERATION : 51, loss : 0.018464328986581263ITERATION : 52, loss : 0.018464328884212847ITERATION : 53, loss : 0.018464328801357652ITERATION : 54, loss : 0.01846432870785061ITERATION : 55, loss : 0.018464328676526864ITERATION : 56, loss : 0.018464328640038857ITERATION : 57, loss : 0.018464328615573497ITERATION : 58, loss : 0.018464328615251015ITERATION : 59, loss : 0.018464328615251015ITERATION : 60, loss : 0.018464328615251015ITERATION : 61, loss : 0.018464328615251015ITERATION : 62, loss : 0.018464328615251015ITERATION : 63, loss : 0.018464328615251015ITERATION : 64, loss : 0.018464328615251015ITERATION : 65, loss : 0.018464328615251015ITERATION : 66, loss : 0.018464328615251015ITERATION : 67, loss : 0.018464328615251015ITERATION : 68, loss : 0.018464328615251015ITERATION : 69, loss : 0.018464328615251015ITERATION : 70, loss : 0.018464328615251015ITERATION : 71, loss : 0.018464328615251015ITERATION : 72, loss : 0.018464328615251015ITERATION : 73, loss : 0.018464328615251015ITERATION : 74, loss : 0.018464328615251015ITERATION : 75, loss : 0.018464328615251015ITERATION : 76, loss : 0.018464328615251015ITERATION : 77, loss : 0.018464328615251015ITERATION : 78, loss : 0.018464328615251015ITERATION : 79, loss : 0.018464328615251015ITERATION : 80, loss : 0.018464328615251015ITERATION : 81, loss : 0.018464328615251015ITERATION : 82, loss : 0.018464328615251015ITERATION : 83, loss : 0.018464328615251015ITERATION : 84, loss : 0.018464328615251015ITERATION : 85, loss : 0.018464328615251015ITERATION : 86, loss : 0.018464328615251015ITERATION : 87, loss : 0.018464328615251015ITERATION : 88, loss : 0.018464328615251015ITERATION : 89, loss : 0.018464328615251015ITERATION : 90, loss : 0.018464328615251015ITERATION : 91, loss : 0.018464328615251015ITERATION : 92, loss : 0.018464328615251015ITERATION : 93, loss : 0.018464328615251015ITERATION : 94, loss : 0.018464328615251015ITERATION : 95, loss : 0.018464328615251015ITERATION : 96, loss : 0.018464328615251015ITERATION : 97, loss : 0.018464328615251015ITERATION : 98, loss : 0.018464328615251015ITERATION : 99, loss : 0.018464328615251015ITERATION : 100, loss : 0.018464328615251015
ITERATION : 1, loss : 0.043542449568420814ITERATION : 2, loss : 0.0427331184213244ITERATION : 3, loss : 0.04320138531836772ITERATION : 4, loss : 0.04374126796951565ITERATION : 5, loss : 0.04416280016150793ITERATION : 6, loss : 0.04447554493624744ITERATION : 7, loss : 0.04470730271637303ITERATION : 8, loss : 0.04487944614837297ITERATION : 9, loss : 0.04500719441115102ITERATION : 10, loss : 0.045101690279481826ITERATION : 11, loss : 0.045171299814655216ITERATION : 12, loss : 0.045222361162045845ITERATION : 13, loss : 0.045259668891060485ITERATION : 14, loss : 0.04528683047450622ITERATION : 15, loss : 0.04530654101534275ITERATION : 16, loss : 0.0453208025054818ITERATION : 17, loss : 0.04533109296271186ITERATION : 18, loss : 0.04533849916393757ITERATION : 19, loss : 0.045343815881883055ITERATION : 20, loss : 0.0453476228510491ITERATION : 21, loss : 0.04535034181730347ITERATION : 22, loss : 0.04535227869984604ITERATION : 23, loss : 0.04535365422842715ITERATION : 24, loss : 0.04535462807856143ITERATION : 25, loss : 0.045355315326951154ITERATION : 26, loss : 0.045355798453661625ITERATION : 27, loss : 0.04535613665496194ITERATION : 28, loss : 0.04535637210847308ITERATION : 29, loss : 0.0453565354124043ITERATION : 30, loss : 0.045356647821697474ITERATION : 31, loss : 0.045356724641876296ITERATION : 32, loss : 0.04535677658126122ITERATION : 33, loss : 0.045356811268443ITERATION : 34, loss : 0.04535683414520163ITERATION : 35, loss : 0.045356848906351326ITERATION : 36, loss : 0.045356858257572144ITERATION : 37, loss : 0.04535686417287518ITERATION : 38, loss : 0.045356867349391726ITERATION : 39, loss : 0.045356868985722415ITERATION : 40, loss : 0.04535686967096413ITERATION : 41, loss : 0.045356869745563785ITERATION : 42, loss : 0.045356869745691794ITERATION : 43, loss : 0.04535686942164914ITERATION : 44, loss : 0.04535686897268735ITERATION : 45, loss : 0.04535686845288992ITERATION : 46, loss : 0.04535686794699498ITERATION : 47, loss : 0.045356867420079466ITERATION : 48, loss : 0.04535686699945064ITERATION : 49, loss : 0.045356866575142304ITERATION : 50, loss : 0.04535686626063866ITERATION : 51, loss : 0.04535686597671514ITERATION : 52, loss : 0.04535686576299119ITERATION : 53, loss : 0.045356865567294555ITERATION : 54, loss : 0.0453568654183665ITERATION : 55, loss : 0.04535686528670221ITERATION : 56, loss : 0.04535686518261254ITERATION : 57, loss : 0.04535686511377068ITERATION : 58, loss : 0.045356865057345826ITERATION : 59, loss : 0.04535686500022044ITERATION : 60, loss : 0.04535686495734459ITERATION : 61, loss : 0.04535686492216052ITERATION : 62, loss : 0.045356864908757816ITERATION : 63, loss : 0.045356864883379734ITERATION : 64, loss : 0.04535686487483862ITERATION : 65, loss : 0.04535686487152594ITERATION : 66, loss : 0.04535686486833274ITERATION : 67, loss : 0.04535686486638118ITERATION : 68, loss : 0.04535686486286872ITERATION : 69, loss : 0.045356864862477024ITERATION : 70, loss : 0.045356864862477024ITERATION : 71, loss : 0.045356864862477024ITERATION : 72, loss : 0.045356864862477024ITERATION : 73, loss : 0.045356864862477024ITERATION : 74, loss : 0.045356864862477024ITERATION : 75, loss : 0.045356864862477024ITERATION : 76, loss : 0.045356864862477024ITERATION : 77, loss : 0.045356864862477024ITERATION : 78, loss : 0.045356864862477024ITERATION : 79, loss : 0.045356864862477024ITERATION : 80, loss : 0.045356864862477024ITERATION : 81, loss : 0.045356864862477024ITERATION : 82, loss : 0.045356864862477024ITERATION : 83, loss : 0.045356864862477024ITERATION : 84, loss : 0.045356864862477024ITERATION : 85, loss : 0.045356864862477024ITERATION : 86, loss : 0.045356864862477024ITERATION : 87, loss : 0.045356864862477024ITERATION : 88, loss : 0.045356864862477024ITERATION : 89, loss : 0.045356864862477024ITERATION : 90, loss : 0.045356864862477024ITERATION : 91, loss : 0.045356864862477024ITERATION : 92, loss : 0.045356864862477024ITERATION : 93, loss : 0.045356864862477024ITERATION : 94, loss : 0.045356864862477024ITERATION : 95, loss : 0.045356864862477024ITERATION : 96, loss : 0.045356864862477024ITERATION : 97, loss : 0.045356864862477024ITERATION : 98, loss : 0.045356864862477024ITERATION : 99, loss : 0.045356864862477024ITERATION : 100, loss : 0.045356864862477024
ITERATION : 1, loss : 0.051696418086170266ITERATION : 2, loss : 0.05555030236678296ITERATION : 3, loss : 0.05363772932033567ITERATION : 4, loss : 0.05252862904093849ITERATION : 5, loss : 0.05122906710342092ITERATION : 6, loss : 0.050517758013897684ITERATION : 7, loss : 0.05010571560879708ITERATION : 8, loss : 0.04985608141321436ITERATION : 9, loss : 0.04969908049170124ITERATION : 10, loss : 0.04959732267687051ITERATION : 11, loss : 0.04952980169193711ITERATION : 12, loss : 0.04948418865838168ITERATION : 13, loss : 0.049452959430519874ITERATION : 14, loss : 0.04943136558078763ITERATION : 15, loss : 0.0494163259658239ITERATION : 16, loss : 0.04940579604612768ITERATION : 17, loss : 0.04939839555956771ITERATION : 18, loss : 0.04939318022895339ITERATION : 19, loss : 0.04938949762069756ITERATION : 20, loss : 0.04938689362980057ITERATION : 21, loss : 0.04938505044303818ITERATION : 22, loss : 0.049383744806495565ITERATION : 23, loss : 0.0493828194806025ITERATION : 24, loss : 0.04938216348122509ITERATION : 25, loss : 0.049381698233772156ITERATION : 26, loss : 0.04938136823682097ITERATION : 27, loss : 0.0493811341163612ITERATION : 28, loss : 0.04938096796166888ITERATION : 29, loss : 0.04938085007950355ITERATION : 30, loss : 0.049380766431198056ITERATION : 31, loss : 0.04938070705802916ITERATION : 32, loss : 0.04938066492798474ITERATION : 33, loss : 0.04938063503256092ITERATION : 34, loss : 0.04938061378658811ITERATION : 35, loss : 0.04938059872872553ITERATION : 36, loss : 0.04938058804560129ITERATION : 37, loss : 0.049380580457028984ITERATION : 38, loss : 0.04938057507373906ITERATION : 39, loss : 0.049380571246302914ITERATION : 40, loss : 0.04938056853320966ITERATION : 41, loss : 0.04938056660951754ITERATION : 42, loss : 0.049380565237514266ITERATION : 43, loss : 0.049380564270380636ITERATION : 44, loss : 0.049380563576520375ITERATION : 45, loss : 0.04938056311267822ITERATION : 46, loss : 0.04938056276871483ITERATION : 47, loss : 0.049380562528362544ITERATION : 48, loss : 0.049380562362443675ITERATION : 49, loss : 0.049380562228690215ITERATION : 50, loss : 0.049380562120529777ITERATION : 51, loss : 0.04938056207686982ITERATION : 52, loss : 0.04938056204969731ITERATION : 53, loss : 0.049380562022518004ITERATION : 54, loss : 0.049380562010297606ITERATION : 55, loss : 0.049380562003506094ITERATION : 56, loss : 0.04938056194569956ITERATION : 57, loss : 0.04938056194569956ITERATION : 58, loss : 0.04938056194569956ITERATION : 59, loss : 0.04938056194569956ITERATION : 60, loss : 0.04938056194569956ITERATION : 61, loss : 0.04938056194569956ITERATION : 62, loss : 0.04938056194569956ITERATION : 63, loss : 0.04938056194569956ITERATION : 64, loss : 0.04938056194569956ITERATION : 65, loss : 0.04938056194569956ITERATION : 66, loss : 0.04938056194569956ITERATION : 67, loss : 0.04938056194569956ITERATION : 68, loss : 0.04938056194569956ITERATION : 69, loss : 0.04938056194569956ITERATION : 70, loss : 0.04938056194569956ITERATION : 71, loss : 0.04938056194569956ITERATION : 72, loss : 0.04938056194569956ITERATION : 73, loss : 0.04938056194569956ITERATION : 74, loss : 0.04938056194569956ITERATION : 75, loss : 0.04938056194569956ITERATION : 76, loss : 0.04938056194569956ITERATION : 77, loss : 0.04938056194569956ITERATION : 78, loss : 0.04938056194569956ITERATION : 79, loss : 0.04938056194569956ITERATION : 80, loss : 0.04938056194569956ITERATION : 81, loss : 0.04938056194569956ITERATION : 82, loss : 0.04938056194569956ITERATION : 83, loss : 0.04938056194569956ITERATION : 84, loss : 0.04938056194569956ITERATION : 85, loss : 0.04938056194569956ITERATION : 86, loss : 0.04938056194569956ITERATION : 87, loss : 0.04938056194569956ITERATION : 88, loss : 0.04938056194569956ITERATION : 89, loss : 0.04938056194569956ITERATION : 90, loss : 0.04938056194569956ITERATION : 91, loss : 0.04938056194569956ITERATION : 92, loss : 0.04938056194569956ITERATION : 93, loss : 0.04938056194569956ITERATION : 94, loss : 0.04938056194569956ITERATION : 95, loss : 0.04938056194569956ITERATION : 96, loss : 0.04938056194569956ITERATION : 97, loss : 0.04938056194569956ITERATION : 98, loss : 0.04938056194569956ITERATION : 99, loss : 0.04938056194569956ITERATION : 100, loss : 0.04938056194569956
ITERATION : 1, loss : 0.04753675770542578ITERATION : 2, loss : 0.040303458711231366ITERATION : 3, loss : 0.038558679274402416ITERATION : 4, loss : 0.03832709070056677ITERATION : 5, loss : 0.038294154697093755ITERATION : 6, loss : 0.03832356447543ITERATION : 7, loss : 0.03836794045245405ITERATION : 8, loss : 0.0384101865659962ITERATION : 9, loss : 0.03844508428493661ITERATION : 10, loss : 0.038472049115664785ITERATION : 11, loss : 0.03849211724983502ITERATION : 12, loss : 0.03850670932897803ITERATION : 13, loss : 0.03851715780618062ITERATION : 14, loss : 0.038524561353611854ITERATION : 15, loss : 0.03852976825813597ITERATION : 16, loss : 0.038533411087474954ITERATION : 17, loss : 0.03853594985868811ITERATION : 18, loss : 0.0385377142356057ITERATION : 19, loss : 0.0385389380313706ITERATION : 20, loss : 0.038539785712714734ITERATION : 21, loss : 0.03854037206631505ITERATION : 22, loss : 0.03854077745746934ITERATION : 23, loss : 0.038541057600183455ITERATION : 24, loss : 0.03854125111749401ITERATION : 25, loss : 0.03854138476032686ITERATION : 26, loss : 0.03854147705470815ITERATION : 27, loss : 0.038541540793094436ITERATION : 28, loss : 0.03854158480968417ITERATION : 29, loss : 0.038541615213034584ITERATION : 30, loss : 0.03854163619857744ITERATION : 31, loss : 0.03854165072060202ITERATION : 32, loss : 0.038541660741342944ITERATION : 33, loss : 0.03854166762571129ITERATION : 34, loss : 0.038541672406254494ITERATION : 35, loss : 0.03854167567651602ITERATION : 36, loss : 0.03854167798931858ITERATION : 37, loss : 0.038541679603938166ITERATION : 38, loss : 0.03854168058166691ITERATION : 39, loss : 0.038541681318987484ITERATION : 40, loss : 0.03854168171224514ITERATION : 41, loss : 0.03854168225345411ITERATION : 42, loss : 0.03854168250344139ITERATION : 43, loss : 0.03854168261651537ITERATION : 44, loss : 0.038541682756141615ITERATION : 45, loss : 0.03854168275778996ITERATION : 46, loss : 0.03854168275778996ITERATION : 47, loss : 0.03854168275778996ITERATION : 48, loss : 0.03854168275778996ITERATION : 49, loss : 0.03854168275778996ITERATION : 50, loss : 0.03854168275778996ITERATION : 51, loss : 0.03854168275778996ITERATION : 52, loss : 0.03854168275778996ITERATION : 53, loss : 0.03854168275778996ITERATION : 54, loss : 0.03854168275778996ITERATION : 55, loss : 0.03854168275778996ITERATION : 56, loss : 0.03854168275778996ITERATION : 57, loss : 0.03854168275778996ITERATION : 58, loss : 0.03854168275778996ITERATION : 59, loss : 0.03854168275778996ITERATION : 60, loss : 0.03854168275778996ITERATION : 61, loss : 0.03854168275778996ITERATION : 62, loss : 0.03854168275778996ITERATION : 63, loss : 0.03854168275778996ITERATION : 64, loss : 0.03854168275778996ITERATION : 65, loss : 0.03854168275778996ITERATION : 66, loss : 0.03854168275778996ITERATION : 67, loss : 0.03854168275778996ITERATION : 68, loss : 0.03854168275778996ITERATION : 69, loss : 0.03854168275778996ITERATION : 70, loss : 0.03854168275778996ITERATION : 71, loss : 0.03854168275778996ITERATION : 72, loss : 0.03854168275778996ITERATION : 73, loss : 0.03854168275778996ITERATION : 74, loss : 0.03854168275778996ITERATION : 75, loss : 0.03854168275778996ITERATION : 76, loss : 0.03854168275778996ITERATION : 77, loss : 0.03854168275778996ITERATION : 78, loss : 0.03854168275778996ITERATION : 79, loss : 0.03854168275778996ITERATION : 80, loss : 0.03854168275778996ITERATION : 81, loss : 0.03854168275778996ITERATION : 82, loss : 0.03854168275778996ITERATION : 83, loss : 0.03854168275778996ITERATION : 84, loss : 0.03854168275778996ITERATION : 85, loss : 0.03854168275778996ITERATION : 86, loss : 0.03854168275778996ITERATION : 87, loss : 0.03854168275778996ITERATION : 88, loss : 0.03854168275778996ITERATION : 89, loss : 0.03854168275778996ITERATION : 90, loss : 0.03854168275778996ITERATION : 91, loss : 0.03854168275778996ITERATION : 92, loss : 0.03854168275778996ITERATION : 93, loss : 0.03854168275778996ITERATION : 94, loss : 0.03854168275778996ITERATION : 95, loss : 0.03854168275778996ITERATION : 96, loss : 0.03854168275778996ITERATION : 97, loss : 0.03854168275778996ITERATION : 98, loss : 0.03854168275778996ITERATION : 99, loss : 0.03854168275778996ITERATION : 100, loss : 0.03854168275778996
ITERATION : 1, loss : 0.05275954638038907ITERATION : 2, loss : 0.04297136819902953ITERATION : 3, loss : 0.03677966701035571ITERATION : 4, loss : 0.0330001358040481ITERATION : 5, loss : 0.030616923826913667ITERATION : 6, loss : 0.02908142282463121ITERATION : 7, loss : 0.02807765911077937ITERATION : 8, loss : 0.027403523002363463ITERATION : 9, loss : 0.026944703972018032ITERATION : 10, loss : 0.026629254910310974ITERATION : 11, loss : 0.026410734879238666ITERATION : 12, loss : 0.026258519794744185ITERATION : 13, loss : 0.02615206328563573ITERATION : 14, loss : 0.02607739304815628ITERATION : 15, loss : 0.02602490888759327ITERATION : 16, loss : 0.025987963866564615ITERATION : 17, loss : 0.025961929652137047ITERATION : 18, loss : 0.025943570159474866ITERATION : 19, loss : 0.025930616002563685ITERATION : 20, loss : 0.025921472308732648ITERATION : 21, loss : 0.02591501654375243ITERATION : 22, loss : 0.025910457698872732ITERATION : 23, loss : 0.025907237982139644ITERATION : 24, loss : 0.025904963825929096ITERATION : 25, loss : 0.025903357444301448ITERATION : 26, loss : 0.025902222711032468ITERATION : 27, loss : 0.025901421130299376ITERATION : 28, loss : 0.025900854871841194ITERATION : 29, loss : 0.025900454862782595ITERATION : 30, loss : 0.025900172281735766ITERATION : 31, loss : 0.025899972670987446ITERATION : 32, loss : 0.025899831653590905ITERATION : 33, loss : 0.02589973204677163ITERATION : 34, loss : 0.02589966167874672ITERATION : 35, loss : 0.025899611977484728ITERATION : 36, loss : 0.02589957685981333ITERATION : 37, loss : 0.025899552035602017ITERATION : 38, loss : 0.02589953452662672ITERATION : 39, loss : 0.025899522157560115ITERATION : 40, loss : 0.025899513434536194ITERATION : 41, loss : 0.025899507275692384ITERATION : 42, loss : 0.025899502929850098ITERATION : 43, loss : 0.025899499846714868ITERATION : 44, loss : 0.0258994976733015ITERATION : 45, loss : 0.0258994961330871ITERATION : 46, loss : 0.025899495052150927ITERATION : 47, loss : 0.025899494302467133ITERATION : 48, loss : 0.025899493764325723ITERATION : 49, loss : 0.0258994933767866ITERATION : 50, loss : 0.02589949313659307ITERATION : 51, loss : 0.025899492954158096ITERATION : 52, loss : 0.02589949281295039ITERATION : 53, loss : 0.025899492734859392ITERATION : 54, loss : 0.025899492659429386ITERATION : 55, loss : 0.025899492647527084ITERATION : 56, loss : 0.025899492647527084ITERATION : 57, loss : 0.025899492647527084ITERATION : 58, loss : 0.025899492647527084ITERATION : 59, loss : 0.025899492647527084ITERATION : 60, loss : 0.025899492647527084ITERATION : 61, loss : 0.025899492647527084ITERATION : 62, loss : 0.025899492647527084ITERATION : 63, loss : 0.025899492647527084ITERATION : 64, loss : 0.025899492647527084ITERATION : 65, loss : 0.025899492647527084ITERATION : 66, loss : 0.025899492647527084ITERATION : 67, loss : 0.025899492647527084ITERATION : 68, loss : 0.025899492647527084ITERATION : 69, loss : 0.025899492647527084ITERATION : 70, loss : 0.025899492647527084ITERATION : 71, loss : 0.025899492647527084ITERATION : 72, loss : 0.025899492647527084ITERATION : 73, loss : 0.025899492647527084ITERATION : 74, loss : 0.025899492647527084ITERATION : 75, loss : 0.025899492647527084ITERATION : 76, loss : 0.025899492647527084ITERATION : 77, loss : 0.025899492647527084ITERATION : 78, loss : 0.025899492647527084ITERATION : 79, loss : 0.025899492647527084ITERATION : 80, loss : 0.025899492647527084ITERATION : 81, loss : 0.025899492647527084ITERATION : 82, loss : 0.025899492647527084ITERATION : 83, loss : 0.025899492647527084ITERATION : 84, loss : 0.025899492647527084ITERATION : 85, loss : 0.025899492647527084ITERATION : 86, loss : 0.025899492647527084ITERATION : 87, loss : 0.025899492647527084ITERATION : 88, loss : 0.025899492647527084ITERATION : 89, loss : 0.025899492647527084ITERATION : 90, loss : 0.025899492647527084ITERATION : 91, loss : 0.025899492647527084ITERATION : 92, loss : 0.025899492647527084ITERATION : 93, loss : 0.025899492647527084ITERATION : 94, loss : 0.025899492647527084ITERATION : 95, loss : 0.025899492647527084ITERATION : 96, loss : 0.025899492647527084ITERATION : 97, loss : 0.025899492647527084ITERATION : 98, loss : 0.025899492647527084ITERATION : 99, loss : 0.025899492647527084ITERATION : 100, loss : 0.025899492647527084
ITERATION : 1, loss : 0.03214311688486773ITERATION : 2, loss : 0.021948141435622907ITERATION : 3, loss : 0.019696687627886605ITERATION : 4, loss : 0.019439731097020962ITERATION : 5, loss : 0.019805109420171843ITERATION : 6, loss : 0.020285191741525807ITERATION : 7, loss : 0.020715730904148435ITERATION : 8, loss : 0.02105696222960893ITERATION : 9, loss : 0.02120343984039727ITERATION : 10, loss : 0.021290926033507903ITERATION : 11, loss : 0.021357125133064703ITERATION : 12, loss : 0.0214056409634872ITERATION : 13, loss : 0.02144049472442166ITERATION : 14, loss : 0.021465209780903365ITERATION : 15, loss : 0.021482583586887986ITERATION : 16, loss : 0.021494725415046826ITERATION : 17, loss : 0.021503177607270593ITERATION : 18, loss : 0.02150904635760468ITERATION : 19, loss : 0.02151311484070362ITERATION : 20, loss : 0.02151593287318282ITERATION : 21, loss : 0.021517884063701198ITERATION : 22, loss : 0.021519235105878564ITERATION : 23, loss : 0.02152017086475998ITERATION : 24, loss : 0.021520819364473785ITERATION : 25, loss : 0.021521269037422994ITERATION : 26, loss : 0.021521581123312684ITERATION : 27, loss : 0.02152179787969259ITERATION : 28, loss : 0.02152194858173936ITERATION : 29, loss : 0.021522053436470057ITERATION : 30, loss : 0.021522126442026467ITERATION : 31, loss : 0.021522177354247045ITERATION : 32, loss : 0.02152221285045436ITERATION : 33, loss : 0.02152223764224766ITERATION : 34, loss : 0.021522254964349553ITERATION : 35, loss : 0.0215222670938471ITERATION : 36, loss : 0.02152227558707543ITERATION : 37, loss : 0.021522281530219105ITERATION : 38, loss : 0.021522285709751936ITERATION : 39, loss : 0.021522288641473042ITERATION : 40, loss : 0.02152229069331678ITERATION : 41, loss : 0.021522292123964ITERATION : 42, loss : 0.021522293124364305ITERATION : 43, loss : 0.021522293826891505ITERATION : 44, loss : 0.021522294312163633ITERATION : 45, loss : 0.021522294658324916ITERATION : 46, loss : 0.021522294909904982ITERATION : 47, loss : 0.021522295073538627ITERATION : 48, loss : 0.021522295189952304ITERATION : 49, loss : 0.021522295295084645ITERATION : 50, loss : 0.021522295346700776ITERATION : 51, loss : 0.02152229539517571ITERATION : 52, loss : 0.02152229542029952ITERATION : 53, loss : 0.021522295443529134ITERATION : 54, loss : 0.02152229545758532ITERATION : 55, loss : 0.02152229546813502ITERATION : 56, loss : 0.021522295468592613ITERATION : 57, loss : 0.021522295468592613ITERATION : 58, loss : 0.021522295468592613ITERATION : 59, loss : 0.021522295468592613ITERATION : 60, loss : 0.021522295468592613ITERATION : 61, loss : 0.021522295468592613ITERATION : 62, loss : 0.021522295468592613ITERATION : 63, loss : 0.021522295468592613ITERATION : 64, loss : 0.021522295468592613ITERATION : 65, loss : 0.021522295468592613ITERATION : 66, loss : 0.021522295468592613ITERATION : 67, loss : 0.021522295468592613ITERATION : 68, loss : 0.021522295468592613ITERATION : 69, loss : 0.021522295468592613ITERATION : 70, loss : 0.021522295468592613ITERATION : 71, loss : 0.021522295468592613ITERATION : 72, loss : 0.021522295468592613ITERATION : 73, loss : 0.021522295468592613ITERATION : 74, loss : 0.021522295468592613ITERATION : 75, loss : 0.021522295468592613ITERATION : 76, loss : 0.021522295468592613ITERATION : 77, loss : 0.021522295468592613ITERATION : 78, loss : 0.021522295468592613ITERATION : 79, loss : 0.021522295468592613ITERATION : 80, loss : 0.021522295468592613ITERATION : 81, loss : 0.021522295468592613ITERATION : 82, loss : 0.021522295468592613ITERATION : 83, loss : 0.021522295468592613ITERATION : 84, loss : 0.021522295468592613ITERATION : 85, loss : 0.021522295468592613ITERATION : 86, loss : 0.021522295468592613ITERATION : 87, loss : 0.021522295468592613ITERATION : 88, loss : 0.021522295468592613ITERATION : 89, loss : 0.021522295468592613ITERATION : 90, loss : 0.021522295468592613ITERATION : 91, loss : 0.021522295468592613ITERATION : 92, loss : 0.021522295468592613ITERATION : 93, loss : 0.021522295468592613ITERATION : 94, loss : 0.021522295468592613ITERATION : 95, loss : 0.021522295468592613ITERATION : 96, loss : 0.021522295468592613ITERATION : 97, loss : 0.021522295468592613ITERATION : 98, loss : 0.021522295468592613ITERATION : 99, loss : 0.021522295468592613ITERATION : 100, loss : 0.021522295468592613
gradient norm in None layer : 0.0007540272803895616
gradient norm in None layer : 4.2776833488086267e-05
gradient norm in None layer : 5.395292626916267e-05
gradient norm in None layer : 0.000602195783832088
gradient norm in None layer : 5.464504711750432e-05
gradient norm in None layer : 6.618384194896604e-05
gradient norm in None layer : 0.00024779026503882797
gradient norm in None layer : 1.2514333944526318e-05
gradient norm in None layer : 1.1381159465559804e-05
gradient norm in None layer : 0.00023221674982986106
gradient norm in None layer : 1.0433165509064773e-05
gradient norm in None layer : 1.043044650358475e-05
gradient norm in None layer : 8.393161661621004e-05
gradient norm in None layer : 3.000292727037835e-06
gradient norm in None layer : 2.338885949974707e-06
gradient norm in None layer : 7.146180034621244e-05
gradient norm in None layer : 3.5099176476955252e-06
gradient norm in None layer : 2.3405757131563318e-06
gradient norm in None layer : 9.696919975750824e-05
gradient norm in None layer : 2.0282372813187744e-06
gradient norm in None layer : 0.0002098313922351084
gradient norm in None layer : 1.5937887115028348e-05
gradient norm in None layer : 1.185935127209e-05
gradient norm in None layer : 0.0002662602242560522
gradient norm in None layer : 2.6937164254918216e-05
gradient norm in None layer : 3.2225412407166e-05
gradient norm in None layer : 0.00042114608978299024
gradient norm in None layer : 4.20055236358248e-06
gradient norm in None layer : 0.0006766503636005377
gradient norm in None layer : 5.687186588195943e-05
gradient norm in None layer : 6.896695835389375e-05
gradient norm in None layer : 0.000843562432810087
gradient norm in None layer : 6.782278804255633e-05
gradient norm in None layer : 8.684791820464713e-05
gradient norm in None layer : 5.9400735132706676e-05
gradient norm in None layer : 1.0406584416654998e-05
Total gradient norm: 0.0016026261928655272
invariance loss : 4.221732794853748, avg_den : 0.423980712890625, density loss : 0.323980712890625, mse loss : 0.02994203398021025, solver time : 135.90079402923584 sec , total loss : 0.03448774748795463, running loss : 0.04613105651496438
saving checkpoint
Epoch 0/10 , batch 21/12500 
ITERATION : 1, loss : 0.039934550972746875ITERATION : 2, loss : 0.05291930844134276ITERATION : 3, loss : 0.047117483159605934ITERATION : 4, loss : 0.04270644988259252ITERATION : 5, loss : 0.040170273868464224ITERATION : 6, loss : 0.038681178663791324ITERATION : 7, loss : 0.03778185779703695ITERATION : 8, loss : 0.03722475910396827ITERATION : 9, loss : 0.03687283702086762ITERATION : 10, loss : 0.03664741888856435ITERATION : 11, loss : 0.03650167262945592ITERATION : 12, loss : 0.03640686264523067ITERATION : 13, loss : 0.03634494973046036ITERATION : 14, loss : 0.03630442483081569ITERATION : 15, loss : 0.03627786436917581ITERATION : 16, loss : 0.036260445244925936ITERATION : 17, loss : 0.03624901926926535ITERATION : 18, loss : 0.03624152558543036ITERATION : 19, loss : 0.03623661274577975ITERATION : 20, loss : 0.036233393859328084ITERATION : 21, loss : 0.036231286371770866ITERATION : 22, loss : 0.03622990771052517ITERATION : 23, loss : 0.03622900674156419ITERATION : 24, loss : 0.03622841863350764ITERATION : 25, loss : 0.03622803527196059ITERATION : 26, loss : 0.03622778575990625ITERATION : 27, loss : 0.03622762355606708ITERATION : 28, loss : 0.03622751834733834ITERATION : 29, loss : 0.03622745019672355ITERATION : 30, loss : 0.03622740619035058ITERATION : 31, loss : 0.03622737785914221ITERATION : 32, loss : 0.03622735967468199ITERATION : 33, loss : 0.03622734797309542ITERATION : 34, loss : 0.03622734056705122ITERATION : 35, loss : 0.036227335836273425ITERATION : 36, loss : 0.03622733284003525ITERATION : 37, loss : 0.036227330979073484ITERATION : 38, loss : 0.03622732982531338ITERATION : 39, loss : 0.036227329070585616ITERATION : 40, loss : 0.03622732861506927ITERATION : 41, loss : 0.036227328359019664ITERATION : 42, loss : 0.03622732823309874ITERATION : 43, loss : 0.03622732815767264ITERATION : 44, loss : 0.036227328131229176ITERATION : 45, loss : 0.036227328118395775ITERATION : 46, loss : 0.03622732811613638ITERATION : 47, loss : 0.03622732812098228ITERATION : 48, loss : 0.03622732812569149ITERATION : 49, loss : 0.03622732813112889ITERATION : 50, loss : 0.0362273281373834ITERATION : 51, loss : 0.036227328140931915ITERATION : 52, loss : 0.0362273281442455ITERATION : 53, loss : 0.0362273281442455ITERATION : 54, loss : 0.0362273281442455ITERATION : 55, loss : 0.0362273281442455ITERATION : 56, loss : 0.0362273281442455ITERATION : 57, loss : 0.0362273281442455ITERATION : 58, loss : 0.0362273281442455ITERATION : 59, loss : 0.0362273281442455ITERATION : 60, loss : 0.0362273281442455ITERATION : 61, loss : 0.0362273281442455ITERATION : 62, loss : 0.0362273281442455ITERATION : 63, loss : 0.0362273281442455ITERATION : 64, loss : 0.0362273281442455ITERATION : 65, loss : 0.0362273281442455ITERATION : 66, loss : 0.0362273281442455ITERATION : 67, loss : 0.0362273281442455ITERATION : 68, loss : 0.0362273281442455ITERATION : 69, loss : 0.0362273281442455ITERATION : 70, loss : 0.0362273281442455ITERATION : 71, loss : 0.0362273281442455ITERATION : 72, loss : 0.0362273281442455ITERATION : 73, loss : 0.0362273281442455ITERATION : 74, loss : 0.0362273281442455ITERATION : 75, loss : 0.0362273281442455ITERATION : 76, loss : 0.0362273281442455ITERATION : 77, loss : 0.0362273281442455ITERATION : 78, loss : 0.0362273281442455ITERATION : 79, loss : 0.0362273281442455ITERATION : 80, loss : 0.0362273281442455ITERATION : 81, loss : 0.0362273281442455ITERATION : 82, loss : 0.0362273281442455ITERATION : 83, loss : 0.0362273281442455ITERATION : 84, loss : 0.0362273281442455ITERATION : 85, loss : 0.0362273281442455ITERATION : 86, loss : 0.0362273281442455ITERATION : 87, loss : 0.0362273281442455ITERATION : 88, loss : 0.0362273281442455ITERATION : 89, loss : 0.0362273281442455ITERATION : 90, loss : 0.0362273281442455ITERATION : 91, loss : 0.0362273281442455ITERATION : 92, loss : 0.0362273281442455ITERATION : 93, loss : 0.0362273281442455ITERATION : 94, loss : 0.0362273281442455ITERATION : 95, loss : 0.0362273281442455ITERATION : 96, loss : 0.0362273281442455ITERATION : 97, loss : 0.0362273281442455ITERATION : 98, loss : 0.0362273281442455ITERATION : 99, loss : 0.0362273281442455ITERATION : 100, loss : 0.0362273281442455
ITERATION : 1, loss : 0.06281368591169768ITERATION : 2, loss : 0.046431559983025394ITERATION : 3, loss : 0.042054845581666375ITERATION : 4, loss : 0.040322099085254796ITERATION : 5, loss : 0.03938369323999941ITERATION : 6, loss : 0.03877747746113161ITERATION : 7, loss : 0.03835244377698657ITERATION : 8, loss : 0.038043173276767595ITERATION : 9, loss : 0.03781411033856244ITERATION : 10, loss : 0.03764292334352378ITERATION : 11, loss : 0.03751438826412591ITERATION : 12, loss : 0.037417643932698606ITERATION : 13, loss : 0.037344742848538136ITERATION : 14, loss : 0.03728978541798615ITERATION : 15, loss : 0.0372483544731902ITERATION : 16, loss : 0.0372171281566977ITERATION : 17, loss : 0.037193601715967375ITERATION : 18, loss : 0.0371758843027488ITERATION : 19, loss : 0.03716254793928575ITERATION : 20, loss : 0.037152514031783976ITERATION : 21, loss : 0.03714496840286571ITERATION : 22, loss : 0.03713929657647885ITERATION : 23, loss : 0.03713503490737209ITERATION : 24, loss : 0.03713183435530865ITERATION : 25, loss : 0.037129431584936375ITERATION : 26, loss : 0.037127628411599986ITERATION : 27, loss : 0.037126275672325494ITERATION : 28, loss : 0.03712526123815933ITERATION : 29, loss : 0.03712450072560171ITERATION : 30, loss : 0.037123930752571685ITERATION : 31, loss : 0.03712350369155846ITERATION : 32, loss : 0.03712318384415719ITERATION : 33, loss : 0.03712294437398559ITERATION : 34, loss : 0.03712276505224612ITERATION : 35, loss : 0.03712263077614887ITERATION : 36, loss : 0.03712253033685018ITERATION : 37, loss : 0.03712245518613639ITERATION : 38, loss : 0.037122399026326074ITERATION : 39, loss : 0.037122356944404786ITERATION : 40, loss : 0.037122325529752476ITERATION : 41, loss : 0.037122302118512605ITERATION : 42, loss : 0.037122284577912716ITERATION : 43, loss : 0.037122271515477145ITERATION : 44, loss : 0.03712226180599373ITERATION : 45, loss : 0.03712225451614819ITERATION : 46, loss : 0.03712224904087882ITERATION : 47, loss : 0.037122244951842694ITERATION : 48, loss : 0.03712224185627174ITERATION : 49, loss : 0.03712223954096569ITERATION : 50, loss : 0.03712223779238099ITERATION : 51, loss : 0.03712223649334115ITERATION : 52, loss : 0.03712223560449671ITERATION : 53, loss : 0.0371222348690693ITERATION : 54, loss : 0.03712223432203993ITERATION : 55, loss : 0.03712223389879675ITERATION : 56, loss : 0.0371222336386279ITERATION : 57, loss : 0.03712223339637109ITERATION : 58, loss : 0.03712223326992968ITERATION : 59, loss : 0.03712223310535606ITERATION : 60, loss : 0.03712223310421646ITERATION : 61, loss : 0.03712223310421646ITERATION : 62, loss : 0.03712223310421646ITERATION : 63, loss : 0.03712223310421646ITERATION : 64, loss : 0.03712223310421646ITERATION : 65, loss : 0.03712223310421646ITERATION : 66, loss : 0.03712223310421646ITERATION : 67, loss : 0.03712223310421646ITERATION : 68, loss : 0.03712223310421646ITERATION : 69, loss : 0.03712223310421646ITERATION : 70, loss : 0.03712223310421646ITERATION : 71, loss : 0.03712223310421646ITERATION : 72, loss : 0.03712223310421646ITERATION : 73, loss : 0.03712223310421646ITERATION : 74, loss : 0.03712223310421646ITERATION : 75, loss : 0.03712223310421646ITERATION : 76, loss : 0.03712223310421646ITERATION : 77, loss : 0.03712223310421646ITERATION : 78, loss : 0.03712223310421646ITERATION : 79, loss : 0.03712223310421646ITERATION : 80, loss : 0.03712223310421646ITERATION : 81, loss : 0.03712223310421646ITERATION : 82, loss : 0.03712223310421646ITERATION : 83, loss : 0.03712223310421646ITERATION : 84, loss : 0.03712223310421646ITERATION : 85, loss : 0.03712223310421646ITERATION : 86, loss : 0.03712223310421646ITERATION : 87, loss : 0.03712223310421646ITERATION : 88, loss : 0.03712223310421646ITERATION : 89, loss : 0.03712223310421646ITERATION : 90, loss : 0.03712223310421646ITERATION : 91, loss : 0.03712223310421646ITERATION : 92, loss : 0.03712223310421646ITERATION : 93, loss : 0.03712223310421646ITERATION : 94, loss : 0.03712223310421646ITERATION : 95, loss : 0.03712223310421646ITERATION : 96, loss : 0.03712223310421646ITERATION : 97, loss : 0.03712223310421646ITERATION : 98, loss : 0.03712223310421646ITERATION : 99, loss : 0.03712223310421646ITERATION : 100, loss : 0.03712223310421646
ITERATION : 1, loss : 0.025788888573045943ITERATION : 2, loss : 0.018566561169386403ITERATION : 3, loss : 0.016750998374574173ITERATION : 4, loss : 0.016369541635495095ITERATION : 5, loss : 0.016336985835051183ITERATION : 6, loss : 0.016409909352646263ITERATION : 7, loss : 0.016501723335399503ITERATION : 8, loss : 0.016504569295356684ITERATION : 9, loss : 0.016410836592859922ITERATION : 10, loss : 0.01635051095863615ITERATION : 11, loss : 0.01631115253626866ITERATION : 12, loss : 0.016285209818056787ITERATION : 13, loss : 0.016267980621683612ITERATION : 14, loss : 0.01625647522359195ITERATION : 15, loss : 0.016248761534444794ITERATION : 16, loss : 0.01624357512182298ITERATION : 17, loss : 0.01624008083752736ITERATION : 18, loss : 0.01623772308933741ITERATION : 19, loss : 0.016236130653524933ITERATION : 20, loss : 0.016235054249319904ITERATION : 21, loss : 0.016234326309528584ITERATION : 22, loss : 0.0162338338172817ITERATION : 23, loss : 0.016233500565422332ITERATION : 24, loss : 0.016233275054583322ITERATION : 25, loss : 0.01623312241000329ITERATION : 26, loss : 0.016233019093409542ITERATION : 27, loss : 0.016232949182621973ITERATION : 28, loss : 0.016232901839873113ITERATION : 29, loss : 0.016232869809442205ITERATION : 30, loss : 0.01623284812313214ITERATION : 31, loss : 0.016232833450559852ITERATION : 32, loss : 0.01623282350975475ITERATION : 33, loss : 0.016232816782528834ITERATION : 34, loss : 0.01623281224923129ITERATION : 35, loss : 0.01623280919112265ITERATION : 36, loss : 0.016232807136614583ITERATION : 37, loss : 0.016232805743192653ITERATION : 38, loss : 0.016232804818930812ITERATION : 39, loss : 0.016232804164797842ITERATION : 40, loss : 0.016232803739957925ITERATION : 41, loss : 0.01623280345355827ITERATION : 42, loss : 0.01623280328084656ITERATION : 43, loss : 0.01623280316668927ITERATION : 44, loss : 0.01623280308911652ITERATION : 45, loss : 0.016232803037056775ITERATION : 46, loss : 0.016232802992346078ITERATION : 47, loss : 0.016232802959325793ITERATION : 48, loss : 0.016232802948210584ITERATION : 49, loss : 0.016232802938291536ITERATION : 50, loss : 0.016232802932625807ITERATION : 51, loss : 0.016232802932625807ITERATION : 52, loss : 0.016232802932625807ITERATION : 53, loss : 0.016232802932625807ITERATION : 54, loss : 0.016232802932625807ITERATION : 55, loss : 0.016232802932625807ITERATION : 56, loss : 0.016232802932625807ITERATION : 57, loss : 0.016232802932625807ITERATION : 58, loss : 0.016232802932625807ITERATION : 59, loss : 0.016232802932625807ITERATION : 60, loss : 0.016232802932625807ITERATION : 61, loss : 0.016232802932625807ITERATION : 62, loss : 0.016232802932625807ITERATION : 63, loss : 0.016232802932625807ITERATION : 64, loss : 0.016232802932625807ITERATION : 65, loss : 0.016232802932625807ITERATION : 66, loss : 0.016232802932625807ITERATION : 67, loss : 0.016232802932625807ITERATION : 68, loss : 0.016232802932625807ITERATION : 69, loss : 0.016232802932625807ITERATION : 70, loss : 0.016232802932625807ITERATION : 71, loss : 0.016232802932625807ITERATION : 72, loss : 0.016232802932625807ITERATION : 73, loss : 0.016232802932625807ITERATION : 74, loss : 0.016232802932625807ITERATION : 75, loss : 0.016232802932625807ITERATION : 76, loss : 0.016232802932625807ITERATION : 77, loss : 0.016232802932625807ITERATION : 78, loss : 0.016232802932625807ITERATION : 79, loss : 0.016232802932625807ITERATION : 80, loss : 0.016232802932625807ITERATION : 81, loss : 0.016232802932625807ITERATION : 82, loss : 0.016232802932625807ITERATION : 83, loss : 0.016232802932625807ITERATION : 84, loss : 0.016232802932625807ITERATION : 85, loss : 0.016232802932625807ITERATION : 86, loss : 0.016232802932625807ITERATION : 87, loss : 0.016232802932625807ITERATION : 88, loss : 0.016232802932625807ITERATION : 89, loss : 0.016232802932625807ITERATION : 90, loss : 0.016232802932625807ITERATION : 91, loss : 0.016232802932625807ITERATION : 92, loss : 0.016232802932625807ITERATION : 93, loss : 0.016232802932625807ITERATION : 94, loss : 0.016232802932625807ITERATION : 95, loss : 0.016232802932625807ITERATION : 96, loss : 0.016232802932625807ITERATION : 97, loss : 0.016232802932625807ITERATION : 98, loss : 0.016232802932625807ITERATION : 99, loss : 0.016232802932625807ITERATION : 100, loss : 0.016232802932625807
ITERATION : 1, loss : 0.03164419661122054ITERATION : 2, loss : 0.026777430782969524ITERATION : 3, loss : 0.024325912822625764ITERATION : 4, loss : 0.02266314112690768ITERATION : 5, loss : 0.021580272890257428ITERATION : 6, loss : 0.020889467308558955ITERATION : 7, loss : 0.020449176650337943ITERATION : 8, loss : 0.02016541500418349ITERATION : 9, loss : 0.019979572812909862ITERATION : 10, loss : 0.01985584615680969ITERATION : 11, loss : 0.019772273203220076ITERATION : 12, loss : 0.019715155711526156ITERATION : 13, loss : 0.01967576360275902ITERATION : 14, loss : 0.019648411645063056ITERATION : 15, loss : 0.019629325622947773ITERATION : 16, loss : 0.01961596003717781ITERATION : 17, loss : 0.019606576629668866ITERATION : 18, loss : 0.019599977127563027ITERATION : 19, loss : 0.019595329741401265ITERATION : 20, loss : 0.019592054207894036ITERATION : 21, loss : 0.019589744180981278ITERATION : 22, loss : 0.019588114385687982ITERATION : 23, loss : 0.01958696421465156ITERATION : 24, loss : 0.019586152361383915ITERATION : 25, loss : 0.019585579239661796ITERATION : 26, loss : 0.019585174626699692ITERATION : 27, loss : 0.01958488899130909ITERATION : 28, loss : 0.01958468732720244ITERATION : 29, loss : 0.01958454495708281ITERATION : 30, loss : 0.019584444451562377ITERATION : 31, loss : 0.01958437349970424ITERATION : 32, loss : 0.019584323413952713ITERATION : 33, loss : 0.019584288060014776ITERATION : 34, loss : 0.019584263104303598ITERATION : 35, loss : 0.01958424549433335ITERATION : 36, loss : 0.01958423306841521ITERATION : 37, loss : 0.01958422429514694ITERATION : 38, loss : 0.01958421811606328ITERATION : 39, loss : 0.019584213744608846ITERATION : 40, loss : 0.01958421066028578ITERATION : 41, loss : 0.01958420849059023ITERATION : 42, loss : 0.01958420696091966ITERATION : 43, loss : 0.019584205876030027ITERATION : 44, loss : 0.01958420510785748ITERATION : 45, loss : 0.01958420457894258ITERATION : 46, loss : 0.01958420417077049ITERATION : 47, loss : 0.019584203905862307ITERATION : 48, loss : 0.01958420373555118ITERATION : 49, loss : 0.01958420359872733ITERATION : 50, loss : 0.01958420350460329ITERATION : 51, loss : 0.019584203416503642ITERATION : 52, loss : 0.0195842033902673ITERATION : 53, loss : 0.01958420336075953ITERATION : 54, loss : 0.019584203360683186ITERATION : 55, loss : 0.019584203360683186ITERATION : 56, loss : 0.019584203360683186ITERATION : 57, loss : 0.019584203360683186ITERATION : 58, loss : 0.019584203360683186ITERATION : 59, loss : 0.019584203360683186ITERATION : 60, loss : 0.019584203360683186ITERATION : 61, loss : 0.019584203360683186ITERATION : 62, loss : 0.019584203360683186ITERATION : 63, loss : 0.019584203360683186ITERATION : 64, loss : 0.019584203360683186ITERATION : 65, loss : 0.019584203360683186ITERATION : 66, loss : 0.019584203360683186ITERATION : 67, loss : 0.019584203360683186ITERATION : 68, loss : 0.019584203360683186ITERATION : 69, loss : 0.019584203360683186ITERATION : 70, loss : 0.019584203360683186ITERATION : 71, loss : 0.019584203360683186ITERATION : 72, loss : 0.019584203360683186ITERATION : 73, loss : 0.019584203360683186ITERATION : 74, loss : 0.019584203360683186ITERATION : 75, loss : 0.019584203360683186ITERATION : 76, loss : 0.019584203360683186ITERATION : 77, loss : 0.019584203360683186ITERATION : 78, loss : 0.019584203360683186ITERATION : 79, loss : 0.019584203360683186ITERATION : 80, loss : 0.019584203360683186ITERATION : 81, loss : 0.019584203360683186ITERATION : 82, loss : 0.019584203360683186ITERATION : 83, loss : 0.019584203360683186ITERATION : 84, loss : 0.019584203360683186ITERATION : 85, loss : 0.019584203360683186ITERATION : 86, loss : 0.019584203360683186ITERATION : 87, loss : 0.019584203360683186ITERATION : 88, loss : 0.019584203360683186ITERATION : 89, loss : 0.019584203360683186ITERATION : 90, loss : 0.019584203360683186ITERATION : 91, loss : 0.019584203360683186ITERATION : 92, loss : 0.019584203360683186ITERATION : 93, loss : 0.019584203360683186ITERATION : 94, loss : 0.019584203360683186ITERATION : 95, loss : 0.019584203360683186ITERATION : 96, loss : 0.019584203360683186ITERATION : 97, loss : 0.019584203360683186ITERATION : 98, loss : 0.019584203360683186ITERATION : 99, loss : 0.019584203360683186ITERATION : 100, loss : 0.019584203360683186
ITERATION : 1, loss : 0.0324510694990642ITERATION : 2, loss : 0.02101294139354214ITERATION : 3, loss : 0.018453403892467773ITERATION : 4, loss : 0.01786588790868807ITERATION : 5, loss : 0.017797237954950154ITERATION : 6, loss : 0.017859292065437484ITERATION : 7, loss : 0.01793991584515625ITERATION : 8, loss : 0.018009176790296653ITERATION : 9, loss : 0.01806212052736847ITERATION : 10, loss : 0.018100733486594595ITERATION : 11, loss : 0.018128297658987193ITERATION : 12, loss : 0.018147772264803ITERATION : 13, loss : 0.018161461480578043ITERATION : 14, loss : 0.018171060100197697ITERATION : 15, loss : 0.018177782530611218ITERATION : 16, loss : 0.018182488381239554ITERATION : 17, loss : 0.01818578236076921ITERATION : 18, loss : 0.018188088253558977ITERATION : 19, loss : 0.018189702854360463ITERATION : 20, loss : 0.018190833791057844ITERATION : 21, loss : 0.018191626075458343ITERATION : 22, loss : 0.01819218142952439ITERATION : 23, loss : 0.01819257075605517ITERATION : 24, loss : 0.018192843773876463ITERATION : 25, loss : 0.01819303534472743ITERATION : 26, loss : 0.01819316981042008ITERATION : 27, loss : 0.01819326425634222ITERATION : 28, loss : 0.018193330618237614ITERATION : 29, loss : 0.018193377215795878ITERATION : 30, loss : 0.018193409989820587ITERATION : 31, loss : 0.018193433028312653ITERATION : 32, loss : 0.01819344922355871ITERATION : 33, loss : 0.01819346059085555ITERATION : 34, loss : 0.018193468588041804ITERATION : 35, loss : 0.018193474227735304ITERATION : 36, loss : 0.018193478246687222ITERATION : 37, loss : 0.01819348104537921ITERATION : 38, loss : 0.018193483019485205ITERATION : 39, loss : 0.01819348441688274ITERATION : 40, loss : 0.018193485370801406ITERATION : 41, loss : 0.018193486083230088ITERATION : 42, loss : 0.018193486534168964ITERATION : 43, loss : 0.018193486908401422ITERATION : 44, loss : 0.018193487100910368ITERATION : 45, loss : 0.0181934872478836ITERATION : 46, loss : 0.01819348737031092ITERATION : 47, loss : 0.018193487379614213ITERATION : 48, loss : 0.018193487379614213ITERATION : 49, loss : 0.018193487379614213ITERATION : 50, loss : 0.018193487379614213ITERATION : 51, loss : 0.018193487379614213ITERATION : 52, loss : 0.018193487379614213ITERATION : 53, loss : 0.018193487379614213ITERATION : 54, loss : 0.018193487379614213ITERATION : 55, loss : 0.018193487379614213ITERATION : 56, loss : 0.018193487379614213ITERATION : 57, loss : 0.018193487379614213ITERATION : 58, loss : 0.018193487379614213ITERATION : 59, loss : 0.018193487379614213ITERATION : 60, loss : 0.018193487379614213ITERATION : 61, loss : 0.018193487379614213ITERATION : 62, loss : 0.018193487379614213ITERATION : 63, loss : 0.018193487379614213ITERATION : 64, loss : 0.018193487379614213ITERATION : 65, loss : 0.018193487379614213ITERATION : 66, loss : 0.018193487379614213ITERATION : 67, loss : 0.018193487379614213ITERATION : 68, loss : 0.018193487379614213ITERATION : 69, loss : 0.018193487379614213ITERATION : 70, loss : 0.018193487379614213ITERATION : 71, loss : 0.018193487379614213ITERATION : 72, loss : 0.018193487379614213ITERATION : 73, loss : 0.018193487379614213ITERATION : 74, loss : 0.018193487379614213ITERATION : 75, loss : 0.018193487379614213ITERATION : 76, loss : 0.018193487379614213ITERATION : 77, loss : 0.018193487379614213ITERATION : 78, loss : 0.018193487379614213ITERATION : 79, loss : 0.018193487379614213ITERATION : 80, loss : 0.018193487379614213ITERATION : 81, loss : 0.018193487379614213ITERATION : 82, loss : 0.018193487379614213ITERATION : 83, loss : 0.018193487379614213ITERATION : 84, loss : 0.018193487379614213ITERATION : 85, loss : 0.018193487379614213ITERATION : 86, loss : 0.018193487379614213ITERATION : 87, loss : 0.018193487379614213ITERATION : 88, loss : 0.018193487379614213ITERATION : 89, loss : 0.018193487379614213ITERATION : 90, loss : 0.018193487379614213ITERATION : 91, loss : 0.018193487379614213ITERATION : 92, loss : 0.018193487379614213ITERATION : 93, loss : 0.018193487379614213ITERATION : 94, loss : 0.018193487379614213ITERATION : 95, loss : 0.018193487379614213ITERATION : 96, loss : 0.018193487379614213ITERATION : 97, loss : 0.018193487379614213ITERATION : 98, loss : 0.018193487379614213ITERATION : 99, loss : 0.018193487379614213ITERATION : 100, loss : 0.018193487379614213
ITERATION : 1, loss : 0.02235695153499185ITERATION : 2, loss : 0.022247478508184416ITERATION : 3, loss : 0.022438122748013434ITERATION : 4, loss : 0.022594086904454087ITERATION : 5, loss : 0.022685026387094697ITERATION : 6, loss : 0.022731365160373787ITERATION : 7, loss : 0.022753210178799943ITERATION : 8, loss : 0.02276280945218526ITERATION : 9, loss : 0.022766617822563725ITERATION : 10, loss : 0.0227678255356632ITERATION : 11, loss : 0.0227679486321272ITERATION : 12, loss : 0.022767680922393493ITERATION : 13, loss : 0.02276732343725763ITERATION : 14, loss : 0.022766994137487884ITERATION : 15, loss : 0.022766729094025422ITERATION : 16, loss : 0.022766530348943614ITERATION : 17, loss : 0.022766387908022684ITERATION : 18, loss : 0.022766288939908015ITERATION : 19, loss : 0.022766221959528037ITERATION : 20, loss : 0.02276617751618736ITERATION : 21, loss : 0.022766148696386053ITERATION : 22, loss : 0.022766130433286343ITERATION : 23, loss : 0.022766119039736996ITERATION : 24, loss : 0.02276611224973867ITERATION : 25, loss : 0.022766108344685527ITERATION : 26, loss : 0.022766106296349706ITERATION : 27, loss : 0.02276610526488396ITERATION : 28, loss : 0.022766104933130745ITERATION : 29, loss : 0.022766104810086358ITERATION : 30, loss : 0.02276610494372211ITERATION : 31, loss : 0.022766105214566464ITERATION : 32, loss : 0.022766105546259553ITERATION : 33, loss : 0.022766105830016713ITERATION : 34, loss : 0.02276610612498878ITERATION : 35, loss : 0.02276610634725487ITERATION : 36, loss : 0.022766106550390273ITERATION : 37, loss : 0.022766106691030834ITERATION : 38, loss : 0.022766106840063086ITERATION : 39, loss : 0.022766106930483386ITERATION : 40, loss : 0.02276610700206143ITERATION : 41, loss : 0.022766107070405675ITERATION : 42, loss : 0.022766107113858056ITERATION : 43, loss : 0.022766107138115995ITERATION : 44, loss : 0.022766107173175083ITERATION : 45, loss : 0.022766107201526813ITERATION : 46, loss : 0.02276610721501451ITERATION : 47, loss : 0.022766107217116336ITERATION : 48, loss : 0.02276610723617342ITERATION : 49, loss : 0.022766107229286816ITERATION : 50, loss : 0.022766107229672716ITERATION : 51, loss : 0.022766107229672716ITERATION : 52, loss : 0.022766107229672716ITERATION : 53, loss : 0.022766107229672716ITERATION : 54, loss : 0.022766107229672716ITERATION : 55, loss : 0.022766107229672716ITERATION : 56, loss : 0.022766107229672716ITERATION : 57, loss : 0.022766107229672716ITERATION : 58, loss : 0.022766107229672716ITERATION : 59, loss : 0.022766107229672716ITERATION : 60, loss : 0.022766107229672716ITERATION : 61, loss : 0.022766107229672716ITERATION : 62, loss : 0.022766107229672716ITERATION : 63, loss : 0.022766107229672716ITERATION : 64, loss : 0.022766107229672716ITERATION : 65, loss : 0.022766107229672716ITERATION : 66, loss : 0.022766107229672716ITERATION : 67, loss : 0.022766107229672716ITERATION : 68, loss : 0.022766107229672716ITERATION : 69, loss : 0.022766107229672716ITERATION : 70, loss : 0.022766107229672716ITERATION : 71, loss : 0.022766107229672716ITERATION : 72, loss : 0.022766107229672716ITERATION : 73, loss : 0.022766107229672716ITERATION : 74, loss : 0.022766107229672716ITERATION : 75, loss : 0.022766107229672716ITERATION : 76, loss : 0.022766107229672716ITERATION : 77, loss : 0.022766107229672716ITERATION : 78, loss : 0.022766107229672716ITERATION : 79, loss : 0.022766107229672716ITERATION : 80, loss : 0.022766107229672716ITERATION : 81, loss : 0.022766107229672716ITERATION : 82, loss : 0.022766107229672716ITERATION : 83, loss : 0.022766107229672716ITERATION : 84, loss : 0.022766107229672716ITERATION : 85, loss : 0.022766107229672716ITERATION : 86, loss : 0.022766107229672716ITERATION : 87, loss : 0.022766107229672716ITERATION : 88, loss : 0.022766107229672716ITERATION : 89, loss : 0.022766107229672716ITERATION : 90, loss : 0.022766107229672716ITERATION : 91, loss : 0.022766107229672716ITERATION : 92, loss : 0.022766107229672716ITERATION : 93, loss : 0.022766107229672716ITERATION : 94, loss : 0.022766107229672716ITERATION : 95, loss : 0.022766107229672716ITERATION : 96, loss : 0.022766107229672716ITERATION : 97, loss : 0.022766107229672716ITERATION : 98, loss : 0.022766107229672716ITERATION : 99, loss : 0.022766107229672716ITERATION : 100, loss : 0.022766107229672716
ITERATION : 1, loss : 0.03625159703730598ITERATION : 2, loss : 0.040551432660255245ITERATION : 3, loss : 0.047874858462609596ITERATION : 4, loss : 0.04889036646167144ITERATION : 5, loss : 0.044161676511114856ITERATION : 6, loss : 0.04114357769099843ITERATION : 7, loss : 0.03916168791097959ITERATION : 8, loss : 0.037834749799460256ITERATION : 9, loss : 0.036933931595618336ITERATION : 10, loss : 0.03631622721862811ITERATION : 11, loss : 0.03588956232698366ITERATION : 12, loss : 0.0355932917960733ITERATION : 13, loss : 0.03538677495931029ITERATION : 14, loss : 0.03524241858529174ITERATION : 15, loss : 0.035141307277488765ITERATION : 16, loss : 0.03507037982675394ITERATION : 17, loss : 0.03502057006467077ITERATION : 18, loss : 0.03498556107780634ITERATION : 19, loss : 0.03496093878033928ITERATION : 20, loss : 0.03494361268773736ITERATION : 21, loss : 0.034931415983740596ITERATION : 22, loss : 0.03492282728697201ITERATION : 23, loss : 0.034916777767587386ITERATION : 24, loss : 0.03491251523402001ITERATION : 25, loss : 0.03490951128527623ITERATION : 26, loss : 0.03490739435384128ITERATION : 27, loss : 0.034905901880524774ITERATION : 28, loss : 0.034904849453427714ITERATION : 29, loss : 0.034904107202112254ITERATION : 30, loss : 0.03490358365016964ITERATION : 31, loss : 0.034903214348521894ITERATION : 32, loss : 0.03490295375897799ITERATION : 33, loss : 0.034902769921308516ITERATION : 34, loss : 0.03490264015817915ITERATION : 35, loss : 0.03490254859872121ITERATION : 36, loss : 0.03490248401273223ITERATION : 37, loss : 0.03490243832335888ITERATION : 38, loss : 0.034902406056469125ITERATION : 39, loss : 0.03490238331590426ITERATION : 40, loss : 0.03490236724370606ITERATION : 41, loss : 0.0349023558980463ITERATION : 42, loss : 0.03490234788630835ITERATION : 43, loss : 0.034902342197625294ITERATION : 44, loss : 0.03490233820561965ITERATION : 45, loss : 0.034902335404193664ITERATION : 46, loss : 0.034902333409874274ITERATION : 47, loss : 0.03490233190150294ITERATION : 48, loss : 0.034902330956185966ITERATION : 49, loss : 0.0349023301880826ITERATION : 50, loss : 0.034902329790400886ITERATION : 51, loss : 0.034902329350253766ITERATION : 52, loss : 0.0349023291452985ITERATION : 53, loss : 0.034902328947272944ITERATION : 54, loss : 0.03490232891294193ITERATION : 55, loss : 0.034902328912286784ITERATION : 56, loss : 0.034902328912286784ITERATION : 57, loss : 0.034902328912286784ITERATION : 58, loss : 0.034902328912286784ITERATION : 59, loss : 0.034902328912286784ITERATION : 60, loss : 0.034902328912286784ITERATION : 61, loss : 0.034902328912286784ITERATION : 62, loss : 0.034902328912286784ITERATION : 63, loss : 0.034902328912286784ITERATION : 64, loss : 0.034902328912286784ITERATION : 65, loss : 0.034902328912286784ITERATION : 66, loss : 0.034902328912286784ITERATION : 67, loss : 0.034902328912286784ITERATION : 68, loss : 0.034902328912286784ITERATION : 69, loss : 0.034902328912286784ITERATION : 70, loss : 0.034902328912286784ITERATION : 71, loss : 0.034902328912286784ITERATION : 72, loss : 0.034902328912286784ITERATION : 73, loss : 0.034902328912286784ITERATION : 74, loss : 0.034902328912286784ITERATION : 75, loss : 0.034902328912286784ITERATION : 76, loss : 0.034902328912286784ITERATION : 77, loss : 0.034902328912286784ITERATION : 78, loss : 0.034902328912286784ITERATION : 79, loss : 0.034902328912286784ITERATION : 80, loss : 0.034902328912286784ITERATION : 81, loss : 0.034902328912286784ITERATION : 82, loss : 0.034902328912286784ITERATION : 83, loss : 0.034902328912286784ITERATION : 84, loss : 0.034902328912286784ITERATION : 85, loss : 0.034902328912286784ITERATION : 86, loss : 0.034902328912286784ITERATION : 87, loss : 0.034902328912286784ITERATION : 88, loss : 0.034902328912286784ITERATION : 89, loss : 0.034902328912286784ITERATION : 90, loss : 0.034902328912286784ITERATION : 91, loss : 0.034902328912286784ITERATION : 92, loss : 0.034902328912286784ITERATION : 93, loss : 0.034902328912286784ITERATION : 94, loss : 0.034902328912286784ITERATION : 95, loss : 0.034902328912286784ITERATION : 96, loss : 0.034902328912286784ITERATION : 97, loss : 0.034902328912286784ITERATION : 98, loss : 0.034902328912286784ITERATION : 99, loss : 0.034902328912286784ITERATION : 100, loss : 0.034902328912286784
ITERATION : 1, loss : 0.02244309455033416ITERATION : 2, loss : 0.023247877661171562ITERATION : 3, loss : 0.02271433472454027ITERATION : 4, loss : 0.022887817269387295ITERATION : 5, loss : 0.02324685631477824ITERATION : 6, loss : 0.02362081834138607ITERATION : 7, loss : 0.023950918233352308ITERATION : 8, loss : 0.02422201185869111ITERATION : 9, loss : 0.024436155527677753ITERATION : 10, loss : 0.024601384065535355ITERATION : 11, loss : 0.024726940765007012ITERATION : 12, loss : 0.024821366415582594ITERATION : 13, loss : 0.02489186652691093ITERATION : 14, loss : 0.02494423195141369ITERATION : 15, loss : 0.024982983447388273ITERATION : 16, loss : 0.025011583583304315ITERATION : 17, loss : 0.025032650763168563ITERATION : 18, loss : 0.025048147771518422ITERATION : 19, loss : 0.025059536025537763ITERATION : 20, loss : 0.02506789905410344ITERATION : 21, loss : 0.025074037552100824ITERATION : 22, loss : 0.02507854175000222ITERATION : 23, loss : 0.025081846125729126ITERATION : 24, loss : 0.02508426998143776ITERATION : 25, loss : 0.02508604785215578ITERATION : 26, loss : 0.02508735186511906ITERATION : 27, loss : 0.02508830836288229ITERATION : 28, loss : 0.02508900992238248ITERATION : 29, loss : 0.02508952454836937ITERATION : 30, loss : 0.025089902130537012ITERATION : 31, loss : 0.025090179121638228ITERATION : 32, loss : 0.025090382312394473ITERATION : 33, loss : 0.025090531437602807ITERATION : 34, loss : 0.025090640894762796ITERATION : 35, loss : 0.025090721192249674ITERATION : 36, loss : 0.025090780176499256ITERATION : 37, loss : 0.025090823444916647ITERATION : 38, loss : 0.025090855180392953ITERATION : 39, loss : 0.02509087852445424ITERATION : 40, loss : 0.025090895599395854ITERATION : 41, loss : 0.02509090821423913ITERATION : 42, loss : 0.02509091746554631ITERATION : 43, loss : 0.025090924204891275ITERATION : 44, loss : 0.02509092916102993ITERATION : 45, loss : 0.02509093279542313ITERATION : 46, loss : 0.02509093540158725ITERATION : 47, loss : 0.02509093732957668ITERATION : 48, loss : 0.025090938692789867ITERATION : 49, loss : 0.025090939764199873ITERATION : 50, loss : 0.025090940495524613ITERATION : 51, loss : 0.025090941020044125ITERATION : 52, loss : 0.025090941447322594ITERATION : 53, loss : 0.025090941723271337ITERATION : 54, loss : 0.02509094203117856ITERATION : 55, loss : 0.025090942156619056ITERATION : 56, loss : 0.025090942351453975ITERATION : 57, loss : 0.025090942367783992ITERATION : 58, loss : 0.02509094247699586ITERATION : 59, loss : 0.025090942482273497ITERATION : 60, loss : 0.025090942482273497ITERATION : 61, loss : 0.025090942482273497ITERATION : 62, loss : 0.025090942482273497ITERATION : 63, loss : 0.025090942482273497ITERATION : 64, loss : 0.025090942482273497ITERATION : 65, loss : 0.025090942482273497ITERATION : 66, loss : 0.025090942482273497ITERATION : 67, loss : 0.025090942482273497ITERATION : 68, loss : 0.025090942482273497ITERATION : 69, loss : 0.025090942482273497ITERATION : 70, loss : 0.025090942482273497ITERATION : 71, loss : 0.025090942482273497ITERATION : 72, loss : 0.025090942482273497ITERATION : 73, loss : 0.025090942482273497ITERATION : 74, loss : 0.025090942482273497ITERATION : 75, loss : 0.025090942482273497ITERATION : 76, loss : 0.025090942482273497ITERATION : 77, loss : 0.025090942482273497ITERATION : 78, loss : 0.025090942482273497ITERATION : 79, loss : 0.025090942482273497ITERATION : 80, loss : 0.025090942482273497ITERATION : 81, loss : 0.025090942482273497ITERATION : 82, loss : 0.025090942482273497ITERATION : 83, loss : 0.025090942482273497ITERATION : 84, loss : 0.025090942482273497ITERATION : 85, loss : 0.025090942482273497ITERATION : 86, loss : 0.025090942482273497ITERATION : 87, loss : 0.025090942482273497ITERATION : 88, loss : 0.025090942482273497ITERATION : 89, loss : 0.025090942482273497ITERATION : 90, loss : 0.025090942482273497ITERATION : 91, loss : 0.025090942482273497ITERATION : 92, loss : 0.025090942482273497ITERATION : 93, loss : 0.025090942482273497ITERATION : 94, loss : 0.025090942482273497ITERATION : 95, loss : 0.025090942482273497ITERATION : 96, loss : 0.025090942482273497ITERATION : 97, loss : 0.025090942482273497ITERATION : 98, loss : 0.025090942482273497ITERATION : 99, loss : 0.025090942482273497ITERATION : 100, loss : 0.025090942482273497
gradient norm in None layer : 0.0008389512788393091
gradient norm in None layer : 5.2890932821282115e-05
gradient norm in None layer : 5.726818961375585e-05
gradient norm in None layer : 0.0007179429599255167
gradient norm in None layer : 5.3530975146683295e-05
gradient norm in None layer : 6.330302176931796e-05
gradient norm in None layer : 0.00026244470571475706
gradient norm in None layer : 1.1017698522186504e-05
gradient norm in None layer : 1.1188048751612485e-05
gradient norm in None layer : 0.0002481956330388039
gradient norm in None layer : 9.603003525282542e-06
gradient norm in None layer : 1.0049823785810618e-05
gradient norm in None layer : 7.464671410555948e-05
gradient norm in None layer : 2.2435727860605516e-06
gradient norm in None layer : 1.8609323083001801e-06
gradient norm in None layer : 6.40811436227464e-05
gradient norm in None layer : 3.009507062419718e-06
gradient norm in None layer : 2.1961563545604344e-06
gradient norm in None layer : 9.209688009452769e-05
gradient norm in None layer : 7.67873794417757e-07
gradient norm in None layer : 0.00020512875593761104
gradient norm in None layer : 1.5332782449856678e-05
gradient norm in None layer : 1.0697603130798703e-05
gradient norm in None layer : 0.00024285649106283293
gradient norm in None layer : 2.3597586612429165e-05
gradient norm in None layer : 2.336920772045454e-05
gradient norm in None layer : 0.0003716969582100052
gradient norm in None layer : 2.48380886615814e-06
gradient norm in None layer : 0.0006423920654505333
gradient norm in None layer : 4.916674311746411e-05
gradient norm in None layer : 5.8159873687095615e-05
gradient norm in None layer : 0.0007747841048400546
gradient norm in None layer : 6.0773260557449866e-05
gradient norm in None layer : 7.187019376285295e-05
gradient norm in None layer : 5.408997727245343e-05
gradient norm in None layer : 8.724801130336438e-06
Total gradient norm: 0.001628688285814471
invariance loss : 4.495322751264775, avg_den : 0.42557525634765625, density loss : 0.32557525634765627, mse loss : 0.02626492919320227, solver time : 126.12400960922241 sec , total loss : 0.0310858272008147, running loss : 0.045414617023814385
Epoch 0/10 , batch 22/12500 
ITERATION : 1, loss : 0.05722356155422383ITERATION : 2, loss : 0.04130040290403438ITERATION : 3, loss : 0.03409925846493358ITERATION : 4, loss : 0.03097118574193604ITERATION : 5, loss : 0.029966809975147283ITERATION : 6, loss : 0.02993744803713423ITERATION : 7, loss : 0.029010357152475286ITERATION : 8, loss : 0.0280033656891366ITERATION : 9, loss : 0.027346814098477123ITERATION : 10, loss : 0.02691053981763933ITERATION : 11, loss : 0.026616552019344195ITERATION : 12, loss : 0.026416421797454646ITERATION : 13, loss : 0.02627918608122388ITERATION : 14, loss : 0.026184587781073913ITERATION : 15, loss : 0.026119138425045693ITERATION : 16, loss : 0.02607373766622982ITERATION : 17, loss : 0.02604218593554041ITERATION : 18, loss : 0.026020230149949836ITERATION : 19, loss : 0.026004937837015633ITERATION : 20, loss : 0.025994279726202718ITERATION : 21, loss : 0.025986848046156263ITERATION : 22, loss : 0.02598166441525053ITERATION : 23, loss : 0.025978047974475272ITERATION : 24, loss : 0.02597552447399355ITERATION : 25, loss : 0.025973763388819602ITERATION : 26, loss : 0.025972534286926332ITERATION : 27, loss : 0.025971676389280204ITERATION : 28, loss : 0.02597107759645382ITERATION : 29, loss : 0.025970659646517607ITERATION : 30, loss : 0.025970367873036415ITERATION : 31, loss : 0.025970164127385245ITERATION : 32, loss : 0.025970021894175046ITERATION : 33, loss : 0.025969922597153795ITERATION : 34, loss : 0.025969853369542065ITERATION : 35, loss : 0.02596980499550425ITERATION : 36, loss : 0.025969771288309414ITERATION : 37, loss : 0.025969747712576045ITERATION : 38, loss : 0.025969731300223068ITERATION : 39, loss : 0.025969719811255454ITERATION : 40, loss : 0.02596971184131502ITERATION : 41, loss : 0.025969706248105396ITERATION : 42, loss : 0.025969702379579535ITERATION : 43, loss : 0.025969699644234467ITERATION : 44, loss : 0.02596969777255894ITERATION : 45, loss : 0.025969696440201567ITERATION : 46, loss : 0.02596969554519818ITERATION : 47, loss : 0.02596969490914972ITERATION : 48, loss : 0.025969694458403508ITERATION : 49, loss : 0.02596969412669259ITERATION : 50, loss : 0.02596969391874474ITERATION : 51, loss : 0.025969693664426377ITERATION : 52, loss : 0.02596969360978511ITERATION : 53, loss : 0.02596969360751696ITERATION : 54, loss : 0.025969693555289018ITERATION : 55, loss : 0.025969693554965423ITERATION : 56, loss : 0.025969693554965423ITERATION : 57, loss : 0.025969693554965423ITERATION : 58, loss : 0.025969693554965423ITERATION : 59, loss : 0.025969693554965423ITERATION : 60, loss : 0.025969693554965423ITERATION : 61, loss : 0.025969693554965423ITERATION : 62, loss : 0.025969693554965423ITERATION : 63, loss : 0.025969693554965423ITERATION : 64, loss : 0.025969693554965423ITERATION : 65, loss : 0.025969693554965423ITERATION : 66, loss : 0.025969693554965423ITERATION : 67, loss : 0.025969693554965423ITERATION : 68, loss : 0.025969693554965423ITERATION : 69, loss : 0.025969693554965423ITERATION : 70, loss : 0.025969693554965423ITERATION : 71, loss : 0.025969693554965423ITERATION : 72, loss : 0.025969693554965423ITERATION : 73, loss : 0.025969693554965423ITERATION : 74, loss : 0.025969693554965423ITERATION : 75, loss : 0.025969693554965423ITERATION : 76, loss : 0.025969693554965423ITERATION : 77, loss : 0.025969693554965423ITERATION : 78, loss : 0.025969693554965423ITERATION : 79, loss : 0.025969693554965423ITERATION : 80, loss : 0.025969693554965423ITERATION : 81, loss : 0.025969693554965423ITERATION : 82, loss : 0.025969693554965423ITERATION : 83, loss : 0.025969693554965423ITERATION : 84, loss : 0.025969693554965423ITERATION : 85, loss : 0.025969693554965423ITERATION : 86, loss : 0.025969693554965423ITERATION : 87, loss : 0.025969693554965423ITERATION : 88, loss : 0.025969693554965423ITERATION : 89, loss : 0.025969693554965423ITERATION : 90, loss : 0.025969693554965423ITERATION : 91, loss : 0.025969693554965423ITERATION : 92, loss : 0.025969693554965423ITERATION : 93, loss : 0.025969693554965423ITERATION : 94, loss : 0.025969693554965423ITERATION : 95, loss : 0.025969693554965423ITERATION : 96, loss : 0.025969693554965423ITERATION : 97, loss : 0.025969693554965423ITERATION : 98, loss : 0.025969693554965423ITERATION : 99, loss : 0.025969693554965423ITERATION : 100, loss : 0.025969693554965423
ITERATION : 1, loss : 0.05883261271630217ITERATION : 2, loss : 0.07143405423318924ITERATION : 3, loss : 0.07462083877201514ITERATION : 4, loss : 0.0747578280433278ITERATION : 5, loss : 0.07632076300233435ITERATION : 6, loss : 0.07829503339972446ITERATION : 7, loss : 0.08022172959217422ITERATION : 8, loss : 0.08181293130115148ITERATION : 9, loss : 0.08154263194568578ITERATION : 10, loss : 0.08132671107914777ITERATION : 11, loss : 0.08115502716218696ITERATION : 12, loss : 0.0810189785117926ITERATION : 13, loss : 0.08070917936322082ITERATION : 14, loss : 0.08042221753327314ITERATION : 15, loss : 0.08021100043351775ITERATION : 16, loss : 0.0800552440033648ITERATION : 17, loss : 0.07994022612869849ITERATION : 18, loss : 0.07985520225450483ITERATION : 19, loss : 0.07979230051987196ITERATION : 20, loss : 0.07974573662994758ITERATION : 21, loss : 0.07971125085252713ITERATION : 22, loss : 0.07968570009851114ITERATION : 23, loss : 0.07966676393180695ITERATION : 24, loss : 0.07965272635321768ITERATION : 25, loss : 0.0796423182080052ITERATION : 26, loss : 0.07963459970722456ITERATION : 27, loss : 0.07962887487311512ITERATION : 28, loss : 0.0796246282828987ITERATION : 29, loss : 0.07962147761680237ITERATION : 30, loss : 0.07961914007612766ITERATION : 31, loss : 0.0796174055054845ITERATION : 32, loss : 0.07961611846211282ITERATION : 33, loss : 0.07961516326799854ITERATION : 34, loss : 0.07961445426135022ITERATION : 35, loss : 0.07961392800997948ITERATION : 36, loss : 0.07961353739033411ITERATION : 37, loss : 0.07961324739504409ITERATION : 38, loss : 0.079613032081614ITERATION : 39, loss : 0.07961287230006112ITERATION : 40, loss : 0.07961275364130711ITERATION : 41, loss : 0.07961266554847125ITERATION : 42, loss : 0.07961260017284101ITERATION : 43, loss : 0.07961255158732677ITERATION : 44, loss : 0.07961251547273218ITERATION : 45, loss : 0.07961248869701637ITERATION : 46, loss : 0.07961246881245643ITERATION : 47, loss : 0.07961245404000362ITERATION : 48, loss : 0.0796124431308423ITERATION : 49, loss : 0.07961243499459163ITERATION : 50, loss : 0.07961242911996025ITERATION : 51, loss : 0.07961242460446612ITERATION : 52, loss : 0.07961242117448317ITERATION : 53, loss : 0.07961241867465187ITERATION : 54, loss : 0.07961241680093034ITERATION : 55, loss : 0.07961241561503687ITERATION : 56, loss : 0.07961241467954797ITERATION : 57, loss : 0.07961241400047125ITERATION : 58, loss : 0.07961241343581177ITERATION : 59, loss : 0.07961241317258573ITERATION : 60, loss : 0.07961241303509595ITERATION : 61, loss : 0.07961241265255285ITERATION : 62, loss : 0.07961241264333939ITERATION : 63, loss : 0.07961241264333939ITERATION : 64, loss : 0.07961241264333939ITERATION : 65, loss : 0.07961241264333939ITERATION : 66, loss : 0.07961241264333939ITERATION : 67, loss : 0.07961241264333939ITERATION : 68, loss : 0.07961241264333939ITERATION : 69, loss : 0.07961241264333939ITERATION : 70, loss : 0.07961241264333939ITERATION : 71, loss : 0.07961241264333939ITERATION : 72, loss : 0.07961241264333939ITERATION : 73, loss : 0.07961241264333939ITERATION : 74, loss : 0.07961241264333939ITERATION : 75, loss : 0.07961241264333939ITERATION : 76, loss : 0.07961241264333939ITERATION : 77, loss : 0.07961241264333939ITERATION : 78, loss : 0.07961241264333939ITERATION : 79, loss : 0.07961241264333939ITERATION : 80, loss : 0.07961241264333939ITERATION : 81, loss : 0.07961241264333939ITERATION : 82, loss : 0.07961241264333939ITERATION : 83, loss : 0.07961241264333939ITERATION : 84, loss : 0.07961241264333939ITERATION : 85, loss : 0.07961241264333939ITERATION : 86, loss : 0.07961241264333939ITERATION : 87, loss : 0.07961241264333939ITERATION : 88, loss : 0.07961241264333939ITERATION : 89, loss : 0.07961241264333939ITERATION : 90, loss : 0.07961241264333939ITERATION : 91, loss : 0.07961241264333939ITERATION : 92, loss : 0.07961241264333939ITERATION : 93, loss : 0.07961241264333939ITERATION : 94, loss : 0.07961241264333939ITERATION : 95, loss : 0.07961241264333939ITERATION : 96, loss : 0.07961241264333939ITERATION : 97, loss : 0.07961241264333939ITERATION : 98, loss : 0.07961241264333939ITERATION : 99, loss : 0.07961241264333939ITERATION : 100, loss : 0.07961241264333939
ITERATION : 1, loss : 0.03382606525890023ITERATION : 2, loss : 0.027622029828781324ITERATION : 3, loss : 0.02535432770117347ITERATION : 4, loss : 0.02483800262609302ITERATION : 5, loss : 0.025005072216996924ITERATION : 6, loss : 0.025394780247607763ITERATION : 7, loss : 0.02581258373432372ITERATION : 8, loss : 0.02618394690768463ITERATION : 9, loss : 0.026487656334637885ITERATION : 10, loss : 0.02672495599450862ITERATION : 11, loss : 0.02690526875680763ITERATION : 12, loss : 0.02703981105771572ITERATION : 13, loss : 0.027138967164334903ITERATION : 14, loss : 0.02721141403605118ITERATION : 15, loss : 0.027264020333417443ITERATION : 16, loss : 0.0273020490357349ITERATION : 17, loss : 0.02732945029802496ITERATION : 18, loss : 0.02734914674634176ITERATION : 19, loss : 0.02736327990551709ITERATION : 20, loss : 0.027373407966644107ITERATION : 21, loss : 0.027380658966269598ITERATION : 22, loss : 0.027385846711327547ITERATION : 23, loss : 0.02738955637784585ITERATION : 24, loss : 0.02739220805996854ITERATION : 25, loss : 0.027394103181706087ITERATION : 26, loss : 0.027395457360803028ITERATION : 27, loss : 0.027396424898456114ITERATION : 28, loss : 0.027397116214029876ITERATION : 29, loss : 0.027397610051191946ITERATION : 30, loss : 0.027397962923325195ITERATION : 31, loss : 0.027398214992659372ITERATION : 32, loss : 0.027398395118100563ITERATION : 33, loss : 0.02739852386907333ITERATION : 34, loss : 0.027398615856486182ITERATION : 35, loss : 0.027398681627258278ITERATION : 36, loss : 0.02739872861142727ITERATION : 37, loss : 0.027398762218164188ITERATION : 38, loss : 0.02739878625148593ITERATION : 39, loss : 0.027398803321722454ITERATION : 40, loss : 0.02739881564855549ITERATION : 41, loss : 0.027398824431944097ITERATION : 42, loss : 0.027398830755393778ITERATION : 43, loss : 0.02739883527860249ITERATION : 44, loss : 0.027398838513922125ITERATION : 45, loss : 0.027398840796615462ITERATION : 46, loss : 0.027398842433444606ITERATION : 47, loss : 0.02739884357760206ITERATION : 48, loss : 0.027398844413017327ITERATION : 49, loss : 0.02739884501076968ITERATION : 50, loss : 0.02739884542445135ITERATION : 51, loss : 0.02739884573077283ITERATION : 52, loss : 0.02739884592665496ITERATION : 53, loss : 0.02739884608454773ITERATION : 54, loss : 0.02739884618037453ITERATION : 55, loss : 0.027398846268118466ITERATION : 56, loss : 0.02739884630593153ITERATION : 57, loss : 0.027398846348815682ITERATION : 58, loss : 0.027398846363966826ITERATION : 59, loss : 0.027398846387826813ITERATION : 60, loss : 0.027398846407572345ITERATION : 61, loss : 0.027398846408510816ITERATION : 62, loss : 0.027398846408510816ITERATION : 63, loss : 0.027398846408510816ITERATION : 64, loss : 0.027398846408510816ITERATION : 65, loss : 0.027398846408510816ITERATION : 66, loss : 0.027398846408510816ITERATION : 67, loss : 0.027398846408510816ITERATION : 68, loss : 0.027398846408510816ITERATION : 69, loss : 0.027398846408510816ITERATION : 70, loss : 0.027398846408510816ITERATION : 71, loss : 0.027398846408510816ITERATION : 72, loss : 0.027398846408510816ITERATION : 73, loss : 0.027398846408510816ITERATION : 74, loss : 0.027398846408510816ITERATION : 75, loss : 0.027398846408510816ITERATION : 76, loss : 0.027398846408510816ITERATION : 77, loss : 0.027398846408510816ITERATION : 78, loss : 0.027398846408510816ITERATION : 79, loss : 0.027398846408510816ITERATION : 80, loss : 0.027398846408510816ITERATION : 81, loss : 0.027398846408510816ITERATION : 82, loss : 0.027398846408510816ITERATION : 83, loss : 0.027398846408510816ITERATION : 84, loss : 0.027398846408510816ITERATION : 85, loss : 0.027398846408510816ITERATION : 86, loss : 0.027398846408510816ITERATION : 87, loss : 0.027398846408510816ITERATION : 88, loss : 0.027398846408510816ITERATION : 89, loss : 0.027398846408510816ITERATION : 90, loss : 0.027398846408510816ITERATION : 91, loss : 0.027398846408510816ITERATION : 92, loss : 0.027398846408510816ITERATION : 93, loss : 0.027398846408510816ITERATION : 94, loss : 0.027398846408510816ITERATION : 95, loss : 0.027398846408510816ITERATION : 96, loss : 0.027398846408510816ITERATION : 97, loss : 0.027398846408510816ITERATION : 98, loss : 0.027398846408510816ITERATION : 99, loss : 0.027398846408510816ITERATION : 100, loss : 0.027398846408510816
ITERATION : 1, loss : 0.011003878750903997ITERATION : 2, loss : 0.011618465122824034ITERATION : 3, loss : 0.011984939112787696ITERATION : 4, loss : 0.011753802640166797ITERATION : 5, loss : 0.011748883104523466ITERATION : 6, loss : 0.011875218854931058ITERATION : 7, loss : 0.01201723232816722ITERATION : 8, loss : 0.012127222337129682ITERATION : 9, loss : 0.01220920726764511ITERATION : 10, loss : 0.012268986247200124ITERATION : 11, loss : 0.012311989459087307ITERATION : 12, loss : 0.012342659547293322ITERATION : 13, loss : 0.01236441078814367ITERATION : 14, loss : 0.012379779011439222ITERATION : 15, loss : 0.012390609780524673ITERATION : 16, loss : 0.012398229822949572ITERATION : 17, loss : 0.012403584592447857ITERATION : 18, loss : 0.012407344414189215ITERATION : 19, loss : 0.012409982996108671ITERATION : 20, loss : 0.012411834001604527ITERATION : 21, loss : 0.012413132102482708ITERATION : 22, loss : 0.01241404235119205ITERATION : 23, loss : 0.012414680465255387ITERATION : 24, loss : 0.012415127844379347ITERATION : 25, loss : 0.012415441419887019ITERATION : 26, loss : 0.01241566126982754ITERATION : 27, loss : 0.012415815368007592ITERATION : 28, loss : 0.012415923369996727ITERATION : 29, loss : 0.012415999007614411ITERATION : 30, loss : 0.01241605207606874ITERATION : 31, loss : 0.012416089267627065ITERATION : 32, loss : 0.012416115386616125ITERATION : 33, loss : 0.0124161336577564ITERATION : 34, loss : 0.012416146523203352ITERATION : 35, loss : 0.012416155502122628ITERATION : 36, loss : 0.012416161856722155ITERATION : 37, loss : 0.012416166261602336ITERATION : 38, loss : 0.012416169376035598ITERATION : 39, loss : 0.012416171513915376ITERATION : 40, loss : 0.01241617308243906ITERATION : 41, loss : 0.012416174145453396ITERATION : 42, loss : 0.012416174924527098ITERATION : 43, loss : 0.012416175406808927ITERATION : 44, loss : 0.012416175815826781ITERATION : 45, loss : 0.012416176056321953ITERATION : 46, loss : 0.012416176249692297ITERATION : 47, loss : 0.01241617633436238ITERATION : 48, loss : 0.01241617643928249ITERATION : 49, loss : 0.012416176488486244ITERATION : 50, loss : 0.01241617653346122ITERATION : 51, loss : 0.012416176570619115ITERATION : 52, loss : 0.01241617656878384ITERATION : 53, loss : 0.01241617656878384ITERATION : 54, loss : 0.01241617656878384ITERATION : 55, loss : 0.01241617656878384ITERATION : 56, loss : 0.01241617656878384ITERATION : 57, loss : 0.01241617656878384ITERATION : 58, loss : 0.01241617656878384ITERATION : 59, loss : 0.01241617656878384ITERATION : 60, loss : 0.01241617656878384ITERATION : 61, loss : 0.01241617656878384ITERATION : 62, loss : 0.01241617656878384ITERATION : 63, loss : 0.01241617656878384ITERATION : 64, loss : 0.01241617656878384ITERATION : 65, loss : 0.01241617656878384ITERATION : 66, loss : 0.01241617656878384ITERATION : 67, loss : 0.01241617656878384ITERATION : 68, loss : 0.01241617656878384ITERATION : 69, loss : 0.01241617656878384ITERATION : 70, loss : 0.01241617656878384ITERATION : 71, loss : 0.01241617656878384ITERATION : 72, loss : 0.01241617656878384ITERATION : 73, loss : 0.01241617656878384ITERATION : 74, loss : 0.01241617656878384ITERATION : 75, loss : 0.01241617656878384ITERATION : 76, loss : 0.01241617656878384ITERATION : 77, loss : 0.01241617656878384ITERATION : 78, loss : 0.01241617656878384ITERATION : 79, loss : 0.01241617656878384ITERATION : 80, loss : 0.01241617656878384ITERATION : 81, loss : 0.01241617656878384ITERATION : 82, loss : 0.01241617656878384ITERATION : 83, loss : 0.01241617656878384ITERATION : 84, loss : 0.01241617656878384ITERATION : 85, loss : 0.01241617656878384ITERATION : 86, loss : 0.01241617656878384ITERATION : 87, loss : 0.01241617656878384ITERATION : 88, loss : 0.01241617656878384ITERATION : 89, loss : 0.01241617656878384ITERATION : 90, loss : 0.01241617656878384ITERATION : 91, loss : 0.01241617656878384ITERATION : 92, loss : 0.01241617656878384ITERATION : 93, loss : 0.01241617656878384ITERATION : 94, loss : 0.01241617656878384ITERATION : 95, loss : 0.01241617656878384ITERATION : 96, loss : 0.01241617656878384ITERATION : 97, loss : 0.01241617656878384ITERATION : 98, loss : 0.01241617656878384ITERATION : 99, loss : 0.01241617656878384ITERATION : 100, loss : 0.01241617656878384
ITERATION : 1, loss : 0.02382828833226084ITERATION : 2, loss : 0.02102817535847892ITERATION : 3, loss : 0.019380529443287248ITERATION : 4, loss : 0.018216714292557137ITERATION : 5, loss : 0.017382505079459595ITERATION : 6, loss : 0.01678910429987308ITERATION : 7, loss : 0.01636900252670059ITERATION : 8, loss : 0.016071800600252577ITERATION : 9, loss : 0.01586121363539437ITERATION : 10, loss : 0.01571163227946128ITERATION : 11, loss : 0.015605112669125334ITERATION : 12, loss : 0.015529085844557936ITERATION : 13, loss : 0.015474721306914825ITERATION : 14, loss : 0.015435789844875371ITERATION : 15, loss : 0.015407879352981926ITERATION : 16, loss : 0.01538785333897234ITERATION : 17, loss : 0.01537347578671726ITERATION : 18, loss : 0.015363149010418789ITERATION : 19, loss : 0.015355729381894825ITERATION : 20, loss : 0.015350397229670172ITERATION : 21, loss : 0.015346564668443837ITERATION : 22, loss : 0.015343809587337585ITERATION : 23, loss : 0.015341828909536233ITERATION : 24, loss : 0.01534040485766145ITERATION : 25, loss : 0.015339380962101043ITERATION : 26, loss : 0.015338644723565926ITERATION : 27, loss : 0.015338115335799478ITERATION : 28, loss : 0.015337734647384702ITERATION : 29, loss : 0.015337460900785325ITERATION : 30, loss : 0.01533726404051591ITERATION : 31, loss : 0.015337122471536183ITERATION : 32, loss : 0.015337020653025902ITERATION : 33, loss : 0.015336947427539169ITERATION : 34, loss : 0.015336894763797772ITERATION : 35, loss : 0.015336856879480594ITERATION : 36, loss : 0.015336829635710212ITERATION : 37, loss : 0.015336810041861064ITERATION : 38, loss : 0.015336795978833416ITERATION : 39, loss : 0.015336785825633447ITERATION : 40, loss : 0.015336778559196722ITERATION : 41, loss : 0.015336773322838731ITERATION : 42, loss : 0.015336769560304275ITERATION : 43, loss : 0.01533676685320155ITERATION : 44, loss : 0.015336764918507294ITERATION : 45, loss : 0.015336763524784865ITERATION : 46, loss : 0.015336762508284741ITERATION : 47, loss : 0.015336761742953465ITERATION : 48, loss : 0.015336761240329856ITERATION : 49, loss : 0.01533676086172585ITERATION : 50, loss : 0.01533676058527604ITERATION : 51, loss : 0.01533676040501556ITERATION : 52, loss : 0.015336760252996871ITERATION : 53, loss : 0.015336760182232546ITERATION : 54, loss : 0.01533676011359722ITERATION : 55, loss : 0.015336760092647896ITERATION : 56, loss : 0.015336760083018671ITERATION : 57, loss : 0.015336760073310017ITERATION : 58, loss : 0.015336760073310017ITERATION : 59, loss : 0.015336760073310017ITERATION : 60, loss : 0.015336760073310017ITERATION : 61, loss : 0.015336760073310017ITERATION : 62, loss : 0.015336760073310017ITERATION : 63, loss : 0.015336760073310017ITERATION : 64, loss : 0.015336760073310017ITERATION : 65, loss : 0.015336760073310017ITERATION : 66, loss : 0.015336760073310017ITERATION : 67, loss : 0.015336760073310017ITERATION : 68, loss : 0.015336760073310017ITERATION : 69, loss : 0.015336760073310017ITERATION : 70, loss : 0.015336760073310017ITERATION : 71, loss : 0.015336760073310017ITERATION : 72, loss : 0.015336760073310017ITERATION : 73, loss : 0.015336760073310017ITERATION : 74, loss : 0.015336760073310017ITERATION : 75, loss : 0.015336760073310017ITERATION : 76, loss : 0.015336760073310017ITERATION : 77, loss : 0.015336760073310017ITERATION : 78, loss : 0.015336760073310017ITERATION : 79, loss : 0.015336760073310017ITERATION : 80, loss : 0.015336760073310017ITERATION : 81, loss : 0.015336760073310017ITERATION : 82, loss : 0.015336760073310017ITERATION : 83, loss : 0.015336760073310017ITERATION : 84, loss : 0.015336760073310017ITERATION : 85, loss : 0.015336760073310017ITERATION : 86, loss : 0.015336760073310017ITERATION : 87, loss : 0.015336760073310017ITERATION : 88, loss : 0.015336760073310017ITERATION : 89, loss : 0.015336760073310017ITERATION : 90, loss : 0.015336760073310017ITERATION : 91, loss : 0.015336760073310017ITERATION : 92, loss : 0.015336760073310017ITERATION : 93, loss : 0.015336760073310017ITERATION : 94, loss : 0.015336760073310017ITERATION : 95, loss : 0.015336760073310017ITERATION : 96, loss : 0.015336760073310017ITERATION : 97, loss : 0.015336760073310017ITERATION : 98, loss : 0.015336760073310017ITERATION : 99, loss : 0.015336760073310017ITERATION : 100, loss : 0.015336760073310017
ITERATION : 1, loss : 0.02315416651156753ITERATION : 2, loss : 0.015788118172097913ITERATION : 3, loss : 0.014970043190572033ITERATION : 4, loss : 0.015563791748817458ITERATION : 5, loss : 0.016418197162307982ITERATION : 6, loss : 0.017221616270509527ITERATION : 7, loss : 0.01789635258034091ITERATION : 8, loss : 0.018437108976092482ITERATION : 9, loss : 0.01886042572759303ITERATION : 10, loss : 0.01918745098272281ITERATION : 11, loss : 0.019438056787926157ITERATION : 12, loss : 0.019629099052341167ITERATION : 13, loss : 0.019774217919797797ITERATION : 14, loss : 0.019884175771494365ITERATION : 15, loss : 0.019967338864986233ITERATION : 16, loss : 0.020030149556122256ITERATION : 17, loss : 0.020077538015864767ITERATION : 18, loss : 0.020113260813771464ITERATION : 19, loss : 0.020140171336660812ITERATION : 20, loss : 0.020160432203084968ITERATION : 21, loss : 0.020175679384924455ITERATION : 22, loss : 0.02018714903715111ITERATION : 23, loss : 0.020195774111828855ITERATION : 24, loss : 0.02020225815215981ITERATION : 25, loss : 0.020207131337155685ITERATION : 26, loss : 0.02021079303072865ITERATION : 27, loss : 0.02021354384651833ITERATION : 28, loss : 0.020215610024050562ITERATION : 29, loss : 0.0202171615982106ITERATION : 30, loss : 0.020218326605991762ITERATION : 31, loss : 0.02021920122690023ITERATION : 32, loss : 0.02021985772733111ITERATION : 33, loss : 0.020220350493453054ITERATION : 34, loss : 0.020220720258090477ITERATION : 35, loss : 0.020220997707138092ITERATION : 36, loss : 0.020221205903952696ITERATION : 37, loss : 0.020221362097132146ITERATION : 38, loss : 0.020221479262652595ITERATION : 39, loss : 0.020221567138467286ITERATION : 40, loss : 0.020221633058352183ITERATION : 41, loss : 0.020221682487637657ITERATION : 42, loss : 0.02022171956115931ITERATION : 43, loss : 0.02022174737482321ITERATION : 44, loss : 0.02022176821342797ITERATION : 45, loss : 0.020221783830976385ITERATION : 46, loss : 0.020221795490338316ITERATION : 47, loss : 0.02022180424369827ITERATION : 48, loss : 0.0202218107899014ITERATION : 49, loss : 0.020221815780541434ITERATION : 50, loss : 0.020221819496650435ITERATION : 51, loss : 0.020221822282915202ITERATION : 52, loss : 0.020221824429331ITERATION : 53, loss : 0.02022182596595307ITERATION : 54, loss : 0.020221827078700284ITERATION : 55, loss : 0.020221827936988266ITERATION : 56, loss : 0.02022182856014881ITERATION : 57, loss : 0.020221829074860247ITERATION : 58, loss : 0.020221829396854876ITERATION : 59, loss : 0.020221829687905275ITERATION : 60, loss : 0.020221829866557104ITERATION : 61, loss : 0.020221829995213745ITERATION : 62, loss : 0.02022183008072616ITERATION : 63, loss : 0.02022183015206804ITERATION : 64, loss : 0.02022183020295893ITERATION : 65, loss : 0.02022183023538785ITERATION : 66, loss : 0.020221830289842674ITERATION : 67, loss : 0.02022183031944249ITERATION : 68, loss : 0.020221830334780193ITERATION : 69, loss : 0.020221830336785023ITERATION : 70, loss : 0.020221830345505343ITERATION : 71, loss : 0.020221830345505343ITERATION : 72, loss : 0.020221830345505343ITERATION : 73, loss : 0.020221830345505343ITERATION : 74, loss : 0.020221830345505343ITERATION : 75, loss : 0.020221830345505343ITERATION : 76, loss : 0.020221830345505343ITERATION : 77, loss : 0.020221830345505343ITERATION : 78, loss : 0.020221830345505343ITERATION : 79, loss : 0.020221830345505343ITERATION : 80, loss : 0.020221830345505343ITERATION : 81, loss : 0.020221830345505343ITERATION : 82, loss : 0.020221830345505343ITERATION : 83, loss : 0.020221830345505343ITERATION : 84, loss : 0.020221830345505343ITERATION : 85, loss : 0.020221830345505343ITERATION : 86, loss : 0.020221830345505343ITERATION : 87, loss : 0.020221830345505343ITERATION : 88, loss : 0.020221830345505343ITERATION : 89, loss : 0.020221830345505343ITERATION : 90, loss : 0.020221830345505343ITERATION : 91, loss : 0.020221830345505343ITERATION : 92, loss : 0.020221830345505343ITERATION : 93, loss : 0.020221830345505343ITERATION : 94, loss : 0.020221830345505343ITERATION : 95, loss : 0.020221830345505343ITERATION : 96, loss : 0.020221830345505343ITERATION : 97, loss : 0.020221830345505343ITERATION : 98, loss : 0.020221830345505343ITERATION : 99, loss : 0.020221830345505343ITERATION : 100, loss : 0.020221830345505343
ITERATION : 1, loss : 0.03775401258237703ITERATION : 2, loss : 0.030271016892341022ITERATION : 3, loss : 0.028108705271634674ITERATION : 4, loss : 0.02757869211619646ITERATION : 5, loss : 0.02755338076234318ITERATION : 6, loss : 0.02767512732716208ITERATION : 7, loss : 0.02781557531613354ITERATION : 8, loss : 0.027934710806621143ITERATION : 9, loss : 0.02802516778803963ITERATION : 10, loss : 0.028090289478105004ITERATION : 11, loss : 0.028135840097017174ITERATION : 12, loss : 0.028167178065546628ITERATION : 13, loss : 0.028188527462509265ITERATION : 14, loss : 0.028202986445468408ITERATION : 15, loss : 0.02821274386938456ITERATION : 16, loss : 0.028219314615410825ITERATION : 17, loss : 0.028223733598830503ITERATION : 18, loss : 0.02822670348479055ITERATION : 19, loss : 0.028228698663804182ITERATION : 20, loss : 0.028230039032289236ITERATION : 21, loss : 0.028230939111249038ITERATION : 22, loss : 0.028231543756429063ITERATION : 23, loss : 0.028231950009053836ITERATION : 24, loss : 0.02823222288240846ITERATION : 25, loss : 0.02823240630320126ITERATION : 26, loss : 0.0282325295240676ITERATION : 27, loss : 0.02823261232080577ITERATION : 28, loss : 0.028232667992108388ITERATION : 29, loss : 0.02823270548266543ITERATION : 30, loss : 0.028232730606890133ITERATION : 31, loss : 0.028232747577996768ITERATION : 32, loss : 0.028232758997787925ITERATION : 33, loss : 0.028232766646939265ITERATION : 34, loss : 0.028232771838635377ITERATION : 35, loss : 0.02823277531535523ITERATION : 36, loss : 0.02823277764097869ITERATION : 37, loss : 0.028232779297619675ITERATION : 38, loss : 0.02823278026906566ITERATION : 39, loss : 0.028232781022274635ITERATION : 40, loss : 0.02823278147396264ITERATION : 41, loss : 0.02823278166543148ITERATION : 42, loss : 0.0282327819389684ITERATION : 43, loss : 0.02823278205353145ITERATION : 44, loss : 0.028232782106629314ITERATION : 45, loss : 0.028232782107718877ITERATION : 46, loss : 0.028232782107718877ITERATION : 47, loss : 0.028232782107718877ITERATION : 48, loss : 0.028232782107718877ITERATION : 49, loss : 0.028232782107718877ITERATION : 50, loss : 0.028232782107718877ITERATION : 51, loss : 0.028232782107718877ITERATION : 52, loss : 0.028232782107718877ITERATION : 53, loss : 0.028232782107718877ITERATION : 54, loss : 0.028232782107718877ITERATION : 55, loss : 0.028232782107718877ITERATION : 56, loss : 0.028232782107718877ITERATION : 57, loss : 0.028232782107718877ITERATION : 58, loss : 0.028232782107718877ITERATION : 59, loss : 0.028232782107718877ITERATION : 60, loss : 0.028232782107718877ITERATION : 61, loss : 0.028232782107718877ITERATION : 62, loss : 0.028232782107718877ITERATION : 63, loss : 0.028232782107718877ITERATION : 64, loss : 0.028232782107718877ITERATION : 65, loss : 0.028232782107718877ITERATION : 66, loss : 0.028232782107718877ITERATION : 67, loss : 0.028232782107718877ITERATION : 68, loss : 0.028232782107718877ITERATION : 69, loss : 0.028232782107718877ITERATION : 70, loss : 0.028232782107718877ITERATION : 71, loss : 0.028232782107718877ITERATION : 72, loss : 0.028232782107718877ITERATION : 73, loss : 0.028232782107718877ITERATION : 74, loss : 0.028232782107718877ITERATION : 75, loss : 0.028232782107718877ITERATION : 76, loss : 0.028232782107718877ITERATION : 77, loss : 0.028232782107718877ITERATION : 78, loss : 0.028232782107718877ITERATION : 79, loss : 0.028232782107718877ITERATION : 80, loss : 0.028232782107718877ITERATION : 81, loss : 0.028232782107718877ITERATION : 82, loss : 0.028232782107718877ITERATION : 83, loss : 0.028232782107718877ITERATION : 84, loss : 0.028232782107718877ITERATION : 85, loss : 0.028232782107718877ITERATION : 86, loss : 0.028232782107718877ITERATION : 87, loss : 0.028232782107718877ITERATION : 88, loss : 0.028232782107718877ITERATION : 89, loss : 0.028232782107718877ITERATION : 90, loss : 0.028232782107718877ITERATION : 91, loss : 0.028232782107718877ITERATION : 92, loss : 0.028232782107718877ITERATION : 93, loss : 0.028232782107718877ITERATION : 94, loss : 0.028232782107718877ITERATION : 95, loss : 0.028232782107718877ITERATION : 96, loss : 0.028232782107718877ITERATION : 97, loss : 0.028232782107718877ITERATION : 98, loss : 0.028232782107718877ITERATION : 99, loss : 0.028232782107718877ITERATION : 100, loss : 0.028232782107718877
ITERATION : 1, loss : 0.002557296412514366ITERATION : 2, loss : 0.0023859762656268687ITERATION : 3, loss : 0.002874396315453791ITERATION : 4, loss : 0.003225510361291556ITERATION : 5, loss : 0.0034888328261780525ITERATION : 6, loss : 0.0036876119019295765ITERATION : 7, loss : 0.003836035589256588ITERATION : 8, loss : 0.003945553276260353ITERATION : 9, loss : 0.004025609831286432ITERATION : 10, loss : 0.0040837323091987435ITERATION : 11, loss : 0.004125726504476586ITERATION : 12, loss : 0.004155965104113594ITERATION : 13, loss : 0.004177687620270992ITERATION : 14, loss : 0.004193267102673036ITERATION : 15, loss : 0.0042044283156802715ITERATION : 16, loss : 0.004212418236670778ITERATION : 17, loss : 0.004218135061239875ITERATION : 18, loss : 0.004222224143645099ITERATION : 19, loss : 0.004225148356225789ITERATION : 20, loss : 0.00422723928204598ITERATION : 21, loss : 0.004228734274363332ITERATION : 22, loss : 0.004229803194780655ITERATION : 23, loss : 0.00423056745872202ITERATION : 24, loss : 0.004231113921604925ITERATION : 25, loss : 0.00423150467891653ITERATION : 26, loss : 0.004231784083490809ITERATION : 27, loss : 0.004231983912443844ITERATION : 28, loss : 0.004232126824371464ITERATION : 29, loss : 0.004232229052722575ITERATION : 30, loss : 0.004232302169068577ITERATION : 31, loss : 0.004232354500797247ITERATION : 32, loss : 0.0042323919065238936ITERATION : 33, loss : 0.004232418701268734ITERATION : 34, loss : 0.004232437854952981ITERATION : 35, loss : 0.004232451582951587ITERATION : 36, loss : 0.004232461390615578ITERATION : 37, loss : 0.004232468404930566ITERATION : 38, loss : 0.004232473439864238ITERATION : 39, loss : 0.004232477048246415ITERATION : 40, loss : 0.004232479615106591ITERATION : 41, loss : 0.004232481456315943ITERATION : 42, loss : 0.004232482775827427ITERATION : 43, loss : 0.004232483706400933ITERATION : 44, loss : 0.00423248440381276ITERATION : 45, loss : 0.0042324848800244915ITERATION : 46, loss : 0.004232485213687663ITERATION : 47, loss : 0.004232485434888039ITERATION : 48, loss : 0.004232485602232726ITERATION : 49, loss : 0.0042324857227568ITERATION : 50, loss : 0.00423248582302247ITERATION : 51, loss : 0.004232485893560139ITERATION : 52, loss : 0.004232485938437039ITERATION : 53, loss : 0.004232485956046507ITERATION : 54, loss : 0.004232485965292533ITERATION : 55, loss : 0.0042324859910036075ITERATION : 56, loss : 0.004232485992771705ITERATION : 57, loss : 0.004232485993559932ITERATION : 58, loss : 0.004232485993559932ITERATION : 59, loss : 0.004232485993559932ITERATION : 60, loss : 0.004232485993559932ITERATION : 61, loss : 0.004232485993559932ITERATION : 62, loss : 0.004232485993559932ITERATION : 63, loss : 0.004232485993559932ITERATION : 64, loss : 0.004232485993559932ITERATION : 65, loss : 0.004232485993559932ITERATION : 66, loss : 0.004232485993559932ITERATION : 67, loss : 0.004232485993559932ITERATION : 68, loss : 0.004232485993559932ITERATION : 69, loss : 0.004232485993559932ITERATION : 70, loss : 0.004232485993559932ITERATION : 71, loss : 0.004232485993559932ITERATION : 72, loss : 0.004232485993559932ITERATION : 73, loss : 0.004232485993559932ITERATION : 74, loss : 0.004232485993559932ITERATION : 75, loss : 0.004232485993559932ITERATION : 76, loss : 0.004232485993559932ITERATION : 77, loss : 0.004232485993559932ITERATION : 78, loss : 0.004232485993559932ITERATION : 79, loss : 0.004232485993559932ITERATION : 80, loss : 0.004232485993559932ITERATION : 81, loss : 0.004232485993559932ITERATION : 82, loss : 0.004232485993559932ITERATION : 83, loss : 0.004232485993559932ITERATION : 84, loss : 0.004232485993559932ITERATION : 85, loss : 0.004232485993559932ITERATION : 86, loss : 0.004232485993559932ITERATION : 87, loss : 0.004232485993559932ITERATION : 88, loss : 0.004232485993559932ITERATION : 89, loss : 0.004232485993559932ITERATION : 90, loss : 0.004232485993559932ITERATION : 91, loss : 0.004232485993559932ITERATION : 92, loss : 0.004232485993559932ITERATION : 93, loss : 0.004232485993559932ITERATION : 94, loss : 0.004232485993559932ITERATION : 95, loss : 0.004232485993559932ITERATION : 96, loss : 0.004232485993559932ITERATION : 97, loss : 0.004232485993559932ITERATION : 98, loss : 0.004232485993559932ITERATION : 99, loss : 0.004232485993559932ITERATION : 100, loss : 0.004232485993559932
gradient norm in None layer : 0.0006855058351789539
gradient norm in None layer : 3.147280746877442e-05
gradient norm in None layer : 3.485139130188967e-05
gradient norm in None layer : 0.00045079960510225654
gradient norm in None layer : 4.282871830659161e-05
gradient norm in None layer : 5.0082266291920175e-05
gradient norm in None layer : 0.00022195465763837457
gradient norm in None layer : 1.2043436741916505e-05
gradient norm in None layer : 7.886698099505017e-06
gradient norm in None layer : 0.00023166427068278142
gradient norm in None layer : 1.0172219160772164e-05
gradient norm in None layer : 7.534242707256196e-06
gradient norm in None layer : 7.142704924059499e-05
gradient norm in None layer : 2.749516061308664e-06
gradient norm in None layer : 1.7189238404655598e-06
gradient norm in None layer : 7.048733614317871e-05
gradient norm in None layer : 3.7401466410334456e-06
gradient norm in None layer : 2.276028057053017e-06
gradient norm in None layer : 9.411952508455342e-05
gradient norm in None layer : 2.184499271414593e-06
gradient norm in None layer : 0.00019261323230848292
gradient norm in None layer : 1.563197485850573e-05
gradient norm in None layer : 1.0222516011526293e-05
gradient norm in None layer : 0.0002727184363196882
gradient norm in None layer : 2.2909528815104384e-05
gradient norm in None layer : 4.093123029891301e-05
gradient norm in None layer : 0.0003661669154099323
gradient norm in None layer : 4.042122270129801e-06
gradient norm in None layer : 0.0005551387842081287
gradient norm in None layer : 5.1078378516218946e-05
gradient norm in None layer : 6.550531837962433e-05
gradient norm in None layer : 0.0006639709089128641
gradient norm in None layer : 4.7220857324683907e-05
gradient norm in None layer : 6.757282902604941e-05
gradient norm in None layer : 4.339069089392813e-05
gradient norm in None layer : 4.177366635336868e-06
Total gradient norm: 0.001347108178990864
invariance loss : 4.200902797085567, avg_den : 0.44176483154296875, density loss : 0.34176483154296877, mse loss : 0.026677623461961703, solver time : 138.24127984046936 sec , total loss : 0.031220291090590237, running loss : 0.044769420390486016
Epoch 0/10 , batch 23/12500 
ITERATION : 1, loss : 0.036252847992360865ITERATION : 2, loss : 0.03485322488789538ITERATION : 3, loss : 0.035322514289931675ITERATION : 4, loss : 0.03585025310894391ITERATION : 5, loss : 0.03624180417368781ITERATION : 6, loss : 0.03651172010839074ITERATION : 7, loss : 0.036695943118979384ITERATION : 8, loss : 0.03682269774031771ITERATION : 9, loss : 0.03691103191218733ITERATION : 10, loss : 0.036973381340806195ITERATION : 11, loss : 0.03701788241070167ITERATION : 12, loss : 0.037049935753847664ITERATION : 13, loss : 0.0370731917163296ITERATION : 14, loss : 0.03709016110175416ITERATION : 15, loss : 0.0371025978676261ITERATION : 16, loss : 0.03711174369688536ITERATION : 17, loss : 0.03711848709715701ITERATION : 18, loss : 0.03712346920804652ITERATION : 19, loss : 0.03712715583952055ITERATION : 20, loss : 0.03712988717584284ITERATION : 21, loss : 0.0371319126587251ITERATION : 22, loss : 0.03713341569487322ITERATION : 23, loss : 0.03713453206600745ITERATION : 24, loss : 0.03713536141575674ITERATION : 25, loss : 0.03713597780827326ITERATION : 26, loss : 0.037136436076712874ITERATION : 27, loss : 0.03713677693179201ITERATION : 28, loss : 0.03713703055613869ITERATION : 29, loss : 0.037137219030169664ITERATION : 30, loss : 0.03713735941634359ITERATION : 31, loss : 0.03713746404528376ITERATION : 32, loss : 0.03713754189773178ITERATION : 33, loss : 0.03713759987981875ITERATION : 34, loss : 0.037137642994157526ITERATION : 35, loss : 0.03713767504553408ITERATION : 36, loss : 0.03713769892040219ITERATION : 37, loss : 0.03713771668775555ITERATION : 38, loss : 0.03713773003198331ITERATION : 39, loss : 0.03713773986451002ITERATION : 40, loss : 0.03713774720689084ITERATION : 41, loss : 0.037137752662350094ITERATION : 42, loss : 0.03713775672004912ITERATION : 43, loss : 0.037137759763151056ITERATION : 44, loss : 0.03713776202166692ITERATION : 45, loss : 0.03713776370189868ITERATION : 46, loss : 0.03713776491824721ITERATION : 47, loss : 0.037137765807944895ITERATION : 48, loss : 0.03713776640756786ITERATION : 49, loss : 0.03713776697218554ITERATION : 50, loss : 0.03713776729778589ITERATION : 51, loss : 0.0371377675703238ITERATION : 52, loss : 0.03713776778746085ITERATION : 53, loss : 0.037137767924497606ITERATION : 54, loss : 0.037137768037735115ITERATION : 55, loss : 0.0371377680920325ITERATION : 56, loss : 0.03713776821454637ITERATION : 57, loss : 0.037137768228491304ITERATION : 58, loss : 0.03713776825494969ITERATION : 59, loss : 0.03713776825494969ITERATION : 60, loss : 0.03713776825494969ITERATION : 61, loss : 0.03713776825494969ITERATION : 62, loss : 0.03713776825494969ITERATION : 63, loss : 0.03713776825494969ITERATION : 64, loss : 0.03713776825494969ITERATION : 65, loss : 0.03713776825494969ITERATION : 66, loss : 0.03713776825494969ITERATION : 67, loss : 0.03713776825494969ITERATION : 68, loss : 0.03713776825494969ITERATION : 69, loss : 0.03713776825494969ITERATION : 70, loss : 0.03713776825494969ITERATION : 71, loss : 0.03713776825494969ITERATION : 72, loss : 0.03713776825494969ITERATION : 73, loss : 0.03713776825494969ITERATION : 74, loss : 0.03713776825494969ITERATION : 75, loss : 0.03713776825494969ITERATION : 76, loss : 0.03713776825494969ITERATION : 77, loss : 0.03713776825494969ITERATION : 78, loss : 0.03713776825494969ITERATION : 79, loss : 0.03713776825494969ITERATION : 80, loss : 0.03713776825494969ITERATION : 81, loss : 0.03713776825494969ITERATION : 82, loss : 0.03713776825494969ITERATION : 83, loss : 0.03713776825494969ITERATION : 84, loss : 0.03713776825494969ITERATION : 85, loss : 0.03713776825494969ITERATION : 86, loss : 0.03713776825494969ITERATION : 87, loss : 0.03713776825494969ITERATION : 88, loss : 0.03713776825494969ITERATION : 89, loss : 0.03713776825494969ITERATION : 90, loss : 0.03713776825494969ITERATION : 91, loss : 0.03713776825494969ITERATION : 92, loss : 0.03713776825494969ITERATION : 93, loss : 0.03713776825494969ITERATION : 94, loss : 0.03713776825494969ITERATION : 95, loss : 0.03713776825494969ITERATION : 96, loss : 0.03713776825494969ITERATION : 97, loss : 0.03713776825494969ITERATION : 98, loss : 0.03713776825494969ITERATION : 99, loss : 0.03713776825494969ITERATION : 100, loss : 0.03713776825494969
ITERATION : 1, loss : 0.03439861877107417ITERATION : 2, loss : 0.030314286752645185ITERATION : 3, loss : 0.029525216068543043ITERATION : 4, loss : 0.029468681590982558ITERATION : 5, loss : 0.02962578963862256ITERATION : 6, loss : 0.029834057012318154ITERATION : 7, loss : 0.030033406065727086ITERATION : 8, loss : 0.03020354415368135ITERATION : 9, loss : 0.030340815050599868ITERATION : 10, loss : 0.030448103022657443ITERATION : 11, loss : 0.030530335654268877ITERATION : 12, loss : 0.030592569660461123ITERATION : 13, loss : 0.030639265710254306ITERATION : 14, loss : 0.03067409348506305ITERATION : 15, loss : 0.030699958164087587ITERATION : 16, loss : 0.030719106624074777ITERATION : 17, loss : 0.030733250295233577ITERATION : 18, loss : 0.030743679660582186ITERATION : 19, loss : 0.03075136041234086ITERATION : 20, loss : 0.030757011778532857ITERATION : 21, loss : 0.030761167219541287ITERATION : 22, loss : 0.030764221251233823ITERATION : 23, loss : 0.030766465028831583ITERATION : 24, loss : 0.03076811325487182ITERATION : 25, loss : 0.0307693238658387ITERATION : 26, loss : 0.03077021300447419ITERATION : 27, loss : 0.030770866028730533ITERATION : 28, loss : 0.030771345660932578ITERATION : 29, loss : 0.03077169810448427ITERATION : 30, loss : 0.03077195701577435ITERATION : 31, loss : 0.030772147301870532ITERATION : 32, loss : 0.03077228716279692ITERATION : 33, loss : 0.03077238993790894ITERATION : 34, loss : 0.03077246553783588ITERATION : 35, loss : 0.030772521239408723ITERATION : 36, loss : 0.03077256220391649ITERATION : 37, loss : 0.030772592388034436ITERATION : 38, loss : 0.030772614575664343ITERATION : 39, loss : 0.030772630949665054ITERATION : 40, loss : 0.030772642961660886ITERATION : 41, loss : 0.03077265186728986ITERATION : 42, loss : 0.030772658378090584ITERATION : 43, loss : 0.03077266321299237ITERATION : 44, loss : 0.030772666740647357ITERATION : 45, loss : 0.03077266941132998ITERATION : 46, loss : 0.03077267140040692ITERATION : 47, loss : 0.03077267284525907ITERATION : 48, loss : 0.030772673903467676ITERATION : 49, loss : 0.030772674702192415ITERATION : 50, loss : 0.03077267526153058ITERATION : 51, loss : 0.030772675671601583ITERATION : 52, loss : 0.030772675975746722ITERATION : 53, loss : 0.03077267619687825ITERATION : 54, loss : 0.0307726763771618ITERATION : 55, loss : 0.0307726764850369ITERATION : 56, loss : 0.03077267655800064ITERATION : 57, loss : 0.03077267661803467ITERATION : 58, loss : 0.030772676663695504ITERATION : 59, loss : 0.030772676716316463ITERATION : 60, loss : 0.030772676741949456ITERATION : 61, loss : 0.03077267675713248ITERATION : 62, loss : 0.03077267677451546ITERATION : 63, loss : 0.030772676778825617ITERATION : 64, loss : 0.030772676778825617ITERATION : 65, loss : 0.030772676778825617ITERATION : 66, loss : 0.030772676778825617ITERATION : 67, loss : 0.030772676778825617ITERATION : 68, loss : 0.030772676778825617ITERATION : 69, loss : 0.030772676778825617ITERATION : 70, loss : 0.030772676778825617ITERATION : 71, loss : 0.030772676778825617ITERATION : 72, loss : 0.030772676778825617ITERATION : 73, loss : 0.030772676778825617ITERATION : 74, loss : 0.030772676778825617ITERATION : 75, loss : 0.030772676778825617ITERATION : 76, loss : 0.030772676778825617ITERATION : 77, loss : 0.030772676778825617ITERATION : 78, loss : 0.030772676778825617ITERATION : 79, loss : 0.030772676778825617ITERATION : 80, loss : 0.030772676778825617ITERATION : 81, loss : 0.030772676778825617ITERATION : 82, loss : 0.030772676778825617ITERATION : 83, loss : 0.030772676778825617ITERATION : 84, loss : 0.030772676778825617ITERATION : 85, loss : 0.030772676778825617ITERATION : 86, loss : 0.030772676778825617ITERATION : 87, loss : 0.030772676778825617ITERATION : 88, loss : 0.030772676778825617ITERATION : 89, loss : 0.030772676778825617ITERATION : 90, loss : 0.030772676778825617ITERATION : 91, loss : 0.030772676778825617ITERATION : 92, loss : 0.030772676778825617ITERATION : 93, loss : 0.030772676778825617ITERATION : 94, loss : 0.030772676778825617ITERATION : 95, loss : 0.030772676778825617ITERATION : 96, loss : 0.030772676778825617ITERATION : 97, loss : 0.030772676778825617ITERATION : 98, loss : 0.030772676778825617ITERATION : 99, loss : 0.030772676778825617ITERATION : 100, loss : 0.030772676778825617
ITERATION : 1, loss : 0.02085422532474747ITERATION : 2, loss : 0.018551713446857156ITERATION : 3, loss : 0.01927295970119485ITERATION : 4, loss : 0.020803720700986385ITERATION : 5, loss : 0.02245603194694979ITERATION : 6, loss : 0.023938393517935603ITERATION : 7, loss : 0.025151057263111513ITERATION : 8, loss : 0.026089894424367682ITERATION : 9, loss : 0.02679130878788492ITERATION : 10, loss : 0.027302938171291802ITERATION : 11, loss : 0.02767003432297936ITERATION : 12, loss : 0.027930418280830316ITERATION : 13, loss : 0.028113624393663716ITERATION : 14, loss : 0.028241794021224502ITERATION : 15, loss : 0.028331098029374613ITERATION : 16, loss : 0.028393142813392926ITERATION : 17, loss : 0.02843616070369685ITERATION : 18, loss : 0.028465943150553875ITERATION : 19, loss : 0.028486541140501195ITERATION : 20, loss : 0.028500776649959897ITERATION : 21, loss : 0.028510610144865906ITERATION : 22, loss : 0.028517400354889682ITERATION : 23, loss : 0.028522088123919112ITERATION : 24, loss : 0.028525323971574645ITERATION : 25, loss : 0.02852755754940996ITERATION : 26, loss : 0.02852909884464408ITERATION : 27, loss : 0.028530162680040973ITERATION : 28, loss : 0.02853089711598699ITERATION : 29, loss : 0.02853140413589013ITERATION : 30, loss : 0.028531754042140582ITERATION : 31, loss : 0.028531995698386235ITERATION : 32, loss : 0.028532162594517085ITERATION : 33, loss : 0.02853227769413103ITERATION : 34, loss : 0.028532357334735307ITERATION : 35, loss : 0.028532412153282682ITERATION : 36, loss : 0.028532450121696477ITERATION : 37, loss : 0.02853247636013933ITERATION : 38, loss : 0.028532494546777795ITERATION : 39, loss : 0.02853250706841076ITERATION : 40, loss : 0.028532515789692042ITERATION : 41, loss : 0.028532521786906608ITERATION : 42, loss : 0.028532525877859673ITERATION : 43, loss : 0.028532528793617513ITERATION : 44, loss : 0.028532530773145097ITERATION : 45, loss : 0.028532532110235133ITERATION : 46, loss : 0.02853253311867049ITERATION : 47, loss : 0.02853253370265413ITERATION : 48, loss : 0.028532534160031212ITERATION : 49, loss : 0.028532534567927578ITERATION : 50, loss : 0.028532534673388992ITERATION : 51, loss : 0.028532534808550235ITERATION : 52, loss : 0.028532534808550235ITERATION : 53, loss : 0.028532534808550235ITERATION : 54, loss : 0.028532534808550235ITERATION : 55, loss : 0.028532534808550235ITERATION : 56, loss : 0.028532534808550235ITERATION : 57, loss : 0.028532534808550235ITERATION : 58, loss : 0.028532534808550235ITERATION : 59, loss : 0.028532534808550235ITERATION : 60, loss : 0.028532534808550235ITERATION : 61, loss : 0.028532534808550235ITERATION : 62, loss : 0.028532534808550235ITERATION : 63, loss : 0.028532534808550235ITERATION : 64, loss : 0.028532534808550235ITERATION : 65, loss : 0.028532534808550235ITERATION : 66, loss : 0.028532534808550235ITERATION : 67, loss : 0.028532534808550235ITERATION : 68, loss : 0.028532534808550235ITERATION : 69, loss : 0.028532534808550235ITERATION : 70, loss : 0.028532534808550235ITERATION : 71, loss : 0.028532534808550235ITERATION : 72, loss : 0.028532534808550235ITERATION : 73, loss : 0.028532534808550235ITERATION : 74, loss : 0.028532534808550235ITERATION : 75, loss : 0.028532534808550235ITERATION : 76, loss : 0.028532534808550235ITERATION : 77, loss : 0.028532534808550235ITERATION : 78, loss : 0.028532534808550235ITERATION : 79, loss : 0.028532534808550235ITERATION : 80, loss : 0.028532534808550235ITERATION : 81, loss : 0.028532534808550235ITERATION : 82, loss : 0.028532534808550235ITERATION : 83, loss : 0.028532534808550235ITERATION : 84, loss : 0.028532534808550235ITERATION : 85, loss : 0.028532534808550235ITERATION : 86, loss : 0.028532534808550235ITERATION : 87, loss : 0.028532534808550235ITERATION : 88, loss : 0.028532534808550235ITERATION : 89, loss : 0.028532534808550235ITERATION : 90, loss : 0.028532534808550235ITERATION : 91, loss : 0.028532534808550235ITERATION : 92, loss : 0.028532534808550235ITERATION : 93, loss : 0.028532534808550235ITERATION : 94, loss : 0.028532534808550235ITERATION : 95, loss : 0.028532534808550235ITERATION : 96, loss : 0.028532534808550235ITERATION : 97, loss : 0.028532534808550235ITERATION : 98, loss : 0.028532534808550235ITERATION : 99, loss : 0.028532534808550235ITERATION : 100, loss : 0.028532534808550235
ITERATION : 1, loss : 0.02978705798886908ITERATION : 2, loss : 0.029171227534053903ITERATION : 3, loss : 0.029248646111827276ITERATION : 4, loss : 0.029296722717772897ITERATION : 5, loss : 0.029297207229641832ITERATION : 6, loss : 0.029274123462571505ITERATION : 7, loss : 0.029243812803602567ITERATION : 8, loss : 0.029214393008439317ITERATION : 9, loss : 0.029189092840998746ITERATION : 10, loss : 0.02916867254573036ITERATION : 11, loss : 0.02915280063906466ITERATION : 12, loss : 0.029140758663192046ITERATION : 13, loss : 0.029131770127661973ITERATION : 14, loss : 0.029125136996970336ITERATION : 15, loss : 0.02912028213283545ITERATION : 16, loss : 0.029116750202345883ITERATION : 17, loss : 0.029114192444521955ITERATION : 18, loss : 0.0291123464523626ITERATION : 19, loss : 0.029111017728714175ITERATION : 20, loss : 0.029110063366684837ITERATION : 21, loss : 0.029109378880455877ITERATION : 22, loss : 0.029108888555609804ITERATION : 23, loss : 0.029108537729305457ITERATION : 24, loss : 0.02910828683238006ITERATION : 25, loss : 0.029108107703917693ITERATION : 26, loss : 0.029107979677611185ITERATION : 27, loss : 0.029107888352792547ITERATION : 28, loss : 0.02910782321921205ITERATION : 29, loss : 0.02910777674327762ITERATION : 30, loss : 0.029107743568523133ITERATION : 31, loss : 0.02910771988328491ITERATION : 32, loss : 0.02910770308677673ITERATION : 33, loss : 0.02910769100954031ITERATION : 34, loss : 0.02910768248299581ITERATION : 35, loss : 0.029107676404444123ITERATION : 36, loss : 0.029107672114495053ITERATION : 37, loss : 0.02910766906178074ITERATION : 38, loss : 0.029107666894079207ITERATION : 39, loss : 0.02910766536908493ITERATION : 40, loss : 0.029107664283497554ITERATION : 41, loss : 0.029107663529808363ITERATION : 42, loss : 0.029107662990102194ITERATION : 43, loss : 0.02910766264857755ITERATION : 44, loss : 0.02910766237684978ITERATION : 45, loss : 0.029107662163918177ITERATION : 46, loss : 0.029107662057565445ITERATION : 47, loss : 0.02910766193683231ITERATION : 48, loss : 0.02910766184379055ITERATION : 49, loss : 0.029107661843661682ITERATION : 50, loss : 0.029107661843661682ITERATION : 51, loss : 0.029107661843661682ITERATION : 52, loss : 0.029107661843661682ITERATION : 53, loss : 0.029107661843661682ITERATION : 54, loss : 0.029107661843661682ITERATION : 55, loss : 0.029107661843661682ITERATION : 56, loss : 0.029107661843661682ITERATION : 57, loss : 0.029107661843661682ITERATION : 58, loss : 0.029107661843661682ITERATION : 59, loss : 0.029107661843661682ITERATION : 60, loss : 0.029107661843661682ITERATION : 61, loss : 0.029107661843661682ITERATION : 62, loss : 0.029107661843661682ITERATION : 63, loss : 0.029107661843661682ITERATION : 64, loss : 0.029107661843661682ITERATION : 65, loss : 0.029107661843661682ITERATION : 66, loss : 0.029107661843661682ITERATION : 67, loss : 0.029107661843661682ITERATION : 68, loss : 0.029107661843661682ITERATION : 69, loss : 0.029107661843661682ITERATION : 70, loss : 0.029107661843661682ITERATION : 71, loss : 0.029107661843661682ITERATION : 72, loss : 0.029107661843661682ITERATION : 73, loss : 0.029107661843661682ITERATION : 74, loss : 0.029107661843661682ITERATION : 75, loss : 0.029107661843661682ITERATION : 76, loss : 0.029107661843661682ITERATION : 77, loss : 0.029107661843661682ITERATION : 78, loss : 0.029107661843661682ITERATION : 79, loss : 0.029107661843661682ITERATION : 80, loss : 0.029107661843661682ITERATION : 81, loss : 0.029107661843661682ITERATION : 82, loss : 0.029107661843661682ITERATION : 83, loss : 0.029107661843661682ITERATION : 84, loss : 0.029107661843661682ITERATION : 85, loss : 0.029107661843661682ITERATION : 86, loss : 0.029107661843661682ITERATION : 87, loss : 0.029107661843661682ITERATION : 88, loss : 0.029107661843661682ITERATION : 89, loss : 0.029107661843661682ITERATION : 90, loss : 0.029107661843661682ITERATION : 91, loss : 0.029107661843661682ITERATION : 92, loss : 0.029107661843661682ITERATION : 93, loss : 0.029107661843661682ITERATION : 94, loss : 0.029107661843661682ITERATION : 95, loss : 0.029107661843661682ITERATION : 96, loss : 0.029107661843661682ITERATION : 97, loss : 0.029107661843661682ITERATION : 98, loss : 0.029107661843661682ITERATION : 99, loss : 0.029107661843661682ITERATION : 100, loss : 0.029107661843661682
ITERATION : 1, loss : 0.03762502405782036ITERATION : 2, loss : 0.04005912514742998ITERATION : 3, loss : 0.04073996941644117ITERATION : 4, loss : 0.04092860975924715ITERATION : 5, loss : 0.04091377596213424ITERATION : 6, loss : 0.040845514865206524ITERATION : 7, loss : 0.04077806879172812ITERATION : 8, loss : 0.040725625296459386ITERATION : 9, loss : 0.0406885714935156ITERATION : 10, loss : 0.04066365003349702ITERATION : 11, loss : 0.04064738561518754ITERATION : 12, loss : 0.04063700237039486ITERATION : 13, loss : 0.04063050293376909ITERATION : 14, loss : 0.04062651963110432ITERATION : 15, loss : 0.04062414121323166ITERATION : 16, loss : 0.04062277078981492ITERATION : 17, loss : 0.04062202264539788ITERATION : 18, loss : 0.04062165021707226ITERATION : 19, loss : 0.04062149813532654ITERATION : 20, loss : 0.0406214697237381ITERATION : 21, loss : 0.040621505886777085ITERATION : 22, loss : 0.04062157156901605ITERATION : 23, loss : 0.040621646634019105ITERATION : 24, loss : 0.040621720105783854ITERATION : 25, loss : 0.04062178668530417ITERATION : 26, loss : 0.04062184422896846ITERATION : 27, loss : 0.0406218923556255ITERATION : 28, loss : 0.04062193176029378ITERATION : 29, loss : 0.04062196344835663ITERATION : 30, loss : 0.04062198863600454ITERATION : 31, loss : 0.040622008343889196ITERATION : 32, loss : 0.04062202376569445ITERATION : 33, loss : 0.04062203565358847ITERATION : 34, loss : 0.040622044811014744ITERATION : 35, loss : 0.04062205184070858ITERATION : 36, loss : 0.040622057153355755ITERATION : 37, loss : 0.04062206117999304ITERATION : 38, loss : 0.04062206424812616ITERATION : 39, loss : 0.04062206652276398ITERATION : 40, loss : 0.0406220682855063ITERATION : 41, loss : 0.04062206959136132ITERATION : 42, loss : 0.040622070608335586ITERATION : 43, loss : 0.040622071334888855ITERATION : 44, loss : 0.040622071915705375ITERATION : 45, loss : 0.04062207232341719ITERATION : 46, loss : 0.04062207263332681ITERATION : 47, loss : 0.04062207285339274ITERATION : 48, loss : 0.04062207305134001ITERATION : 49, loss : 0.04062207315813626ITERATION : 50, loss : 0.040622073251400354ITERATION : 51, loss : 0.04062207330935585ITERATION : 52, loss : 0.040622073381362166ITERATION : 53, loss : 0.040622073401101966ITERATION : 54, loss : 0.04062207340267488ITERATION : 55, loss : 0.04062207340267488ITERATION : 56, loss : 0.04062207340267488ITERATION : 57, loss : 0.04062207340267488ITERATION : 58, loss : 0.04062207340267488ITERATION : 59, loss : 0.04062207340267488ITERATION : 60, loss : 0.04062207340267488ITERATION : 61, loss : 0.04062207340267488ITERATION : 62, loss : 0.04062207340267488ITERATION : 63, loss : 0.04062207340267488ITERATION : 64, loss : 0.04062207340267488ITERATION : 65, loss : 0.04062207340267488ITERATION : 66, loss : 0.04062207340267488ITERATION : 67, loss : 0.04062207340267488ITERATION : 68, loss : 0.04062207340267488ITERATION : 69, loss : 0.04062207340267488ITERATION : 70, loss : 0.04062207340267488ITERATION : 71, loss : 0.04062207340267488ITERATION : 72, loss : 0.04062207340267488ITERATION : 73, loss : 0.04062207340267488ITERATION : 74, loss : 0.04062207340267488ITERATION : 75, loss : 0.04062207340267488ITERATION : 76, loss : 0.04062207340267488ITERATION : 77, loss : 0.04062207340267488ITERATION : 78, loss : 0.04062207340267488ITERATION : 79, loss : 0.04062207340267488ITERATION : 80, loss : 0.04062207340267488ITERATION : 81, loss : 0.04062207340267488ITERATION : 82, loss : 0.04062207340267488ITERATION : 83, loss : 0.04062207340267488ITERATION : 84, loss : 0.04062207340267488ITERATION : 85, loss : 0.04062207340267488ITERATION : 86, loss : 0.04062207340267488ITERATION : 87, loss : 0.04062207340267488ITERATION : 88, loss : 0.04062207340267488ITERATION : 89, loss : 0.04062207340267488ITERATION : 90, loss : 0.04062207340267488ITERATION : 91, loss : 0.04062207340267488ITERATION : 92, loss : 0.04062207340267488ITERATION : 93, loss : 0.04062207340267488ITERATION : 94, loss : 0.04062207340267488ITERATION : 95, loss : 0.04062207340267488ITERATION : 96, loss : 0.04062207340267488ITERATION : 97, loss : 0.04062207340267488ITERATION : 98, loss : 0.04062207340267488ITERATION : 99, loss : 0.04062207340267488ITERATION : 100, loss : 0.04062207340267488
ITERATION : 1, loss : 0.03584855772703913ITERATION : 2, loss : 0.029030181862696965ITERATION : 3, loss : 0.026758215822394425ITERATION : 4, loss : 0.025841822131259275ITERATION : 5, loss : 0.025412750466200344ITERATION : 6, loss : 0.0251743236422873ITERATION : 7, loss : 0.02501907386972084ITERATION : 8, loss : 0.02490663545595347ITERATION : 9, loss : 0.02482062313758665ITERATION : 10, loss : 0.024753316497456025ITERATION : 11, loss : 0.02470027885412208ITERATION : 12, loss : 0.02465847432776932ITERATION : 13, loss : 0.024625598636398485ITERATION : 14, loss : 0.02459982208232903ITERATION : 15, loss : 0.024579671895937284ITERATION : 16, loss : 0.02456396257489273ITERATION : 17, loss : 0.024551744321133627ITERATION : 18, loss : 0.02454226060274675ITERATION : 19, loss : 0.02453491204296932ITERATION : 20, loss : 0.02452922644943843ITERATION : 21, loss : 0.024524832822727216ITERATION : 22, loss : 0.024521441202609677ITERATION : 23, loss : 0.024518825430131317ITERATION : 24, loss : 0.02451680952631443ITERATION : 25, loss : 0.024515256977624317ITERATION : 26, loss : 0.024514061920257985ITERATION : 27, loss : 0.024513142451027717ITERATION : 28, loss : 0.024512435348709342ITERATION : 29, loss : 0.02451189177426504ITERATION : 30, loss : 0.02451147399403936ITERATION : 31, loss : 0.02451115300607692ITERATION : 32, loss : 0.024510906404376497ITERATION : 33, loss : 0.024510717054757943ITERATION : 34, loss : 0.024510571645211032ITERATION : 35, loss : 0.024510459973206607ITERATION : 36, loss : 0.0245103742225601ITERATION : 37, loss : 0.024510308436141447ITERATION : 38, loss : 0.02451025789658331ITERATION : 39, loss : 0.02451021913321798ITERATION : 40, loss : 0.024510189369631747ITERATION : 41, loss : 0.024510166548609817ITERATION : 42, loss : 0.024510149031981905ITERATION : 43, loss : 0.02451013563016213ITERATION : 44, loss : 0.024510125291032397ITERATION : 45, loss : 0.024510117378649592ITERATION : 46, loss : 0.02451011131051719ITERATION : 47, loss : 0.024510106739169556ITERATION : 48, loss : 0.02451010313409395ITERATION : 49, loss : 0.024510100313591698ITERATION : 50, loss : 0.02451009825098891ITERATION : 51, loss : 0.024510096640188227ITERATION : 52, loss : 0.02451009541206376ITERATION : 53, loss : 0.02451009448524946ITERATION : 54, loss : 0.024510093732947304ITERATION : 55, loss : 0.024510093205370564ITERATION : 56, loss : 0.024510092813558686ITERATION : 57, loss : 0.024510092528622057ITERATION : 58, loss : 0.024510092290392697ITERATION : 59, loss : 0.024510092140458444ITERATION : 60, loss : 0.024510092012573263ITERATION : 61, loss : 0.02451009188884342ITERATION : 62, loss : 0.024510091844072303ITERATION : 63, loss : 0.024510091811630375ITERATION : 64, loss : 0.024510091697497263ITERATION : 65, loss : 0.024510091692536984ITERATION : 66, loss : 0.024510091692536984ITERATION : 67, loss : 0.024510091692536984ITERATION : 68, loss : 0.024510091692536984ITERATION : 69, loss : 0.024510091692536984ITERATION : 70, loss : 0.024510091692536984ITERATION : 71, loss : 0.024510091692536984ITERATION : 72, loss : 0.024510091692536984ITERATION : 73, loss : 0.024510091692536984ITERATION : 74, loss : 0.024510091692536984ITERATION : 75, loss : 0.024510091692536984ITERATION : 76, loss : 0.024510091692536984ITERATION : 77, loss : 0.024510091692536984ITERATION : 78, loss : 0.024510091692536984ITERATION : 79, loss : 0.024510091692536984ITERATION : 80, loss : 0.024510091692536984ITERATION : 81, loss : 0.024510091692536984ITERATION : 82, loss : 0.024510091692536984ITERATION : 83, loss : 0.024510091692536984ITERATION : 84, loss : 0.024510091692536984ITERATION : 85, loss : 0.024510091692536984ITERATION : 86, loss : 0.024510091692536984ITERATION : 87, loss : 0.024510091692536984ITERATION : 88, loss : 0.024510091692536984ITERATION : 89, loss : 0.024510091692536984ITERATION : 90, loss : 0.024510091692536984ITERATION : 91, loss : 0.024510091692536984ITERATION : 92, loss : 0.024510091692536984ITERATION : 93, loss : 0.024510091692536984ITERATION : 94, loss : 0.024510091692536984ITERATION : 95, loss : 0.024510091692536984ITERATION : 96, loss : 0.024510091692536984ITERATION : 97, loss : 0.024510091692536984ITERATION : 98, loss : 0.024510091692536984ITERATION : 99, loss : 0.024510091692536984ITERATION : 100, loss : 0.024510091692536984
ITERATION : 1, loss : 0.03394001621493799ITERATION : 2, loss : 0.025924007306757674ITERATION : 3, loss : 0.023367048749809962ITERATION : 4, loss : 0.021971932446980553ITERATION : 5, loss : 0.0211571617823194ITERATION : 6, loss : 0.02066507410585193ITERATION : 7, loss : 0.02035976647375768ITERATION : 8, loss : 0.02016568985626213ITERATION : 9, loss : 0.02003971577922096ITERATION : 10, loss : 0.019956555407964205ITERATION : 11, loss : 0.019900941427326815ITERATION : 12, loss : 0.019863388780271173ITERATION : 13, loss : 0.019837852712497387ITERATION : 14, loss : 0.019820399490744066ITERATION : 15, loss : 0.01980842652042213ITERATION : 16, loss : 0.019800190724851237ITERATION : 17, loss : 0.019794514184634338ITERATION : 18, loss : 0.01979059550762486ITERATION : 19, loss : 0.01978788708169636ITERATION : 20, loss : 0.019786013188909577ITERATION : 21, loss : 0.019784715626355646ITERATION : 22, loss : 0.019783816391730234ITERATION : 23, loss : 0.019783192805526862ITERATION : 24, loss : 0.019782760068615693ITERATION : 25, loss : 0.019782459626175825ITERATION : 26, loss : 0.01978225088125478ITERATION : 27, loss : 0.0197821057543405ITERATION : 28, loss : 0.019782004796444517ITERATION : 29, loss : 0.019781934538229733ITERATION : 30, loss : 0.01978188561821398ITERATION : 31, loss : 0.019781851531622614ITERATION : 32, loss : 0.01978182778207123ITERATION : 33, loss : 0.01978181114898028ITERATION : 34, loss : 0.019781799587921485ITERATION : 35, loss : 0.019781791536017466ITERATION : 36, loss : 0.019781785922287732ITERATION : 37, loss : 0.019781781944100636ITERATION : 38, loss : 0.019781779206458433ITERATION : 39, loss : 0.0197817772870689ITERATION : 40, loss : 0.019781775934838628ITERATION : 41, loss : 0.019781774983180157ITERATION : 42, loss : 0.019781774324035634ITERATION : 43, loss : 0.01978177384826411ITERATION : 44, loss : 0.01978177348911083ITERATION : 45, loss : 0.019781773273373496ITERATION : 46, loss : 0.019781773112357955ITERATION : 47, loss : 0.019781772990118705ITERATION : 48, loss : 0.019781772940205902ITERATION : 49, loss : 0.019781772872665086ITERATION : 50, loss : 0.0197817728376156ITERATION : 51, loss : 0.01978177282355834ITERATION : 52, loss : 0.019781772816345897ITERATION : 53, loss : 0.019781772816345897ITERATION : 54, loss : 0.019781772816345897ITERATION : 55, loss : 0.019781772816345897ITERATION : 56, loss : 0.019781772816345897ITERATION : 57, loss : 0.019781772816345897ITERATION : 58, loss : 0.019781772816345897ITERATION : 59, loss : 0.019781772816345897ITERATION : 60, loss : 0.019781772816345897ITERATION : 61, loss : 0.019781772816345897ITERATION : 62, loss : 0.019781772816345897ITERATION : 63, loss : 0.019781772816345897ITERATION : 64, loss : 0.019781772816345897ITERATION : 65, loss : 0.019781772816345897ITERATION : 66, loss : 0.019781772816345897ITERATION : 67, loss : 0.019781772816345897ITERATION : 68, loss : 0.019781772816345897ITERATION : 69, loss : 0.019781772816345897ITERATION : 70, loss : 0.019781772816345897ITERATION : 71, loss : 0.019781772816345897ITERATION : 72, loss : 0.019781772816345897ITERATION : 73, loss : 0.019781772816345897ITERATION : 74, loss : 0.019781772816345897ITERATION : 75, loss : 0.019781772816345897ITERATION : 76, loss : 0.019781772816345897ITERATION : 77, loss : 0.019781772816345897ITERATION : 78, loss : 0.019781772816345897ITERATION : 79, loss : 0.019781772816345897ITERATION : 80, loss : 0.019781772816345897ITERATION : 81, loss : 0.019781772816345897ITERATION : 82, loss : 0.019781772816345897ITERATION : 83, loss : 0.019781772816345897ITERATION : 84, loss : 0.019781772816345897ITERATION : 85, loss : 0.019781772816345897ITERATION : 86, loss : 0.019781772816345897ITERATION : 87, loss : 0.019781772816345897ITERATION : 88, loss : 0.019781772816345897ITERATION : 89, loss : 0.019781772816345897ITERATION : 90, loss : 0.019781772816345897ITERATION : 91, loss : 0.019781772816345897ITERATION : 92, loss : 0.019781772816345897ITERATION : 93, loss : 0.019781772816345897ITERATION : 94, loss : 0.019781772816345897ITERATION : 95, loss : 0.019781772816345897ITERATION : 96, loss : 0.019781772816345897ITERATION : 97, loss : 0.019781772816345897ITERATION : 98, loss : 0.019781772816345897ITERATION : 99, loss : 0.019781772816345897ITERATION : 100, loss : 0.019781772816345897
ITERATION : 1, loss : 0.04454436901534354ITERATION : 2, loss : 0.034608354373477986ITERATION : 3, loss : 0.03167730176135208ITERATION : 4, loss : 0.028837390366838037ITERATION : 5, loss : 0.02640565924363629ITERATION : 6, loss : 0.024876540281981916ITERATION : 7, loss : 0.023892142688054046ITERATION : 8, loss : 0.02324782274629113ITERATION : 9, loss : 0.022821006325447476ITERATION : 10, loss : 0.02253578617191154ITERATION : 11, loss : 0.0223439621216678ITERATION : 12, loss : 0.022214340294952308ITERATION : 13, loss : 0.02212644133606349ITERATION : 14, loss : 0.02206667556466424ITERATION : 15, loss : 0.022025953421415313ITERATION : 16, loss : 0.02199815975309914ITERATION : 17, loss : 0.021979162740987566ITERATION : 18, loss : 0.021966161666313908ITERATION : 19, loss : 0.021957253603592606ITERATION : 20, loss : 0.021951143073672463ITERATION : 21, loss : 0.021946946792960477ITERATION : 22, loss : 0.021944061840841175ITERATION : 23, loss : 0.02194207618018828ITERATION : 24, loss : 0.02194070771789307ITERATION : 25, loss : 0.02193976351365333ITERATION : 26, loss : 0.02193911116917329ITERATION : 27, loss : 0.021938659797063213ITERATION : 28, loss : 0.021938347145247616ITERATION : 29, loss : 0.02193813011022533ITERATION : 30, loss : 0.021937979271391597ITERATION : 31, loss : 0.02193787430576128ITERATION : 32, loss : 0.021937801061141463ITERATION : 33, loss : 0.02193774989027646ITERATION : 34, loss : 0.021937714064678576ITERATION : 35, loss : 0.02193768902798562ITERATION : 36, loss : 0.021937671509365736ITERATION : 37, loss : 0.021937659113582034ITERATION : 38, loss : 0.021937650330404422ITERATION : 39, loss : 0.021937644124733854ITERATION : 40, loss : 0.021937639693820322ITERATION : 41, loss : 0.021937636624031368ITERATION : 42, loss : 0.021937634375060452ITERATION : 43, loss : 0.021937632837512665ITERATION : 44, loss : 0.02193763172397264ITERATION : 45, loss : 0.021937630945764595ITERATION : 46, loss : 0.021937630415028712ITERATION : 47, loss : 0.021937630009113286ITERATION : 48, loss : 0.021937629705394188ITERATION : 49, loss : 0.021937629496274223ITERATION : 50, loss : 0.021937629347983226ITERATION : 51, loss : 0.021937629264861987ITERATION : 52, loss : 0.02193762917840678ITERATION : 53, loss : 0.021937629142433846ITERATION : 54, loss : 0.02193762910918502ITERATION : 55, loss : 0.021937629083886783ITERATION : 56, loss : 0.021937629073225683ITERATION : 57, loss : 0.021937629067202417ITERATION : 58, loss : 0.021937629062825644ITERATION : 59, loss : 0.021937629056462626ITERATION : 60, loss : 0.021937629055483496ITERATION : 61, loss : 0.021937629055483496ITERATION : 62, loss : 0.021937629055483496ITERATION : 63, loss : 0.021937629055483496ITERATION : 64, loss : 0.021937629055483496ITERATION : 65, loss : 0.021937629055483496ITERATION : 66, loss : 0.021937629055483496ITERATION : 67, loss : 0.021937629055483496ITERATION : 68, loss : 0.021937629055483496ITERATION : 69, loss : 0.021937629055483496ITERATION : 70, loss : 0.021937629055483496ITERATION : 71, loss : 0.021937629055483496ITERATION : 72, loss : 0.021937629055483496ITERATION : 73, loss : 0.021937629055483496ITERATION : 74, loss : 0.021937629055483496ITERATION : 75, loss : 0.021937629055483496ITERATION : 76, loss : 0.021937629055483496ITERATION : 77, loss : 0.021937629055483496ITERATION : 78, loss : 0.021937629055483496ITERATION : 79, loss : 0.021937629055483496ITERATION : 80, loss : 0.021937629055483496ITERATION : 81, loss : 0.021937629055483496ITERATION : 82, loss : 0.021937629055483496ITERATION : 83, loss : 0.021937629055483496ITERATION : 84, loss : 0.021937629055483496ITERATION : 85, loss : 0.021937629055483496ITERATION : 86, loss : 0.021937629055483496ITERATION : 87, loss : 0.021937629055483496ITERATION : 88, loss : 0.021937629055483496ITERATION : 89, loss : 0.021937629055483496ITERATION : 90, loss : 0.021937629055483496ITERATION : 91, loss : 0.021937629055483496ITERATION : 92, loss : 0.021937629055483496ITERATION : 93, loss : 0.021937629055483496ITERATION : 94, loss : 0.021937629055483496ITERATION : 95, loss : 0.021937629055483496ITERATION : 96, loss : 0.021937629055483496ITERATION : 97, loss : 0.021937629055483496ITERATION : 98, loss : 0.021937629055483496ITERATION : 99, loss : 0.021937629055483496ITERATION : 100, loss : 0.021937629055483496
gradient norm in None layer : 0.0006673580799395113
gradient norm in None layer : 3.0099327935709025e-05
gradient norm in None layer : 3.434021481137483e-05
gradient norm in None layer : 0.0004358397146936121
gradient norm in None layer : 4.390303887537231e-05
gradient norm in None layer : 4.860456353502257e-05
gradient norm in None layer : 0.00015620133920760446
gradient norm in None layer : 7.362580467675779e-06
gradient norm in None layer : 6.298587283164239e-06
gradient norm in None layer : 0.00014358121699239296
gradient norm in None layer : 7.393770294274786e-06
gradient norm in None layer : 5.640114595320673e-06
gradient norm in None layer : 4.981393567170159e-05
gradient norm in None layer : 1.8298725772567033e-06
gradient norm in None layer : 1.4340980166678937e-06
gradient norm in None layer : 4.4430537734467915e-05
gradient norm in None layer : 2.295325413040661e-06
gradient norm in None layer : 1.5356636859624152e-06
gradient norm in None layer : 6.523058596618177e-05
gradient norm in None layer : 1.1885658711478408e-06
gradient norm in None layer : 0.00013019191435127434
gradient norm in None layer : 9.357397336922466e-06
gradient norm in None layer : 6.809657435328496e-06
gradient norm in None layer : 0.00016754438876395925
gradient norm in None layer : 1.854932701224793e-05
gradient norm in None layer : 2.4479667511205087e-05
gradient norm in None layer : 0.0002891641140516817
gradient norm in None layer : 4.021259312262653e-06
gradient norm in None layer : 0.0005587817958384742
gradient norm in None layer : 4.63446308884334e-05
gradient norm in None layer : 5.28651445032409e-05
gradient norm in None layer : 0.0006817736536011944
gradient norm in None layer : 6.12343889116903e-05
gradient norm in None layer : 7.941991003891501e-05
gradient norm in None layer : 5.4429986756970686e-05
gradient norm in None layer : 1.0810145664976964e-05
Total gradient norm: 0.0012729807191598178
invariance loss : 4.268296794589196, avg_den : 0.4147796630859375, density loss : 0.3147796630859375, mse loss : 0.029050276081628558, solver time : 148.71796250343323 sec , total loss : 0.03363335253930369, running loss : 0.04428524352739113
Epoch 0/10 , batch 24/12500 
ITERATION : 1, loss : 0.02842572426586885ITERATION : 2, loss : 0.028409003790913483ITERATION : 3, loss : 0.03072243053390919ITERATION : 4, loss : 0.03121125575425157ITERATION : 5, loss : 0.031561793408844206ITERATION : 6, loss : 0.03190704231272971ITERATION : 7, loss : 0.031984063169155176ITERATION : 8, loss : 0.03182401856262472ITERATION : 9, loss : 0.03172609607109194ITERATION : 10, loss : 0.031664611361770005ITERATION : 11, loss : 0.031625066069619664ITERATION : 12, loss : 0.03159907444727626ITERATION : 13, loss : 0.03158166556918424ITERATION : 14, loss : 0.031569818583964945ITERATION : 15, loss : 0.03156165120039065ITERATION : 16, loss : 0.03155596223105612ITERATION : 17, loss : 0.03155196760038263ITERATION : 18, loss : 0.0315491456707842ITERATION : 19, loss : 0.03154714273267063ITERATION : 20, loss : 0.03154571637202192ITERATION : 21, loss : 0.03154469813174714ITERATION : 22, loss : 0.031543969867223055ITERATION : 23, loss : 0.031543448341817805ITERATION : 24, loss : 0.03154307458260037ITERATION : 25, loss : 0.03154280656843095ITERATION : 26, loss : 0.031542614201009324ITERATION : 27, loss : 0.031542476161509014ITERATION : 28, loss : 0.03154237717232605ITERATION : 29, loss : 0.03154230611832306ITERATION : 30, loss : 0.031542255130433836ITERATION : 31, loss : 0.031542218550717434ITERATION : 32, loss : 0.03154219227998353ITERATION : 33, loss : 0.031542173455413484ITERATION : 34, loss : 0.03154215994296208ITERATION : 35, loss : 0.03154215025949685ITERATION : 36, loss : 0.03154214328314527ITERATION : 37, loss : 0.03154213829637946ITERATION : 38, loss : 0.03154213472026512ITERATION : 39, loss : 0.0315421321454612ITERATION : 40, loss : 0.03154213028206719ITERATION : 41, loss : 0.03154212895580682ITERATION : 42, loss : 0.03154212799332605ITERATION : 43, loss : 0.03154212735114954ITERATION : 44, loss : 0.031542126847599515ITERATION : 45, loss : 0.031542126521631596ITERATION : 46, loss : 0.03154212627079259ITERATION : 47, loss : 0.03154212608621521ITERATION : 48, loss : 0.03154212595728497ITERATION : 49, loss : 0.03154212582779236ITERATION : 50, loss : 0.031542125805261244ITERATION : 51, loss : 0.03154212575832049ITERATION : 52, loss : 0.031542125718689196ITERATION : 53, loss : 0.03154212570397956ITERATION : 54, loss : 0.03154212570397956ITERATION : 55, loss : 0.03154212570397956ITERATION : 56, loss : 0.03154212570397956ITERATION : 57, loss : 0.03154212570397956ITERATION : 58, loss : 0.03154212570397956ITERATION : 59, loss : 0.03154212570397956ITERATION : 60, loss : 0.03154212570397956ITERATION : 61, loss : 0.03154212570397956ITERATION : 62, loss : 0.03154212570397956ITERATION : 63, loss : 0.03154212570397956ITERATION : 64, loss : 0.03154212570397956ITERATION : 65, loss : 0.03154212570397956ITERATION : 66, loss : 0.03154212570397956ITERATION : 67, loss : 0.03154212570397956ITERATION : 68, loss : 0.03154212570397956ITERATION : 69, loss : 0.03154212570397956ITERATION : 70, loss : 0.03154212570397956ITERATION : 71, loss : 0.03154212570397956ITERATION : 72, loss : 0.03154212570397956ITERATION : 73, loss : 0.03154212570397956ITERATION : 74, loss : 0.03154212570397956ITERATION : 75, loss : 0.03154212570397956ITERATION : 76, loss : 0.03154212570397956ITERATION : 77, loss : 0.03154212570397956ITERATION : 78, loss : 0.03154212570397956ITERATION : 79, loss : 0.03154212570397956ITERATION : 80, loss : 0.03154212570397956ITERATION : 81, loss : 0.03154212570397956ITERATION : 82, loss : 0.03154212570397956ITERATION : 83, loss : 0.03154212570397956ITERATION : 84, loss : 0.03154212570397956ITERATION : 85, loss : 0.03154212570397956ITERATION : 86, loss : 0.03154212570397956ITERATION : 87, loss : 0.03154212570397956ITERATION : 88, loss : 0.03154212570397956ITERATION : 89, loss : 0.03154212570397956ITERATION : 90, loss : 0.03154212570397956ITERATION : 91, loss : 0.03154212570397956ITERATION : 92, loss : 0.03154212570397956ITERATION : 93, loss : 0.03154212570397956ITERATION : 94, loss : 0.03154212570397956ITERATION : 95, loss : 0.03154212570397956ITERATION : 96, loss : 0.03154212570397956ITERATION : 97, loss : 0.03154212570397956ITERATION : 98, loss : 0.03154212570397956ITERATION : 99, loss : 0.03154212570397956ITERATION : 100, loss : 0.03154212570397956
ITERATION : 1, loss : 0.04865611822287433ITERATION : 2, loss : 0.039247005408603264ITERATION : 3, loss : 0.03603095839537251ITERATION : 4, loss : 0.03457370434023762ITERATION : 5, loss : 0.033827872169219494ITERATION : 6, loss : 0.033311223587869934ITERATION : 7, loss : 0.032991338516869355ITERATION : 8, loss : 0.032784336214110234ITERATION : 9, loss : 0.03264558620143589ITERATION : 10, loss : 0.03254995743102398ITERATION : 11, loss : 0.03248259140398934ITERATION : 12, loss : 0.03243432289892011ITERATION : 13, loss : 0.0323992851722038ITERATION : 14, loss : 0.032373599909642815ITERATION : 15, loss : 0.03235463158753531ITERATION : 16, loss : 0.032340547134932215ITERATION : 17, loss : 0.03233004721687301ITERATION : 18, loss : 0.032322196840051644ITERATION : 19, loss : 0.032316315148622654ITERATION : 20, loss : 0.03231190191088068ITERATION : 21, loss : 0.03230858699050995ITERATION : 22, loss : 0.032306095277174256ITERATION : 23, loss : 0.032304221365362006ITERATION : 24, loss : 0.03230281167297693ITERATION : 25, loss : 0.0323017509670593ITERATION : 26, loss : 0.03230095276556371ITERATION : 27, loss : 0.03230035205909319ITERATION : 28, loss : 0.032299900009746974ITERATION : 29, loss : 0.03229955983001022ITERATION : 30, loss : 0.032299303825958754ITERATION : 31, loss : 0.032299111194655235ITERATION : 32, loss : 0.03229896624220627ITERATION : 33, loss : 0.032298857185220874ITERATION : 34, loss : 0.03229877510877894ITERATION : 35, loss : 0.03229871339067146ITERATION : 36, loss : 0.032298666960650986ITERATION : 37, loss : 0.03229863204422414ITERATION : 38, loss : 0.032298605771165834ITERATION : 39, loss : 0.03229858604754269ITERATION : 40, loss : 0.03229857120442887ITERATION : 41, loss : 0.03229856003741553ITERATION : 42, loss : 0.03229855163986889ITERATION : 43, loss : 0.03229854533806345ITERATION : 44, loss : 0.03229854060666491ITERATION : 45, loss : 0.03229853702613858ITERATION : 46, loss : 0.03229853437142851ITERATION : 47, loss : 0.03229853233615257ITERATION : 48, loss : 0.032298530838959026ITERATION : 49, loss : 0.03229852967856821ITERATION : 50, loss : 0.032298528833196706ITERATION : 51, loss : 0.03229852819061745ITERATION : 52, loss : 0.03229852767108199ITERATION : 53, loss : 0.03229852732486223ITERATION : 54, loss : 0.032298527062809064ITERATION : 55, loss : 0.032298526839518726ITERATION : 56, loss : 0.032298526703781395ITERATION : 57, loss : 0.03229852657235578ITERATION : 58, loss : 0.03229852652526314ITERATION : 59, loss : 0.03229852646719828ITERATION : 60, loss : 0.032298526437041716ITERATION : 61, loss : 0.032298526406319195ITERATION : 62, loss : 0.03229852638789466ITERATION : 63, loss : 0.03229852638777892ITERATION : 64, loss : 0.03229852638777892ITERATION : 65, loss : 0.03229852638777892ITERATION : 66, loss : 0.03229852638777892ITERATION : 67, loss : 0.03229852638777892ITERATION : 68, loss : 0.03229852638777892ITERATION : 69, loss : 0.03229852638777892ITERATION : 70, loss : 0.03229852638777892ITERATION : 71, loss : 0.03229852638777892ITERATION : 72, loss : 0.03229852638777892ITERATION : 73, loss : 0.03229852638777892ITERATION : 74, loss : 0.03229852638777892ITERATION : 75, loss : 0.03229852638777892ITERATION : 76, loss : 0.03229852638777892ITERATION : 77, loss : 0.03229852638777892ITERATION : 78, loss : 0.03229852638777892ITERATION : 79, loss : 0.03229852638777892ITERATION : 80, loss : 0.03229852638777892ITERATION : 81, loss : 0.03229852638777892ITERATION : 82, loss : 0.03229852638777892ITERATION : 83, loss : 0.03229852638777892ITERATION : 84, loss : 0.03229852638777892ITERATION : 85, loss : 0.03229852638777892ITERATION : 86, loss : 0.03229852638777892ITERATION : 87, loss : 0.03229852638777892ITERATION : 88, loss : 0.03229852638777892ITERATION : 89, loss : 0.03229852638777892ITERATION : 90, loss : 0.03229852638777892ITERATION : 91, loss : 0.03229852638777892ITERATION : 92, loss : 0.03229852638777892ITERATION : 93, loss : 0.03229852638777892ITERATION : 94, loss : 0.03229852638777892ITERATION : 95, loss : 0.03229852638777892ITERATION : 96, loss : 0.03229852638777892ITERATION : 97, loss : 0.03229852638777892ITERATION : 98, loss : 0.03229852638777892ITERATION : 99, loss : 0.03229852638777892ITERATION : 100, loss : 0.03229852638777892
ITERATION : 1, loss : 0.032284975214512214ITERATION : 2, loss : 0.03267930327805128ITERATION : 3, loss : 0.03082264413474031ITERATION : 4, loss : 0.02996740639151648ITERATION : 5, loss : 0.02976306188462985ITERATION : 6, loss : 0.02976326883059029ITERATION : 7, loss : 0.029816962412701046ITERATION : 8, loss : 0.0298745470415159ITERATION : 9, loss : 0.029922170671898767ITERATION : 10, loss : 0.02995805253153617ITERATION : 11, loss : 0.02998395827153729ITERATION : 12, loss : 0.030002261080670802ITERATION : 13, loss : 0.030015044294479088ITERATION : 14, loss : 0.03002391652942564ITERATION : 15, loss : 0.030030053061767738ITERATION : 16, loss : 0.030034289312837314ITERATION : 17, loss : 0.030037210815588442ITERATION : 18, loss : 0.030039224457808007ITERATION : 19, loss : 0.030040611976338158ITERATION : 20, loss : 0.0300415680662312ITERATION : 21, loss : 0.030042226842038115ITERATION : 22, loss : 0.03004268080609677ITERATION : 23, loss : 0.030042993648626056ITERATION : 24, loss : 0.03004320928323007ITERATION : 25, loss : 0.030043357933397172ITERATION : 26, loss : 0.030043460431816695ITERATION : 27, loss : 0.03004353107649841ITERATION : 28, loss : 0.03004357981009667ITERATION : 29, loss : 0.030043613439136724ITERATION : 30, loss : 0.030043636592459365ITERATION : 31, loss : 0.03004365260774669ITERATION : 32, loss : 0.030043663613854524ITERATION : 33, loss : 0.030043671259098712ITERATION : 34, loss : 0.03004367649109827ITERATION : 35, loss : 0.030043680150600226ITERATION : 36, loss : 0.030043682643295447ITERATION : 37, loss : 0.030043684408190146ITERATION : 38, loss : 0.0300436855590224ITERATION : 39, loss : 0.030043686402038132ITERATION : 40, loss : 0.030043686952947037ITERATION : 41, loss : 0.03004368738791198ITERATION : 42, loss : 0.030043687580542454ITERATION : 43, loss : 0.030043687821027353ITERATION : 44, loss : 0.03004368790831679ITERATION : 45, loss : 0.0300436880130311ITERATION : 46, loss : 0.030043688022737363ITERATION : 47, loss : 0.030043688025893293ITERATION : 48, loss : 0.030043688025893293ITERATION : 49, loss : 0.030043688025893293ITERATION : 50, loss : 0.030043688025893293ITERATION : 51, loss : 0.030043688025893293ITERATION : 52, loss : 0.030043688025893293ITERATION : 53, loss : 0.030043688025893293ITERATION : 54, loss : 0.030043688025893293ITERATION : 55, loss : 0.030043688025893293ITERATION : 56, loss : 0.030043688025893293ITERATION : 57, loss : 0.030043688025893293ITERATION : 58, loss : 0.030043688025893293ITERATION : 59, loss : 0.030043688025893293ITERATION : 60, loss : 0.030043688025893293ITERATION : 61, loss : 0.030043688025893293ITERATION : 62, loss : 0.030043688025893293ITERATION : 63, loss : 0.030043688025893293ITERATION : 64, loss : 0.030043688025893293ITERATION : 65, loss : 0.030043688025893293ITERATION : 66, loss : 0.030043688025893293ITERATION : 67, loss : 0.030043688025893293ITERATION : 68, loss : 0.030043688025893293ITERATION : 69, loss : 0.030043688025893293ITERATION : 70, loss : 0.030043688025893293ITERATION : 71, loss : 0.030043688025893293ITERATION : 72, loss : 0.030043688025893293ITERATION : 73, loss : 0.030043688025893293ITERATION : 74, loss : 0.030043688025893293ITERATION : 75, loss : 0.030043688025893293ITERATION : 76, loss : 0.030043688025893293ITERATION : 77, loss : 0.030043688025893293ITERATION : 78, loss : 0.030043688025893293ITERATION : 79, loss : 0.030043688025893293ITERATION : 80, loss : 0.030043688025893293ITERATION : 81, loss : 0.030043688025893293ITERATION : 82, loss : 0.030043688025893293ITERATION : 83, loss : 0.030043688025893293ITERATION : 84, loss : 0.030043688025893293ITERATION : 85, loss : 0.030043688025893293ITERATION : 86, loss : 0.030043688025893293ITERATION : 87, loss : 0.030043688025893293ITERATION : 88, loss : 0.030043688025893293ITERATION : 89, loss : 0.030043688025893293ITERATION : 90, loss : 0.030043688025893293ITERATION : 91, loss : 0.030043688025893293ITERATION : 92, loss : 0.030043688025893293ITERATION : 93, loss : 0.030043688025893293ITERATION : 94, loss : 0.030043688025893293ITERATION : 95, loss : 0.030043688025893293ITERATION : 96, loss : 0.030043688025893293ITERATION : 97, loss : 0.030043688025893293ITERATION : 98, loss : 0.030043688025893293ITERATION : 99, loss : 0.030043688025893293ITERATION : 100, loss : 0.030043688025893293
ITERATION : 1, loss : 0.029172777346265027ITERATION : 2, loss : 0.031064199903197368ITERATION : 3, loss : 0.03241238184273998ITERATION : 4, loss : 0.030419243796409593ITERATION : 5, loss : 0.02878715769305144ITERATION : 6, loss : 0.027764546457998383ITERATION : 7, loss : 0.02708892844693279ITERATION : 8, loss : 0.02662780522304235ITERATION : 9, loss : 0.026306432735490944ITERATION : 10, loss : 0.026079330138345812ITERATION : 11, loss : 0.025917324644863834ITERATION : 12, loss : 0.025800998674196853ITERATION : 13, loss : 0.025717086305437932ITERATION : 14, loss : 0.025656355155275156ITERATION : 15, loss : 0.025612295932347373ITERATION : 16, loss : 0.025580275474646263ITERATION : 17, loss : 0.02555697378570821ITERATION : 18, loss : 0.025539999938957832ITERATION : 19, loss : 0.02552762621823709ITERATION : 20, loss : 0.02551860058941178ITERATION : 21, loss : 0.0255120141490252ITERATION : 22, loss : 0.025507205805172243ITERATION : 23, loss : 0.025503694472472218ITERATION : 24, loss : 0.025501129720989786ITERATION : 25, loss : 0.02549925580847906ITERATION : 26, loss : 0.025497886469209325ITERATION : 27, loss : 0.025496885728307005ITERATION : 28, loss : 0.025496154189561665ITERATION : 29, loss : 0.025495619381280303ITERATION : 30, loss : 0.02549522838736735ITERATION : 31, loss : 0.02549494250350419ITERATION : 32, loss : 0.025494733300470664ITERATION : 33, loss : 0.02549458030283161ITERATION : 34, loss : 0.025494468343491755ITERATION : 35, loss : 0.025494386437926134ITERATION : 36, loss : 0.025494326572868903ITERATION : 37, loss : 0.025494282691127787ITERATION : 38, loss : 0.02549425059748587ITERATION : 39, loss : 0.02549422711925524ITERATION : 40, loss : 0.02549420994890106ITERATION : 41, loss : 0.02549419741611707ITERATION : 42, loss : 0.02549418818137844ITERATION : 43, loss : 0.025494181500878493ITERATION : 44, loss : 0.02549417659323483ITERATION : 45, loss : 0.02549417304192205ITERATION : 46, loss : 0.02549417040858954ITERATION : 47, loss : 0.02549416853518319ITERATION : 48, loss : 0.025494167117280763ITERATION : 49, loss : 0.025494166160030144ITERATION : 50, loss : 0.025494165319203555ITERATION : 51, loss : 0.025494164717541904ITERATION : 52, loss : 0.025494164318311294ITERATION : 53, loss : 0.025494164119476054ITERATION : 54, loss : 0.025494163921494728ITERATION : 55, loss : 0.02549416373137604ITERATION : 56, loss : 0.025494163683254526ITERATION : 57, loss : 0.025494163558415373ITERATION : 58, loss : 0.025494163556748422ITERATION : 59, loss : 0.025494163556748422ITERATION : 60, loss : 0.025494163556748422ITERATION : 61, loss : 0.025494163556748422ITERATION : 62, loss : 0.025494163556748422ITERATION : 63, loss : 0.025494163556748422ITERATION : 64, loss : 0.025494163556748422ITERATION : 65, loss : 0.025494163556748422ITERATION : 66, loss : 0.025494163556748422ITERATION : 67, loss : 0.025494163556748422ITERATION : 68, loss : 0.025494163556748422ITERATION : 69, loss : 0.025494163556748422ITERATION : 70, loss : 0.025494163556748422ITERATION : 71, loss : 0.025494163556748422ITERATION : 72, loss : 0.025494163556748422ITERATION : 73, loss : 0.025494163556748422ITERATION : 74, loss : 0.025494163556748422ITERATION : 75, loss : 0.025494163556748422ITERATION : 76, loss : 0.025494163556748422ITERATION : 77, loss : 0.025494163556748422ITERATION : 78, loss : 0.025494163556748422ITERATION : 79, loss : 0.025494163556748422ITERATION : 80, loss : 0.025494163556748422ITERATION : 81, loss : 0.025494163556748422ITERATION : 82, loss : 0.025494163556748422ITERATION : 83, loss : 0.025494163556748422ITERATION : 84, loss : 0.025494163556748422ITERATION : 85, loss : 0.025494163556748422ITERATION : 86, loss : 0.025494163556748422ITERATION : 87, loss : 0.025494163556748422ITERATION : 88, loss : 0.025494163556748422ITERATION : 89, loss : 0.025494163556748422ITERATION : 90, loss : 0.025494163556748422ITERATION : 91, loss : 0.025494163556748422ITERATION : 92, loss : 0.025494163556748422ITERATION : 93, loss : 0.025494163556748422ITERATION : 94, loss : 0.025494163556748422ITERATION : 95, loss : 0.025494163556748422ITERATION : 96, loss : 0.025494163556748422ITERATION : 97, loss : 0.025494163556748422ITERATION : 98, loss : 0.025494163556748422ITERATION : 99, loss : 0.025494163556748422ITERATION : 100, loss : 0.025494163556748422
ITERATION : 1, loss : 0.03260004030674562ITERATION : 2, loss : 0.03277625621724871ITERATION : 3, loss : 0.035943744865095724ITERATION : 4, loss : 0.040907885316076927ITERATION : 5, loss : 0.044856903816736536ITERATION : 6, loss : 0.047611984159842406ITERATION : 7, loss : 0.04943917622782481ITERATION : 8, loss : 0.050622164578180216ITERATION : 9, loss : 0.05137637598920859ITERATION : 10, loss : 0.05185368662067029ITERATION : 11, loss : 0.052154803769379095ITERATION : 12, loss : 0.05234455662461743ITERATION : 13, loss : 0.052464097444121345ITERATION : 14, loss : 0.05253939103088577ITERATION : 15, loss : 0.05258679013369017ITERATION : 16, loss : 0.0526165961126493ITERATION : 17, loss : 0.05263530345481909ITERATION : 18, loss : 0.052647012116685585ITERATION : 19, loss : 0.05265431182780192ITERATION : 20, loss : 0.05265883919651449ITERATION : 21, loss : 0.052661627957517415ITERATION : 22, loss : 0.05266333076131896ITERATION : 23, loss : 0.052664358581541386ITERATION : 24, loss : 0.05266496942780202ITERATION : 25, loss : 0.05266532495156062ITERATION : 26, loss : 0.05266552583894258ITERATION : 27, loss : 0.05266563450037248ITERATION : 28, loss : 0.05266568906077112ITERATION : 29, loss : 0.0526657127581635ITERATION : 30, loss : 0.05266571958754219ITERATION : 31, loss : 0.05266571776645578ITERATION : 32, loss : 0.05266571201368733ITERATION : 33, loss : 0.05266570493552114ITERATION : 34, loss : 0.0526656979161907ITERATION : 35, loss : 0.05266569165418791ITERATION : 36, loss : 0.05266568609027298ITERATION : 37, loss : 0.05266568160770909ITERATION : 38, loss : 0.05266567812694712ITERATION : 39, loss : 0.052665675290252914ITERATION : 40, loss : 0.05266567301681249ITERATION : 41, loss : 0.052665671202183836ITERATION : 42, loss : 0.05266566984974584ITERATION : 43, loss : 0.05266566882338587ITERATION : 44, loss : 0.052665667950368464ITERATION : 45, loss : 0.052665667299982774ITERATION : 46, loss : 0.052665666895465896ITERATION : 47, loss : 0.05266566652227684ITERATION : 48, loss : 0.05266566628960835ITERATION : 49, loss : 0.052665666087554734ITERATION : 50, loss : 0.052665665999336ITERATION : 51, loss : 0.05266566589241569ITERATION : 52, loss : 0.052665665882158805ITERATION : 53, loss : 0.052665665882158805ITERATION : 54, loss : 0.052665665882158805ITERATION : 55, loss : 0.052665665882158805ITERATION : 56, loss : 0.052665665882158805ITERATION : 57, loss : 0.052665665882158805ITERATION : 58, loss : 0.052665665882158805ITERATION : 59, loss : 0.052665665882158805ITERATION : 60, loss : 0.052665665882158805ITERATION : 61, loss : 0.052665665882158805ITERATION : 62, loss : 0.052665665882158805ITERATION : 63, loss : 0.052665665882158805ITERATION : 64, loss : 0.052665665882158805ITERATION : 65, loss : 0.052665665882158805ITERATION : 66, loss : 0.052665665882158805ITERATION : 67, loss : 0.052665665882158805ITERATION : 68, loss : 0.052665665882158805ITERATION : 69, loss : 0.052665665882158805ITERATION : 70, loss : 0.052665665882158805ITERATION : 71, loss : 0.052665665882158805ITERATION : 72, loss : 0.052665665882158805ITERATION : 73, loss : 0.052665665882158805ITERATION : 74, loss : 0.052665665882158805ITERATION : 75, loss : 0.052665665882158805ITERATION : 76, loss : 0.052665665882158805ITERATION : 77, loss : 0.052665665882158805ITERATION : 78, loss : 0.052665665882158805ITERATION : 79, loss : 0.052665665882158805ITERATION : 80, loss : 0.052665665882158805ITERATION : 81, loss : 0.052665665882158805ITERATION : 82, loss : 0.052665665882158805ITERATION : 83, loss : 0.052665665882158805ITERATION : 84, loss : 0.052665665882158805ITERATION : 85, loss : 0.052665665882158805ITERATION : 86, loss : 0.052665665882158805ITERATION : 87, loss : 0.052665665882158805ITERATION : 88, loss : 0.052665665882158805ITERATION : 89, loss : 0.052665665882158805ITERATION : 90, loss : 0.052665665882158805ITERATION : 91, loss : 0.052665665882158805ITERATION : 92, loss : 0.052665665882158805ITERATION : 93, loss : 0.052665665882158805ITERATION : 94, loss : 0.052665665882158805ITERATION : 95, loss : 0.052665665882158805ITERATION : 96, loss : 0.052665665882158805ITERATION : 97, loss : 0.052665665882158805ITERATION : 98, loss : 0.052665665882158805ITERATION : 99, loss : 0.052665665882158805ITERATION : 100, loss : 0.052665665882158805
ITERATION : 1, loss : 0.011276692051113148ITERATION : 2, loss : 0.011710761789735085ITERATION : 3, loss : 0.011881339158479644ITERATION : 4, loss : 0.011983009954426709ITERATION : 5, loss : 0.012056320081320984ITERATION : 6, loss : 0.012113073280610752ITERATION : 7, loss : 0.012157271775725823ITERATION : 8, loss : 0.012191191614728796ITERATION : 9, loss : 0.012216771459394133ITERATION : 10, loss : 0.012235770362027035ITERATION : 11, loss : 0.012249712352672314ITERATION : 12, loss : 0.012259849838657058ITERATION : 13, loss : 0.01226717060872984ITERATION : 14, loss : 0.01227243053965758ITERATION : 15, loss : 0.012276195760953469ITERATION : 16, loss : 0.012278883749272428ITERATION : 17, loss : 0.012280798913357456ITERATION : 18, loss : 0.012282161566537806ITERATION : 19, loss : 0.012283130135744285ITERATION : 20, loss : 0.012283818045773024ITERATION : 21, loss : 0.01228430639591542ITERATION : 22, loss : 0.012284652908998422ITERATION : 23, loss : 0.0122848987701051ITERATION : 24, loss : 0.012285073178725486ITERATION : 25, loss : 0.012285196930641182ITERATION : 26, loss : 0.012285284690703247ITERATION : 27, loss : 0.01228534690419136ITERATION : 28, loss : 0.012285390976547449ITERATION : 29, loss : 0.012285422283448916ITERATION : 30, loss : 0.012285444495326609ITERATION : 31, loss : 0.012285460248299322ITERATION : 32, loss : 0.012285471451052554ITERATION : 33, loss : 0.012285479362287329ITERATION : 34, loss : 0.01228548501687565ITERATION : 35, loss : 0.012285489010288147ITERATION : 36, loss : 0.012285491837803763ITERATION : 37, loss : 0.012285493827963175ITERATION : 38, loss : 0.012285495240088502ITERATION : 39, loss : 0.01228549623615755ITERATION : 40, loss : 0.012285496935611106ITERATION : 41, loss : 0.012285497435191778ITERATION : 42, loss : 0.012285497787128044ITERATION : 43, loss : 0.012285498036285472ITERATION : 44, loss : 0.0122854982166156ITERATION : 45, loss : 0.012285498333074466ITERATION : 46, loss : 0.012285498422414427ITERATION : 47, loss : 0.01228549848086734ITERATION : 48, loss : 0.012285498522540716ITERATION : 49, loss : 0.012285498556405912ITERATION : 50, loss : 0.012285498577212695ITERATION : 51, loss : 0.012285498584963158ITERATION : 52, loss : 0.012285498601115226ITERATION : 53, loss : 0.012285498609076394ITERATION : 54, loss : 0.012285498608987776ITERATION : 55, loss : 0.012285498608987776ITERATION : 56, loss : 0.012285498608987776ITERATION : 57, loss : 0.012285498608987776ITERATION : 58, loss : 0.012285498608987776ITERATION : 59, loss : 0.012285498608987776ITERATION : 60, loss : 0.012285498608987776ITERATION : 61, loss : 0.012285498608987776ITERATION : 62, loss : 0.012285498608987776ITERATION : 63, loss : 0.012285498608987776ITERATION : 64, loss : 0.012285498608987776ITERATION : 65, loss : 0.012285498608987776ITERATION : 66, loss : 0.012285498608987776ITERATION : 67, loss : 0.012285498608987776ITERATION : 68, loss : 0.012285498608987776ITERATION : 69, loss : 0.012285498608987776ITERATION : 70, loss : 0.012285498608987776ITERATION : 71, loss : 0.012285498608987776ITERATION : 72, loss : 0.012285498608987776ITERATION : 73, loss : 0.012285498608987776ITERATION : 74, loss : 0.012285498608987776ITERATION : 75, loss : 0.012285498608987776ITERATION : 76, loss : 0.012285498608987776ITERATION : 77, loss : 0.012285498608987776ITERATION : 78, loss : 0.012285498608987776ITERATION : 79, loss : 0.012285498608987776ITERATION : 80, loss : 0.012285498608987776ITERATION : 81, loss : 0.012285498608987776ITERATION : 82, loss : 0.012285498608987776ITERATION : 83, loss : 0.012285498608987776ITERATION : 84, loss : 0.012285498608987776ITERATION : 85, loss : 0.012285498608987776ITERATION : 86, loss : 0.012285498608987776ITERATION : 87, loss : 0.012285498608987776ITERATION : 88, loss : 0.012285498608987776ITERATION : 89, loss : 0.012285498608987776ITERATION : 90, loss : 0.012285498608987776ITERATION : 91, loss : 0.012285498608987776ITERATION : 92, loss : 0.012285498608987776ITERATION : 93, loss : 0.012285498608987776ITERATION : 94, loss : 0.012285498608987776ITERATION : 95, loss : 0.012285498608987776ITERATION : 96, loss : 0.012285498608987776ITERATION : 97, loss : 0.012285498608987776ITERATION : 98, loss : 0.012285498608987776ITERATION : 99, loss : 0.012285498608987776ITERATION : 100, loss : 0.012285498608987776
ITERATION : 1, loss : 0.04224992102762346ITERATION : 2, loss : 0.05129126050161527ITERATION : 3, loss : 0.05108079660417788ITERATION : 4, loss : 0.050277801996646894ITERATION : 5, loss : 0.050306451795991323ITERATION : 6, loss : 0.05058778310365492ITERATION : 7, loss : 0.05091435358652788ITERATION : 8, loss : 0.051211983498119ITERATION : 9, loss : 0.05145868514419117ITERATION : 10, loss : 0.05165331972244354ITERATION : 11, loss : 0.05180240969596892ITERATION : 12, loss : 0.05191445050468792ITERATION : 13, loss : 0.05199755983017866ITERATION : 14, loss : 0.052058647094975444ITERATION : 15, loss : 0.0521032524436584ITERATION : 16, loss : 0.05213566635970656ITERATION : 17, loss : 0.05215913714153642ITERATION : 18, loss : 0.052176087131042985ITERATION : 19, loss : 0.05218830393346968ITERATION : 20, loss : 0.052197095387459924ITERATION : 21, loss : 0.05220341505956781ITERATION : 22, loss : 0.05220795373027209ITERATION : 23, loss : 0.05221121146615591ITERATION : 24, loss : 0.052213548559075826ITERATION : 25, loss : 0.05221522451185676ITERATION : 26, loss : 0.052216426208342444ITERATION : 27, loss : 0.05221728733995994ITERATION : 28, loss : 0.05221790433825508ITERATION : 29, loss : 0.05221834651149413ITERATION : 30, loss : 0.05221866328093777ITERATION : 31, loss : 0.052218890211418095ITERATION : 32, loss : 0.05221905276309534ITERATION : 33, loss : 0.05221916922034578ITERATION : 34, loss : 0.05221925265114029ITERATION : 35, loss : 0.05221931233426455ITERATION : 36, loss : 0.052219355230391847ITERATION : 37, loss : 0.05221938579650612ITERATION : 38, loss : 0.052219407851283275ITERATION : 39, loss : 0.05221942356234908ITERATION : 40, loss : 0.052219434819248206ITERATION : 41, loss : 0.05221944272535978ITERATION : 42, loss : 0.052219448547146385ITERATION : 43, loss : 0.05221945279294623ITERATION : 44, loss : 0.05221945583629625ITERATION : 45, loss : 0.052219457761368195ITERATION : 46, loss : 0.05221945922614389ITERATION : 47, loss : 0.052219460286635315ITERATION : 48, loss : 0.052219461162713886ITERATION : 49, loss : 0.05221946157950378ITERATION : 50, loss : 0.05221946197049064ITERATION : 51, loss : 0.05221946220660463ITERATION : 52, loss : 0.0522194622345777ITERATION : 53, loss : 0.0522194622345777ITERATION : 54, loss : 0.0522194622345777ITERATION : 55, loss : 0.0522194622345777ITERATION : 56, loss : 0.0522194622345777ITERATION : 57, loss : 0.0522194622345777ITERATION : 58, loss : 0.0522194622345777ITERATION : 59, loss : 0.0522194622345777ITERATION : 60, loss : 0.0522194622345777ITERATION : 61, loss : 0.0522194622345777ITERATION : 62, loss : 0.0522194622345777ITERATION : 63, loss : 0.0522194622345777ITERATION : 64, loss : 0.0522194622345777ITERATION : 65, loss : 0.0522194622345777ITERATION : 66, loss : 0.0522194622345777ITERATION : 67, loss : 0.0522194622345777ITERATION : 68, loss : 0.0522194622345777ITERATION : 69, loss : 0.0522194622345777ITERATION : 70, loss : 0.0522194622345777ITERATION : 71, loss : 0.0522194622345777ITERATION : 72, loss : 0.0522194622345777ITERATION : 73, loss : 0.0522194622345777ITERATION : 74, loss : 0.0522194622345777ITERATION : 75, loss : 0.0522194622345777ITERATION : 76, loss : 0.0522194622345777ITERATION : 77, loss : 0.0522194622345777ITERATION : 78, loss : 0.0522194622345777ITERATION : 79, loss : 0.0522194622345777ITERATION : 80, loss : 0.0522194622345777ITERATION : 81, loss : 0.0522194622345777ITERATION : 82, loss : 0.0522194622345777ITERATION : 83, loss : 0.0522194622345777ITERATION : 84, loss : 0.0522194622345777ITERATION : 85, loss : 0.0522194622345777ITERATION : 86, loss : 0.0522194622345777ITERATION : 87, loss : 0.0522194622345777ITERATION : 88, loss : 0.0522194622345777ITERATION : 89, loss : 0.0522194622345777ITERATION : 90, loss : 0.0522194622345777ITERATION : 91, loss : 0.0522194622345777ITERATION : 92, loss : 0.0522194622345777ITERATION : 93, loss : 0.0522194622345777ITERATION : 94, loss : 0.0522194622345777ITERATION : 95, loss : 0.0522194622345777ITERATION : 96, loss : 0.0522194622345777ITERATION : 97, loss : 0.0522194622345777ITERATION : 98, loss : 0.0522194622345777ITERATION : 99, loss : 0.0522194622345777ITERATION : 100, loss : 0.0522194622345777
ITERATION : 1, loss : 0.024236792167340326ITERATION : 2, loss : 0.025829305948114247ITERATION : 3, loss : 0.026338077618163713ITERATION : 4, loss : 0.026951010041288772ITERATION : 5, loss : 0.027478521008980922ITERATION : 6, loss : 0.027893860523507657ITERATION : 7, loss : 0.028209961817971127ITERATION : 8, loss : 0.02840513998173751ITERATION : 9, loss : 0.02831257795467382ITERATION : 10, loss : 0.028246653121925116ITERATION : 11, loss : 0.028200024159441065ITERATION : 12, loss : 0.028167177107791125ITERATION : 13, loss : 0.028144100467014262ITERATION : 14, loss : 0.028127920135900212ITERATION : 15, loss : 0.02811659283449372ITERATION : 16, loss : 0.02810867390868563ITERATION : 17, loss : 0.02810314382677012ITERATION : 18, loss : 0.028099286306465625ITERATION : 19, loss : 0.028096598070864228ITERATION : 20, loss : 0.028094726448959845ITERATION : 21, loss : 0.02809342435799329ITERATION : 22, loss : 0.028092519346456127ITERATION : 23, loss : 0.028091890682631352ITERATION : 24, loss : 0.02809145440189055ITERATION : 25, loss : 0.028091151849612278ITERATION : 26, loss : 0.028090942215629734ITERATION : 27, loss : 0.02809079694957056ITERATION : 28, loss : 0.028090696528392387ITERATION : 29, loss : 0.02809062708240584ITERATION : 30, loss : 0.02809057904666985ITERATION : 31, loss : 0.028090545830680054ITERATION : 32, loss : 0.028090523043200304ITERATION : 33, loss : 0.028090507209972228ITERATION : 34, loss : 0.02809049637583744ITERATION : 35, loss : 0.028090488861339365ITERATION : 36, loss : 0.028090483691246636ITERATION : 37, loss : 0.0280904802095846ITERATION : 38, loss : 0.02809047782207319ITERATION : 39, loss : 0.028090476287971247ITERATION : 40, loss : 0.028090475183127888ITERATION : 41, loss : 0.028090474506050463ITERATION : 42, loss : 0.028090474033263743ITERATION : 43, loss : 0.028090473704529165ITERATION : 44, loss : 0.028090473477093612ITERATION : 45, loss : 0.02809047332607597ITERATION : 46, loss : 0.028090473253443514ITERATION : 47, loss : 0.028090473210901173ITERATION : 48, loss : 0.028090473133813517ITERATION : 49, loss : 0.028090473128429883ITERATION : 50, loss : 0.028090473106386113ITERATION : 51, loss : 0.028090473106199127ITERATION : 52, loss : 0.028090473106199127ITERATION : 53, loss : 0.028090473106199127ITERATION : 54, loss : 0.028090473106199127ITERATION : 55, loss : 0.028090473106199127ITERATION : 56, loss : 0.028090473106199127ITERATION : 57, loss : 0.028090473106199127ITERATION : 58, loss : 0.028090473106199127ITERATION : 59, loss : 0.028090473106199127ITERATION : 60, loss : 0.028090473106199127ITERATION : 61, loss : 0.028090473106199127ITERATION : 62, loss : 0.028090473106199127ITERATION : 63, loss : 0.028090473106199127ITERATION : 64, loss : 0.028090473106199127ITERATION : 65, loss : 0.028090473106199127ITERATION : 66, loss : 0.028090473106199127ITERATION : 67, loss : 0.028090473106199127ITERATION : 68, loss : 0.028090473106199127ITERATION : 69, loss : 0.028090473106199127ITERATION : 70, loss : 0.028090473106199127ITERATION : 71, loss : 0.028090473106199127ITERATION : 72, loss : 0.028090473106199127ITERATION : 73, loss : 0.028090473106199127ITERATION : 74, loss : 0.028090473106199127ITERATION : 75, loss : 0.028090473106199127ITERATION : 76, loss : 0.028090473106199127ITERATION : 77, loss : 0.028090473106199127ITERATION : 78, loss : 0.028090473106199127ITERATION : 79, loss : 0.028090473106199127ITERATION : 80, loss : 0.028090473106199127ITERATION : 81, loss : 0.028090473106199127ITERATION : 82, loss : 0.028090473106199127ITERATION : 83, loss : 0.028090473106199127ITERATION : 84, loss : 0.028090473106199127ITERATION : 85, loss : 0.028090473106199127ITERATION : 86, loss : 0.028090473106199127ITERATION : 87, loss : 0.028090473106199127ITERATION : 88, loss : 0.028090473106199127ITERATION : 89, loss : 0.028090473106199127ITERATION : 90, loss : 0.028090473106199127ITERATION : 91, loss : 0.028090473106199127ITERATION : 92, loss : 0.028090473106199127ITERATION : 93, loss : 0.028090473106199127ITERATION : 94, loss : 0.028090473106199127ITERATION : 95, loss : 0.028090473106199127ITERATION : 96, loss : 0.028090473106199127ITERATION : 97, loss : 0.028090473106199127ITERATION : 98, loss : 0.028090473106199127ITERATION : 99, loss : 0.028090473106199127ITERATION : 100, loss : 0.028090473106199127
gradient norm in None layer : 0.0006813980692562227
gradient norm in None layer : 3.322271997460901e-05
gradient norm in None layer : 4.134541872463263e-05
gradient norm in None layer : 0.0004844890755113663
gradient norm in None layer : 3.865703359625534e-05
gradient norm in None layer : 4.722803064397415e-05
gradient norm in None layer : 0.00022935314405660606
gradient norm in None layer : 1.0599175973305692e-05
gradient norm in None layer : 8.282621019963543e-06
gradient norm in None layer : 0.00019613521503153005
gradient norm in None layer : 8.751767737638201e-06
gradient norm in None layer : 7.5320175177169905e-06
gradient norm in None layer : 7.284699005151317e-05
gradient norm in None layer : 2.5830713700646375e-06
gradient norm in None layer : 1.7401528477054366e-06
gradient norm in None layer : 6.285790745419193e-05
gradient norm in None layer : 3.189466121621059e-06
gradient norm in None layer : 1.7968550534053885e-06
gradient norm in None layer : 8.255053581788904e-05
gradient norm in None layer : 1.7293653737531118e-06
gradient norm in None layer : 0.00016289951111275833
gradient norm in None layer : 1.277810094161096e-05
gradient norm in None layer : 8.33788881535134e-06
gradient norm in None layer : 0.00020560077547903587
gradient norm in None layer : 2.178636921106968e-05
gradient norm in None layer : 3.0168005624813772e-05
gradient norm in None layer : 0.00031467063763449796
gradient norm in None layer : 5.465200212554839e-06
gradient norm in None layer : 0.000483115414985679
gradient norm in None layer : 4.533773280586217e-05
gradient norm in None layer : 5.510094252585927e-05
gradient norm in None layer : 0.0006252190970869869
gradient norm in None layer : 4.8274148484951066e-05
gradient norm in None layer : 6.193986567048063e-05
gradient norm in None layer : 4.239904387643528e-05
gradient norm in None layer : 5.676613931662052e-06
Total gradient norm: 0.0012727594167074002
invariance loss : 4.23899686699543, avg_den : 0.43805694580078125, density loss : 0.33805694580078127, mse loss : 0.03307995043829045, solver time : 118.89951300621033 sec , total loss : 0.03765700425108666, running loss : 0.04400906689087845
Epoch 0/10 , batch 25/12500 
ITERATION : 1, loss : 0.023032573939787773ITERATION : 2, loss : 0.019181310499044403ITERATION : 3, loss : 0.01612036431220647ITERATION : 4, loss : 0.015221453775664535ITERATION : 5, loss : 0.01487597376870658ITERATION : 6, loss : 0.014754304881840308ITERATION : 7, loss : 0.014721391287283588ITERATION : 8, loss : 0.014721437936093082ITERATION : 9, loss : 0.014731764260028174ITERATION : 10, loss : 0.014743598101437619ITERATION : 11, loss : 0.01475393609575045ITERATION : 12, loss : 0.014762078537321558ITERATION : 13, loss : 0.014768168007778887ITERATION : 14, loss : 0.014772590344565026ITERATION : 15, loss : 0.014775745153023281ITERATION : 16, loss : 0.014777970330742402ITERATION : 17, loss : 0.014779528248829252ITERATION : 18, loss : 0.014780613573511136ITERATION : 19, loss : 0.014781367125449939ITERATION : 20, loss : 0.014781889085826097ITERATION : 21, loss : 0.014782250076056084ITERATION : 22, loss : 0.014782499439956605ITERATION : 23, loss : 0.014782671559763644ITERATION : 24, loss : 0.014782790287406505ITERATION : 25, loss : 0.014782872149729767ITERATION : 26, loss : 0.014782928579360432ITERATION : 27, loss : 0.014782967460939267ITERATION : 28, loss : 0.014782994243245697ITERATION : 29, loss : 0.014783012697880564ITERATION : 30, loss : 0.014783025407473137ITERATION : 31, loss : 0.01478303415667277ITERATION : 32, loss : 0.014783040184546892ITERATION : 33, loss : 0.014783044325215388ITERATION : 34, loss : 0.014783047183212536ITERATION : 35, loss : 0.014783049145744838ITERATION : 36, loss : 0.014783050499576138ITERATION : 37, loss : 0.014783051429307551ITERATION : 38, loss : 0.01478305207163058ITERATION : 39, loss : 0.014783052508768366ITERATION : 40, loss : 0.014783052819763672ITERATION : 41, loss : 0.014783053024599514ITERATION : 42, loss : 0.014783053164358385ITERATION : 43, loss : 0.014783053262773672ITERATION : 44, loss : 0.01478305332603139ITERATION : 45, loss : 0.014783053378541772ITERATION : 46, loss : 0.014783053409182432ITERATION : 47, loss : 0.014783053422441097ITERATION : 48, loss : 0.014783053436766296ITERATION : 49, loss : 0.014783053452184084ITERATION : 50, loss : 0.014783053452126304ITERATION : 51, loss : 0.01478305345709974ITERATION : 52, loss : 0.014783053457401696ITERATION : 53, loss : 0.01478305345736911ITERATION : 54, loss : 0.01478305345736911ITERATION : 55, loss : 0.01478305345736911ITERATION : 56, loss : 0.01478305345736911ITERATION : 57, loss : 0.01478305345736911ITERATION : 58, loss : 0.01478305345736911ITERATION : 59, loss : 0.01478305345736911ITERATION : 60, loss : 0.01478305345736911ITERATION : 61, loss : 0.01478305345736911ITERATION : 62, loss : 0.01478305345736911ITERATION : 63, loss : 0.01478305345736911ITERATION : 64, loss : 0.01478305345736911ITERATION : 65, loss : 0.01478305345736911ITERATION : 66, loss : 0.01478305345736911ITERATION : 67, loss : 0.01478305345736911ITERATION : 68, loss : 0.01478305345736911ITERATION : 69, loss : 0.01478305345736911ITERATION : 70, loss : 0.01478305345736911ITERATION : 71, loss : 0.01478305345736911ITERATION : 72, loss : 0.01478305345736911ITERATION : 73, loss : 0.01478305345736911ITERATION : 74, loss : 0.01478305345736911ITERATION : 75, loss : 0.01478305345736911ITERATION : 76, loss : 0.01478305345736911ITERATION : 77, loss : 0.01478305345736911ITERATION : 78, loss : 0.01478305345736911ITERATION : 79, loss : 0.01478305345736911ITERATION : 80, loss : 0.01478305345736911ITERATION : 81, loss : 0.01478305345736911ITERATION : 82, loss : 0.01478305345736911ITERATION : 83, loss : 0.01478305345736911ITERATION : 84, loss : 0.01478305345736911ITERATION : 85, loss : 0.01478305345736911ITERATION : 86, loss : 0.01478305345736911ITERATION : 87, loss : 0.01478305345736911ITERATION : 88, loss : 0.01478305345736911ITERATION : 89, loss : 0.01478305345736911ITERATION : 90, loss : 0.01478305345736911ITERATION : 91, loss : 0.01478305345736911ITERATION : 92, loss : 0.01478305345736911ITERATION : 93, loss : 0.01478305345736911ITERATION : 94, loss : 0.01478305345736911ITERATION : 95, loss : 0.01478305345736911ITERATION : 96, loss : 0.01478305345736911ITERATION : 97, loss : 0.01478305345736911ITERATION : 98, loss : 0.01478305345736911ITERATION : 99, loss : 0.01478305345736911ITERATION : 100, loss : 0.01478305345736911
ITERATION : 1, loss : 0.03660735155437736ITERATION : 2, loss : 0.032511882158311994ITERATION : 3, loss : 0.03400515522763446ITERATION : 4, loss : 0.03563255595896242ITERATION : 5, loss : 0.033762275849412916ITERATION : 6, loss : 0.03255234799002452ITERATION : 7, loss : 0.03173644051012408ITERATION : 8, loss : 0.031171924107417724ITERATION : 9, loss : 0.030775182233301838ITERATION : 10, loss : 0.03049368575671519ITERATION : 11, loss : 0.03029278445634684ITERATION : 12, loss : 0.030148872459193305ITERATION : 13, loss : 0.030045534068677923ITERATION : 14, loss : 0.029971206275274637ITERATION : 15, loss : 0.02991767991258907ITERATION : 16, loss : 0.0298790962960555ITERATION : 17, loss : 0.02985126165448406ITERATION : 18, loss : 0.0298311666667526ITERATION : 19, loss : 0.029816649827908277ITERATION : 20, loss : 0.029806155672422174ITERATION : 21, loss : 0.029798564822278575ITERATION : 22, loss : 0.02979307054220602ITERATION : 23, loss : 0.029789091018321553ITERATION : 24, loss : 0.029786207162567968ITERATION : 25, loss : 0.029784115747193446ITERATION : 26, loss : 0.029782598067881916ITERATION : 27, loss : 0.02978149614600184ITERATION : 28, loss : 0.029780695494182757ITERATION : 29, loss : 0.029780113338166353ITERATION : 30, loss : 0.0297796899340421ITERATION : 31, loss : 0.02977938161234048ITERATION : 32, loss : 0.029779157068041056ITERATION : 33, loss : 0.029778993326050943ITERATION : 34, loss : 0.029778873848623835ITERATION : 35, loss : 0.029778786678757958ITERATION : 36, loss : 0.029778723006544248ITERATION : 37, loss : 0.02977867649937686ITERATION : 38, loss : 0.02977864247316628ITERATION : 39, loss : 0.029778617570922733ITERATION : 40, loss : 0.029778599340742673ITERATION : 41, loss : 0.0297785859842172ITERATION : 42, loss : 0.02977857620889886ITERATION : 43, loss : 0.02977856901537844ITERATION : 44, loss : 0.02977856366794839ITERATION : 45, loss : 0.0297785598276338ITERATION : 46, loss : 0.029778556878365586ITERATION : 47, loss : 0.029778554826426273ITERATION : 48, loss : 0.029778553479346385ITERATION : 49, loss : 0.029778552270745887ITERATION : 50, loss : 0.029778551623891203ITERATION : 51, loss : 0.029778550973510887ITERATION : 52, loss : 0.029778550386619387ITERATION : 53, loss : 0.029778550149945838ITERATION : 54, loss : 0.02977854994163205ITERATION : 55, loss : 0.029778549818508405ITERATION : 56, loss : 0.02977854979509631ITERATION : 57, loss : 0.02977854975150021ITERATION : 58, loss : 0.029778549751094563ITERATION : 59, loss : 0.029778549751094563ITERATION : 60, loss : 0.029778549751094563ITERATION : 61, loss : 0.029778549751094563ITERATION : 62, loss : 0.029778549751094563ITERATION : 63, loss : 0.029778549751094563ITERATION : 64, loss : 0.029778549751094563ITERATION : 65, loss : 0.029778549751094563ITERATION : 66, loss : 0.029778549751094563ITERATION : 67, loss : 0.029778549751094563ITERATION : 68, loss : 0.029778549751094563ITERATION : 69, loss : 0.029778549751094563ITERATION : 70, loss : 0.029778549751094563ITERATION : 71, loss : 0.029778549751094563ITERATION : 72, loss : 0.029778549751094563ITERATION : 73, loss : 0.029778549751094563ITERATION : 74, loss : 0.029778549751094563ITERATION : 75, loss : 0.029778549751094563ITERATION : 76, loss : 0.029778549751094563ITERATION : 77, loss : 0.029778549751094563ITERATION : 78, loss : 0.029778549751094563ITERATION : 79, loss : 0.029778549751094563ITERATION : 80, loss : 0.029778549751094563ITERATION : 81, loss : 0.029778549751094563ITERATION : 82, loss : 0.029778549751094563ITERATION : 83, loss : 0.029778549751094563ITERATION : 84, loss : 0.029778549751094563ITERATION : 85, loss : 0.029778549751094563ITERATION : 86, loss : 0.029778549751094563ITERATION : 87, loss : 0.029778549751094563ITERATION : 88, loss : 0.029778549751094563ITERATION : 89, loss : 0.029778549751094563ITERATION : 90, loss : 0.029778549751094563ITERATION : 91, loss : 0.029778549751094563ITERATION : 92, loss : 0.029778549751094563ITERATION : 93, loss : 0.029778549751094563ITERATION : 94, loss : 0.029778549751094563ITERATION : 95, loss : 0.029778549751094563ITERATION : 96, loss : 0.029778549751094563ITERATION : 97, loss : 0.029778549751094563ITERATION : 98, loss : 0.029778549751094563ITERATION : 99, loss : 0.029778549751094563ITERATION : 100, loss : 0.029778549751094563
ITERATION : 1, loss : 0.023267170774311487ITERATION : 2, loss : 0.032456993248172ITERATION : 3, loss : 0.03416299509180085ITERATION : 4, loss : 0.034970873871796156ITERATION : 5, loss : 0.03543820251101056ITERATION : 6, loss : 0.03573565166921811ITERATION : 7, loss : 0.035932792374730536ITERATION : 8, loss : 0.03606599613787887ITERATION : 9, loss : 0.03615703713242543ITERATION : 10, loss : 0.036219773468496294ITERATION : 11, loss : 0.03626328529223769ITERATION : 12, loss : 0.03629362337984979ITERATION : 13, loss : 0.03631486869717044ITERATION : 14, loss : 0.03632980001131646ITERATION : 15, loss : 0.0363403247684325ITERATION : 16, loss : 0.036347761229790485ITERATION : 17, loss : 0.03635302570328932ITERATION : 18, loss : 0.03635675857964875ITERATION : 19, loss : 0.036359408495289916ITERATION : 20, loss : 0.03636129179151031ITERATION : 21, loss : 0.03636263099853941ITERATION : 22, loss : 0.03636358403223573ITERATION : 23, loss : 0.03636426255155ITERATION : 24, loss : 0.036364745767563655ITERATION : 25, loss : 0.03636509000677116ITERATION : 26, loss : 0.03636533533844469ITERATION : 27, loss : 0.03636551021408322ITERATION : 28, loss : 0.036365635026503154ITERATION : 29, loss : 0.036365723987187334ITERATION : 30, loss : 0.03636578729913881ITERATION : 31, loss : 0.03636583252438542ITERATION : 32, loss : 0.03636586481509334ITERATION : 33, loss : 0.036365887879143285ITERATION : 34, loss : 0.03636590422400977ITERATION : 35, loss : 0.03636591586976663ITERATION : 36, loss : 0.036365924088783676ITERATION : 37, loss : 0.036365929982089235ITERATION : 38, loss : 0.03636593409438287ITERATION : 39, loss : 0.036365937075232696ITERATION : 40, loss : 0.03636593916312376ITERATION : 41, loss : 0.036365940659009215ITERATION : 42, loss : 0.036365941707127564ITERATION : 43, loss : 0.03636594243963159ITERATION : 44, loss : 0.036365942984013994ITERATION : 45, loss : 0.036365943354069064ITERATION : 46, loss : 0.03636594363893131ITERATION : 47, loss : 0.03636594382570874ITERATION : 48, loss : 0.03636594395685032ITERATION : 49, loss : 0.03636594402420192ITERATION : 50, loss : 0.03636594411283694ITERATION : 51, loss : 0.03636594413332152ITERATION : 52, loss : 0.03636594418139621ITERATION : 53, loss : 0.036365944183375624ITERATION : 54, loss : 0.036365944183375624ITERATION : 55, loss : 0.036365944183375624ITERATION : 56, loss : 0.036365944183375624ITERATION : 57, loss : 0.036365944183375624ITERATION : 58, loss : 0.036365944183375624ITERATION : 59, loss : 0.036365944183375624ITERATION : 60, loss : 0.036365944183375624ITERATION : 61, loss : 0.036365944183375624ITERATION : 62, loss : 0.036365944183375624ITERATION : 63, loss : 0.036365944183375624ITERATION : 64, loss : 0.036365944183375624ITERATION : 65, loss : 0.036365944183375624ITERATION : 66, loss : 0.036365944183375624ITERATION : 67, loss : 0.036365944183375624ITERATION : 68, loss : 0.036365944183375624ITERATION : 69, loss : 0.036365944183375624ITERATION : 70, loss : 0.036365944183375624ITERATION : 71, loss : 0.036365944183375624ITERATION : 72, loss : 0.036365944183375624ITERATION : 73, loss : 0.036365944183375624ITERATION : 74, loss : 0.036365944183375624ITERATION : 75, loss : 0.036365944183375624ITERATION : 76, loss : 0.036365944183375624ITERATION : 77, loss : 0.036365944183375624ITERATION : 78, loss : 0.036365944183375624ITERATION : 79, loss : 0.036365944183375624ITERATION : 80, loss : 0.036365944183375624ITERATION : 81, loss : 0.036365944183375624ITERATION : 82, loss : 0.036365944183375624ITERATION : 83, loss : 0.036365944183375624ITERATION : 84, loss : 0.036365944183375624ITERATION : 85, loss : 0.036365944183375624ITERATION : 86, loss : 0.036365944183375624ITERATION : 87, loss : 0.036365944183375624ITERATION : 88, loss : 0.036365944183375624ITERATION : 89, loss : 0.036365944183375624ITERATION : 90, loss : 0.036365944183375624ITERATION : 91, loss : 0.036365944183375624ITERATION : 92, loss : 0.036365944183375624ITERATION : 93, loss : 0.036365944183375624ITERATION : 94, loss : 0.036365944183375624ITERATION : 95, loss : 0.036365944183375624ITERATION : 96, loss : 0.036365944183375624ITERATION : 97, loss : 0.036365944183375624ITERATION : 98, loss : 0.036365944183375624ITERATION : 99, loss : 0.036365944183375624ITERATION : 100, loss : 0.036365944183375624
ITERATION : 1, loss : 0.05065325457991765ITERATION : 2, loss : 0.053720384948444645ITERATION : 3, loss : 0.05503092947337795ITERATION : 4, loss : 0.055502895632643316ITERATION : 5, loss : 0.0557779910004255ITERATION : 6, loss : 0.05598273955389277ITERATION : 7, loss : 0.056144159099000085ITERATION : 8, loss : 0.05588840166299969ITERATION : 9, loss : 0.055681387172915944ITERATION : 10, loss : 0.05553629318235589ITERATION : 11, loss : 0.055433280829094315ITERATION : 12, loss : 0.05535948707544608ITERATION : 13, loss : 0.05530629057737498ITERATION : 14, loss : 0.05526777130442176ITERATION : 15, loss : 0.0552397906656541ITERATION : 16, loss : 0.05521941825578518ITERATION : 17, loss : 0.05520456053367667ITERATION : 18, loss : 0.05519371130977682ITERATION : 19, loss : 0.055185781816228914ITERATION : 20, loss : 0.055179982663843385ITERATION : 21, loss : 0.05517573947315188ITERATION : 22, loss : 0.055172633596181894ITERATION : 23, loss : 0.055170359551331295ITERATION : 24, loss : 0.055168694202797415ITERATION : 25, loss : 0.055167474530317596ITERATION : 26, loss : 0.05516658104707404ITERATION : 27, loss : 0.055165926524115984ITERATION : 28, loss : 0.05516544703624953ITERATION : 29, loss : 0.0551650957205288ITERATION : 30, loss : 0.05516483842060418ITERATION : 31, loss : 0.055164649797536894ITERATION : 32, loss : 0.055164511562128864ITERATION : 33, loss : 0.05516441035899105ITERATION : 34, loss : 0.05516433629895155ITERATION : 35, loss : 0.05516428187206553ITERATION : 36, loss : 0.05516424195980619ITERATION : 37, loss : 0.05516421275530909ITERATION : 38, loss : 0.055164191472225496ITERATION : 39, loss : 0.055164175886050396ITERATION : 40, loss : 0.05516416446550851ITERATION : 41, loss : 0.055164156040458004ITERATION : 42, loss : 0.05516414998870255ITERATION : 43, loss : 0.05516414547447618ITERATION : 44, loss : 0.055164142237909164ITERATION : 45, loss : 0.05516413987503728ITERATION : 46, loss : 0.05516413802409485ITERATION : 47, loss : 0.05516413639742962ITERATION : 48, loss : 0.05516413543472943ITERATION : 49, loss : 0.05516413479371449ITERATION : 50, loss : 0.0551641343074732ITERATION : 51, loss : 0.05516413392277012ITERATION : 52, loss : 0.05516413376458611ITERATION : 53, loss : 0.05516413354746069ITERATION : 54, loss : 0.05516413353064057ITERATION : 55, loss : 0.05516413353064057ITERATION : 56, loss : 0.05516413353064057ITERATION : 57, loss : 0.05516413353064057ITERATION : 58, loss : 0.05516413353064057ITERATION : 59, loss : 0.05516413353064057ITERATION : 60, loss : 0.05516413353064057ITERATION : 61, loss : 0.05516413353064057ITERATION : 62, loss : 0.05516413353064057ITERATION : 63, loss : 0.05516413353064057ITERATION : 64, loss : 0.05516413353064057ITERATION : 65, loss : 0.05516413353064057ITERATION : 66, loss : 0.05516413353064057ITERATION : 67, loss : 0.05516413353064057ITERATION : 68, loss : 0.05516413353064057ITERATION : 69, loss : 0.05516413353064057ITERATION : 70, loss : 0.05516413353064057ITERATION : 71, loss : 0.05516413353064057ITERATION : 72, loss : 0.05516413353064057ITERATION : 73, loss : 0.05516413353064057ITERATION : 74, loss : 0.05516413353064057ITERATION : 75, loss : 0.05516413353064057ITERATION : 76, loss : 0.05516413353064057ITERATION : 77, loss : 0.05516413353064057ITERATION : 78, loss : 0.05516413353064057ITERATION : 79, loss : 0.05516413353064057ITERATION : 80, loss : 0.05516413353064057ITERATION : 81, loss : 0.05516413353064057ITERATION : 82, loss : 0.05516413353064057ITERATION : 83, loss : 0.05516413353064057ITERATION : 84, loss : 0.05516413353064057ITERATION : 85, loss : 0.05516413353064057ITERATION : 86, loss : 0.05516413353064057ITERATION : 87, loss : 0.05516413353064057ITERATION : 88, loss : 0.05516413353064057ITERATION : 89, loss : 0.05516413353064057ITERATION : 90, loss : 0.05516413353064057ITERATION : 91, loss : 0.05516413353064057ITERATION : 92, loss : 0.05516413353064057ITERATION : 93, loss : 0.05516413353064057ITERATION : 94, loss : 0.05516413353064057ITERATION : 95, loss : 0.05516413353064057ITERATION : 96, loss : 0.05516413353064057ITERATION : 97, loss : 0.05516413353064057ITERATION : 98, loss : 0.05516413353064057ITERATION : 99, loss : 0.05516413353064057ITERATION : 100, loss : 0.05516413353064057
ITERATION : 1, loss : 0.02136912520430637ITERATION : 2, loss : 0.023819155816714788ITERATION : 3, loss : 0.022092069990372352ITERATION : 4, loss : 0.021750936590170453ITERATION : 5, loss : 0.021771980886524753ITERATION : 6, loss : 0.021869975858135163ITERATION : 7, loss : 0.021965290632071394ITERATION : 8, loss : 0.022040295334396384ITERATION : 9, loss : 0.022095251659732293ITERATION : 10, loss : 0.02213445683808989ITERATION : 11, loss : 0.02216219256262023ITERATION : 12, loss : 0.022181809381534044ITERATION : 13, loss : 0.022195727774373272ITERATION : 14, loss : 0.022205645918684395ITERATION : 15, loss : 0.022212744660310965ITERATION : 16, loss : 0.022217845773743288ITERATION : 17, loss : 0.022221524023754175ITERATION : 18, loss : 0.022224183869257164ITERATION : 19, loss : 0.02222611180702616ITERATION : 20, loss : 0.022227511799935845ITERATION : 21, loss : 0.022228529892712817ITERATION : 22, loss : 0.022229271066251048ITERATION : 23, loss : 0.022229811143939585ITERATION : 24, loss : 0.02223020520013976ITERATION : 25, loss : 0.022230492671806965ITERATION : 26, loss : 0.022230702556916473ITERATION : 27, loss : 0.0222308558809916ITERATION : 28, loss : 0.022230967690140498ITERATION : 29, loss : 0.022231049603056264ITERATION : 30, loss : 0.022231109356872357ITERATION : 31, loss : 0.02223115294464434ITERATION : 32, loss : 0.022231184938763342ITERATION : 33, loss : 0.022231208366651233ITERATION : 34, loss : 0.02223122546000041ITERATION : 35, loss : 0.022231237905582235ITERATION : 36, loss : 0.022231246986035427ITERATION : 37, loss : 0.02223125370500409ITERATION : 38, loss : 0.022231258606232568ITERATION : 39, loss : 0.022231262130670556ITERATION : 40, loss : 0.022231264712503022ITERATION : 41, loss : 0.022231266592452475ITERATION : 42, loss : 0.022231267966802314ITERATION : 43, loss : 0.02223126890934768ITERATION : 44, loss : 0.02223126962485277ITERATION : 45, loss : 0.022231270123139756ITERATION : 46, loss : 0.022231270510056904ITERATION : 47, loss : 0.022231270770516686ITERATION : 48, loss : 0.022231270974581416ITERATION : 49, loss : 0.022231271091986592ITERATION : 50, loss : 0.02223127119452418ITERATION : 51, loss : 0.022231271261761384ITERATION : 52, loss : 0.022231271335549464ITERATION : 53, loss : 0.0222312713711033ITERATION : 54, loss : 0.022231271396492874ITERATION : 55, loss : 0.022231271418181823ITERATION : 56, loss : 0.022231271418242136ITERATION : 57, loss : 0.022231271418242136ITERATION : 58, loss : 0.022231271418242136ITERATION : 59, loss : 0.022231271418242136ITERATION : 60, loss : 0.022231271418242136ITERATION : 61, loss : 0.022231271418242136ITERATION : 62, loss : 0.022231271418242136ITERATION : 63, loss : 0.022231271418242136ITERATION : 64, loss : 0.022231271418242136ITERATION : 65, loss : 0.022231271418242136ITERATION : 66, loss : 0.022231271418242136ITERATION : 67, loss : 0.022231271418242136ITERATION : 68, loss : 0.022231271418242136ITERATION : 69, loss : 0.022231271418242136ITERATION : 70, loss : 0.022231271418242136ITERATION : 71, loss : 0.022231271418242136ITERATION : 72, loss : 0.022231271418242136ITERATION : 73, loss : 0.022231271418242136ITERATION : 74, loss : 0.022231271418242136ITERATION : 75, loss : 0.022231271418242136ITERATION : 76, loss : 0.022231271418242136ITERATION : 77, loss : 0.022231271418242136ITERATION : 78, loss : 0.022231271418242136ITERATION : 79, loss : 0.022231271418242136ITERATION : 80, loss : 0.022231271418242136ITERATION : 81, loss : 0.022231271418242136ITERATION : 82, loss : 0.022231271418242136ITERATION : 83, loss : 0.022231271418242136ITERATION : 84, loss : 0.022231271418242136ITERATION : 85, loss : 0.022231271418242136ITERATION : 86, loss : 0.022231271418242136ITERATION : 87, loss : 0.022231271418242136ITERATION : 88, loss : 0.022231271418242136ITERATION : 89, loss : 0.022231271418242136ITERATION : 90, loss : 0.022231271418242136ITERATION : 91, loss : 0.022231271418242136ITERATION : 92, loss : 0.022231271418242136ITERATION : 93, loss : 0.022231271418242136ITERATION : 94, loss : 0.022231271418242136ITERATION : 95, loss : 0.022231271418242136ITERATION : 96, loss : 0.022231271418242136ITERATION : 97, loss : 0.022231271418242136ITERATION : 98, loss : 0.022231271418242136ITERATION : 99, loss : 0.022231271418242136ITERATION : 100, loss : 0.022231271418242136
ITERATION : 1, loss : 0.043586725928147045ITERATION : 2, loss : 0.04061773110113956ITERATION : 3, loss : 0.039842882044758714ITERATION : 4, loss : 0.03910917595916822ITERATION : 5, loss : 0.03855126838497094ITERATION : 6, loss : 0.03818672158677336ITERATION : 7, loss : 0.037942092116110666ITERATION : 8, loss : 0.0377749665273963ITERATION : 9, loss : 0.03765928465844144ITERATION : 10, loss : 0.037578419964896866ITERATION : 11, loss : 0.037521475558539ITERATION : 12, loss : 0.03748115649938937ITERATION : 13, loss : 0.03745249525098087ITERATION : 14, loss : 0.03743206277604434ITERATION : 15, loss : 0.03741746688376337ITERATION : 16, loss : 0.03740702540682935ITERATION : 17, loss : 0.03739954835949157ITERATION : 18, loss : 0.03739419043499547ITERATION : 19, loss : 0.03739034917534528ITERATION : 20, loss : 0.037387594277694405ITERATION : 21, loss : 0.03738561812117063ITERATION : 22, loss : 0.03738420032342167ITERATION : 23, loss : 0.03738318301570319ITERATION : 24, loss : 0.037382453034115304ITERATION : 25, loss : 0.037381929136811896ITERATION : 26, loss : 0.03738155319608604ITERATION : 27, loss : 0.03738128336642034ITERATION : 28, loss : 0.03738108969321494ITERATION : 29, loss : 0.037380950691817884ITERATION : 30, loss : 0.037380850960494635ITERATION : 31, loss : 0.03738077934635604ITERATION : 32, loss : 0.03738072795958956ITERATION : 33, loss : 0.0373806910577691ITERATION : 34, loss : 0.03738066458751941ITERATION : 35, loss : 0.037380645559803714ITERATION : 36, loss : 0.037380631906455845ITERATION : 37, loss : 0.03738062212247244ITERATION : 38, loss : 0.03738061508710743ITERATION : 39, loss : 0.03738061006014637ITERATION : 40, loss : 0.03738060642300721ITERATION : 41, loss : 0.03738060384756624ITERATION : 42, loss : 0.03738060195467196ITERATION : 43, loss : 0.03738060063912312ITERATION : 44, loss : 0.037380599666196834ITERATION : 45, loss : 0.03738059900670222ITERATION : 46, loss : 0.037380598492487455ITERATION : 47, loss : 0.03738059813398977ITERATION : 48, loss : 0.03738059788334766ITERATION : 49, loss : 0.037380597680575715ITERATION : 50, loss : 0.03738059756753671ITERATION : 51, loss : 0.03738059746369103ITERATION : 52, loss : 0.037380597433004106ITERATION : 53, loss : 0.0373805973692837ITERATION : 54, loss : 0.03738059735774073ITERATION : 55, loss : 0.037380597316096165ITERATION : 56, loss : 0.037380597331201866ITERATION : 57, loss : 0.037380597331201866ITERATION : 58, loss : 0.037380597331201866ITERATION : 59, loss : 0.037380597331201866ITERATION : 60, loss : 0.037380597331201866ITERATION : 61, loss : 0.037380597331201866ITERATION : 62, loss : 0.037380597331201866ITERATION : 63, loss : 0.037380597331201866ITERATION : 64, loss : 0.037380597331201866ITERATION : 65, loss : 0.037380597331201866ITERATION : 66, loss : 0.037380597331201866ITERATION : 67, loss : 0.037380597331201866ITERATION : 68, loss : 0.037380597331201866ITERATION : 69, loss : 0.037380597331201866ITERATION : 70, loss : 0.037380597331201866ITERATION : 71, loss : 0.037380597331201866ITERATION : 72, loss : 0.037380597331201866ITERATION : 73, loss : 0.037380597331201866ITERATION : 74, loss : 0.037380597331201866ITERATION : 75, loss : 0.037380597331201866ITERATION : 76, loss : 0.037380597331201866ITERATION : 77, loss : 0.037380597331201866ITERATION : 78, loss : 0.037380597331201866ITERATION : 79, loss : 0.037380597331201866ITERATION : 80, loss : 0.037380597331201866ITERATION : 81, loss : 0.037380597331201866ITERATION : 82, loss : 0.037380597331201866ITERATION : 83, loss : 0.037380597331201866ITERATION : 84, loss : 0.037380597331201866ITERATION : 85, loss : 0.037380597331201866ITERATION : 86, loss : 0.037380597331201866ITERATION : 87, loss : 0.037380597331201866ITERATION : 88, loss : 0.037380597331201866ITERATION : 89, loss : 0.037380597331201866ITERATION : 90, loss : 0.037380597331201866ITERATION : 91, loss : 0.037380597331201866ITERATION : 92, loss : 0.037380597331201866ITERATION : 93, loss : 0.037380597331201866ITERATION : 94, loss : 0.037380597331201866ITERATION : 95, loss : 0.037380597331201866ITERATION : 96, loss : 0.037380597331201866ITERATION : 97, loss : 0.037380597331201866ITERATION : 98, loss : 0.037380597331201866ITERATION : 99, loss : 0.037380597331201866ITERATION : 100, loss : 0.037380597331201866
ITERATION : 1, loss : 0.036679255563843806ITERATION : 2, loss : 0.040968195872274155ITERATION : 3, loss : 0.04419804451350522ITERATION : 4, loss : 0.0468008552417292ITERATION : 5, loss : 0.04871882406294405ITERATION : 6, loss : 0.05007538843263437ITERATION : 7, loss : 0.05100845112063222ITERATION : 8, loss : 0.051638184850938754ITERATION : 9, loss : 0.052058026199146235ITERATION : 10, loss : 0.05233583253459362ITERATION : 11, loss : 0.052518857151304624ITERATION : 12, loss : 0.052639164679853204ITERATION : 13, loss : 0.05271817320742712ITERATION : 14, loss : 0.05277005585917835ITERATION : 15, loss : 0.05280414097821106ITERATION : 16, loss : 0.05282655098513021ITERATION : 17, loss : 0.05284129952140565ITERATION : 18, loss : 0.05285101626147872ITERATION : 19, loss : 0.05285742529836573ITERATION : 20, loss : 0.05286165782955952ITERATION : 21, loss : 0.05286445664287031ITERATION : 22, loss : 0.0528663095715112ITERATION : 23, loss : 0.05286753796536697ITERATION : 24, loss : 0.05286835321157417ITERATION : 25, loss : 0.05286889499651636ITERATION : 26, loss : 0.05286925554967892ITERATION : 27, loss : 0.052869495686771746ITERATION : 28, loss : 0.05286965589619818ITERATION : 29, loss : 0.052869763102972964ITERATION : 30, loss : 0.052869834552604765ITERATION : 31, loss : 0.05286988266989037ITERATION : 32, loss : 0.052869915025380985ITERATION : 33, loss : 0.05286993662524857ITERATION : 34, loss : 0.05286995114380809ITERATION : 35, loss : 0.052869960809376663ITERATION : 36, loss : 0.05286996753762799ITERATION : 37, loss : 0.05286997207546658ITERATION : 38, loss : 0.05286997499910164ITERATION : 39, loss : 0.05286997697067519ITERATION : 40, loss : 0.052869978251270544ITERATION : 41, loss : 0.052869979195911444ITERATION : 42, loss : 0.05286997983259432ITERATION : 43, loss : 0.052869980210654824ITERATION : 44, loss : 0.05286998048666353ITERATION : 45, loss : 0.05286998061530335ITERATION : 46, loss : 0.05286998090132624ITERATION : 47, loss : 0.052869980903023917ITERATION : 48, loss : 0.052869980903023917ITERATION : 49, loss : 0.052869980903023917ITERATION : 50, loss : 0.052869980903023917ITERATION : 51, loss : 0.052869980903023917ITERATION : 52, loss : 0.052869980903023917ITERATION : 53, loss : 0.052869980903023917ITERATION : 54, loss : 0.052869980903023917ITERATION : 55, loss : 0.052869980903023917ITERATION : 56, loss : 0.052869980903023917ITERATION : 57, loss : 0.052869980903023917ITERATION : 58, loss : 0.052869980903023917ITERATION : 59, loss : 0.052869980903023917ITERATION : 60, loss : 0.052869980903023917ITERATION : 61, loss : 0.052869980903023917ITERATION : 62, loss : 0.052869980903023917ITERATION : 63, loss : 0.052869980903023917ITERATION : 64, loss : 0.052869980903023917ITERATION : 65, loss : 0.052869980903023917ITERATION : 66, loss : 0.052869980903023917ITERATION : 67, loss : 0.052869980903023917ITERATION : 68, loss : 0.052869980903023917ITERATION : 69, loss : 0.052869980903023917ITERATION : 70, loss : 0.052869980903023917ITERATION : 71, loss : 0.052869980903023917ITERATION : 72, loss : 0.052869980903023917ITERATION : 73, loss : 0.052869980903023917ITERATION : 74, loss : 0.052869980903023917ITERATION : 75, loss : 0.052869980903023917ITERATION : 76, loss : 0.052869980903023917ITERATION : 77, loss : 0.052869980903023917ITERATION : 78, loss : 0.052869980903023917ITERATION : 79, loss : 0.052869980903023917ITERATION : 80, loss : 0.052869980903023917ITERATION : 81, loss : 0.052869980903023917ITERATION : 82, loss : 0.052869980903023917ITERATION : 83, loss : 0.052869980903023917ITERATION : 84, loss : 0.052869980903023917ITERATION : 85, loss : 0.052869980903023917ITERATION : 86, loss : 0.052869980903023917ITERATION : 87, loss : 0.052869980903023917ITERATION : 88, loss : 0.052869980903023917ITERATION : 89, loss : 0.052869980903023917ITERATION : 90, loss : 0.052869980903023917ITERATION : 91, loss : 0.052869980903023917ITERATION : 92, loss : 0.052869980903023917ITERATION : 93, loss : 0.052869980903023917ITERATION : 94, loss : 0.052869980903023917ITERATION : 95, loss : 0.052869980903023917ITERATION : 96, loss : 0.052869980903023917ITERATION : 97, loss : 0.052869980903023917ITERATION : 98, loss : 0.052869980903023917ITERATION : 99, loss : 0.052869980903023917ITERATION : 100, loss : 0.052869980903023917
ITERATION : 1, loss : 0.060418291334350976ITERATION : 2, loss : 0.049346065897249484ITERATION : 3, loss : 0.046204975060856274ITERATION : 4, loss : 0.04193346748435669ITERATION : 5, loss : 0.03692753638188518ITERATION : 6, loss : 0.03399745529999083ITERATION : 7, loss : 0.03224316915394249ITERATION : 8, loss : 0.031076664880602737ITERATION : 9, loss : 0.030279184658044488ITERATION : 10, loss : 0.02973050535658467ITERATION : 11, loss : 0.02933737608450537ITERATION : 12, loss : 0.02905154995472904ITERATION : 13, loss : 0.028841356120194204ITERATION : 14, loss : 0.02868541458922846ITERATION : 15, loss : 0.028568940518566666ITERATION : 16, loss : 0.02848149698697224ITERATION : 17, loss : 0.028415592240037237ITERATION : 18, loss : 0.02836577479650582ITERATION : 19, loss : 0.0283280338056256ITERATION : 20, loss : 0.028299393751016626ITERATION : 21, loss : 0.02827763233888163ITERATION : 22, loss : 0.028261081560937977ITERATION : 23, loss : 0.02824848453328883ITERATION : 24, loss : 0.028238891528015227ITERATION : 25, loss : 0.028231583245164085ITERATION : 26, loss : 0.028226013494055582ITERATION : 27, loss : 0.028221767877662833ITERATION : 28, loss : 0.02821853084912027ITERATION : 29, loss : 0.02821606246132927ITERATION : 30, loss : 0.028214180015915934ITERATION : 31, loss : 0.028212744270057297ITERATION : 32, loss : 0.02821164918182286ITERATION : 33, loss : 0.028210813863187092ITERATION : 34, loss : 0.028210176611848894ITERATION : 35, loss : 0.028209690546237873ITERATION : 36, loss : 0.028209319727152772ITERATION : 37, loss : 0.028209036797741217ITERATION : 38, loss : 0.028208820991868177ITERATION : 39, loss : 0.028208656216375198ITERATION : 40, loss : 0.028208530641036466ITERATION : 41, loss : 0.028208434882414205ITERATION : 42, loss : 0.028208361811157986ITERATION : 43, loss : 0.02820830605320741ITERATION : 44, loss : 0.02820826349697884ITERATION : 45, loss : 0.028208231043590857ITERATION : 46, loss : 0.028208206303621277ITERATION : 47, loss : 0.028208187424769923ITERATION : 48, loss : 0.028208173030991022ITERATION : 49, loss : 0.02820816219996593ITERATION : 50, loss : 0.028208153749553544ITERATION : 51, loss : 0.02820814732678063ITERATION : 52, loss : 0.028208142450125564ITERATION : 53, loss : 0.028208138658833634ITERATION : 54, loss : 0.02820813584267494ITERATION : 55, loss : 0.02820813370120431ITERATION : 56, loss : 0.02820813209630443ITERATION : 57, loss : 0.028208130697205ITERATION : 58, loss : 0.028208129712496133ITERATION : 59, loss : 0.028208129123689984ITERATION : 60, loss : 0.028208128597043667ITERATION : 61, loss : 0.028208128263339696ITERATION : 62, loss : 0.028208128043969213ITERATION : 63, loss : 0.028208127564680913ITERATION : 64, loss : 0.028208127499226302ITERATION : 65, loss : 0.02820812746601271ITERATION : 66, loss : 0.02820812746601271ITERATION : 67, loss : 0.02820812746601271ITERATION : 68, loss : 0.02820812746601271ITERATION : 69, loss : 0.02820812746601271ITERATION : 70, loss : 0.02820812746601271ITERATION : 71, loss : 0.02820812746601271ITERATION : 72, loss : 0.02820812746601271ITERATION : 73, loss : 0.02820812746601271ITERATION : 74, loss : 0.02820812746601271ITERATION : 75, loss : 0.02820812746601271ITERATION : 76, loss : 0.02820812746601271ITERATION : 77, loss : 0.02820812746601271ITERATION : 78, loss : 0.02820812746601271ITERATION : 79, loss : 0.02820812746601271ITERATION : 80, loss : 0.02820812746601271ITERATION : 81, loss : 0.02820812746601271ITERATION : 82, loss : 0.02820812746601271ITERATION : 83, loss : 0.02820812746601271ITERATION : 84, loss : 0.02820812746601271ITERATION : 85, loss : 0.02820812746601271ITERATION : 86, loss : 0.02820812746601271ITERATION : 87, loss : 0.02820812746601271ITERATION : 88, loss : 0.02820812746601271ITERATION : 89, loss : 0.02820812746601271ITERATION : 90, loss : 0.02820812746601271ITERATION : 91, loss : 0.02820812746601271ITERATION : 92, loss : 0.02820812746601271ITERATION : 93, loss : 0.02820812746601271ITERATION : 94, loss : 0.02820812746601271ITERATION : 95, loss : 0.02820812746601271ITERATION : 96, loss : 0.02820812746601271ITERATION : 97, loss : 0.02820812746601271ITERATION : 98, loss : 0.02820812746601271ITERATION : 99, loss : 0.02820812746601271ITERATION : 100, loss : 0.02820812746601271
gradient norm in None layer : 0.0004434260684995475
gradient norm in None layer : 1.9697824608751123e-05
gradient norm in None layer : 2.3232032266364264e-05
gradient norm in None layer : 0.0002877796608551509
gradient norm in None layer : 2.6421542597729222e-05
gradient norm in None layer : 3.147346897599966e-05
gradient norm in None layer : 0.00013323566992461296
gradient norm in None layer : 6.876103830747264e-06
gradient norm in None layer : 4.793912990251779e-06
gradient norm in None layer : 0.00012801845693409666
gradient norm in None layer : 6.075803705671036e-06
gradient norm in None layer : 4.815612004586167e-06
gradient norm in None layer : 4.558573824103989e-05
gradient norm in None layer : 1.6829901074846937e-06
gradient norm in None layer : 1.1128573878224557e-06
gradient norm in None layer : 4.2134269229405954e-05
gradient norm in None layer : 2.2915098047370084e-06
gradient norm in None layer : 1.390258079629075e-06
gradient norm in None layer : 5.900110273894599e-05
gradient norm in None layer : 1.4750567246246654e-06
gradient norm in None layer : 0.00011823211538410875
gradient norm in None layer : 8.965836343589413e-06
gradient norm in None layer : 6.100693418525147e-06
gradient norm in None layer : 0.00015867281605820756
gradient norm in None layer : 1.4197016505478118e-05
gradient norm in None layer : 2.19671118661422e-05
gradient norm in None layer : 0.00024163871681548064
gradient norm in None layer : 4.329386381702462e-06
gradient norm in None layer : 0.0003558104390175159
gradient norm in None layer : 3.341734744966442e-05
gradient norm in None layer : 3.941022229930001e-05
gradient norm in None layer : 0.00046076153661649496
gradient norm in None layer : 4.376404268253255e-05
gradient norm in None layer : 6.15753165378426e-05
gradient norm in None layer : 3.971973415882618e-05
gradient norm in None layer : 8.132162483903739e-06
Total gradient norm: 0.000878047461101896
invariance loss : 4.165767322476432, avg_den : 0.43482208251953125, density loss : 0.33482208251953127, mse loss : 0.03459770725512006, solver time : 132.92419815063477 sec , total loss : 0.03909829666011602, running loss : 0.043812636081647954
Epoch 0/10 , batch 26/12500 
ITERATION : 1, loss : 0.027157476975353406ITERATION : 2, loss : 0.023694369432880804ITERATION : 3, loss : 0.024448018317872574ITERATION : 4, loss : 0.02706871667564135ITERATION : 5, loss : 0.030574442388308715ITERATION : 6, loss : 0.03378427160806395ITERATION : 7, loss : 0.03515660185648858ITERATION : 8, loss : 0.03508389222174449ITERATION : 9, loss : 0.03503895387093177ITERATION : 10, loss : 0.03501050913006623ITERATION : 11, loss : 0.034992076499751876ITERATION : 12, loss : 0.034979877838435355ITERATION : 13, loss : 0.034971662540656076ITERATION : 14, loss : 0.034966054125743815ITERATION : 15, loss : 0.03496218706124331ITERATION : 16, loss : 0.03495950181816345ITERATION : 17, loss : 0.0349576286727697ITERATION : 18, loss : 0.03495631825117284ITERATION : 19, loss : 0.03495540002907831ITERATION : 20, loss : 0.034954756116092285ITERATION : 21, loss : 0.03495430456519395ITERATION : 22, loss : 0.03495398795011826ITERATION : 23, loss : 0.03495376604129219ITERATION : 24, loss : 0.03495361063942347ITERATION : 25, loss : 0.03495350187634534ITERATION : 26, loss : 0.03495342579973229ITERATION : 27, loss : 0.03495337266591666ITERATION : 28, loss : 0.03495333556215873ITERATION : 29, loss : 0.034953309703381015ITERATION : 30, loss : 0.03495329166716602ITERATION : 31, loss : 0.03495327912490865ITERATION : 32, loss : 0.03495327039956894ITERATION : 33, loss : 0.03495326432961795ITERATION : 34, loss : 0.03495326014173569ITERATION : 35, loss : 0.034953257194754145ITERATION : 36, loss : 0.034953255169682604ITERATION : 37, loss : 0.034953253792300534ITERATION : 38, loss : 0.03495325281642289ITERATION : 39, loss : 0.03495325214171164ITERATION : 40, loss : 0.034953251688308415ITERATION : 41, loss : 0.03495325136967294ITERATION : 42, loss : 0.03495325114655023ITERATION : 43, loss : 0.03495325101190499ITERATION : 44, loss : 0.034953250909483966ITERATION : 45, loss : 0.034953250847603985ITERATION : 46, loss : 0.034953250800976034ITERATION : 47, loss : 0.034953250770867854ITERATION : 48, loss : 0.03495325075425209ITERATION : 49, loss : 0.03495325074085916ITERATION : 50, loss : 0.034953250735481736ITERATION : 51, loss : 0.034953250729788644ITERATION : 52, loss : 0.03495325072497769ITERATION : 53, loss : 0.03495325072703784ITERATION : 54, loss : 0.03495325071658595ITERATION : 55, loss : 0.03495325072369721ITERATION : 56, loss : 0.03495325072255654ITERATION : 57, loss : 0.03495325072123625ITERATION : 58, loss : 0.03495325072123625ITERATION : 59, loss : 0.03495325072123625ITERATION : 60, loss : 0.03495325072123625ITERATION : 61, loss : 0.03495325072123625ITERATION : 62, loss : 0.03495325072123625ITERATION : 63, loss : 0.03495325072123625ITERATION : 64, loss : 0.03495325072123625ITERATION : 65, loss : 0.03495325072123625ITERATION : 66, loss : 0.03495325072123625ITERATION : 67, loss : 0.03495325072123625ITERATION : 68, loss : 0.03495325072123625ITERATION : 69, loss : 0.03495325072123625ITERATION : 70, loss : 0.03495325072123625ITERATION : 71, loss : 0.03495325072123625ITERATION : 72, loss : 0.03495325072123625ITERATION : 73, loss : 0.03495325072123625ITERATION : 74, loss : 0.03495325072123625ITERATION : 75, loss : 0.03495325072123625ITERATION : 76, loss : 0.03495325072123625ITERATION : 77, loss : 0.03495325072123625ITERATION : 78, loss : 0.03495325072123625ITERATION : 79, loss : 0.03495325072123625ITERATION : 80, loss : 0.03495325072123625ITERATION : 81, loss : 0.03495325072123625ITERATION : 82, loss : 0.03495325072123625ITERATION : 83, loss : 0.03495325072123625ITERATION : 84, loss : 0.03495325072123625ITERATION : 85, loss : 0.03495325072123625ITERATION : 86, loss : 0.03495325072123625ITERATION : 87, loss : 0.03495325072123625ITERATION : 88, loss : 0.03495325072123625ITERATION : 89, loss : 0.03495325072123625ITERATION : 90, loss : 0.03495325072123625ITERATION : 91, loss : 0.03495325072123625ITERATION : 92, loss : 0.03495325072123625ITERATION : 93, loss : 0.03495325072123625ITERATION : 94, loss : 0.03495325072123625ITERATION : 95, loss : 0.03495325072123625ITERATION : 96, loss : 0.03495325072123625ITERATION : 97, loss : 0.03495325072123625ITERATION : 98, loss : 0.03495325072123625ITERATION : 99, loss : 0.03495325072123625ITERATION : 100, loss : 0.03495325072123625
ITERATION : 1, loss : 0.0438168585022185ITERATION : 2, loss : 0.0443214771868126ITERATION : 3, loss : 0.04693205541033779ITERATION : 4, loss : 0.04895142525503535ITERATION : 5, loss : 0.0478840602317976ITERATION : 6, loss : 0.04725676805249643ITERATION : 7, loss : 0.04687273065877219ITERATION : 8, loss : 0.04662953978394384ITERATION : 9, loss : 0.04647154541513911ITERATION : 10, loss : 0.0463670211112247ITERATION : 11, loss : 0.04629700837663115ITERATION : 12, loss : 0.046249718287095155ITERATION : 13, loss : 0.046217593912983246ITERATION : 14, loss : 0.04619568527796688ITERATION : 15, loss : 0.04618070104483119ITERATION : 16, loss : 0.04617043105080823ITERATION : 17, loss : 0.04616338032135232ITERATION : 18, loss : 0.04615853327531363ITERATION : 19, loss : 0.04615519748945402ITERATION : 20, loss : 0.046152899292728346ITERATION : 21, loss : 0.04615131451504062ITERATION : 22, loss : 0.046150220849805225ITERATION : 23, loss : 0.046149465595792936ITERATION : 24, loss : 0.04614894343136368ITERATION : 25, loss : 0.04614858224370778ITERATION : 26, loss : 0.04614833222267478ITERATION : 27, loss : 0.046148159112100344ITERATION : 28, loss : 0.04614803909275344ITERATION : 29, loss : 0.04614795576806482ITERATION : 30, loss : 0.046147897856220824ITERATION : 31, loss : 0.04614785771863083ITERATION : 32, loss : 0.04614782980232372ITERATION : 33, loss : 0.04614781024691517ITERATION : 34, loss : 0.0461477968169904ITERATION : 35, loss : 0.046147787395241016ITERATION : 36, loss : 0.04614778077825931ITERATION : 37, loss : 0.046147776200807025ITERATION : 38, loss : 0.04614777293537986ITERATION : 39, loss : 0.04614777064389579ITERATION : 40, loss : 0.04614776887990152ITERATION : 41, loss : 0.04614776772797094ITERATION : 42, loss : 0.046147766944550604ITERATION : 43, loss : 0.046147766347830375ITERATION : 44, loss : 0.046147765989626825ITERATION : 45, loss : 0.04614776576658642ITERATION : 46, loss : 0.04614776556238231ITERATION : 47, loss : 0.046147765393050925ITERATION : 48, loss : 0.04614776539133835ITERATION : 49, loss : 0.04614776539133835ITERATION : 50, loss : 0.04614776539133835ITERATION : 51, loss : 0.04614776539133835ITERATION : 52, loss : 0.04614776539133835ITERATION : 53, loss : 0.04614776539133835ITERATION : 54, loss : 0.04614776539133835ITERATION : 55, loss : 0.04614776539133835ITERATION : 56, loss : 0.04614776539133835ITERATION : 57, loss : 0.04614776539133835ITERATION : 58, loss : 0.04614776539133835ITERATION : 59, loss : 0.04614776539133835ITERATION : 60, loss : 0.04614776539133835ITERATION : 61, loss : 0.04614776539133835ITERATION : 62, loss : 0.04614776539133835ITERATION : 63, loss : 0.04614776539133835ITERATION : 64, loss : 0.04614776539133835ITERATION : 65, loss : 0.04614776539133835ITERATION : 66, loss : 0.04614776539133835ITERATION : 67, loss : 0.04614776539133835ITERATION : 68, loss : 0.04614776539133835ITERATION : 69, loss : 0.04614776539133835ITERATION : 70, loss : 0.04614776539133835ITERATION : 71, loss : 0.04614776539133835ITERATION : 72, loss : 0.04614776539133835ITERATION : 73, loss : 0.04614776539133835ITERATION : 74, loss : 0.04614776539133835ITERATION : 75, loss : 0.04614776539133835ITERATION : 76, loss : 0.04614776539133835ITERATION : 77, loss : 0.04614776539133835ITERATION : 78, loss : 0.04614776539133835ITERATION : 79, loss : 0.04614776539133835ITERATION : 80, loss : 0.04614776539133835ITERATION : 81, loss : 0.04614776539133835ITERATION : 82, loss : 0.04614776539133835ITERATION : 83, loss : 0.04614776539133835ITERATION : 84, loss : 0.04614776539133835ITERATION : 85, loss : 0.04614776539133835ITERATION : 86, loss : 0.04614776539133835ITERATION : 87, loss : 0.04614776539133835ITERATION : 88, loss : 0.04614776539133835ITERATION : 89, loss : 0.04614776539133835ITERATION : 90, loss : 0.04614776539133835ITERATION : 91, loss : 0.04614776539133835ITERATION : 92, loss : 0.04614776539133835ITERATION : 93, loss : 0.04614776539133835ITERATION : 94, loss : 0.04614776539133835ITERATION : 95, loss : 0.04614776539133835ITERATION : 96, loss : 0.04614776539133835ITERATION : 97, loss : 0.04614776539133835ITERATION : 98, loss : 0.04614776539133835ITERATION : 99, loss : 0.04614776539133835ITERATION : 100, loss : 0.04614776539133835
ITERATION : 1, loss : 0.024827674328562782ITERATION : 2, loss : 0.021241872581834718ITERATION : 3, loss : 0.02001721080135238ITERATION : 4, loss : 0.02088831182832153ITERATION : 5, loss : 0.018741705297161678ITERATION : 6, loss : 0.01744680865042376ITERATION : 7, loss : 0.016644682574086325ITERATION : 8, loss : 0.016134434894145382ITERATION : 9, loss : 0.015801988948364445ITERATION : 10, loss : 0.015580987436353102ITERATION : 11, loss : 0.015431692562439014ITERATION : 12, loss : 0.015329581967749062ITERATION : 13, loss : 0.015259090929443843ITERATION : 14, loss : 0.01521009351768895ITERATION : 15, loss : 0.015175865898614025ITERATION : 16, loss : 0.015151869994452293ITERATION : 17, loss : 0.015135004198967603ITERATION : 18, loss : 0.015123128742003266ITERATION : 19, loss : 0.015114756507311847ITERATION : 20, loss : 0.015108849014608398ITERATION : 21, loss : 0.015104678182596445ITERATION : 22, loss : 0.015101732340201443ITERATION : 23, loss : 0.01509965133754135ITERATION : 24, loss : 0.015098180920524906ITERATION : 25, loss : 0.015097142024384395ITERATION : 26, loss : 0.015096407874093552ITERATION : 27, loss : 0.015095889099458613ITERATION : 28, loss : 0.01509552250824067ITERATION : 29, loss : 0.015095263499155989ITERATION : 30, loss : 0.01509508058038498ITERATION : 31, loss : 0.015094951335710127ITERATION : 32, loss : 0.015094860052302782ITERATION : 33, loss : 0.015094795545241029ITERATION : 34, loss : 0.015094749996241705ITERATION : 35, loss : 0.015094717866402034ITERATION : 36, loss : 0.015094695195869904ITERATION : 37, loss : 0.015094679157514265ITERATION : 38, loss : 0.01509466785126935ITERATION : 39, loss : 0.015094659832066731ITERATION : 40, loss : 0.0150946542011171ITERATION : 41, loss : 0.015094650194909756ITERATION : 42, loss : 0.015094647354733386ITERATION : 43, loss : 0.015094645337745545ITERATION : 44, loss : 0.0150946439032872ITERATION : 45, loss : 0.015094642925942179ITERATION : 46, loss : 0.01509464226614722ITERATION : 47, loss : 0.01509464176909324ITERATION : 48, loss : 0.015094641426449543ITERATION : 49, loss : 0.015094641202993848ITERATION : 50, loss : 0.015094641079795686ITERATION : 51, loss : 0.015094640935593224ITERATION : 52, loss : 0.015094640933344377ITERATION : 53, loss : 0.015094640933344377ITERATION : 54, loss : 0.015094640933344377ITERATION : 55, loss : 0.015094640933344377ITERATION : 56, loss : 0.015094640933344377ITERATION : 57, loss : 0.015094640933344377ITERATION : 58, loss : 0.015094640933344377ITERATION : 59, loss : 0.015094640933344377ITERATION : 60, loss : 0.015094640933344377ITERATION : 61, loss : 0.015094640933344377ITERATION : 62, loss : 0.015094640933344377ITERATION : 63, loss : 0.015094640933344377ITERATION : 64, loss : 0.015094640933344377ITERATION : 65, loss : 0.015094640933344377ITERATION : 66, loss : 0.015094640933344377ITERATION : 67, loss : 0.015094640933344377ITERATION : 68, loss : 0.015094640933344377ITERATION : 69, loss : 0.015094640933344377ITERATION : 70, loss : 0.015094640933344377ITERATION : 71, loss : 0.015094640933344377ITERATION : 72, loss : 0.015094640933344377ITERATION : 73, loss : 0.015094640933344377ITERATION : 74, loss : 0.015094640933344377ITERATION : 75, loss : 0.015094640933344377ITERATION : 76, loss : 0.015094640933344377ITERATION : 77, loss : 0.015094640933344377ITERATION : 78, loss : 0.015094640933344377ITERATION : 79, loss : 0.015094640933344377ITERATION : 80, loss : 0.015094640933344377ITERATION : 81, loss : 0.015094640933344377ITERATION : 82, loss : 0.015094640933344377ITERATION : 83, loss : 0.015094640933344377ITERATION : 84, loss : 0.015094640933344377ITERATION : 85, loss : 0.015094640933344377ITERATION : 86, loss : 0.015094640933344377ITERATION : 87, loss : 0.015094640933344377ITERATION : 88, loss : 0.015094640933344377ITERATION : 89, loss : 0.015094640933344377ITERATION : 90, loss : 0.015094640933344377ITERATION : 91, loss : 0.015094640933344377ITERATION : 92, loss : 0.015094640933344377ITERATION : 93, loss : 0.015094640933344377ITERATION : 94, loss : 0.015094640933344377ITERATION : 95, loss : 0.015094640933344377ITERATION : 96, loss : 0.015094640933344377ITERATION : 97, loss : 0.015094640933344377ITERATION : 98, loss : 0.015094640933344377ITERATION : 99, loss : 0.015094640933344377ITERATION : 100, loss : 0.015094640933344377
ITERATION : 1, loss : 0.05100169070163568ITERATION : 2, loss : 0.05130776112397956ITERATION : 3, loss : 0.04954306676410436ITERATION : 4, loss : 0.049770947252364243ITERATION : 5, loss : 0.05054117309982356ITERATION : 6, loss : 0.051357399369016885ITERATION : 7, loss : 0.051584647237255435ITERATION : 8, loss : 0.05147786301719066ITERATION : 9, loss : 0.05138977866701513ITERATION : 10, loss : 0.051318070153908414ITERATION : 11, loss : 0.05126049917720348ITERATION : 12, loss : 0.0512149075184807ITERATION : 13, loss : 0.05117926406418547ITERATION : 14, loss : 0.05116752340016611ITERATION : 15, loss : 0.05116993747431869ITERATION : 16, loss : 0.05116996943461605ITERATION : 17, loss : 0.051168877527514145ITERATION : 18, loss : 0.05116736583020209ITERATION : 19, loss : 0.051165807424893135ITERATION : 20, loss : 0.05116438180085591ITERATION : 21, loss : 0.05116316012093487ITERATION : 22, loss : 0.05116215547072324ITERATION : 23, loss : 0.05116135232927223ITERATION : 24, loss : 0.051160723283498175ITERATION : 25, loss : 0.0511602386584455ITERATION : 26, loss : 0.05115986972781908ITERATION : 27, loss : 0.051159591708439825ITERATION : 28, loss : 0.051159383812577064ITERATION : 29, loss : 0.05115922946197106ITERATION : 30, loss : 0.05115911526283745ITERATION : 31, loss : 0.05115903139012274ITERATION : 32, loss : 0.05115897034482865ITERATION : 33, loss : 0.05115892562087529ITERATION : 34, loss : 0.05115889308276925ITERATION : 35, loss : 0.051158869743858026ITERATION : 36, loss : 0.05115885292388579ITERATION : 37, loss : 0.05115884069279719ITERATION : 38, loss : 0.05115883185820745ITERATION : 39, loss : 0.051158825422299006ITERATION : 40, loss : 0.05115882080596852ITERATION : 41, loss : 0.051158817436222845ITERATION : 42, loss : 0.05115881505990031ITERATION : 43, loss : 0.05115881332911871ITERATION : 44, loss : 0.051158812094001374ITERATION : 45, loss : 0.051158811182255404ITERATION : 46, loss : 0.05115881053846147ITERATION : 47, loss : 0.05115881014613619ITERATION : 48, loss : 0.051158809761026ITERATION : 49, loss : 0.051158809585239176ITERATION : 50, loss : 0.051158809353572456ITERATION : 51, loss : 0.051158809353572456ITERATION : 52, loss : 0.051158809353572456ITERATION : 53, loss : 0.051158809353572456ITERATION : 54, loss : 0.051158809353572456ITERATION : 55, loss : 0.051158809353572456ITERATION : 56, loss : 0.051158809353572456ITERATION : 57, loss : 0.051158809353572456ITERATION : 58, loss : 0.051158809353572456ITERATION : 59, loss : 0.051158809353572456ITERATION : 60, loss : 0.051158809353572456ITERATION : 61, loss : 0.051158809353572456ITERATION : 62, loss : 0.051158809353572456ITERATION : 63, loss : 0.051158809353572456ITERATION : 64, loss : 0.051158809353572456ITERATION : 65, loss : 0.051158809353572456ITERATION : 66, loss : 0.051158809353572456ITERATION : 67, loss : 0.051158809353572456ITERATION : 68, loss : 0.051158809353572456ITERATION : 69, loss : 0.051158809353572456ITERATION : 70, loss : 0.051158809353572456ITERATION : 71, loss : 0.051158809353572456ITERATION : 72, loss : 0.051158809353572456ITERATION : 73, loss : 0.051158809353572456ITERATION : 74, loss : 0.051158809353572456ITERATION : 75, loss : 0.051158809353572456ITERATION : 76, loss : 0.051158809353572456ITERATION : 77, loss : 0.051158809353572456ITERATION : 78, loss : 0.051158809353572456ITERATION : 79, loss : 0.051158809353572456ITERATION : 80, loss : 0.051158809353572456ITERATION : 81, loss : 0.051158809353572456ITERATION : 82, loss : 0.051158809353572456ITERATION : 83, loss : 0.051158809353572456ITERATION : 84, loss : 0.051158809353572456ITERATION : 85, loss : 0.051158809353572456ITERATION : 86, loss : 0.051158809353572456ITERATION : 87, loss : 0.051158809353572456ITERATION : 88, loss : 0.051158809353572456ITERATION : 89, loss : 0.051158809353572456ITERATION : 90, loss : 0.051158809353572456ITERATION : 91, loss : 0.051158809353572456ITERATION : 92, loss : 0.051158809353572456ITERATION : 93, loss : 0.051158809353572456ITERATION : 94, loss : 0.051158809353572456ITERATION : 95, loss : 0.051158809353572456ITERATION : 96, loss : 0.051158809353572456ITERATION : 97, loss : 0.051158809353572456ITERATION : 98, loss : 0.051158809353572456ITERATION : 99, loss : 0.051158809353572456ITERATION : 100, loss : 0.051158809353572456
ITERATION : 1, loss : 0.02025040132201707ITERATION : 2, loss : 0.01718633862536381ITERATION : 3, loss : 0.017937302581746962ITERATION : 4, loss : 0.017537336302162417ITERATION : 5, loss : 0.017340293886796376ITERATION : 6, loss : 0.01724403303087878ITERATION : 7, loss : 0.01720043924246726ITERATION : 8, loss : 0.017184407612392573ITERATION : 9, loss : 0.01718243659093988ITERATION : 10, loss : 0.017187203277204115ITERATION : 11, loss : 0.01719475661227281ITERATION : 12, loss : 0.017203011409997748ITERATION : 13, loss : 0.017210920876937676ITERATION : 14, loss : 0.017218012546221996ITERATION : 15, loss : 0.01722412536891881ITERATION : 16, loss : 0.017229261027018286ITERATION : 17, loss : 0.01723349934435115ITERATION : 18, loss : 0.01723695199627622ITERATION : 19, loss : 0.017239737214376424ITERATION : 20, loss : 0.017241966784321713ITERATION : 21, loss : 0.017243740697015883ITERATION : 22, loss : 0.017245145064659167ITERATION : 23, loss : 0.017246252149785098ITERATION : 24, loss : 0.017247122017358618ITERATION : 25, loss : 0.017247803252951405ITERATION : 26, loss : 0.017248335424187033ITERATION : 27, loss : 0.01724875003218974ITERATION : 28, loss : 0.01724907258631973ITERATION : 29, loss : 0.017249322969106723ITERATION : 30, loss : 0.017249516854464995ITERATION : 31, loss : 0.017249666984513863ITERATION : 32, loss : 0.01724978281200038ITERATION : 33, loss : 0.0172498721817044ITERATION : 34, loss : 0.01724994118831581ITERATION : 35, loss : 0.017249994212516346ITERATION : 36, loss : 0.017250035151399508ITERATION : 37, loss : 0.017250066429499892ITERATION : 38, loss : 0.017250090515182035ITERATION : 39, loss : 0.017250108997123585ITERATION : 40, loss : 0.017250123106587304ITERATION : 41, loss : 0.01725013390996885ITERATION : 42, loss : 0.017250142199061697ITERATION : 43, loss : 0.01725014854235684ITERATION : 44, loss : 0.01725015340837929ITERATION : 45, loss : 0.01725015714618291ITERATION : 46, loss : 0.017250160002698536ITERATION : 47, loss : 0.01725016218761189ITERATION : 48, loss : 0.017250163893497452ITERATION : 49, loss : 0.017250165170067375ITERATION : 50, loss : 0.017250166128891083ITERATION : 51, loss : 0.017250166882598953ITERATION : 52, loss : 0.01725016745151744ITERATION : 53, loss : 0.01725016789054873ITERATION : 54, loss : 0.01725016820657263ITERATION : 55, loss : 0.017250168462241585ITERATION : 56, loss : 0.017250168652945614ITERATION : 57, loss : 0.017250168810468047ITERATION : 58, loss : 0.01725016892103021ITERATION : 59, loss : 0.017250168992469995ITERATION : 60, loss : 0.017250169035426963ITERATION : 61, loss : 0.017250169099698353ITERATION : 62, loss : 0.017250169122538492ITERATION : 63, loss : 0.01725016912717927ITERATION : 64, loss : 0.01725016912717927ITERATION : 65, loss : 0.01725016912717927ITERATION : 66, loss : 0.01725016912717927ITERATION : 67, loss : 0.01725016912717927ITERATION : 68, loss : 0.01725016912717927ITERATION : 69, loss : 0.01725016912717927ITERATION : 70, loss : 0.01725016912717927ITERATION : 71, loss : 0.01725016912717927ITERATION : 72, loss : 0.01725016912717927ITERATION : 73, loss : 0.01725016912717927ITERATION : 74, loss : 0.01725016912717927ITERATION : 75, loss : 0.01725016912717927ITERATION : 76, loss : 0.01725016912717927ITERATION : 77, loss : 0.01725016912717927ITERATION : 78, loss : 0.01725016912717927ITERATION : 79, loss : 0.01725016912717927ITERATION : 80, loss : 0.01725016912717927ITERATION : 81, loss : 0.01725016912717927ITERATION : 82, loss : 0.01725016912717927ITERATION : 83, loss : 0.01725016912717927ITERATION : 84, loss : 0.01725016912717927ITERATION : 85, loss : 0.01725016912717927ITERATION : 86, loss : 0.01725016912717927ITERATION : 87, loss : 0.01725016912717927ITERATION : 88, loss : 0.01725016912717927ITERATION : 89, loss : 0.01725016912717927ITERATION : 90, loss : 0.01725016912717927ITERATION : 91, loss : 0.01725016912717927ITERATION : 92, loss : 0.01725016912717927ITERATION : 93, loss : 0.01725016912717927ITERATION : 94, loss : 0.01725016912717927ITERATION : 95, loss : 0.01725016912717927ITERATION : 96, loss : 0.01725016912717927ITERATION : 97, loss : 0.01725016912717927ITERATION : 98, loss : 0.01725016912717927ITERATION : 99, loss : 0.01725016912717927ITERATION : 100, loss : 0.01725016912717927
ITERATION : 1, loss : 0.03027694871198581ITERATION : 2, loss : 0.030717719371822442ITERATION : 3, loss : 0.032342782572297ITERATION : 4, loss : 0.03403689327703858ITERATION : 5, loss : 0.03546588723342063ITERATION : 6, loss : 0.03659643924394464ITERATION : 7, loss : 0.03746953136958131ITERATION : 8, loss : 0.038132392233314855ITERATION : 9, loss : 0.038630220773632956ITERATION : 10, loss : 0.03900139946004163ITERATION : 11, loss : 0.03927675713435173ITERATION : 12, loss : 0.03948030018939959ITERATION : 13, loss : 0.039630369938930106ITERATION : 14, loss : 0.039740805682565425ITERATION : 15, loss : 0.03982196160312138ITERATION : 16, loss : 0.03988153876158792ITERATION : 17, loss : 0.03992524067937174ITERATION : 18, loss : 0.03995727860936596ITERATION : 19, loss : 0.03998075511564012ITERATION : 20, loss : 0.0399979520594528ITERATION : 21, loss : 0.04001054553378007ITERATION : 22, loss : 0.0400197659594026ITERATION : 23, loss : 0.04002651557089913ITERATION : 24, loss : 0.040031455691946806ITERATION : 25, loss : 0.04003507109750377ITERATION : 26, loss : 0.040037716708880386ITERATION : 27, loss : 0.04003965247052999ITERATION : 28, loss : 0.040041068755653164ITERATION : 29, loss : 0.04004210484127584ITERATION : 30, loss : 0.040042862787819336ITERATION : 31, loss : 0.04004341718691909ITERATION : 32, loss : 0.04004382270742392ITERATION : 33, loss : 0.040044119288032094ITERATION : 34, loss : 0.04004433623158307ITERATION : 35, loss : 0.040044494888036ITERATION : 36, loss : 0.040044610898244924ITERATION : 37, loss : 0.04004469574990006ITERATION : 38, loss : 0.04004475777804153ITERATION : 39, loss : 0.04004480311753325ITERATION : 40, loss : 0.04004483629183658ITERATION : 41, loss : 0.040044860543330264ITERATION : 42, loss : 0.04004487827488998ITERATION : 43, loss : 0.040044891239009874ITERATION : 44, loss : 0.04004490071118363ITERATION : 45, loss : 0.04004490762275988ITERATION : 46, loss : 0.04004491266614053ITERATION : 47, loss : 0.04004491633721267ITERATION : 48, loss : 0.04004491902797877ITERATION : 49, loss : 0.04004492102297456ITERATION : 50, loss : 0.04004492243410737ITERATION : 51, loss : 0.040044923485932346ITERATION : 52, loss : 0.04004492421878801ITERATION : 53, loss : 0.04004492474102354ITERATION : 54, loss : 0.04004492516788511ITERATION : 55, loss : 0.04004492544615812ITERATION : 56, loss : 0.04004492564724799ITERATION : 57, loss : 0.04004492582441333ITERATION : 58, loss : 0.04004492591778414ITERATION : 59, loss : 0.04004492602338608ITERATION : 60, loss : 0.04004492606223181ITERATION : 61, loss : 0.04004492609220311ITERATION : 62, loss : 0.040044926117099124ITERATION : 63, loss : 0.04004492614934732ITERATION : 64, loss : 0.04004492615280851ITERATION : 65, loss : 0.04004492615280851ITERATION : 66, loss : 0.04004492615280851ITERATION : 67, loss : 0.04004492615280851ITERATION : 68, loss : 0.04004492615280851ITERATION : 69, loss : 0.04004492615280851ITERATION : 70, loss : 0.04004492615280851ITERATION : 71, loss : 0.04004492615280851ITERATION : 72, loss : 0.04004492615280851ITERATION : 73, loss : 0.04004492615280851ITERATION : 74, loss : 0.04004492615280851ITERATION : 75, loss : 0.04004492615280851ITERATION : 76, loss : 0.04004492615280851ITERATION : 77, loss : 0.04004492615280851ITERATION : 78, loss : 0.04004492615280851ITERATION : 79, loss : 0.04004492615280851ITERATION : 80, loss : 0.04004492615280851ITERATION : 81, loss : 0.04004492615280851ITERATION : 82, loss : 0.04004492615280851ITERATION : 83, loss : 0.04004492615280851ITERATION : 84, loss : 0.04004492615280851ITERATION : 85, loss : 0.04004492615280851ITERATION : 86, loss : 0.04004492615280851ITERATION : 87, loss : 0.04004492615280851ITERATION : 88, loss : 0.04004492615280851ITERATION : 89, loss : 0.04004492615280851ITERATION : 90, loss : 0.04004492615280851ITERATION : 91, loss : 0.04004492615280851ITERATION : 92, loss : 0.04004492615280851ITERATION : 93, loss : 0.04004492615280851ITERATION : 94, loss : 0.04004492615280851ITERATION : 95, loss : 0.04004492615280851ITERATION : 96, loss : 0.04004492615280851ITERATION : 97, loss : 0.04004492615280851ITERATION : 98, loss : 0.04004492615280851ITERATION : 99, loss : 0.04004492615280851ITERATION : 100, loss : 0.04004492615280851
ITERATION : 1, loss : 0.013394338119590824ITERATION : 2, loss : 0.011252683569280362ITERATION : 3, loss : 0.010808720236787667ITERATION : 4, loss : 0.010789631321197403ITERATION : 5, loss : 0.010869507555814647ITERATION : 6, loss : 0.010960700835424312ITERATION : 7, loss : 0.011039759049803812ITERATION : 8, loss : 0.011102189532383821ITERATION : 9, loss : 0.011149330819693947ITERATION : 10, loss : 0.011184039375655671ITERATION : 11, loss : 0.011209199880985168ITERATION : 12, loss : 0.01122725622130646ITERATION : 13, loss : 0.011240127427849097ITERATION : 14, loss : 0.011249260452620619ITERATION : 15, loss : 0.01125572039546707ITERATION : 16, loss : 0.011260279493833358ITERATION : 17, loss : 0.01126349200808052ITERATION : 18, loss : 0.011265753146637612ITERATION : 19, loss : 0.011267343431984942ITERATION : 20, loss : 0.011268461247675986ITERATION : 21, loss : 0.011269246605816659ITERATION : 22, loss : 0.01126979825446983ITERATION : 23, loss : 0.011270185646969584ITERATION : 24, loss : 0.01127045765956161ITERATION : 25, loss : 0.011270648614720245ITERATION : 26, loss : 0.01127078269263588ITERATION : 27, loss : 0.01127087680163542ITERATION : 28, loss : 0.01127094287518109ITERATION : 29, loss : 0.011270989249155127ITERATION : 30, loss : 0.011271021806915823ITERATION : 31, loss : 0.011271044644424648ITERATION : 32, loss : 0.011271060689246923ITERATION : 33, loss : 0.011271071945042267ITERATION : 34, loss : 0.011271079843816291ITERATION : 35, loss : 0.011271085395310933ITERATION : 36, loss : 0.01127108929368877ITERATION : 37, loss : 0.011271092030955282ITERATION : 38, loss : 0.011271093944699054ITERATION : 39, loss : 0.011271095288378363ITERATION : 40, loss : 0.0112710962285016ITERATION : 41, loss : 0.011271096882068406ITERATION : 42, loss : 0.011271097341395857ITERATION : 43, loss : 0.01127109765432307ITERATION : 44, loss : 0.011271097886745587ITERATION : 45, loss : 0.011271098044406626ITERATION : 46, loss : 0.011271098148831293ITERATION : 47, loss : 0.011271098219714695ITERATION : 48, loss : 0.011271098276542196ITERATION : 49, loss : 0.011271098307516249ITERATION : 50, loss : 0.011271098329217286ITERATION : 51, loss : 0.011271098344588415ITERATION : 52, loss : 0.011271098365732844ITERATION : 53, loss : 0.011271098377488298ITERATION : 54, loss : 0.01127109838103936ITERATION : 55, loss : 0.01127109838103936ITERATION : 56, loss : 0.01127109838103936ITERATION : 57, loss : 0.01127109838103936ITERATION : 58, loss : 0.01127109838103936ITERATION : 59, loss : 0.01127109838103936ITERATION : 60, loss : 0.01127109838103936ITERATION : 61, loss : 0.01127109838103936ITERATION : 62, loss : 0.01127109838103936ITERATION : 63, loss : 0.01127109838103936ITERATION : 64, loss : 0.01127109838103936ITERATION : 65, loss : 0.01127109838103936ITERATION : 66, loss : 0.01127109838103936ITERATION : 67, loss : 0.01127109838103936ITERATION : 68, loss : 0.01127109838103936ITERATION : 69, loss : 0.01127109838103936ITERATION : 70, loss : 0.01127109838103936ITERATION : 71, loss : 0.01127109838103936ITERATION : 72, loss : 0.01127109838103936ITERATION : 73, loss : 0.01127109838103936ITERATION : 74, loss : 0.01127109838103936ITERATION : 75, loss : 0.01127109838103936ITERATION : 76, loss : 0.01127109838103936ITERATION : 77, loss : 0.01127109838103936ITERATION : 78, loss : 0.01127109838103936ITERATION : 79, loss : 0.01127109838103936ITERATION : 80, loss : 0.01127109838103936ITERATION : 81, loss : 0.01127109838103936ITERATION : 82, loss : 0.01127109838103936ITERATION : 83, loss : 0.01127109838103936ITERATION : 84, loss : 0.01127109838103936ITERATION : 85, loss : 0.01127109838103936ITERATION : 86, loss : 0.01127109838103936ITERATION : 87, loss : 0.01127109838103936ITERATION : 88, loss : 0.01127109838103936ITERATION : 89, loss : 0.01127109838103936ITERATION : 90, loss : 0.01127109838103936ITERATION : 91, loss : 0.01127109838103936ITERATION : 92, loss : 0.01127109838103936ITERATION : 93, loss : 0.01127109838103936ITERATION : 94, loss : 0.01127109838103936ITERATION : 95, loss : 0.01127109838103936ITERATION : 96, loss : 0.01127109838103936ITERATION : 97, loss : 0.01127109838103936ITERATION : 98, loss : 0.01127109838103936ITERATION : 99, loss : 0.01127109838103936ITERATION : 100, loss : 0.01127109838103936
ITERATION : 1, loss : 0.042117353063461045ITERATION : 2, loss : 0.04031127616013188ITERATION : 3, loss : 0.040865813191644534ITERATION : 4, loss : 0.041454043318613465ITERATION : 5, loss : 0.041880109651618515ITERATION : 6, loss : 0.04217261986326044ITERATION : 7, loss : 0.042372997389909094ITERATION : 8, loss : 0.04251186639196668ITERATION : 9, loss : 0.04260915393307531ITERATION : 10, loss : 0.04267791099234588ITERATION : 11, loss : 0.0427268383615424ITERATION : 12, loss : 0.04276184012112344ITERATION : 13, loss : 0.042786983104659136ITERATION : 14, loss : 0.042805102208979956ITERATION : 15, loss : 0.042818192601482265ITERATION : 16, loss : 0.04282766894505871ITERATION : 17, loss : 0.04283453996085905ITERATION : 18, loss : 0.042839528537352504ITERATION : 19, loss : 0.04284315424447622ITERATION : 20, loss : 0.042845791704612224ITERATION : 21, loss : 0.042847711792541446ITERATION : 22, loss : 0.04284911050436614ITERATION : 23, loss : 0.04285013004651567ITERATION : 24, loss : 0.042850873648312214ITERATION : 25, loss : 0.04285141618164979ITERATION : 26, loss : 0.04285181220948562ITERATION : 27, loss : 0.04285210134143809ITERATION : 28, loss : 0.042852312551643755ITERATION : 29, loss : 0.042852466901625544ITERATION : 30, loss : 0.04285257971433031ITERATION : 31, loss : 0.042852662229110446ITERATION : 32, loss : 0.042852722559138946ITERATION : 33, loss : 0.042852766703428954ITERATION : 34, loss : 0.04285279901898284ITERATION : 35, loss : 0.042852822708074306ITERATION : 36, loss : 0.04285284001988472ITERATION : 37, loss : 0.04285285269540528ITERATION : 38, loss : 0.042852861985284464ITERATION : 39, loss : 0.04285286879237441ITERATION : 40, loss : 0.04285287379534495ITERATION : 41, loss : 0.04285287744975768ITERATION : 42, loss : 0.0428528801301038ITERATION : 43, loss : 0.04285288208465192ITERATION : 44, loss : 0.04285288348876959ITERATION : 45, loss : 0.04285288455097532ITERATION : 46, loss : 0.042852885354312586ITERATION : 47, loss : 0.04285288592962119ITERATION : 48, loss : 0.042852886327489285ITERATION : 49, loss : 0.042852886582621694ITERATION : 50, loss : 0.04285288680950046ITERATION : 51, loss : 0.04285288693644004ITERATION : 52, loss : 0.042852887055984494ITERATION : 53, loss : 0.04285288714253497ITERATION : 54, loss : 0.04285288724159298ITERATION : 55, loss : 0.04285288726958846ITERATION : 56, loss : 0.04285288729368572ITERATION : 57, loss : 0.042852887294960756ITERATION : 58, loss : 0.042852887294960756ITERATION : 59, loss : 0.042852887294960756ITERATION : 60, loss : 0.042852887294960756ITERATION : 61, loss : 0.042852887294960756ITERATION : 62, loss : 0.042852887294960756ITERATION : 63, loss : 0.042852887294960756ITERATION : 64, loss : 0.042852887294960756ITERATION : 65, loss : 0.042852887294960756ITERATION : 66, loss : 0.042852887294960756ITERATION : 67, loss : 0.042852887294960756ITERATION : 68, loss : 0.042852887294960756ITERATION : 69, loss : 0.042852887294960756ITERATION : 70, loss : 0.042852887294960756ITERATION : 71, loss : 0.042852887294960756ITERATION : 72, loss : 0.042852887294960756ITERATION : 73, loss : 0.042852887294960756ITERATION : 74, loss : 0.042852887294960756ITERATION : 75, loss : 0.042852887294960756ITERATION : 76, loss : 0.042852887294960756ITERATION : 77, loss : 0.042852887294960756ITERATION : 78, loss : 0.042852887294960756ITERATION : 79, loss : 0.042852887294960756ITERATION : 80, loss : 0.042852887294960756ITERATION : 81, loss : 0.042852887294960756ITERATION : 82, loss : 0.042852887294960756ITERATION : 83, loss : 0.042852887294960756ITERATION : 84, loss : 0.042852887294960756ITERATION : 85, loss : 0.042852887294960756ITERATION : 86, loss : 0.042852887294960756ITERATION : 87, loss : 0.042852887294960756ITERATION : 88, loss : 0.042852887294960756ITERATION : 89, loss : 0.042852887294960756ITERATION : 90, loss : 0.042852887294960756ITERATION : 91, loss : 0.042852887294960756ITERATION : 92, loss : 0.042852887294960756ITERATION : 93, loss : 0.042852887294960756ITERATION : 94, loss : 0.042852887294960756ITERATION : 95, loss : 0.042852887294960756ITERATION : 96, loss : 0.042852887294960756ITERATION : 97, loss : 0.042852887294960756ITERATION : 98, loss : 0.042852887294960756ITERATION : 99, loss : 0.042852887294960756ITERATION : 100, loss : 0.042852887294960756
gradient norm in None layer : 0.0010549815070955131
gradient norm in None layer : 4.122919594001453e-05
gradient norm in None layer : 2.483582079423952e-05
gradient norm in None layer : 0.0005714166712214037
gradient norm in None layer : 4.647353578081899e-05
gradient norm in None layer : 3.702245943862013e-05
gradient norm in None layer : 0.0002408102397136691
gradient norm in None layer : 9.131175174404082e-06
gradient norm in None layer : 8.14482608750947e-06
gradient norm in None layer : 0.00020959268493132542
gradient norm in None layer : 9.338531778313175e-06
gradient norm in None layer : 7.479485214378673e-06
gradient norm in None layer : 6.837676830246445e-05
gradient norm in None layer : 2.1603802454820976e-06
gradient norm in None layer : 1.7750998392561715e-06
gradient norm in None layer : 6.069050609799826e-05
gradient norm in None layer : 3.0693756239441694e-06
gradient norm in None layer : 1.9443086138091117e-06
gradient norm in None layer : 8.732340724054958e-05
gradient norm in None layer : 3.220764236142587e-06
gradient norm in None layer : 0.00020026011884716346
gradient norm in None layer : 1.2668531618185479e-05
gradient norm in None layer : 1.0443042103224784e-05
gradient norm in None layer : 0.00020918547077958415
gradient norm in None layer : 2.1587361314673156e-05
gradient norm in None layer : 2.8840641401784212e-05
gradient norm in None layer : 0.00033873835848890143
gradient norm in None layer : 7.0734811158139575e-06
gradient norm in None layer : 0.0005895514492116634
gradient norm in None layer : 5.693850532779254e-05
gradient norm in None layer : 5.219622070840503e-05
gradient norm in None layer : 0.0007600427028268373
gradient norm in None layer : 7.77130548574503e-05
gradient norm in None layer : 9.852173560410977e-05
gradient norm in None layer : 6.692833168043407e-05
gradient norm in None layer : 1.4688077871905766e-05
Total gradient norm: 0.0016478604662900872
invariance loss : 4.229452163117353, avg_den : 0.43402099609375, density loss : 0.33402099609375, mse loss : 0.03234669341943491, solver time : 139.13147115707397 sec , total loss : 0.03691016657864602, running loss : 0.04354715648537865
Epoch 0/10 , batch 27/12500 
ITERATION : 1, loss : 0.011229719783382967ITERATION : 2, loss : 0.011975969363726715ITERATION : 3, loss : 0.013880807004646662ITERATION : 4, loss : 0.015754688540604127ITERATION : 5, loss : 0.017393771814931413ITERATION : 6, loss : 0.018744180496236284ITERATION : 7, loss : 0.01877852638505785ITERATION : 8, loss : 0.018286760386358406ITERATION : 9, loss : 0.017951523338003223ITERATION : 10, loss : 0.017719565023700225ITERATION : 11, loss : 0.017557354012012444ITERATION : 12, loss : 0.017443052777926803ITERATION : 13, loss : 0.017362071499098172ITERATION : 14, loss : 0.017304472710936637ITERATION : 15, loss : 0.01726339002096763ITERATION : 16, loss : 0.017234028650160584ITERATION : 17, loss : 0.01721301408211689ITERATION : 18, loss : 0.017197958040470755ITERATION : 19, loss : 0.017187163048784287ITERATION : 20, loss : 0.017179419092445164ITERATION : 21, loss : 0.01717386178054761ITERATION : 22, loss : 0.017169872622736088ITERATION : 23, loss : 0.017167008607931668ITERATION : 24, loss : 0.017164952158260267ITERATION : 25, loss : 0.017163475374074765ITERATION : 26, loss : 0.017162414893258784ITERATION : 27, loss : 0.01716165323397886ITERATION : 28, loss : 0.017161106245487154ITERATION : 29, loss : 0.01716071348422411ITERATION : 30, loss : 0.01716043141858299ITERATION : 31, loss : 0.01716022885080817ITERATION : 32, loss : 0.01716008333870387ITERATION : 33, loss : 0.017159978827392175ITERATION : 34, loss : 0.017159903843492384ITERATION : 35, loss : 0.017159849915668966ITERATION : 36, loss : 0.017159811163442038ITERATION : 37, loss : 0.017159783354828904ITERATION : 38, loss : 0.017159763420755324ITERATION : 39, loss : 0.017159749137701196ITERATION : 40, loss : 0.017159738897168608ITERATION : 41, loss : 0.017159731544616325ITERATION : 42, loss : 0.017159726285986974ITERATION : 43, loss : 0.017159722508934234ITERATION : 44, loss : 0.017159719807645222ITERATION : 45, loss : 0.01715971786941665ITERATION : 46, loss : 0.01715971648404546ITERATION : 47, loss : 0.01715971548986875ITERATION : 48, loss : 0.017159714779850014ITERATION : 49, loss : 0.017159714278288708ITERATION : 50, loss : 0.017159713922708934ITERATION : 51, loss : 0.017159713691506283ITERATION : 52, loss : 0.017159713508303823ITERATION : 53, loss : 0.017159713341214328ITERATION : 54, loss : 0.01715971323658091ITERATION : 55, loss : 0.017159713163880284ITERATION : 56, loss : 0.017159713133749997ITERATION : 57, loss : 0.017159713105457823ITERATION : 58, loss : 0.017159713103756788ITERATION : 59, loss : 0.017159713103756788ITERATION : 60, loss : 0.017159713103756788ITERATION : 61, loss : 0.017159713103756788ITERATION : 62, loss : 0.017159713103756788ITERATION : 63, loss : 0.017159713103756788ITERATION : 64, loss : 0.017159713103756788ITERATION : 65, loss : 0.017159713103756788ITERATION : 66, loss : 0.017159713103756788ITERATION : 67, loss : 0.017159713103756788ITERATION : 68, loss : 0.017159713103756788ITERATION : 69, loss : 0.017159713103756788ITERATION : 70, loss : 0.017159713103756788ITERATION : 71, loss : 0.017159713103756788ITERATION : 72, loss : 0.017159713103756788ITERATION : 73, loss : 0.017159713103756788ITERATION : 74, loss : 0.017159713103756788ITERATION : 75, loss : 0.017159713103756788ITERATION : 76, loss : 0.017159713103756788ITERATION : 77, loss : 0.017159713103756788ITERATION : 78, loss : 0.017159713103756788ITERATION : 79, loss : 0.017159713103756788ITERATION : 80, loss : 0.017159713103756788ITERATION : 81, loss : 0.017159713103756788ITERATION : 82, loss : 0.017159713103756788ITERATION : 83, loss : 0.017159713103756788ITERATION : 84, loss : 0.017159713103756788ITERATION : 85, loss : 0.017159713103756788ITERATION : 86, loss : 0.017159713103756788ITERATION : 87, loss : 0.017159713103756788ITERATION : 88, loss : 0.017159713103756788ITERATION : 89, loss : 0.017159713103756788ITERATION : 90, loss : 0.017159713103756788ITERATION : 91, loss : 0.017159713103756788ITERATION : 92, loss : 0.017159713103756788ITERATION : 93, loss : 0.017159713103756788ITERATION : 94, loss : 0.017159713103756788ITERATION : 95, loss : 0.017159713103756788ITERATION : 96, loss : 0.017159713103756788ITERATION : 97, loss : 0.017159713103756788ITERATION : 98, loss : 0.017159713103756788ITERATION : 99, loss : 0.017159713103756788ITERATION : 100, loss : 0.017159713103756788
ITERATION : 1, loss : 0.02142849667250639ITERATION : 2, loss : 0.020966195624264883ITERATION : 3, loss : 0.02113923502796664ITERATION : 4, loss : 0.021135543885862786ITERATION : 5, loss : 0.021061374839555252ITERATION : 6, loss : 0.020986706505520474ITERATION : 7, loss : 0.020929300762696012ITERATION : 8, loss : 0.02088883952873763ITERATION : 9, loss : 0.020861145889893244ITERATION : 10, loss : 0.02084230038532923ITERATION : 11, loss : 0.020829418472827165ITERATION : 12, loss : 0.02082053778371931ITERATION : 13, loss : 0.02081435741203147ITERATION : 14, loss : 0.020810018270505503ITERATION : 15, loss : 0.020806948671425303ITERATION : 16, loss : 0.020804763764832396ITERATION : 17, loss : 0.02080320100252055ITERATION : 18, loss : 0.020802079077546567ITERATION : 19, loss : 0.02080127128545862ITERATION : 20, loss : 0.02080068856385643ITERATION : 21, loss : 0.020800267538842336ITERATION : 22, loss : 0.020799962977022315ITERATION : 23, loss : 0.02079974245959631ITERATION : 24, loss : 0.02079958272002893ITERATION : 25, loss : 0.020799466967333192ITERATION : 26, loss : 0.020799383046084186ITERATION : 27, loss : 0.02079932220709752ITERATION : 28, loss : 0.020799278081000617ITERATION : 29, loss : 0.02079924604567605ITERATION : 30, loss : 0.020799222817335858ITERATION : 31, loss : 0.020799205941646977ITERATION : 32, loss : 0.020799193718184197ITERATION : 33, loss : 0.020799184826741247ITERATION : 34, loss : 0.0207991783849967ITERATION : 35, loss : 0.020799173715492576ITERATION : 36, loss : 0.02079917030422531ITERATION : 37, loss : 0.020799167853846123ITERATION : 38, loss : 0.020799166053220658ITERATION : 39, loss : 0.020799164773895793ITERATION : 40, loss : 0.020799163822094673ITERATION : 41, loss : 0.020799163152099162ITERATION : 42, loss : 0.020799162646612007ITERATION : 43, loss : 0.02079916229957324ITERATION : 44, loss : 0.020799162028679395ITERATION : 45, loss : 0.020799161851468333ITERATION : 46, loss : 0.02079916170080702ITERATION : 47, loss : 0.02079916161675198ITERATION : 48, loss : 0.02079916153112432ITERATION : 49, loss : 0.020799161491360015ITERATION : 50, loss : 0.020799161446011214ITERATION : 51, loss : 0.020799161425293214ITERATION : 52, loss : 0.020799161400567097ITERATION : 53, loss : 0.02079916139599323ITERATION : 54, loss : 0.020799161375663913ITERATION : 55, loss : 0.020799161375453984ITERATION : 56, loss : 0.020799161364045932ITERATION : 57, loss : 0.020799161363331035ITERATION : 58, loss : 0.020799161365674557ITERATION : 59, loss : 0.020799161365674557ITERATION : 60, loss : 0.020799161365674557ITERATION : 61, loss : 0.020799161365674557ITERATION : 62, loss : 0.020799161365674557ITERATION : 63, loss : 0.020799161365674557ITERATION : 64, loss : 0.020799161365674557ITERATION : 65, loss : 0.020799161365674557ITERATION : 66, loss : 0.020799161365674557ITERATION : 67, loss : 0.020799161365674557ITERATION : 68, loss : 0.020799161365674557ITERATION : 69, loss : 0.020799161365674557ITERATION : 70, loss : 0.020799161365674557ITERATION : 71, loss : 0.020799161365674557ITERATION : 72, loss : 0.020799161365674557ITERATION : 73, loss : 0.020799161365674557ITERATION : 74, loss : 0.020799161365674557ITERATION : 75, loss : 0.020799161365674557ITERATION : 76, loss : 0.020799161365674557ITERATION : 77, loss : 0.020799161365674557ITERATION : 78, loss : 0.020799161365674557ITERATION : 79, loss : 0.020799161365674557ITERATION : 80, loss : 0.020799161365674557ITERATION : 81, loss : 0.020799161365674557ITERATION : 82, loss : 0.020799161365674557ITERATION : 83, loss : 0.020799161365674557ITERATION : 84, loss : 0.020799161365674557ITERATION : 85, loss : 0.020799161365674557ITERATION : 86, loss : 0.020799161365674557ITERATION : 87, loss : 0.020799161365674557ITERATION : 88, loss : 0.020799161365674557ITERATION : 89, loss : 0.020799161365674557ITERATION : 90, loss : 0.020799161365674557ITERATION : 91, loss : 0.020799161365674557ITERATION : 92, loss : 0.020799161365674557ITERATION : 93, loss : 0.020799161365674557ITERATION : 94, loss : 0.020799161365674557ITERATION : 95, loss : 0.020799161365674557ITERATION : 96, loss : 0.020799161365674557ITERATION : 97, loss : 0.020799161365674557ITERATION : 98, loss : 0.020799161365674557ITERATION : 99, loss : 0.020799161365674557ITERATION : 100, loss : 0.020799161365674557
ITERATION : 1, loss : 0.02446288097245643ITERATION : 2, loss : 0.018975016414195ITERATION : 3, loss : 0.016872030656354467ITERATION : 4, loss : 0.016225788559465413ITERATION : 5, loss : 0.016164045900062714ITERATION : 6, loss : 0.01630730908226456ITERATION : 7, loss : 0.016495948720152493ITERATION : 8, loss : 0.016669014570190346ITERATION : 9, loss : 0.01680863349410487ITERATION : 10, loss : 0.01691433196925262ITERATION : 11, loss : 0.016991524340396143ITERATION : 12, loss : 0.01704668364400051ITERATION : 13, loss : 0.0170855624085194ITERATION : 14, loss : 0.017112725966881726ITERATION : 15, loss : 0.01713159647734523ITERATION : 16, loss : 0.017144657373150695ITERATION : 17, loss : 0.017153675542689997ITERATION : 18, loss : 0.017159892921194856ITERATION : 19, loss : 0.017164175404008237ITERATION : 20, loss : 0.017167123537571626ITERATION : 21, loss : 0.01716915254517655ITERATION : 22, loss : 0.017170548903878508ITERATION : 23, loss : 0.01717150988913572ITERATION : 24, loss : 0.017172171337072432ITERATION : 25, loss : 0.01717262671287782ITERATION : 26, loss : 0.017172940279028742ITERATION : 27, loss : 0.01717315627381776ITERATION : 28, loss : 0.01717330511602362ITERATION : 29, loss : 0.017173407705396545ITERATION : 30, loss : 0.01717347844692851ITERATION : 31, loss : 0.017173527259525263ITERATION : 32, loss : 0.01717356094724426ITERATION : 33, loss : 0.017173584185803715ITERATION : 34, loss : 0.01717360024062588ITERATION : 35, loss : 0.01717361132439168ITERATION : 36, loss : 0.017173618990164147ITERATION : 37, loss : 0.017173624289265842ITERATION : 38, loss : 0.01717362796413153ITERATION : 39, loss : 0.017173630485014187ITERATION : 40, loss : 0.017173632242300046ITERATION : 41, loss : 0.017173633455319387ITERATION : 42, loss : 0.017173634296791033ITERATION : 43, loss : 0.01717363486574778ITERATION : 44, loss : 0.01717363527295439ITERATION : 45, loss : 0.01717363556436641ITERATION : 46, loss : 0.01717363575839691ITERATION : 47, loss : 0.017173635889733824ITERATION : 48, loss : 0.01717363597766002ITERATION : 49, loss : 0.017173636020082032ITERATION : 50, loss : 0.017173636047328203ITERATION : 51, loss : 0.01717363610109167ITERATION : 52, loss : 0.017173636108149228ITERATION : 53, loss : 0.01717363610845061ITERATION : 54, loss : 0.01717363610845061ITERATION : 55, loss : 0.01717363610845061ITERATION : 56, loss : 0.01717363610845061ITERATION : 57, loss : 0.01717363610845061ITERATION : 58, loss : 0.01717363610845061ITERATION : 59, loss : 0.01717363610845061ITERATION : 60, loss : 0.01717363610845061ITERATION : 61, loss : 0.01717363610845061ITERATION : 62, loss : 0.01717363610845061ITERATION : 63, loss : 0.01717363610845061ITERATION : 64, loss : 0.01717363610845061ITERATION : 65, loss : 0.01717363610845061ITERATION : 66, loss : 0.01717363610845061ITERATION : 67, loss : 0.01717363610845061ITERATION : 68, loss : 0.01717363610845061ITERATION : 69, loss : 0.01717363610845061ITERATION : 70, loss : 0.01717363610845061ITERATION : 71, loss : 0.01717363610845061ITERATION : 72, loss : 0.01717363610845061ITERATION : 73, loss : 0.01717363610845061ITERATION : 74, loss : 0.01717363610845061ITERATION : 75, loss : 0.01717363610845061ITERATION : 76, loss : 0.01717363610845061ITERATION : 77, loss : 0.01717363610845061ITERATION : 78, loss : 0.01717363610845061ITERATION : 79, loss : 0.01717363610845061ITERATION : 80, loss : 0.01717363610845061ITERATION : 81, loss : 0.01717363610845061ITERATION : 82, loss : 0.01717363610845061ITERATION : 83, loss : 0.01717363610845061ITERATION : 84, loss : 0.01717363610845061ITERATION : 85, loss : 0.01717363610845061ITERATION : 86, loss : 0.01717363610845061ITERATION : 87, loss : 0.01717363610845061ITERATION : 88, loss : 0.01717363610845061ITERATION : 89, loss : 0.01717363610845061ITERATION : 90, loss : 0.01717363610845061ITERATION : 91, loss : 0.01717363610845061ITERATION : 92, loss : 0.01717363610845061ITERATION : 93, loss : 0.01717363610845061ITERATION : 94, loss : 0.01717363610845061ITERATION : 95, loss : 0.01717363610845061ITERATION : 96, loss : 0.01717363610845061ITERATION : 97, loss : 0.01717363610845061ITERATION : 98, loss : 0.01717363610845061ITERATION : 99, loss : 0.01717363610845061ITERATION : 100, loss : 0.01717363610845061
ITERATION : 1, loss : 0.031913790531759724ITERATION : 2, loss : 0.02737222397547747ITERATION : 3, loss : 0.02655797894898401ITERATION : 4, loss : 0.02693364183155998ITERATION : 5, loss : 0.027661211680804935ITERATION : 6, loss : 0.028394549725280403ITERATION : 7, loss : 0.02901343778990285ITERATION : 8, loss : 0.029495776208583516ITERATION : 9, loss : 0.029856161475843187ITERATION : 10, loss : 0.030118963093109286ITERATION : 11, loss : 0.030307809664809313ITERATION : 12, loss : 0.030442275727942107ITERATION : 13, loss : 0.030537464326646963ITERATION : 14, loss : 0.030604595237547057ITERATION : 15, loss : 0.030651822699251506ITERATION : 16, loss : 0.030684994234751357ITERATION : 17, loss : 0.030708268227639512ITERATION : 18, loss : 0.030724586232521344ITERATION : 19, loss : 0.03073602145114165ITERATION : 20, loss : 0.030744032326856137ITERATION : 21, loss : 0.030749642942684695ITERATION : 22, loss : 0.030753571688241103ITERATION : 23, loss : 0.030756322375142465ITERATION : 24, loss : 0.03075824792895269ITERATION : 25, loss : 0.030759595669856865ITERATION : 26, loss : 0.030760538974082475ITERATION : 27, loss : 0.030761198997607486ITERATION : 28, loss : 0.0307616607600599ITERATION : 29, loss : 0.030761983796669737ITERATION : 30, loss : 0.03076220970292777ITERATION : 31, loss : 0.030762367811895924ITERATION : 32, loss : 0.030762478220616916ITERATION : 33, loss : 0.030762555515117987ITERATION : 34, loss : 0.03076260954541193ITERATION : 35, loss : 0.030762647370907257ITERATION : 36, loss : 0.030762673704134897ITERATION : 37, loss : 0.03076269219306156ITERATION : 38, loss : 0.030762705047477714ITERATION : 39, loss : 0.03076271407419845ITERATION : 40, loss : 0.03076272033682889ITERATION : 41, loss : 0.03076272471079085ITERATION : 42, loss : 0.03076272771450102ITERATION : 43, loss : 0.030762729813231863ITERATION : 44, loss : 0.030762731244375716ITERATION : 45, loss : 0.03076273229660435ITERATION : 46, loss : 0.030762732942112233ITERATION : 47, loss : 0.030762733407052868ITERATION : 48, loss : 0.03076273372371432ITERATION : 49, loss : 0.03076273391764589ITERATION : 50, loss : 0.030762734083598118ITERATION : 51, loss : 0.030762734175653648ITERATION : 52, loss : 0.03076273425309501ITERATION : 53, loss : 0.030762734277632108ITERATION : 54, loss : 0.03076273427800831ITERATION : 55, loss : 0.03076273427800831ITERATION : 56, loss : 0.03076273427800831ITERATION : 57, loss : 0.03076273427800831ITERATION : 58, loss : 0.03076273427800831ITERATION : 59, loss : 0.03076273427800831ITERATION : 60, loss : 0.03076273427800831ITERATION : 61, loss : 0.03076273427800831ITERATION : 62, loss : 0.03076273427800831ITERATION : 63, loss : 0.03076273427800831ITERATION : 64, loss : 0.03076273427800831ITERATION : 65, loss : 0.03076273427800831ITERATION : 66, loss : 0.03076273427800831ITERATION : 67, loss : 0.03076273427800831ITERATION : 68, loss : 0.03076273427800831ITERATION : 69, loss : 0.03076273427800831ITERATION : 70, loss : 0.03076273427800831ITERATION : 71, loss : 0.03076273427800831ITERATION : 72, loss : 0.03076273427800831ITERATION : 73, loss : 0.03076273427800831ITERATION : 74, loss : 0.03076273427800831ITERATION : 75, loss : 0.03076273427800831ITERATION : 76, loss : 0.03076273427800831ITERATION : 77, loss : 0.03076273427800831ITERATION : 78, loss : 0.03076273427800831ITERATION : 79, loss : 0.03076273427800831ITERATION : 80, loss : 0.03076273427800831ITERATION : 81, loss : 0.03076273427800831ITERATION : 82, loss : 0.03076273427800831ITERATION : 83, loss : 0.03076273427800831ITERATION : 84, loss : 0.03076273427800831ITERATION : 85, loss : 0.03076273427800831ITERATION : 86, loss : 0.03076273427800831ITERATION : 87, loss : 0.03076273427800831ITERATION : 88, loss : 0.03076273427800831ITERATION : 89, loss : 0.03076273427800831ITERATION : 90, loss : 0.03076273427800831ITERATION : 91, loss : 0.03076273427800831ITERATION : 92, loss : 0.03076273427800831ITERATION : 93, loss : 0.03076273427800831ITERATION : 94, loss : 0.03076273427800831ITERATION : 95, loss : 0.03076273427800831ITERATION : 96, loss : 0.03076273427800831ITERATION : 97, loss : 0.03076273427800831ITERATION : 98, loss : 0.03076273427800831ITERATION : 99, loss : 0.03076273427800831ITERATION : 100, loss : 0.03076273427800831
ITERATION : 1, loss : 0.008872232727515117ITERATION : 2, loss : 0.008857722944067555ITERATION : 3, loss : 0.007736146937129797ITERATION : 4, loss : 0.007193908990136896ITERATION : 5, loss : 0.0069103833879556355ITERATION : 6, loss : 0.006757588666428817ITERATION : 7, loss : 0.00667409597450946ITERATION : 8, loss : 0.006628013191549552ITERATION : 9, loss : 0.00660231255766423ITERATION : 10, loss : 0.006587802979752513ITERATION : 11, loss : 0.006579491911749194ITERATION : 12, loss : 0.006574651060174184ITERATION : 13, loss : 0.0065717784529255195ITERATION : 14, loss : 0.00657003954833653ITERATION : 15, loss : 0.006568965283410353ITERATION : 16, loss : 0.006568288219611864ITERATION : 17, loss : 0.006567853443025919ITERATION : 18, loss : 0.006567569504904492ITERATION : 19, loss : 0.006567381327026072ITERATION : 20, loss : 0.006567255075823076ITERATION : 21, loss : 0.00656716951941998ITERATION : 22, loss : 0.00656711106394592ITERATION : 23, loss : 0.006567070919574178ITERATION : 24, loss : 0.006567043202615215ITERATION : 25, loss : 0.006567024024363134ITERATION : 26, loss : 0.006567010704447223ITERATION : 27, loss : 0.0065670014704801195ITERATION : 28, loss : 0.0065669950212311635ITERATION : 29, loss : 0.006566990547205543ITERATION : 30, loss : 0.00656698743629674ITERATION : 31, loss : 0.006566985276887163ITERATION : 32, loss : 0.006566983769931914ITERATION : 33, loss : 0.006566982744322261ITERATION : 34, loss : 0.006566982013248253ITERATION : 35, loss : 0.006566981506135761ITERATION : 36, loss : 0.006566981161984634ITERATION : 37, loss : 0.006566980921850623ITERATION : 38, loss : 0.006566980761883318ITERATION : 39, loss : 0.006566980654987001ITERATION : 40, loss : 0.006566980586324908ITERATION : 41, loss : 0.006566980535849365ITERATION : 42, loss : 0.006566980510932554ITERATION : 43, loss : 0.006566980487964437ITERATION : 44, loss : 0.00656698047934289ITERATION : 45, loss : 0.00656698047036093ITERATION : 46, loss : 0.006566980466557976ITERATION : 47, loss : 0.0065669804650974216ITERATION : 48, loss : 0.00656698046127718ITERATION : 49, loss : 0.006566980463985078ITERATION : 50, loss : 0.006566980460860259ITERATION : 51, loss : 0.006566980463468644ITERATION : 52, loss : 0.006566980465553011ITERATION : 53, loss : 0.006566980462583826ITERATION : 54, loss : 0.00656698046313074ITERATION : 55, loss : 0.006566980463178831ITERATION : 56, loss : 0.006566980462923069ITERATION : 57, loss : 0.006566980462956111ITERATION : 58, loss : 0.006566980463431391ITERATION : 59, loss : 0.006566980463174515ITERATION : 60, loss : 0.006566980463287714ITERATION : 61, loss : 0.006566980463287714ITERATION : 62, loss : 0.006566980463287714ITERATION : 63, loss : 0.006566980463287714ITERATION : 64, loss : 0.006566980463287714ITERATION : 65, loss : 0.006566980463287714ITERATION : 66, loss : 0.006566980463287714ITERATION : 67, loss : 0.006566980463287714ITERATION : 68, loss : 0.006566980463287714ITERATION : 69, loss : 0.006566980463287714ITERATION : 70, loss : 0.006566980463287714ITERATION : 71, loss : 0.006566980463287714ITERATION : 72, loss : 0.006566980463287714ITERATION : 73, loss : 0.006566980463287714ITERATION : 74, loss : 0.006566980463287714ITERATION : 75, loss : 0.006566980463287714ITERATION : 76, loss : 0.006566980463287714ITERATION : 77, loss : 0.006566980463287714ITERATION : 78, loss : 0.006566980463287714ITERATION : 79, loss : 0.006566980463287714ITERATION : 80, loss : 0.006566980463287714ITERATION : 81, loss : 0.006566980463287714ITERATION : 82, loss : 0.006566980463287714ITERATION : 83, loss : 0.006566980463287714ITERATION : 84, loss : 0.006566980463287714ITERATION : 85, loss : 0.006566980463287714ITERATION : 86, loss : 0.006566980463287714ITERATION : 87, loss : 0.006566980463287714ITERATION : 88, loss : 0.006566980463287714ITERATION : 89, loss : 0.006566980463287714ITERATION : 90, loss : 0.006566980463287714ITERATION : 91, loss : 0.006566980463287714ITERATION : 92, loss : 0.006566980463287714ITERATION : 93, loss : 0.006566980463287714ITERATION : 94, loss : 0.006566980463287714ITERATION : 95, loss : 0.006566980463287714ITERATION : 96, loss : 0.006566980463287714ITERATION : 97, loss : 0.006566980463287714ITERATION : 98, loss : 0.006566980463287714ITERATION : 99, loss : 0.006566980463287714ITERATION : 100, loss : 0.006566980463287714
ITERATION : 1, loss : 0.01847868411541657ITERATION : 2, loss : 0.012556792611488747ITERATION : 3, loss : 0.010906257835767876ITERATION : 4, loss : 0.010262967906353363ITERATION : 5, loss : 0.009936149332881098ITERATION : 6, loss : 0.009742650414691934ITERATION : 7, loss : 0.009618704448232544ITERATION : 8, loss : 0.009535716647020711ITERATION : 9, loss : 0.009478523054954393ITERATION : 10, loss : 0.009438271560404983ITERATION : 11, loss : 0.009409490580450694ITERATION : 12, loss : 0.009388661749108398ITERATION : 13, loss : 0.009373451147604347ITERATION : 14, loss : 0.009362269391713642ITERATION : 15, loss : 0.009354010127805353ITERATION : 16, loss : 0.009347889095948955ITERATION : 17, loss : 0.009343342327071381ITERATION : 18, loss : 0.00933995982939272ITERATION : 19, loss : 0.009337441106261194ITERATION : 20, loss : 0.009335564516008284ITERATION : 21, loss : 0.009334165957142633ITERATION : 22, loss : 0.009333123560019908ITERATION : 23, loss : 0.009332346658410533ITERATION : 24, loss : 0.00933176772029689ITERATION : 25, loss : 0.009331336378882974ITERATION : 26, loss : 0.009331015107276268ITERATION : 27, loss : 0.009330775855222588ITERATION : 28, loss : 0.009330597736671001ITERATION : 29, loss : 0.009330465176709997ITERATION : 30, loss : 0.009330366515398498ITERATION : 31, loss : 0.00933029311859709ITERATION : 32, loss : 0.009330238543103856ITERATION : 33, loss : 0.00933019797115173ITERATION : 34, loss : 0.00933016779363755ITERATION : 35, loss : 0.009330145372932579ITERATION : 36, loss : 0.009330128692975977ITERATION : 37, loss : 0.009330116316462ITERATION : 38, loss : 0.00933010712225607ITERATION : 39, loss : 0.009330100310433347ITERATION : 40, loss : 0.009330095249531267ITERATION : 41, loss : 0.009330091486946624ITERATION : 42, loss : 0.009330088689217494ITERATION : 43, loss : 0.009330086609178412ITERATION : 44, loss : 0.009330085064795771ITERATION : 45, loss : 0.009330083920395487ITERATION : 46, loss : 0.009330083072342047ITERATION : 47, loss : 0.009330082443461531ITERATION : 48, loss : 0.009330081976222248ITERATION : 49, loss : 0.00933008162555242ITERATION : 50, loss : 0.009330081373542866ITERATION : 51, loss : 0.009330081176264807ITERATION : 52, loss : 0.009330081024617262ITERATION : 53, loss : 0.009330080926872382ITERATION : 54, loss : 0.009330080868135311ITERATION : 55, loss : 0.009330080822687856ITERATION : 56, loss : 0.00933008079015672ITERATION : 57, loss : 0.00933008075714876ITERATION : 58, loss : 0.009330080722920865ITERATION : 59, loss : 0.00933008070774029ITERATION : 60, loss : 0.009330080705296557ITERATION : 61, loss : 0.009330080705296557ITERATION : 62, loss : 0.009330080705296557ITERATION : 63, loss : 0.009330080705296557ITERATION : 64, loss : 0.009330080705296557ITERATION : 65, loss : 0.009330080705296557ITERATION : 66, loss : 0.009330080705296557ITERATION : 67, loss : 0.009330080705296557ITERATION : 68, loss : 0.009330080705296557ITERATION : 69, loss : 0.009330080705296557ITERATION : 70, loss : 0.009330080705296557ITERATION : 71, loss : 0.009330080705296557ITERATION : 72, loss : 0.009330080705296557ITERATION : 73, loss : 0.009330080705296557ITERATION : 74, loss : 0.009330080705296557ITERATION : 75, loss : 0.009330080705296557ITERATION : 76, loss : 0.009330080705296557ITERATION : 77, loss : 0.009330080705296557ITERATION : 78, loss : 0.009330080705296557ITERATION : 79, loss : 0.009330080705296557ITERATION : 80, loss : 0.009330080705296557ITERATION : 81, loss : 0.009330080705296557ITERATION : 82, loss : 0.009330080705296557ITERATION : 83, loss : 0.009330080705296557ITERATION : 84, loss : 0.009330080705296557ITERATION : 85, loss : 0.009330080705296557ITERATION : 86, loss : 0.009330080705296557ITERATION : 87, loss : 0.009330080705296557ITERATION : 88, loss : 0.009330080705296557ITERATION : 89, loss : 0.009330080705296557ITERATION : 90, loss : 0.009330080705296557ITERATION : 91, loss : 0.009330080705296557ITERATION : 92, loss : 0.009330080705296557ITERATION : 93, loss : 0.009330080705296557ITERATION : 94, loss : 0.009330080705296557ITERATION : 95, loss : 0.009330080705296557ITERATION : 96, loss : 0.009330080705296557ITERATION : 97, loss : 0.009330080705296557ITERATION : 98, loss : 0.009330080705296557ITERATION : 99, loss : 0.009330080705296557ITERATION : 100, loss : 0.009330080705296557
ITERATION : 1, loss : 0.015963853918769415ITERATION : 2, loss : 0.015457553626148514ITERATION : 3, loss : 0.015971964632541948ITERATION : 4, loss : 0.016419142414627233ITERATION : 5, loss : 0.01674617815930195ITERATION : 6, loss : 0.01697949825826763ITERATION : 7, loss : 0.017145376400889596ITERATION : 8, loss : 0.01726314024698534ITERATION : 9, loss : 0.017346609677652745ITERATION : 10, loss : 0.017405672680096494ITERATION : 11, loss : 0.017447404115419417ITERATION : 12, loss : 0.017476855154519914ITERATION : 13, loss : 0.01749762119983578ITERATION : 14, loss : 0.01751225396419891ITERATION : 15, loss : 0.01752256021095852ITERATION : 16, loss : 0.01752981694936385ITERATION : 17, loss : 0.017534925380479325ITERATION : 18, loss : 0.01753852103728985ITERATION : 19, loss : 0.017541051711857818ITERATION : 20, loss : 0.017542832738105738ITERATION : 21, loss : 0.017544086185073453ITERATION : 22, loss : 0.017544968300526882ITERATION : 23, loss : 0.01754558913436033ITERATION : 24, loss : 0.017546026082622703ITERATION : 25, loss : 0.017546333596261496ITERATION : 26, loss : 0.01754655004362745ITERATION : 27, loss : 0.017546702394080495ITERATION : 28, loss : 0.017546809649429538ITERATION : 29, loss : 0.017546885143255146ITERATION : 30, loss : 0.017546938289913744ITERATION : 31, loss : 0.01754697570107161ITERATION : 32, loss : 0.017547002055153853ITERATION : 33, loss : 0.017547020608623703ITERATION : 34, loss : 0.017547033664688117ITERATION : 35, loss : 0.017547042855751892ITERATION : 36, loss : 0.017547049322859814ITERATION : 37, loss : 0.017547053870348515ITERATION : 38, loss : 0.017547057086873513ITERATION : 39, loss : 0.01754705933432466ITERATION : 40, loss : 0.01754706093696359ITERATION : 41, loss : 0.017547062050574457ITERATION : 42, loss : 0.017547062823508017ITERATION : 43, loss : 0.01754706337708496ITERATION : 44, loss : 0.017547063785095355ITERATION : 45, loss : 0.017547064016102416ITERATION : 46, loss : 0.017547064206498573ITERATION : 47, loss : 0.01754706432271565ITERATION : 48, loss : 0.01754706444490021ITERATION : 49, loss : 0.01754706449753945ITERATION : 50, loss : 0.01754706455868777ITERATION : 51, loss : 0.017547064566695716ITERATION : 52, loss : 0.017547064595140594ITERATION : 53, loss : 0.017547064596134046ITERATION : 54, loss : 0.017547064596134046ITERATION : 55, loss : 0.017547064596134046ITERATION : 56, loss : 0.017547064596134046ITERATION : 57, loss : 0.017547064596134046ITERATION : 58, loss : 0.017547064596134046ITERATION : 59, loss : 0.017547064596134046ITERATION : 60, loss : 0.017547064596134046ITERATION : 61, loss : 0.017547064596134046ITERATION : 62, loss : 0.017547064596134046ITERATION : 63, loss : 0.017547064596134046ITERATION : 64, loss : 0.017547064596134046ITERATION : 65, loss : 0.017547064596134046ITERATION : 66, loss : 0.017547064596134046ITERATION : 67, loss : 0.017547064596134046ITERATION : 68, loss : 0.017547064596134046ITERATION : 69, loss : 0.017547064596134046ITERATION : 70, loss : 0.017547064596134046ITERATION : 71, loss : 0.017547064596134046ITERATION : 72, loss : 0.017547064596134046ITERATION : 73, loss : 0.017547064596134046ITERATION : 74, loss : 0.017547064596134046ITERATION : 75, loss : 0.017547064596134046ITERATION : 76, loss : 0.017547064596134046ITERATION : 77, loss : 0.017547064596134046ITERATION : 78, loss : 0.017547064596134046ITERATION : 79, loss : 0.017547064596134046ITERATION : 80, loss : 0.017547064596134046ITERATION : 81, loss : 0.017547064596134046ITERATION : 82, loss : 0.017547064596134046ITERATION : 83, loss : 0.017547064596134046ITERATION : 84, loss : 0.017547064596134046ITERATION : 85, loss : 0.017547064596134046ITERATION : 86, loss : 0.017547064596134046ITERATION : 87, loss : 0.017547064596134046ITERATION : 88, loss : 0.017547064596134046ITERATION : 89, loss : 0.017547064596134046ITERATION : 90, loss : 0.017547064596134046ITERATION : 91, loss : 0.017547064596134046ITERATION : 92, loss : 0.017547064596134046ITERATION : 93, loss : 0.017547064596134046ITERATION : 94, loss : 0.017547064596134046ITERATION : 95, loss : 0.017547064596134046ITERATION : 96, loss : 0.017547064596134046ITERATION : 97, loss : 0.017547064596134046ITERATION : 98, loss : 0.017547064596134046ITERATION : 99, loss : 0.017547064596134046ITERATION : 100, loss : 0.017547064596134046
ITERATION : 1, loss : 0.025223656941968414ITERATION : 2, loss : 0.0194486217380917ITERATION : 3, loss : 0.018715668216340863ITERATION : 4, loss : 0.018636150725470264ITERATION : 5, loss : 0.018769595083434138ITERATION : 6, loss : 0.018963419675118623ITERATION : 7, loss : 0.019158294764005025ITERATION : 8, loss : 0.01933133746990447ITERATION : 9, loss : 0.019475705842420345ITERATION : 10, loss : 0.01959177872249911ITERATION : 11, loss : 0.019682903394863425ITERATION : 12, loss : 0.01975330108502403ITERATION : 13, loss : 0.01980708456847113ITERATION : 14, loss : 0.019847855226338862ITERATION : 15, loss : 0.01987859128493789ITERATION : 16, loss : 0.0199016722031867ITERATION : 17, loss : 0.019918957223297232ITERATION : 18, loss : 0.0199318772287185ITERATION : 19, loss : 0.019941522126975576ITERATION : 20, loss : 0.019948716193906972ITERATION : 21, loss : 0.019954079557384707ITERATION : 22, loss : 0.01995807713228856ITERATION : 23, loss : 0.01996105652655343ITERATION : 24, loss : 0.019963277251515598ITERATION : 25, loss : 0.0199649328571835ITERATION : 26, loss : 0.019966167447812593ITERATION : 27, loss : 0.019967088368648456ITERATION : 28, loss : 0.019967775492794087ITERATION : 29, loss : 0.01996828847214238ITERATION : 30, loss : 0.019968671598294345ITERATION : 31, loss : 0.019968957796929605ITERATION : 32, loss : 0.01996917174701187ITERATION : 33, loss : 0.01996933173092181ITERATION : 34, loss : 0.01996945144264515ITERATION : 35, loss : 0.019969541016238098ITERATION : 36, loss : 0.01996960808168809ITERATION : 37, loss : 0.01996965832173865ITERATION : 38, loss : 0.019969695989140605ITERATION : 39, loss : 0.019969724236068333ITERATION : 40, loss : 0.019969745426137844ITERATION : 41, loss : 0.01996976132983576ITERATION : 42, loss : 0.01996977326019989ITERATION : 43, loss : 0.01996978222529954ITERATION : 44, loss : 0.019969788970879768ITERATION : 45, loss : 0.01996979402572599ITERATION : 46, loss : 0.01996979784433056ITERATION : 47, loss : 0.01996980070500662ITERATION : 48, loss : 0.01996980286123876ITERATION : 49, loss : 0.01996980447953449ITERATION : 50, loss : 0.019969805682711325ITERATION : 51, loss : 0.01996980658003726ITERATION : 52, loss : 0.01996980727255741ITERATION : 53, loss : 0.01996980778620713ITERATION : 54, loss : 0.019969808169099646ITERATION : 55, loss : 0.019969808445201558ITERATION : 56, loss : 0.0199698086848348ITERATION : 57, loss : 0.019969808808926983ITERATION : 58, loss : 0.019969808923202482ITERATION : 59, loss : 0.01996980897875705ITERATION : 60, loss : 0.01996980909155161ITERATION : 61, loss : 0.019969809148540685ITERATION : 62, loss : 0.019969809183070942ITERATION : 63, loss : 0.019969809183183897ITERATION : 64, loss : 0.019969809183183897ITERATION : 65, loss : 0.019969809183183897ITERATION : 66, loss : 0.019969809183183897ITERATION : 67, loss : 0.019969809183183897ITERATION : 68, loss : 0.019969809183183897ITERATION : 69, loss : 0.019969809183183897ITERATION : 70, loss : 0.019969809183183897ITERATION : 71, loss : 0.019969809183183897ITERATION : 72, loss : 0.019969809183183897ITERATION : 73, loss : 0.019969809183183897ITERATION : 74, loss : 0.019969809183183897ITERATION : 75, loss : 0.019969809183183897ITERATION : 76, loss : 0.019969809183183897ITERATION : 77, loss : 0.019969809183183897ITERATION : 78, loss : 0.019969809183183897ITERATION : 79, loss : 0.019969809183183897ITERATION : 80, loss : 0.019969809183183897ITERATION : 81, loss : 0.019969809183183897ITERATION : 82, loss : 0.019969809183183897ITERATION : 83, loss : 0.019969809183183897ITERATION : 84, loss : 0.019969809183183897ITERATION : 85, loss : 0.019969809183183897ITERATION : 86, loss : 0.019969809183183897ITERATION : 87, loss : 0.019969809183183897ITERATION : 88, loss : 0.019969809183183897ITERATION : 89, loss : 0.019969809183183897ITERATION : 90, loss : 0.019969809183183897ITERATION : 91, loss : 0.019969809183183897ITERATION : 92, loss : 0.019969809183183897ITERATION : 93, loss : 0.019969809183183897ITERATION : 94, loss : 0.019969809183183897ITERATION : 95, loss : 0.019969809183183897ITERATION : 96, loss : 0.019969809183183897ITERATION : 97, loss : 0.019969809183183897ITERATION : 98, loss : 0.019969809183183897ITERATION : 99, loss : 0.019969809183183897ITERATION : 100, loss : 0.019969809183183897
gradient norm in None layer : 0.0009618428591571368
gradient norm in None layer : 4.420467139485485e-05
gradient norm in None layer : 6.23249785290105e-05
gradient norm in None layer : 0.0006941660776723432
gradient norm in None layer : 6.061637360996012e-05
gradient norm in None layer : 7.897466406525125e-05
gradient norm in None layer : 0.00026025685003230675
gradient norm in None layer : 1.2873805961916438e-05
gradient norm in None layer : 1.2753399089088182e-05
gradient norm in None layer : 0.00022972196676115494
gradient norm in None layer : 1.2376824149430551e-05
gradient norm in None layer : 1.0483741991479364e-05
gradient norm in None layer : 9.602852841445085e-05
gradient norm in None layer : 3.71697873024326e-06
gradient norm in None layer : 2.864108294702984e-06
gradient norm in None layer : 7.804459168689876e-05
gradient norm in None layer : 3.932134544667141e-06
gradient norm in None layer : 3.0129880418227844e-06
gradient norm in None layer : 0.00010963594037306752
gradient norm in None layer : 1.3315430912783977e-06
gradient norm in None layer : 0.00022223628791173252
gradient norm in None layer : 1.7560629676865697e-05
gradient norm in None layer : 1.3374834966528885e-05
gradient norm in None layer : 0.00029432244424909646
gradient norm in None layer : 3.263532173058261e-05
gradient norm in None layer : 3.718093045651688e-05
gradient norm in None layer : 0.0005426096255160706
gradient norm in None layer : 3.558933042694343e-06
gradient norm in None layer : 0.000849952868194133
gradient norm in None layer : 7.32676628076203e-05
gradient norm in None layer : 8.201534556652543e-05
gradient norm in None layer : 0.0011139958294628593
gradient norm in None layer : 0.00010732247395646164
gradient norm in None layer : 0.00014184451391986808
gradient norm in None layer : 9.259896641889956e-05
gradient norm in None layer : 2.1310479026111066e-05
Total gradient norm: 0.0020051911006990566
invariance loss : 4.249175280510154, avg_den : 0.4233856201171875, density loss : 0.3233856201171875, mse loss : 0.01741364747547406, solver time : 127.90798592567444 sec , total loss : 0.021986208376101404, running loss : 0.042748602851701706
Epoch 0/10 , batch 28/12500 
ITERATION : 1, loss : 0.021631740040177993ITERATION : 2, loss : 0.02124535868646439ITERATION : 3, loss : 0.02062845189519457ITERATION : 4, loss : 0.01993724625527609ITERATION : 5, loss : 0.019483056566923073ITERATION : 6, loss : 0.01917901131074781ITERATION : 7, loss : 0.018973673099828992ITERATION : 8, loss : 0.0188339440565585ITERATION : 9, loss : 0.018738179736314ITERATION : 10, loss : 0.01867212108844435ITERATION : 11, loss : 0.018626298295650358ITERATION : 12, loss : 0.018594364428364308ITERATION : 13, loss : 0.018572025759132433ITERATION : 14, loss : 0.018556352387704086ITERATION : 15, loss : 0.0185453296813723ITERATION : 16, loss : 0.018537563463861397ITERATION : 17, loss : 0.01853208375460944ITERATION : 18, loss : 0.018528213119703592ITERATION : 19, loss : 0.018525476677247778ITERATION : 20, loss : 0.018523540750758793ITERATION : 21, loss : 0.01852217036663509ITERATION : 22, loss : 0.018521199946954565ITERATION : 23, loss : 0.01852051245400395ITERATION : 24, loss : 0.018520025261198926ITERATION : 25, loss : 0.01851967991276081ITERATION : 26, loss : 0.018519435077965204ITERATION : 27, loss : 0.01851926145167211ITERATION : 28, loss : 0.018519138216305006ITERATION : 29, loss : 0.018519050786748186ITERATION : 30, loss : 0.018518988776632557ITERATION : 31, loss : 0.01851894473652981ITERATION : 32, loss : 0.018518913552182874ITERATION : 33, loss : 0.018518891349849714ITERATION : 34, loss : 0.018518875586646208ITERATION : 35, loss : 0.018518864423660786ITERATION : 36, loss : 0.018518856525236757ITERATION : 37, loss : 0.018518850859808412ITERATION : 38, loss : 0.018518846799505745ITERATION : 39, loss : 0.018518843931851377ITERATION : 40, loss : 0.018518841881496207ITERATION : 41, loss : 0.018518840426979304ITERATION : 42, loss : 0.01851883940602202ITERATION : 43, loss : 0.018518838695048397ITERATION : 44, loss : 0.018518838181654014ITERATION : 45, loss : 0.018518837825909185ITERATION : 46, loss : 0.018518837571021372ITERATION : 47, loss : 0.01851883739787986ITERATION : 48, loss : 0.018518837274075387ITERATION : 49, loss : 0.018518837184997753ITERATION : 50, loss : 0.018518837125294372ITERATION : 51, loss : 0.018518837069440873ITERATION : 52, loss : 0.01851883704462615ITERATION : 53, loss : 0.01851883702944781ITERATION : 54, loss : 0.018518837002649217ITERATION : 55, loss : 0.018518836998099895ITERATION : 56, loss : 0.018518836988085027ITERATION : 57, loss : 0.01851883698500737ITERATION : 58, loss : 0.01851883698500737ITERATION : 59, loss : 0.01851883698500737ITERATION : 60, loss : 0.01851883698500737ITERATION : 61, loss : 0.01851883698500737ITERATION : 62, loss : 0.01851883698500737ITERATION : 63, loss : 0.01851883698500737ITERATION : 64, loss : 0.01851883698500737ITERATION : 65, loss : 0.01851883698500737ITERATION : 66, loss : 0.01851883698500737ITERATION : 67, loss : 0.01851883698500737ITERATION : 68, loss : 0.01851883698500737ITERATION : 69, loss : 0.01851883698500737ITERATION : 70, loss : 0.01851883698500737ITERATION : 71, loss : 0.01851883698500737ITERATION : 72, loss : 0.01851883698500737ITERATION : 73, loss : 0.01851883698500737ITERATION : 74, loss : 0.01851883698500737ITERATION : 75, loss : 0.01851883698500737ITERATION : 76, loss : 0.01851883698500737ITERATION : 77, loss : 0.01851883698500737ITERATION : 78, loss : 0.01851883698500737ITERATION : 79, loss : 0.01851883698500737ITERATION : 80, loss : 0.01851883698500737ITERATION : 81, loss : 0.01851883698500737ITERATION : 82, loss : 0.01851883698500737ITERATION : 83, loss : 0.01851883698500737ITERATION : 84, loss : 0.01851883698500737ITERATION : 85, loss : 0.01851883698500737ITERATION : 86, loss : 0.01851883698500737ITERATION : 87, loss : 0.01851883698500737ITERATION : 88, loss : 0.01851883698500737ITERATION : 89, loss : 0.01851883698500737ITERATION : 90, loss : 0.01851883698500737ITERATION : 91, loss : 0.01851883698500737ITERATION : 92, loss : 0.01851883698500737ITERATION : 93, loss : 0.01851883698500737ITERATION : 94, loss : 0.01851883698500737ITERATION : 95, loss : 0.01851883698500737ITERATION : 96, loss : 0.01851883698500737ITERATION : 97, loss : 0.01851883698500737ITERATION : 98, loss : 0.01851883698500737ITERATION : 99, loss : 0.01851883698500737ITERATION : 100, loss : 0.01851883698500737
ITERATION : 1, loss : 0.00985620420525352ITERATION : 2, loss : 0.008390777000321646ITERATION : 3, loss : 0.008170545253204504ITERATION : 4, loss : 0.008115533134903059ITERATION : 5, loss : 0.008095275707177963ITERATION : 6, loss : 0.008085539410410793ITERATION : 7, loss : 0.00808004454718397ITERATION : 8, loss : 0.008076651402272065ITERATION : 9, loss : 0.008074443175092238ITERATION : 10, loss : 0.008072957050793248ITERATION : 11, loss : 0.008071933459618728ITERATION : 12, loss : 0.008071216650668726ITERATION : 13, loss : 0.008070708603350891ITERATION : 14, loss : 0.008070345400448646ITERATION : 15, loss : 0.008070084160340087ITERATION : 16, loss : 0.008069895457472517ITERATION : 17, loss : 0.008069758772511508ITERATION : 18, loss : 0.008069659570828197ITERATION : 19, loss : 0.008069587499604708ITERATION : 20, loss : 0.008069535108440407ITERATION : 21, loss : 0.008069497003465702ITERATION : 22, loss : 0.008069469313134877ITERATION : 23, loss : 0.008069449195310548ITERATION : 24, loss : 0.008069434581881293ITERATION : 25, loss : 0.008069423961678743ITERATION : 26, loss : 0.008069416265146718ITERATION : 27, loss : 0.008069410676718153ITERATION : 28, loss : 0.008069406609629482ITERATION : 29, loss : 0.008069403682896102ITERATION : 30, loss : 0.008069401563675426ITERATION : 31, loss : 0.008069400020624659ITERATION : 32, loss : 0.008069398896115033ITERATION : 33, loss : 0.008069398078388287ITERATION : 34, loss : 0.008069397484956421ITERATION : 35, loss : 0.00806939705227826ITERATION : 36, loss : 0.00806939673943818ITERATION : 37, loss : 0.008069396513174591ITERATION : 38, loss : 0.008069396349711395ITERATION : 39, loss : 0.008069396231115222ITERATION : 40, loss : 0.0080693961487562ITERATION : 41, loss : 0.008069396083974546ITERATION : 42, loss : 0.008069396038691515ITERATION : 43, loss : 0.008069396005378277ITERATION : 44, loss : 0.00806939598189032ITERATION : 45, loss : 0.008069395969038868ITERATION : 46, loss : 0.008069395955977453ITERATION : 47, loss : 0.00806939595108567ITERATION : 48, loss : 0.008069395942349131ITERATION : 49, loss : 0.00806939594090677ITERATION : 50, loss : 0.008069395940906839ITERATION : 51, loss : 0.008069395940906839ITERATION : 52, loss : 0.008069395940906839ITERATION : 53, loss : 0.008069395940906839ITERATION : 54, loss : 0.008069395940906839ITERATION : 55, loss : 0.008069395940906839ITERATION : 56, loss : 0.008069395940906839ITERATION : 57, loss : 0.008069395940906839ITERATION : 58, loss : 0.008069395940906839ITERATION : 59, loss : 0.008069395940906839ITERATION : 60, loss : 0.008069395940906839ITERATION : 61, loss : 0.008069395940906839ITERATION : 62, loss : 0.008069395940906839ITERATION : 63, loss : 0.008069395940906839ITERATION : 64, loss : 0.008069395940906839ITERATION : 65, loss : 0.008069395940906839ITERATION : 66, loss : 0.008069395940906839ITERATION : 67, loss : 0.008069395940906839ITERATION : 68, loss : 0.008069395940906839ITERATION : 69, loss : 0.008069395940906839ITERATION : 70, loss : 0.008069395940906839ITERATION : 71, loss : 0.008069395940906839ITERATION : 72, loss : 0.008069395940906839ITERATION : 73, loss : 0.008069395940906839ITERATION : 74, loss : 0.008069395940906839ITERATION : 75, loss : 0.008069395940906839ITERATION : 76, loss : 0.008069395940906839ITERATION : 77, loss : 0.008069395940906839ITERATION : 78, loss : 0.008069395940906839ITERATION : 79, loss : 0.008069395940906839ITERATION : 80, loss : 0.008069395940906839ITERATION : 81, loss : 0.008069395940906839ITERATION : 82, loss : 0.008069395940906839ITERATION : 83, loss : 0.008069395940906839ITERATION : 84, loss : 0.008069395940906839ITERATION : 85, loss : 0.008069395940906839ITERATION : 86, loss : 0.008069395940906839ITERATION : 87, loss : 0.008069395940906839ITERATION : 88, loss : 0.008069395940906839ITERATION : 89, loss : 0.008069395940906839ITERATION : 90, loss : 0.008069395940906839ITERATION : 91, loss : 0.008069395940906839ITERATION : 92, loss : 0.008069395940906839ITERATION : 93, loss : 0.008069395940906839ITERATION : 94, loss : 0.008069395940906839ITERATION : 95, loss : 0.008069395940906839ITERATION : 96, loss : 0.008069395940906839ITERATION : 97, loss : 0.008069395940906839ITERATION : 98, loss : 0.008069395940906839ITERATION : 99, loss : 0.008069395940906839ITERATION : 100, loss : 0.008069395940906839
ITERATION : 1, loss : 0.03186344948890988ITERATION : 2, loss : 0.03337301136466733ITERATION : 3, loss : 0.035308832220511725ITERATION : 4, loss : 0.0367236836432196ITERATION : 5, loss : 0.03767779990867187ITERATION : 6, loss : 0.03834293722269765ITERATION : 7, loss : 0.038820189243251105ITERATION : 8, loss : 0.03916762764541306ITERATION : 9, loss : 0.03942190663173753ITERATION : 10, loss : 0.039608162152430584ITERATION : 11, loss : 0.03974446244719549ITERATION : 12, loss : 0.03984406105295781ITERATION : 13, loss : 0.03991673808230792ITERATION : 14, loss : 0.03996970847030468ITERATION : 15, loss : 0.04000828128396686ITERATION : 16, loss : 0.0400363521305307ITERATION : 17, loss : 0.04005677181633342ITERATION : 18, loss : 0.040071622174024595ITERATION : 19, loss : 0.040082420920294674ITERATION : 20, loss : 0.040090273490979766ITERATION : 21, loss : 0.04009598375885884ITERATION : 22, loss : 0.0401001368333325ITERATION : 23, loss : 0.040103157617114346ITERATION : 24, loss : 0.04010535532480695ITERATION : 25, loss : 0.040106954459039924ITERATION : 26, loss : 0.0401081184783334ITERATION : 27, loss : 0.04010896561614374ITERATION : 28, loss : 0.04010958259150916ITERATION : 29, loss : 0.0401100319599182ITERATION : 30, loss : 0.040110359245058354ITERATION : 31, loss : 0.04011059778928922ITERATION : 32, loss : 0.040110771644055215ITERATION : 33, loss : 0.04011089819215667ITERATION : 34, loss : 0.04011099060876077ITERATION : 35, loss : 0.040111057851642776ITERATION : 36, loss : 0.0401111069541316ITERATION : 37, loss : 0.04011114291712832ITERATION : 38, loss : 0.040111169103342094ITERATION : 39, loss : 0.04011118817024363ITERATION : 40, loss : 0.04011120209200335ITERATION : 41, loss : 0.04011121223494572ITERATION : 42, loss : 0.04011121952618511ITERATION : 43, loss : 0.040111224941196684ITERATION : 44, loss : 0.040111228784397095ITERATION : 45, loss : 0.04011123169066298ITERATION : 46, loss : 0.04011123377447065ITERATION : 47, loss : 0.04011123525936991ITERATION : 48, loss : 0.0401112364223408ITERATION : 49, loss : 0.04011123727372338ITERATION : 50, loss : 0.040111237853410406ITERATION : 51, loss : 0.04011123824050652ITERATION : 52, loss : 0.040111238598998955ITERATION : 53, loss : 0.04011123880308923ITERATION : 54, loss : 0.040111238989678916ITERATION : 55, loss : 0.040111239080587405ITERATION : 56, loss : 0.040111239160088526ITERATION : 57, loss : 0.040111239196671415ITERATION : 58, loss : 0.04011123922343536ITERATION : 59, loss : 0.04011123924133231ITERATION : 60, loss : 0.04011123938167346ITERATION : 61, loss : 0.040111239380350966ITERATION : 62, loss : 0.040111239380350966ITERATION : 63, loss : 0.040111239380350966ITERATION : 64, loss : 0.040111239380350966ITERATION : 65, loss : 0.040111239380350966ITERATION : 66, loss : 0.040111239380350966ITERATION : 67, loss : 0.040111239380350966ITERATION : 68, loss : 0.040111239380350966ITERATION : 69, loss : 0.040111239380350966ITERATION : 70, loss : 0.040111239380350966ITERATION : 71, loss : 0.040111239380350966ITERATION : 72, loss : 0.040111239380350966ITERATION : 73, loss : 0.040111239380350966ITERATION : 74, loss : 0.040111239380350966ITERATION : 75, loss : 0.040111239380350966ITERATION : 76, loss : 0.040111239380350966ITERATION : 77, loss : 0.040111239380350966ITERATION : 78, loss : 0.040111239380350966ITERATION : 79, loss : 0.040111239380350966ITERATION : 80, loss : 0.040111239380350966ITERATION : 81, loss : 0.040111239380350966ITERATION : 82, loss : 0.040111239380350966ITERATION : 83, loss : 0.040111239380350966ITERATION : 84, loss : 0.040111239380350966ITERATION : 85, loss : 0.040111239380350966ITERATION : 86, loss : 0.040111239380350966ITERATION : 87, loss : 0.040111239380350966ITERATION : 88, loss : 0.040111239380350966ITERATION : 89, loss : 0.040111239380350966ITERATION : 90, loss : 0.040111239380350966ITERATION : 91, loss : 0.040111239380350966ITERATION : 92, loss : 0.040111239380350966ITERATION : 93, loss : 0.040111239380350966ITERATION : 94, loss : 0.040111239380350966ITERATION : 95, loss : 0.040111239380350966ITERATION : 96, loss : 0.040111239380350966ITERATION : 97, loss : 0.040111239380350966ITERATION : 98, loss : 0.040111239380350966ITERATION : 99, loss : 0.040111239380350966ITERATION : 100, loss : 0.040111239380350966
ITERATION : 1, loss : 0.04400410801194029ITERATION : 2, loss : 0.03394554508238548ITERATION : 3, loss : 0.027812648820318548ITERATION : 4, loss : 0.02384350480758411ITERATION : 5, loss : 0.021123592758777496ITERATION : 6, loss : 0.019341257226212084ITERATION : 7, loss : 0.018149766916768503ITERATION : 8, loss : 0.017340530173502802ITERATION : 9, loss : 0.0167842355700469ITERATION : 10, loss : 0.016398369278398477ITERATION : 11, loss : 0.016128948612121004ITERATION : 12, loss : 0.01593993054438752ITERATION : 13, loss : 0.015806862125971762ITERATION : 14, loss : 0.015712949653871905ITERATION : 15, loss : 0.01564655362494773ITERATION : 16, loss : 0.015599552192307305ITERATION : 17, loss : 0.01556625001211421ITERATION : 18, loss : 0.015542639063503984ITERATION : 19, loss : 0.015525891432512458ITERATION : 20, loss : 0.015514008194382367ITERATION : 21, loss : 0.015505574537492137ITERATION : 22, loss : 0.015499588076211306ITERATION : 23, loss : 0.015495338227900675ITERATION : 24, loss : 0.015492320946000298ITERATION : 25, loss : 0.015490178642617923ITERATION : 26, loss : 0.015488657512275234ITERATION : 27, loss : 0.015487577414278836ITERATION : 28, loss : 0.015486810464947457ITERATION : 29, loss : 0.015486265870566245ITERATION : 30, loss : 0.015485879152813487ITERATION : 31, loss : 0.015485604554378292ITERATION : 32, loss : 0.015485409562462358ITERATION : 33, loss : 0.015485271080577734ITERATION : 34, loss : 0.015485172744751013ITERATION : 35, loss : 0.015485102908032665ITERATION : 36, loss : 0.015485053317905149ITERATION : 37, loss : 0.015485018098368325ITERATION : 38, loss : 0.015484993093979996ITERATION : 39, loss : 0.015484975331901268ITERATION : 40, loss : 0.015484962727594762ITERATION : 41, loss : 0.015484953768839278ITERATION : 42, loss : 0.015484947411385001ITERATION : 43, loss : 0.015484942893938588ITERATION : 44, loss : 0.015484939686240113ITERATION : 45, loss : 0.015484937409274566ITERATION : 46, loss : 0.015484935792243448ITERATION : 47, loss : 0.015484934651159486ITERATION : 48, loss : 0.01548493383835549ITERATION : 49, loss : 0.015484933258003106ITERATION : 50, loss : 0.015484932870716341ITERATION : 51, loss : 0.015484932573057877ITERATION : 52, loss : 0.015484932378678044ITERATION : 53, loss : 0.015484932244085951ITERATION : 54, loss : 0.015484932182873602ITERATION : 55, loss : 0.015484932137274476ITERATION : 56, loss : 0.015484932113695258ITERATION : 57, loss : 0.01548493210588446ITERATION : 58, loss : 0.015484932103361531ITERATION : 59, loss : 0.015484932103361531ITERATION : 60, loss : 0.015484932103361531ITERATION : 61, loss : 0.015484932103361531ITERATION : 62, loss : 0.015484932103361531ITERATION : 63, loss : 0.015484932103361531ITERATION : 64, loss : 0.015484932103361531ITERATION : 65, loss : 0.015484932103361531ITERATION : 66, loss : 0.015484932103361531ITERATION : 67, loss : 0.015484932103361531ITERATION : 68, loss : 0.015484932103361531ITERATION : 69, loss : 0.015484932103361531ITERATION : 70, loss : 0.015484932103361531ITERATION : 71, loss : 0.015484932103361531ITERATION : 72, loss : 0.015484932103361531ITERATION : 73, loss : 0.015484932103361531ITERATION : 74, loss : 0.015484932103361531ITERATION : 75, loss : 0.015484932103361531ITERATION : 76, loss : 0.015484932103361531ITERATION : 77, loss : 0.015484932103361531ITERATION : 78, loss : 0.015484932103361531ITERATION : 79, loss : 0.015484932103361531ITERATION : 80, loss : 0.015484932103361531ITERATION : 81, loss : 0.015484932103361531ITERATION : 82, loss : 0.015484932103361531ITERATION : 83, loss : 0.015484932103361531ITERATION : 84, loss : 0.015484932103361531ITERATION : 85, loss : 0.015484932103361531ITERATION : 86, loss : 0.015484932103361531ITERATION : 87, loss : 0.015484932103361531ITERATION : 88, loss : 0.015484932103361531ITERATION : 89, loss : 0.015484932103361531ITERATION : 90, loss : 0.015484932103361531ITERATION : 91, loss : 0.015484932103361531ITERATION : 92, loss : 0.015484932103361531ITERATION : 93, loss : 0.015484932103361531ITERATION : 94, loss : 0.015484932103361531ITERATION : 95, loss : 0.015484932103361531ITERATION : 96, loss : 0.015484932103361531ITERATION : 97, loss : 0.015484932103361531ITERATION : 98, loss : 0.015484932103361531ITERATION : 99, loss : 0.015484932103361531ITERATION : 100, loss : 0.015484932103361531
ITERATION : 1, loss : 0.029249580140350635ITERATION : 2, loss : 0.02422484155435562ITERATION : 3, loss : 0.024654914708443235ITERATION : 4, loss : 0.02627201393320001ITERATION : 5, loss : 0.028043501580794574ITERATION : 6, loss : 0.02938953242703256ITERATION : 7, loss : 0.030509192115568354ITERATION : 8, loss : 0.0313937256779286ITERATION : 9, loss : 0.03207083440883469ITERATION : 10, loss : 0.03257867733249374ITERATION : 11, loss : 0.03295439041720619ITERATION : 12, loss : 0.03322029696017186ITERATION : 13, loss : 0.03307857605425475ITERATION : 14, loss : 0.03297826638128606ITERATION : 15, loss : 0.03290707333235466ITERATION : 16, loss : 0.03285643935212094ITERATION : 17, loss : 0.03282036887420343ITERATION : 18, loss : 0.032794640172169785ITERATION : 19, loss : 0.032776269288325985ITERATION : 20, loss : 0.03276314096445101ITERATION : 21, loss : 0.03275375270129985ITERATION : 22, loss : 0.03274703469217446ITERATION : 23, loss : 0.03274222504481911ITERATION : 24, loss : 0.03273878016144838ITERATION : 25, loss : 0.0327363118025201ITERATION : 26, loss : 0.032734542374978914ITERATION : 27, loss : 0.03273327341182658ITERATION : 28, loss : 0.032732363200918206ITERATION : 29, loss : 0.0327317099283053ITERATION : 30, loss : 0.032731241137847ITERATION : 31, loss : 0.032730904514796835ITERATION : 32, loss : 0.032730662703376975ITERATION : 33, loss : 0.032730488978434974ITERATION : 34, loss : 0.03273036413160355ITERATION : 35, loss : 0.03273027438081646ITERATION : 36, loss : 0.032730209835883914ITERATION : 37, loss : 0.0327301634132374ITERATION : 38, loss : 0.03273013001347124ITERATION : 39, loss : 0.03273010595998564ITERATION : 40, loss : 0.03273008866043562ITERATION : 41, loss : 0.03273007622831751ITERATION : 42, loss : 0.03273006724390183ITERATION : 43, loss : 0.03273006079552348ITERATION : 44, loss : 0.032730056112923275ITERATION : 45, loss : 0.03273005278499774ITERATION : 46, loss : 0.03273005038053013ITERATION : 47, loss : 0.032730048670704226ITERATION : 48, loss : 0.03273004752673196ITERATION : 49, loss : 0.032730046641145455ITERATION : 50, loss : 0.03273004594817083ITERATION : 51, loss : 0.03273004553629404ITERATION : 52, loss : 0.032730045031953606ITERATION : 53, loss : 0.03273004480853477ITERATION : 54, loss : 0.032730044713690384ITERATION : 55, loss : 0.03273004471350292ITERATION : 56, loss : 0.03273004471350292ITERATION : 57, loss : 0.03273004471350292ITERATION : 58, loss : 0.03273004471350292ITERATION : 59, loss : 0.03273004471350292ITERATION : 60, loss : 0.03273004471350292ITERATION : 61, loss : 0.03273004471350292ITERATION : 62, loss : 0.03273004471350292ITERATION : 63, loss : 0.03273004471350292ITERATION : 64, loss : 0.03273004471350292ITERATION : 65, loss : 0.03273004471350292ITERATION : 66, loss : 0.03273004471350292ITERATION : 67, loss : 0.03273004471350292ITERATION : 68, loss : 0.03273004471350292ITERATION : 69, loss : 0.03273004471350292ITERATION : 70, loss : 0.03273004471350292ITERATION : 71, loss : 0.03273004471350292ITERATION : 72, loss : 0.03273004471350292ITERATION : 73, loss : 0.03273004471350292ITERATION : 74, loss : 0.03273004471350292ITERATION : 75, loss : 0.03273004471350292ITERATION : 76, loss : 0.03273004471350292ITERATION : 77, loss : 0.03273004471350292ITERATION : 78, loss : 0.03273004471350292ITERATION : 79, loss : 0.03273004471350292ITERATION : 80, loss : 0.03273004471350292ITERATION : 81, loss : 0.03273004471350292ITERATION : 82, loss : 0.03273004471350292ITERATION : 83, loss : 0.03273004471350292ITERATION : 84, loss : 0.03273004471350292ITERATION : 85, loss : 0.03273004471350292ITERATION : 86, loss : 0.03273004471350292ITERATION : 87, loss : 0.03273004471350292ITERATION : 88, loss : 0.03273004471350292ITERATION : 89, loss : 0.03273004471350292ITERATION : 90, loss : 0.03273004471350292ITERATION : 91, loss : 0.03273004471350292ITERATION : 92, loss : 0.03273004471350292ITERATION : 93, loss : 0.03273004471350292ITERATION : 94, loss : 0.03273004471350292ITERATION : 95, loss : 0.03273004471350292ITERATION : 96, loss : 0.03273004471350292ITERATION : 97, loss : 0.03273004471350292ITERATION : 98, loss : 0.03273004471350292ITERATION : 99, loss : 0.03273004471350292ITERATION : 100, loss : 0.03273004471350292
ITERATION : 1, loss : 0.031714326814254054ITERATION : 2, loss : 0.02428326824245821ITERATION : 3, loss : 0.02266000958476943ITERATION : 4, loss : 0.022311522364407553ITERATION : 5, loss : 0.02228839319590542ITERATION : 6, loss : 0.02234528528038538ITERATION : 7, loss : 0.022412065668056026ITERATION : 8, loss : 0.022469807912803327ITERATION : 9, loss : 0.022515279201713888ITERATION : 10, loss : 0.022549765692352435ITERATION : 11, loss : 0.022575492785598998ITERATION : 12, loss : 0.022594545116797746ITERATION : 13, loss : 0.022608610805497146ITERATION : 14, loss : 0.022618983494633958ITERATION : 15, loss : 0.02262663118102957ITERATION : 16, loss : 0.022632270833173266ITERATION : 17, loss : 0.02263643109480752ITERATION : 18, loss : 0.0226395010259379ITERATION : 19, loss : 0.022641767083914275ITERATION : 20, loss : 0.022643440221214083ITERATION : 21, loss : 0.022644675868191265ITERATION : 22, loss : 0.02264558848082203ITERATION : 23, loss : 0.022646262627954848ITERATION : 24, loss : 0.022646760679866786ITERATION : 25, loss : 0.022647128616662324ITERATION : 26, loss : 0.022647400437917263ITERATION : 27, loss : 0.022647601235141885ITERATION : 28, loss : 0.022647749568458993ITERATION : 29, loss : 0.022647859153593405ITERATION : 30, loss : 0.022647940091654602ITERATION : 31, loss : 0.022647999877274536ITERATION : 32, loss : 0.02264804403943154ITERATION : 33, loss : 0.02264807664734109ITERATION : 34, loss : 0.022648100729405783ITERATION : 35, loss : 0.02264811850964781ITERATION : 36, loss : 0.022648131642866234ITERATION : 37, loss : 0.022648141340104462ITERATION : 38, loss : 0.022648148496110826ITERATION : 39, loss : 0.022648153783768ITERATION : 40, loss : 0.022648157683379085ITERATION : 41, loss : 0.022648160561929254ITERATION : 42, loss : 0.022648162686060767ITERATION : 43, loss : 0.022648164262398605ITERATION : 44, loss : 0.02264816541145175ITERATION : 45, loss : 0.022648166268469903ITERATION : 46, loss : 0.02264816691002013ITERATION : 47, loss : 0.02264816736774641ITERATION : 48, loss : 0.022648167714599127ITERATION : 49, loss : 0.022648167957376855ITERATION : 50, loss : 0.022648168145077984ITERATION : 51, loss : 0.022648168283559263ITERATION : 52, loss : 0.02264816838139937ITERATION : 53, loss : 0.022648168475014045ITERATION : 54, loss : 0.022648168545337154ITERATION : 55, loss : 0.022648168568543414ITERATION : 56, loss : 0.02264816858737879ITERATION : 57, loss : 0.02264816860971745ITERATION : 58, loss : 0.02264816861671773ITERATION : 59, loss : 0.02264816861962225ITERATION : 60, loss : 0.02264816862528594ITERATION : 61, loss : 0.022648168627320723ITERATION : 62, loss : 0.022648168627320723ITERATION : 63, loss : 0.022648168627320723ITERATION : 64, loss : 0.022648168627320723ITERATION : 65, loss : 0.022648168627320723ITERATION : 66, loss : 0.022648168627320723ITERATION : 67, loss : 0.022648168627320723ITERATION : 68, loss : 0.022648168627320723ITERATION : 69, loss : 0.022648168627320723ITERATION : 70, loss : 0.022648168627320723ITERATION : 71, loss : 0.022648168627320723ITERATION : 72, loss : 0.022648168627320723ITERATION : 73, loss : 0.022648168627320723ITERATION : 74, loss : 0.022648168627320723ITERATION : 75, loss : 0.022648168627320723ITERATION : 76, loss : 0.022648168627320723ITERATION : 77, loss : 0.022648168627320723ITERATION : 78, loss : 0.022648168627320723ITERATION : 79, loss : 0.022648168627320723ITERATION : 80, loss : 0.022648168627320723ITERATION : 81, loss : 0.022648168627320723ITERATION : 82, loss : 0.022648168627320723ITERATION : 83, loss : 0.022648168627320723ITERATION : 84, loss : 0.022648168627320723ITERATION : 85, loss : 0.022648168627320723ITERATION : 86, loss : 0.022648168627320723ITERATION : 87, loss : 0.022648168627320723ITERATION : 88, loss : 0.022648168627320723ITERATION : 89, loss : 0.022648168627320723ITERATION : 90, loss : 0.022648168627320723ITERATION : 91, loss : 0.022648168627320723ITERATION : 92, loss : 0.022648168627320723ITERATION : 93, loss : 0.022648168627320723ITERATION : 94, loss : 0.022648168627320723ITERATION : 95, loss : 0.022648168627320723ITERATION : 96, loss : 0.022648168627320723ITERATION : 97, loss : 0.022648168627320723ITERATION : 98, loss : 0.022648168627320723ITERATION : 99, loss : 0.022648168627320723ITERATION : 100, loss : 0.022648168627320723
ITERATION : 1, loss : 0.03008673329126504ITERATION : 2, loss : 0.012700995641926833ITERATION : 3, loss : 0.008340965478004152ITERATION : 4, loss : 0.009941023342540304ITERATION : 5, loss : 0.013053939071343721ITERATION : 6, loss : 0.015970126611037683ITERATION : 7, loss : 0.018268378444442453ITERATION : 8, loss : 0.019903117952642293ITERATION : 9, loss : 0.02098664690808904ITERATION : 10, loss : 0.021685606940460345ITERATION : 11, loss : 0.021869616571152512ITERATION : 12, loss : 0.02180313783330815ITERATION : 13, loss : 0.021761616604932265ITERATION : 14, loss : 0.02173576377601437ITERATION : 15, loss : 0.02171971665012167ITERATION : 16, loss : 0.021709785094925344ITERATION : 17, loss : 0.02170365493992644ITERATION : 18, loss : 0.021699880205024644ITERATION : 19, loss : 0.02169756111515422ITERATION : 20, loss : 0.02169613903311523ITERATION : 21, loss : 0.021695268603776678ITERATION : 22, loss : 0.021694736805479946ITERATION : 23, loss : 0.021694412509919495ITERATION : 24, loss : 0.021694214917231128ITERATION : 25, loss : 0.02169409490932398ITERATION : 26, loss : 0.021694021993093126ITERATION : 27, loss : 0.021693977877113038ITERATION : 28, loss : 0.021693951087686143ITERATION : 29, loss : 0.021693934932429007ITERATION : 30, loss : 0.021693925193833796ITERATION : 31, loss : 0.021693919355787315ITERATION : 32, loss : 0.021693915854168563ITERATION : 33, loss : 0.021693913797116514ITERATION : 34, loss : 0.021693912577019015ITERATION : 35, loss : 0.021693911901245082ITERATION : 36, loss : 0.021693911536286706ITERATION : 37, loss : 0.021693911291454298ITERATION : 38, loss : 0.021693911139554947ITERATION : 39, loss : 0.021693911083866917ITERATION : 40, loss : 0.02169391107021766ITERATION : 41, loss : 0.021693911048667226ITERATION : 42, loss : 0.021693911048667226ITERATION : 43, loss : 0.021693911048667226ITERATION : 44, loss : 0.021693911048667226ITERATION : 45, loss : 0.021693911048667226ITERATION : 46, loss : 0.021693911048667226ITERATION : 47, loss : 0.021693911048667226ITERATION : 48, loss : 0.021693911048667226ITERATION : 49, loss : 0.021693911048667226ITERATION : 50, loss : 0.021693911048667226ITERATION : 51, loss : 0.021693911048667226ITERATION : 52, loss : 0.021693911048667226ITERATION : 53, loss : 0.021693911048667226ITERATION : 54, loss : 0.021693911048667226ITERATION : 55, loss : 0.021693911048667226ITERATION : 56, loss : 0.021693911048667226ITERATION : 57, loss : 0.021693911048667226ITERATION : 58, loss : 0.021693911048667226ITERATION : 59, loss : 0.021693911048667226ITERATION : 60, loss : 0.021693911048667226ITERATION : 61, loss : 0.021693911048667226ITERATION : 62, loss : 0.021693911048667226ITERATION : 63, loss : 0.021693911048667226ITERATION : 64, loss : 0.021693911048667226ITERATION : 65, loss : 0.021693911048667226ITERATION : 66, loss : 0.021693911048667226ITERATION : 67, loss : 0.021693911048667226ITERATION : 68, loss : 0.021693911048667226ITERATION : 69, loss : 0.021693911048667226ITERATION : 70, loss : 0.021693911048667226ITERATION : 71, loss : 0.021693911048667226ITERATION : 72, loss : 0.021693911048667226ITERATION : 73, loss : 0.021693911048667226ITERATION : 74, loss : 0.021693911048667226ITERATION : 75, loss : 0.021693911048667226ITERATION : 76, loss : 0.021693911048667226ITERATION : 77, loss : 0.021693911048667226ITERATION : 78, loss : 0.021693911048667226ITERATION : 79, loss : 0.021693911048667226ITERATION : 80, loss : 0.021693911048667226ITERATION : 81, loss : 0.021693911048667226ITERATION : 82, loss : 0.021693911048667226ITERATION : 83, loss : 0.021693911048667226ITERATION : 84, loss : 0.021693911048667226ITERATION : 85, loss : 0.021693911048667226ITERATION : 86, loss : 0.021693911048667226ITERATION : 87, loss : 0.021693911048667226ITERATION : 88, loss : 0.021693911048667226ITERATION : 89, loss : 0.021693911048667226ITERATION : 90, loss : 0.021693911048667226ITERATION : 91, loss : 0.021693911048667226ITERATION : 92, loss : 0.021693911048667226ITERATION : 93, loss : 0.021693911048667226ITERATION : 94, loss : 0.021693911048667226ITERATION : 95, loss : 0.021693911048667226ITERATION : 96, loss : 0.021693911048667226ITERATION : 97, loss : 0.021693911048667226ITERATION : 98, loss : 0.021693911048667226ITERATION : 99, loss : 0.021693911048667226ITERATION : 100, loss : 0.021693911048667226
ITERATION : 1, loss : 0.04904675121282291ITERATION : 2, loss : 0.035492401148133124ITERATION : 3, loss : 0.026575782933593126ITERATION : 4, loss : 0.021041059634650333ITERATION : 5, loss : 0.017647544590370226ITERATION : 6, loss : 0.01553251019869368ITERATION : 7, loss : 0.014182973369672499ITERATION : 8, loss : 0.013302264793037602ITERATION : 9, loss : 0.012716481476208278ITERATION : 10, loss : 0.01232094428638589ITERATION : 11, loss : 0.012050777239634434ITERATION : 12, loss : 0.011864656088433083ITERATION : 13, loss : 0.011735628414995541ITERATION : 14, loss : 0.011645772427575892ITERATION : 15, loss : 0.011582989576462656ITERATION : 16, loss : 0.011539017887938005ITERATION : 17, loss : 0.011508167323273642ITERATION : 18, loss : 0.011486494319349639ITERATION : 19, loss : 0.011471253816135483ITERATION : 20, loss : 0.011460528452012626ITERATION : 21, loss : 0.011452975715885287ITERATION : 22, loss : 0.01144765446422428ITERATION : 23, loss : 0.01144390372914792ITERATION : 24, loss : 0.01144125887378787ITERATION : 25, loss : 0.01143939315815145ITERATION : 26, loss : 0.011438076547346656ITERATION : 27, loss : 0.011437147206441876ITERATION : 28, loss : 0.011436490840438848ITERATION : 29, loss : 0.011436027239510024ITERATION : 30, loss : 0.011435699644150218ITERATION : 31, loss : 0.011435468082062679ITERATION : 32, loss : 0.011435304366222978ITERATION : 33, loss : 0.011435188589210546ITERATION : 34, loss : 0.011435106652132277ITERATION : 35, loss : 0.011435048652895087ITERATION : 36, loss : 0.011435007535591048ITERATION : 37, loss : 0.011434978474895137ITERATION : 38, loss : 0.011434957833925716ITERATION : 39, loss : 0.011434943200893059ITERATION : 40, loss : 0.011434932859764484ITERATION : 41, loss : 0.011434925507304447ITERATION : 42, loss : 0.011434920339104511ITERATION : 43, loss : 0.011434916674104567ITERATION : 44, loss : 0.011434914081044781ITERATION : 45, loss : 0.0114349122461429ITERATION : 46, loss : 0.011434910938694826ITERATION : 47, loss : 0.011434910007470366ITERATION : 48, loss : 0.01143490938559057ITERATION : 49, loss : 0.01143490891349126ITERATION : 50, loss : 0.011434908663334771ITERATION : 51, loss : 0.011434908374624953ITERATION : 52, loss : 0.011434908260026685ITERATION : 53, loss : 0.011434908125086006ITERATION : 54, loss : 0.011434908067911264ITERATION : 55, loss : 0.011434908008074918ITERATION : 56, loss : 0.011434907975562103ITERATION : 57, loss : 0.011434907975075435ITERATION : 58, loss : 0.011434907975075435ITERATION : 59, loss : 0.011434907975075435ITERATION : 60, loss : 0.011434907975075435ITERATION : 61, loss : 0.011434907975075435ITERATION : 62, loss : 0.011434907975075435ITERATION : 63, loss : 0.011434907975075435ITERATION : 64, loss : 0.011434907975075435ITERATION : 65, loss : 0.011434907975075435ITERATION : 66, loss : 0.011434907975075435ITERATION : 67, loss : 0.011434907975075435ITERATION : 68, loss : 0.011434907975075435ITERATION : 69, loss : 0.011434907975075435ITERATION : 70, loss : 0.011434907975075435ITERATION : 71, loss : 0.011434907975075435ITERATION : 72, loss : 0.011434907975075435ITERATION : 73, loss : 0.011434907975075435ITERATION : 74, loss : 0.011434907975075435ITERATION : 75, loss : 0.011434907975075435ITERATION : 76, loss : 0.011434907975075435ITERATION : 77, loss : 0.011434907975075435ITERATION : 78, loss : 0.011434907975075435ITERATION : 79, loss : 0.011434907975075435ITERATION : 80, loss : 0.011434907975075435ITERATION : 81, loss : 0.011434907975075435ITERATION : 82, loss : 0.011434907975075435ITERATION : 83, loss : 0.011434907975075435ITERATION : 84, loss : 0.011434907975075435ITERATION : 85, loss : 0.011434907975075435ITERATION : 86, loss : 0.011434907975075435ITERATION : 87, loss : 0.011434907975075435ITERATION : 88, loss : 0.011434907975075435ITERATION : 89, loss : 0.011434907975075435ITERATION : 90, loss : 0.011434907975075435ITERATION : 91, loss : 0.011434907975075435ITERATION : 92, loss : 0.011434907975075435ITERATION : 93, loss : 0.011434907975075435ITERATION : 94, loss : 0.011434907975075435ITERATION : 95, loss : 0.011434907975075435ITERATION : 96, loss : 0.011434907975075435ITERATION : 97, loss : 0.011434907975075435ITERATION : 98, loss : 0.011434907975075435ITERATION : 99, loss : 0.011434907975075435ITERATION : 100, loss : 0.011434907975075435
gradient norm in None layer : 0.0006566860110215925
gradient norm in None layer : 3.728313794170442e-05
gradient norm in None layer : 4.282285123207393e-05
gradient norm in None layer : 0.0005206031146866559
gradient norm in None layer : 4.207332541250865e-05
gradient norm in None layer : 4.8579356402768026e-05
gradient norm in None layer : 0.0002210414468300297
gradient norm in None layer : 8.371195157420555e-06
gradient norm in None layer : 8.610001668221742e-06
gradient norm in None layer : 0.00016590976520832693
gradient norm in None layer : 7.44531653304946e-06
gradient norm in None layer : 7.710818314054767e-06
gradient norm in None layer : 5.912879728923548e-05
gradient norm in None layer : 1.9490143096887373e-06
gradient norm in None layer : 1.568416553028781e-06
gradient norm in None layer : 4.882559926765878e-05
gradient norm in None layer : 2.0608258991127637e-06
gradient norm in None layer : 1.733300237825253e-06
gradient norm in None layer : 6.746492405131698e-05
gradient norm in None layer : 9.789579874820247e-07
gradient norm in None layer : 0.0001509169596228445
gradient norm in None layer : 1.0419901727107025e-05
gradient norm in None layer : 8.726235642687384e-06
gradient norm in None layer : 0.000174580516647861
gradient norm in None layer : 2.102600836225136e-05
gradient norm in None layer : 2.1767561709670158e-05
gradient norm in None layer : 0.00030897052055821783
gradient norm in None layer : 2.6694615065169535e-06
gradient norm in None layer : 0.0005134406998184392
gradient norm in None layer : 4.3689072340460345e-05
gradient norm in None layer : 4.874484138153637e-05
gradient norm in None layer : 0.0006631489128296494
gradient norm in None layer : 5.729216204988473e-05
gradient norm in None layer : 6.966264096885296e-05
gradient norm in None layer : 5.0774520842231756e-05
gradient norm in None layer : 9.350560760754402e-06
Total gradient norm: 0.0012903514632704538
invariance loss : 4.281040702066559, avg_den : 0.42455291748046875, density loss : 0.32455291748046877, mse loss : 0.021336429596774127, solver time : 128.22022199630737 sec , total loss : 0.025942023216321154, running loss : 0.04214836786472383
Epoch 0/10 , batch 29/12500 
ITERATION : 1, loss : 0.03239233238923635ITERATION : 2, loss : 0.02745425179950275ITERATION : 3, loss : 0.023776756145169314ITERATION : 4, loss : 0.0221012170981842ITERATION : 5, loss : 0.02130365874892437ITERATION : 6, loss : 0.0209091759594824ITERATION : 7, loss : 0.020706686216089495ITERATION : 8, loss : 0.0205993536043743ITERATION : 9, loss : 0.02054106284503198ITERATION : 10, loss : 0.02050892855573889ITERATION : 11, loss : 0.02049112206005995ITERATION : 12, loss : 0.020481305316435643ITERATION : 13, loss : 0.020475983426907117ITERATION : 14, loss : 0.020473190301514774ITERATION : 15, loss : 0.020471806785321883ITERATION : 16, loss : 0.02047119378680347ITERATION : 17, loss : 0.02047098774230773ITERATION : 18, loss : 0.020470984200882926ITERATION : 19, loss : 0.020471070943625158ITERATION : 20, loss : 0.020471188262617464ITERATION : 21, loss : 0.020471306455647764ITERATION : 22, loss : 0.020471412091685858ITERATION : 23, loss : 0.020471500543869783ITERATION : 24, loss : 0.020471571758899527ITERATION : 25, loss : 0.020471627500307808ITERATION : 26, loss : 0.02047167026123219ITERATION : 27, loss : 0.02047170252269545ITERATION : 28, loss : 0.02047172664460751ITERATION : 29, loss : 0.020471744512125842ITERATION : 30, loss : 0.02047175757997595ITERATION : 31, loss : 0.020471767119335654ITERATION : 32, loss : 0.020471774050214612ITERATION : 33, loss : 0.020471779032113717ITERATION : 34, loss : 0.020471782610553376ITERATION : 35, loss : 0.020471785168401528ITERATION : 36, loss : 0.02047178699234465ITERATION : 37, loss : 0.020471788340171247ITERATION : 38, loss : 0.020471789281371652ITERATION : 39, loss : 0.02047178995377046ITERATION : 40, loss : 0.020471790421433485ITERATION : 41, loss : 0.020471790759424322ITERATION : 42, loss : 0.020471791013381847ITERATION : 43, loss : 0.020471791184401577ITERATION : 44, loss : 0.020471791297166347ITERATION : 45, loss : 0.020471791383135055ITERATION : 46, loss : 0.020471791431502465ITERATION : 47, loss : 0.02047179147403473ITERATION : 48, loss : 0.020471791506806704ITERATION : 49, loss : 0.020471791526977125ITERATION : 50, loss : 0.02047179153410841ITERATION : 51, loss : 0.02047179153410841ITERATION : 52, loss : 0.02047179153410841ITERATION : 53, loss : 0.02047179153410841ITERATION : 54, loss : 0.02047179153410841ITERATION : 55, loss : 0.02047179153410841ITERATION : 56, loss : 0.02047179153410841ITERATION : 57, loss : 0.02047179153410841ITERATION : 58, loss : 0.02047179153410841ITERATION : 59, loss : 0.02047179153410841ITERATION : 60, loss : 0.02047179153410841ITERATION : 61, loss : 0.02047179153410841ITERATION : 62, loss : 0.02047179153410841ITERATION : 63, loss : 0.02047179153410841ITERATION : 64, loss : 0.02047179153410841ITERATION : 65, loss : 0.02047179153410841ITERATION : 66, loss : 0.02047179153410841ITERATION : 67, loss : 0.02047179153410841ITERATION : 68, loss : 0.02047179153410841ITERATION : 69, loss : 0.02047179153410841ITERATION : 70, loss : 0.02047179153410841ITERATION : 71, loss : 0.02047179153410841ITERATION : 72, loss : 0.02047179153410841ITERATION : 73, loss : 0.02047179153410841ITERATION : 74, loss : 0.02047179153410841ITERATION : 75, loss : 0.02047179153410841ITERATION : 76, loss : 0.02047179153410841ITERATION : 77, loss : 0.02047179153410841ITERATION : 78, loss : 0.02047179153410841ITERATION : 79, loss : 0.02047179153410841ITERATION : 80, loss : 0.02047179153410841ITERATION : 81, loss : 0.02047179153410841ITERATION : 82, loss : 0.02047179153410841ITERATION : 83, loss : 0.02047179153410841ITERATION : 84, loss : 0.02047179153410841ITERATION : 85, loss : 0.02047179153410841ITERATION : 86, loss : 0.02047179153410841ITERATION : 87, loss : 0.02047179153410841ITERATION : 88, loss : 0.02047179153410841ITERATION : 89, loss : 0.02047179153410841ITERATION : 90, loss : 0.02047179153410841ITERATION : 91, loss : 0.02047179153410841ITERATION : 92, loss : 0.02047179153410841ITERATION : 93, loss : 0.02047179153410841ITERATION : 94, loss : 0.02047179153410841ITERATION : 95, loss : 0.02047179153410841ITERATION : 96, loss : 0.02047179153410841ITERATION : 97, loss : 0.02047179153410841ITERATION : 98, loss : 0.02047179153410841ITERATION : 99, loss : 0.02047179153410841ITERATION : 100, loss : 0.02047179153410841
ITERATION : 1, loss : 0.0573516777630301ITERATION : 2, loss : 0.04729450674206164ITERATION : 3, loss : 0.0435984421612299ITERATION : 4, loss : 0.042678253157248415ITERATION : 5, loss : 0.042869011292812204ITERATION : 6, loss : 0.04344031680705739ITERATION : 7, loss : 0.04407760376264447ITERATION : 8, loss : 0.0446557449804199ITERATION : 9, loss : 0.045135664034531585ITERATION : 10, loss : 0.045515539817727525ITERATION : 11, loss : 0.04580780337421567ITERATION : 12, loss : 0.04602863623865857ITERATION : 13, loss : 0.046151225360043215ITERATION : 14, loss : 0.04615284730009315ITERATION : 15, loss : 0.04615475421183542ITERATION : 16, loss : 0.04615653976872092ITERATION : 17, loss : 0.046158061465695614ITERATION : 18, loss : 0.04615929554221234ITERATION : 19, loss : 0.046160267672809195ITERATION : 20, loss : 0.046161019470673074ITERATION : 21, loss : 0.046161593846826686ITERATION : 22, loss : 0.0461620290421994ITERATION : 23, loss : 0.04616235686500258ITERATION : 24, loss : 0.046162602771680605ITERATION : 25, loss : 0.046162786667849565ITERATION : 26, loss : 0.046162923875216796ITERATION : 27, loss : 0.046163026070012964ITERATION : 28, loss : 0.0461631020911393ITERATION : 29, loss : 0.0461631585788133ITERATION : 30, loss : 0.046163200516325886ITERATION : 31, loss : 0.04616323162850805ITERATION : 32, loss : 0.04616325469558934ITERATION : 33, loss : 0.0461632717857318ITERATION : 34, loss : 0.0461632844507654ITERATION : 35, loss : 0.04616329382338946ITERATION : 36, loss : 0.046163300759674404ITERATION : 37, loss : 0.046163305896263995ITERATION : 38, loss : 0.046163309693486775ITERATION : 39, loss : 0.04616331249961816ITERATION : 40, loss : 0.04616331456810331ITERATION : 41, loss : 0.04616331609853201ITERATION : 42, loss : 0.0461633172331817ITERATION : 43, loss : 0.04616331806871748ITERATION : 44, loss : 0.04616331868267427ITERATION : 45, loss : 0.046163319140865175ITERATION : 46, loss : 0.04616331948272095ITERATION : 47, loss : 0.0461633197309479ITERATION : 48, loss : 0.04616331991525402ITERATION : 49, loss : 0.046163320046074596ITERATION : 50, loss : 0.04616332014435184ITERATION : 51, loss : 0.046163320219429ITERATION : 52, loss : 0.046163320275604755ITERATION : 53, loss : 0.04616332031470358ITERATION : 54, loss : 0.04616332034939481ITERATION : 55, loss : 0.04616332036956604ITERATION : 56, loss : 0.04616332038760563ITERATION : 57, loss : 0.04616332039533236ITERATION : 58, loss : 0.04616332040722117ITERATION : 59, loss : 0.04616332040691269ITERATION : 60, loss : 0.04616332040691269ITERATION : 61, loss : 0.04616332040691269ITERATION : 62, loss : 0.04616332040691269ITERATION : 63, loss : 0.04616332040691269ITERATION : 64, loss : 0.04616332040691269ITERATION : 65, loss : 0.04616332040691269ITERATION : 66, loss : 0.04616332040691269ITERATION : 67, loss : 0.04616332040691269ITERATION : 68, loss : 0.04616332040691269ITERATION : 69, loss : 0.04616332040691269ITERATION : 70, loss : 0.04616332040691269ITERATION : 71, loss : 0.04616332040691269ITERATION : 72, loss : 0.04616332040691269ITERATION : 73, loss : 0.04616332040691269ITERATION : 74, loss : 0.04616332040691269ITERATION : 75, loss : 0.04616332040691269ITERATION : 76, loss : 0.04616332040691269ITERATION : 77, loss : 0.04616332040691269ITERATION : 78, loss : 0.04616332040691269ITERATION : 79, loss : 0.04616332040691269ITERATION : 80, loss : 0.04616332040691269ITERATION : 81, loss : 0.04616332040691269ITERATION : 82, loss : 0.04616332040691269ITERATION : 83, loss : 0.04616332040691269ITERATION : 84, loss : 0.04616332040691269ITERATION : 85, loss : 0.04616332040691269ITERATION : 86, loss : 0.04616332040691269ITERATION : 87, loss : 0.04616332040691269ITERATION : 88, loss : 0.04616332040691269ITERATION : 89, loss : 0.04616332040691269ITERATION : 90, loss : 0.04616332040691269ITERATION : 91, loss : 0.04616332040691269ITERATION : 92, loss : 0.04616332040691269ITERATION : 93, loss : 0.04616332040691269ITERATION : 94, loss : 0.04616332040691269ITERATION : 95, loss : 0.04616332040691269ITERATION : 96, loss : 0.04616332040691269ITERATION : 97, loss : 0.04616332040691269ITERATION : 98, loss : 0.04616332040691269ITERATION : 99, loss : 0.04616332040691269ITERATION : 100, loss : 0.04616332040691269
ITERATION : 1, loss : 0.01819746756923684ITERATION : 2, loss : 0.010606692490269973ITERATION : 3, loss : 0.010551471463874773ITERATION : 4, loss : 0.011214289780075276ITERATION : 5, loss : 0.012058997152312538ITERATION : 6, loss : 0.01278913507385269ITERATION : 7, loss : 0.013288467476792631ITERATION : 8, loss : 0.013622101879948308ITERATION : 9, loss : 0.013842208066501748ITERATION : 10, loss : 0.013986408578508308ITERATION : 11, loss : 0.014080540067280704ITERATION : 12, loss : 0.01414189114288152ITERATION : 13, loss : 0.014181864453704094ITERATION : 14, loss : 0.014207920963031298ITERATION : 15, loss : 0.01422492145271796ITERATION : 16, loss : 0.014236026777056545ITERATION : 17, loss : 0.014243291169682903ITERATION : 18, loss : 0.014248049978452434ITERATION : 19, loss : 0.014251172257881467ITERATION : 20, loss : 0.0142532238347421ITERATION : 21, loss : 0.014254574004165986ITERATION : 22, loss : 0.014255463920687774ITERATION : 23, loss : 0.014256051424015773ITERATION : 24, loss : 0.014256439889310921ITERATION : 25, loss : 0.014256696940331968ITERATION : 26, loss : 0.014256867407439023ITERATION : 27, loss : 0.01425698053781346ITERATION : 28, loss : 0.01425705584237027ITERATION : 29, loss : 0.014257105882641745ITERATION : 30, loss : 0.01425713920598161ITERATION : 31, loss : 0.014257161493250367ITERATION : 32, loss : 0.014257176357464735ITERATION : 33, loss : 0.01425718638230799ITERATION : 34, loss : 0.014257193076574705ITERATION : 35, loss : 0.014257197516205557ITERATION : 36, loss : 0.014257200448607895ITERATION : 37, loss : 0.014257202425757117ITERATION : 38, loss : 0.01425720374843805ITERATION : 39, loss : 0.014257204635251246ITERATION : 40, loss : 0.01425720521514439ITERATION : 41, loss : 0.014257205610317115ITERATION : 42, loss : 0.014257205839735424ITERATION : 43, loss : 0.01425720598821286ITERATION : 44, loss : 0.014257206078124588ITERATION : 45, loss : 0.014257206158877506ITERATION : 46, loss : 0.0142572062415808ITERATION : 47, loss : 0.014257206291593594ITERATION : 48, loss : 0.014257206294818196ITERATION : 49, loss : 0.014257206294818196ITERATION : 50, loss : 0.014257206294818196ITERATION : 51, loss : 0.014257206294818196ITERATION : 52, loss : 0.014257206294818196ITERATION : 53, loss : 0.014257206294818196ITERATION : 54, loss : 0.014257206294818196ITERATION : 55, loss : 0.014257206294818196ITERATION : 56, loss : 0.014257206294818196ITERATION : 57, loss : 0.014257206294818196ITERATION : 58, loss : 0.014257206294818196ITERATION : 59, loss : 0.014257206294818196ITERATION : 60, loss : 0.014257206294818196ITERATION : 61, loss : 0.014257206294818196ITERATION : 62, loss : 0.014257206294818196ITERATION : 63, loss : 0.014257206294818196ITERATION : 64, loss : 0.014257206294818196ITERATION : 65, loss : 0.014257206294818196ITERATION : 66, loss : 0.014257206294818196ITERATION : 67, loss : 0.014257206294818196ITERATION : 68, loss : 0.014257206294818196ITERATION : 69, loss : 0.014257206294818196ITERATION : 70, loss : 0.014257206294818196ITERATION : 71, loss : 0.014257206294818196ITERATION : 72, loss : 0.014257206294818196ITERATION : 73, loss : 0.014257206294818196ITERATION : 74, loss : 0.014257206294818196ITERATION : 75, loss : 0.014257206294818196ITERATION : 76, loss : 0.014257206294818196ITERATION : 77, loss : 0.014257206294818196ITERATION : 78, loss : 0.014257206294818196ITERATION : 79, loss : 0.014257206294818196ITERATION : 80, loss : 0.014257206294818196ITERATION : 81, loss : 0.014257206294818196ITERATION : 82, loss : 0.014257206294818196ITERATION : 83, loss : 0.014257206294818196ITERATION : 84, loss : 0.014257206294818196ITERATION : 85, loss : 0.014257206294818196ITERATION : 86, loss : 0.014257206294818196ITERATION : 87, loss : 0.014257206294818196ITERATION : 88, loss : 0.014257206294818196ITERATION : 89, loss : 0.014257206294818196ITERATION : 90, loss : 0.014257206294818196ITERATION : 91, loss : 0.014257206294818196ITERATION : 92, loss : 0.014257206294818196ITERATION : 93, loss : 0.014257206294818196ITERATION : 94, loss : 0.014257206294818196ITERATION : 95, loss : 0.014257206294818196ITERATION : 96, loss : 0.014257206294818196ITERATION : 97, loss : 0.014257206294818196ITERATION : 98, loss : 0.014257206294818196ITERATION : 99, loss : 0.014257206294818196ITERATION : 100, loss : 0.014257206294818196
ITERATION : 1, loss : 0.013074048330005642ITERATION : 2, loss : 0.01067897052345648ITERATION : 3, loss : 0.009857359893404846ITERATION : 4, loss : 0.009584046932441984ITERATION : 5, loss : 0.009513235687161047ITERATION : 6, loss : 0.009519096583263838ITERATION : 7, loss : 0.009550527116825195ITERATION : 8, loss : 0.009586281275083066ITERATION : 9, loss : 0.009618232437112049ITERATION : 10, loss : 0.009644057350362327ITERATION : 11, loss : 0.009663843967408053ITERATION : 12, loss : 0.009678527276264247ITERATION : 13, loss : 0.009689203639667148ITERATION : 14, loss : 0.009696862416627004ITERATION : 15, loss : 0.009702306387615536ITERATION : 16, loss : 0.009706151594565955ITERATION : 17, loss : 0.009708855491729604ITERATION : 18, loss : 0.00971075083867716ITERATION : 19, loss : 0.009712076488678934ITERATION : 20, loss : 0.009713002096376986ITERATION : 21, loss : 0.009713647620261936ITERATION : 22, loss : 0.00971409741344144ITERATION : 23, loss : 0.009714410586791103ITERATION : 24, loss : 0.009714628604107777ITERATION : 25, loss : 0.009714780217152294ITERATION : 26, loss : 0.009714885649907419ITERATION : 27, loss : 0.009714958914296745ITERATION : 28, loss : 0.009715009865452625ITERATION : 29, loss : 0.009715045235426451ITERATION : 30, loss : 0.009715069845280882ITERATION : 31, loss : 0.009715086916377246ITERATION : 32, loss : 0.009715098771985207ITERATION : 33, loss : 0.009715106973421265ITERATION : 34, loss : 0.009715112696475922ITERATION : 35, loss : 0.00971511667172507ITERATION : 36, loss : 0.009715119401565662ITERATION : 37, loss : 0.009715121282459913ITERATION : 38, loss : 0.009715122575146213ITERATION : 39, loss : 0.009715123464292442ITERATION : 40, loss : 0.009715124077631455ITERATION : 41, loss : 0.009715124500625644ITERATION : 42, loss : 0.00971512479352711ITERATION : 43, loss : 0.009715124990196274ITERATION : 44, loss : 0.00971512513051404ITERATION : 45, loss : 0.009715125220236662ITERATION : 46, loss : 0.009715125282832221ITERATION : 47, loss : 0.009715125328341226ITERATION : 48, loss : 0.009715125353010468ITERATION : 49, loss : 0.00971512536870124ITERATION : 50, loss : 0.009715125383927362ITERATION : 51, loss : 0.00971512539099781ITERATION : 52, loss : 0.00971512539648612ITERATION : 53, loss : 0.009715125409388289ITERATION : 54, loss : 0.009715125409408826ITERATION : 55, loss : 0.009715125409408826ITERATION : 56, loss : 0.009715125409408826ITERATION : 57, loss : 0.009715125409408826ITERATION : 58, loss : 0.009715125409408826ITERATION : 59, loss : 0.009715125409408826ITERATION : 60, loss : 0.009715125409408826ITERATION : 61, loss : 0.009715125409408826ITERATION : 62, loss : 0.009715125409408826ITERATION : 63, loss : 0.009715125409408826ITERATION : 64, loss : 0.009715125409408826ITERATION : 65, loss : 0.009715125409408826ITERATION : 66, loss : 0.009715125409408826ITERATION : 67, loss : 0.009715125409408826ITERATION : 68, loss : 0.009715125409408826ITERATION : 69, loss : 0.009715125409408826ITERATION : 70, loss : 0.009715125409408826ITERATION : 71, loss : 0.009715125409408826ITERATION : 72, loss : 0.009715125409408826ITERATION : 73, loss : 0.009715125409408826ITERATION : 74, loss : 0.009715125409408826ITERATION : 75, loss : 0.009715125409408826ITERATION : 76, loss : 0.009715125409408826ITERATION : 77, loss : 0.009715125409408826ITERATION : 78, loss : 0.009715125409408826ITERATION : 79, loss : 0.009715125409408826ITERATION : 80, loss : 0.009715125409408826ITERATION : 81, loss : 0.009715125409408826ITERATION : 82, loss : 0.009715125409408826ITERATION : 83, loss : 0.009715125409408826ITERATION : 84, loss : 0.009715125409408826ITERATION : 85, loss : 0.009715125409408826ITERATION : 86, loss : 0.009715125409408826ITERATION : 87, loss : 0.009715125409408826ITERATION : 88, loss : 0.009715125409408826ITERATION : 89, loss : 0.009715125409408826ITERATION : 90, loss : 0.009715125409408826ITERATION : 91, loss : 0.009715125409408826ITERATION : 92, loss : 0.009715125409408826ITERATION : 93, loss : 0.009715125409408826ITERATION : 94, loss : 0.009715125409408826ITERATION : 95, loss : 0.009715125409408826ITERATION : 96, loss : 0.009715125409408826ITERATION : 97, loss : 0.009715125409408826ITERATION : 98, loss : 0.009715125409408826ITERATION : 99, loss : 0.009715125409408826ITERATION : 100, loss : 0.009715125409408826
ITERATION : 1, loss : 0.03742451278226951ITERATION : 2, loss : 0.0299110317989761ITERATION : 3, loss : 0.02709424108764013ITERATION : 4, loss : 0.0258480159789693ITERATION : 5, loss : 0.025224852305223706ITERATION : 6, loss : 0.024884137142055892ITERATION : 7, loss : 0.024710279532542834ITERATION : 8, loss : 0.024650406487676777ITERATION : 9, loss : 0.024606478971966446ITERATION : 10, loss : 0.02457167697220536ITERATION : 11, loss : 0.02454320522368499ITERATION : 12, loss : 0.024519745514055703ITERATION : 13, loss : 0.02450048958691456ITERATION : 14, loss : 0.02448480283984611ITERATION : 15, loss : 0.024472125879313394ITERATION : 16, loss : 0.0244619554185029ITERATION : 17, loss : 0.024453846040198304ITERATION : 18, loss : 0.024447412621981787ITERATION : 19, loss : 0.02444232959072231ITERATION : 20, loss : 0.024438326674614836ITERATION : 21, loss : 0.02443518261642942ITERATION : 22, loss : 0.024432718391325638ITERATION : 23, loss : 0.024430790250258048ITERATION : 24, loss : 0.024429283713560995ITERATION : 25, loss : 0.02442810792058614ITERATION : 26, loss : 0.02442719108752613ITERATION : 27, loss : 0.024426476777096005ITERATION : 28, loss : 0.024425920573256904ITERATION : 29, loss : 0.024425487741618093ITERATION : 30, loss : 0.024425151052331832ITERATION : 31, loss : 0.02442488925408787ITERATION : 32, loss : 0.024424685759292793ITERATION : 33, loss : 0.02442452763727288ITERATION : 34, loss : 0.02442440479266691ITERATION : 35, loss : 0.024424309374031485ITERATION : 36, loss : 0.024424235271989708ITERATION : 37, loss : 0.024424177736161613ITERATION : 38, loss : 0.02442413306571836ITERATION : 39, loss : 0.02442409839294603ITERATION : 40, loss : 0.024424071480114967ITERATION : 41, loss : 0.02442405059131004ITERATION : 42, loss : 0.0244240343807514ITERATION : 43, loss : 0.024424021820375463ITERATION : 44, loss : 0.02442401205385132ITERATION : 45, loss : 0.024424004479512273ITERATION : 46, loss : 0.02442399859951999ITERATION : 47, loss : 0.02442399407110603ITERATION : 48, loss : 0.024423990523499133ITERATION : 49, loss : 0.024423987774541488ITERATION : 50, loss : 0.024423985645146577ITERATION : 51, loss : 0.02442398397805562ITERATION : 52, loss : 0.02442398268241024ITERATION : 53, loss : 0.02442398172781845ITERATION : 54, loss : 0.02442398094883962ITERATION : 55, loss : 0.024423980360753376ITERATION : 56, loss : 0.02442397988865375ITERATION : 57, loss : 0.02442397952261381ITERATION : 58, loss : 0.024423979200119506ITERATION : 59, loss : 0.024423978957390905ITERATION : 60, loss : 0.02442397882302592ITERATION : 61, loss : 0.024423978727439552ITERATION : 62, loss : 0.024423978650027163ITERATION : 63, loss : 0.024423978586843073ITERATION : 64, loss : 0.024423978537607246ITERATION : 65, loss : 0.024423978475786284ITERATION : 66, loss : 0.024423978456353787ITERATION : 67, loss : 0.024423978442333374ITERATION : 68, loss : 0.024423978430974203ITERATION : 69, loss : 0.024423978423731535ITERATION : 70, loss : 0.024423978421174726ITERATION : 71, loss : 0.024423978380526446ITERATION : 72, loss : 0.024423978380289698ITERATION : 73, loss : 0.024423978380289698ITERATION : 74, loss : 0.024423978380289698ITERATION : 75, loss : 0.024423978380289698ITERATION : 76, loss : 0.024423978380289698ITERATION : 77, loss : 0.024423978380289698ITERATION : 78, loss : 0.024423978380289698ITERATION : 79, loss : 0.024423978380289698ITERATION : 80, loss : 0.024423978380289698ITERATION : 81, loss : 0.024423978380289698ITERATION : 82, loss : 0.024423978380289698ITERATION : 83, loss : 0.024423978380289698ITERATION : 84, loss : 0.024423978380289698ITERATION : 85, loss : 0.024423978380289698ITERATION : 86, loss : 0.024423978380289698ITERATION : 87, loss : 0.024423978380289698ITERATION : 88, loss : 0.024423978380289698ITERATION : 89, loss : 0.024423978380289698ITERATION : 90, loss : 0.024423978380289698ITERATION : 91, loss : 0.024423978380289698ITERATION : 92, loss : 0.024423978380289698ITERATION : 93, loss : 0.024423978380289698ITERATION : 94, loss : 0.024423978380289698ITERATION : 95, loss : 0.024423978380289698ITERATION : 96, loss : 0.024423978380289698ITERATION : 97, loss : 0.024423978380289698ITERATION : 98, loss : 0.024423978380289698ITERATION : 99, loss : 0.024423978380289698ITERATION : 100, loss : 0.024423978380289698
ITERATION : 1, loss : 0.03420112064991618ITERATION : 2, loss : 0.031856029119970534ITERATION : 3, loss : 0.030633274474215513ITERATION : 4, loss : 0.029523016294251987ITERATION : 5, loss : 0.028666121547582017ITERATION : 6, loss : 0.02806737614579448ITERATION : 7, loss : 0.027668525669254006ITERATION : 8, loss : 0.02740873338530694ITERATION : 9, loss : 0.02724127101772943ITERATION : 10, loss : 0.027133851928391357ITERATION : 11, loss : 0.027065130389475472ITERATION : 12, loss : 0.027021258147802492ITERATION : 13, loss : 0.026993318312714423ITERATION : 14, loss : 0.02697558313326743ITERATION : 15, loss : 0.026964375579691984ITERATION : 16, loss : 0.026957334317533006ITERATION : 17, loss : 0.02695294433114881ITERATION : 18, loss : 0.026950234225572377ITERATION : 19, loss : 0.02694858250126674ITERATION : 20, loss : 0.026947592752348266ITERATION : 21, loss : 0.026947012932469382ITERATION : 22, loss : 0.02694668426549703ITERATION : 23, loss : 0.026946506745516283ITERATION : 24, loss : 0.02694641821484195ITERATION : 25, loss : 0.02694638105252752ITERATION : 26, loss : 0.026946371877132044ITERATION : 27, loss : 0.026946376935005293ITERATION : 28, loss : 0.02694638834867135ITERATION : 29, loss : 0.02694640178913516ITERATION : 30, loss : 0.026946414908354664ITERATION : 31, loss : 0.026946426650363377ITERATION : 32, loss : 0.026946436457931607ITERATION : 33, loss : 0.026946444686299805ITERATION : 34, loss : 0.026946451230063173ITERATION : 35, loss : 0.026946456417425848ITERATION : 36, loss : 0.02694646031303191ITERATION : 37, loss : 0.02694646327683337ITERATION : 38, loss : 0.026946465559410913ITERATION : 39, loss : 0.026946467370377994ITERATION : 40, loss : 0.02694646870873306ITERATION : 41, loss : 0.026946469778820342ITERATION : 42, loss : 0.02694647053203177ITERATION : 43, loss : 0.02694647110644063ITERATION : 44, loss : 0.026946471544121477ITERATION : 45, loss : 0.026946471941665637ITERATION : 46, loss : 0.02694647216211696ITERATION : 47, loss : 0.02694647235793301ITERATION : 48, loss : 0.026946472430558453ITERATION : 49, loss : 0.02694647252864216ITERATION : 50, loss : 0.026946472538941616ITERATION : 51, loss : 0.026946472541716723ITERATION : 52, loss : 0.02694647254176009ITERATION : 53, loss : 0.02694647254176009ITERATION : 54, loss : 0.02694647254176009ITERATION : 55, loss : 0.02694647254176009ITERATION : 56, loss : 0.02694647254176009ITERATION : 57, loss : 0.02694647254176009ITERATION : 58, loss : 0.02694647254176009ITERATION : 59, loss : 0.02694647254176009ITERATION : 60, loss : 0.02694647254176009ITERATION : 61, loss : 0.02694647254176009ITERATION : 62, loss : 0.02694647254176009ITERATION : 63, loss : 0.02694647254176009ITERATION : 64, loss : 0.02694647254176009ITERATION : 65, loss : 0.02694647254176009ITERATION : 66, loss : 0.02694647254176009ITERATION : 67, loss : 0.02694647254176009ITERATION : 68, loss : 0.02694647254176009ITERATION : 69, loss : 0.02694647254176009ITERATION : 70, loss : 0.02694647254176009ITERATION : 71, loss : 0.02694647254176009ITERATION : 72, loss : 0.02694647254176009ITERATION : 73, loss : 0.02694647254176009ITERATION : 74, loss : 0.02694647254176009ITERATION : 75, loss : 0.02694647254176009ITERATION : 76, loss : 0.02694647254176009ITERATION : 77, loss : 0.02694647254176009ITERATION : 78, loss : 0.02694647254176009ITERATION : 79, loss : 0.02694647254176009ITERATION : 80, loss : 0.02694647254176009ITERATION : 81, loss : 0.02694647254176009ITERATION : 82, loss : 0.02694647254176009ITERATION : 83, loss : 0.02694647254176009ITERATION : 84, loss : 0.02694647254176009ITERATION : 85, loss : 0.02694647254176009ITERATION : 86, loss : 0.02694647254176009ITERATION : 87, loss : 0.02694647254176009ITERATION : 88, loss : 0.02694647254176009ITERATION : 89, loss : 0.02694647254176009ITERATION : 90, loss : 0.02694647254176009ITERATION : 91, loss : 0.02694647254176009ITERATION : 92, loss : 0.02694647254176009ITERATION : 93, loss : 0.02694647254176009ITERATION : 94, loss : 0.02694647254176009ITERATION : 95, loss : 0.02694647254176009ITERATION : 96, loss : 0.02694647254176009ITERATION : 97, loss : 0.02694647254176009ITERATION : 98, loss : 0.02694647254176009ITERATION : 99, loss : 0.02694647254176009ITERATION : 100, loss : 0.02694647254176009
ITERATION : 1, loss : 0.026530476097868756ITERATION : 2, loss : 0.0262932801283298ITERATION : 3, loss : 0.027233649208141916ITERATION : 4, loss : 0.02804840925884936ITERATION : 5, loss : 0.028657638882137213ITERATION : 6, loss : 0.029094017647367503ITERATION : 7, loss : 0.029401023686549493ITERATION : 8, loss : 0.029615397877402697ITERATION : 9, loss : 0.02976475007797178ITERATION : 10, loss : 0.029868848236298866ITERATION : 11, loss : 0.029941533742328837ITERATION : 12, loss : 0.02999240428406472ITERATION : 13, loss : 0.030028094328213954ITERATION : 14, loss : 0.030053192314058726ITERATION : 15, loss : 0.0300708785310525ITERATION : 16, loss : 0.030083364432404093ITERATION : 17, loss : 0.030092192604977416ITERATION : 18, loss : 0.03009844252509192ITERATION : 19, loss : 0.03010287171795748ITERATION : 20, loss : 0.030106013278564506ITERATION : 21, loss : 0.0301082430212289ITERATION : 22, loss : 0.03010982637914033ITERATION : 23, loss : 0.030110951215468272ITERATION : 24, loss : 0.03011175063102106ITERATION : 25, loss : 0.030112318809503533ITERATION : 26, loss : 0.03011272275276219ITERATION : 27, loss : 0.030113009964117973ITERATION : 28, loss : 0.0301132141437649ITERATION : 29, loss : 0.030113359372408173ITERATION : 30, loss : 0.030113462623904318ITERATION : 31, loss : 0.03011353606703584ITERATION : 32, loss : 0.030113588254946585ITERATION : 33, loss : 0.03011362536645273ITERATION : 34, loss : 0.03011365171343466ITERATION : 35, loss : 0.030113670499871476ITERATION : 36, loss : 0.03011368383136734ITERATION : 37, loss : 0.0301136933165247ITERATION : 38, loss : 0.030113700102922494ITERATION : 39, loss : 0.03011370487917911ITERATION : 40, loss : 0.0301137082683974ITERATION : 41, loss : 0.03011371064668596ITERATION : 42, loss : 0.030113712444986063ITERATION : 43, loss : 0.03011371368848438ITERATION : 44, loss : 0.030113714472227198ITERATION : 45, loss : 0.030113715144845992ITERATION : 46, loss : 0.030113715461125127ITERATION : 47, loss : 0.03011371568442284ITERATION : 48, loss : 0.030113715863557884ITERATION : 49, loss : 0.03011371599642461ITERATION : 50, loss : 0.0301137161502554ITERATION : 51, loss : 0.030113716232062084ITERATION : 52, loss : 0.030113716237873185ITERATION : 53, loss : 0.030113716237873185ITERATION : 54, loss : 0.030113716237873185ITERATION : 55, loss : 0.030113716237873185ITERATION : 56, loss : 0.030113716237873185ITERATION : 57, loss : 0.030113716237873185ITERATION : 58, loss : 0.030113716237873185ITERATION : 59, loss : 0.030113716237873185ITERATION : 60, loss : 0.030113716237873185ITERATION : 61, loss : 0.030113716237873185ITERATION : 62, loss : 0.030113716237873185ITERATION : 63, loss : 0.030113716237873185ITERATION : 64, loss : 0.030113716237873185ITERATION : 65, loss : 0.030113716237873185ITERATION : 66, loss : 0.030113716237873185ITERATION : 67, loss : 0.030113716237873185ITERATION : 68, loss : 0.030113716237873185ITERATION : 69, loss : 0.030113716237873185ITERATION : 70, loss : 0.030113716237873185ITERATION : 71, loss : 0.030113716237873185ITERATION : 72, loss : 0.030113716237873185ITERATION : 73, loss : 0.030113716237873185ITERATION : 74, loss : 0.030113716237873185ITERATION : 75, loss : 0.030113716237873185ITERATION : 76, loss : 0.030113716237873185ITERATION : 77, loss : 0.030113716237873185ITERATION : 78, loss : 0.030113716237873185ITERATION : 79, loss : 0.030113716237873185ITERATION : 80, loss : 0.030113716237873185ITERATION : 81, loss : 0.030113716237873185ITERATION : 82, loss : 0.030113716237873185ITERATION : 83, loss : 0.030113716237873185ITERATION : 84, loss : 0.030113716237873185ITERATION : 85, loss : 0.030113716237873185ITERATION : 86, loss : 0.030113716237873185ITERATION : 87, loss : 0.030113716237873185ITERATION : 88, loss : 0.030113716237873185ITERATION : 89, loss : 0.030113716237873185ITERATION : 90, loss : 0.030113716237873185ITERATION : 91, loss : 0.030113716237873185ITERATION : 92, loss : 0.030113716237873185ITERATION : 93, loss : 0.030113716237873185ITERATION : 94, loss : 0.030113716237873185ITERATION : 95, loss : 0.030113716237873185ITERATION : 96, loss : 0.030113716237873185ITERATION : 97, loss : 0.030113716237873185ITERATION : 98, loss : 0.030113716237873185ITERATION : 99, loss : 0.030113716237873185ITERATION : 100, loss : 0.030113716237873185
ITERATION : 1, loss : 0.03986419939180382ITERATION : 2, loss : 0.027531694811282165ITERATION : 3, loss : 0.025614084191816412ITERATION : 4, loss : 0.026029327228068087ITERATION : 5, loss : 0.026612561993228874ITERATION : 6, loss : 0.02712225355918051ITERATION : 7, loss : 0.027514215163117153ITERATION : 8, loss : 0.027800627703712527ITERATION : 9, loss : 0.02800518075483755ITERATION : 10, loss : 0.0281497489918341ITERATION : 11, loss : 0.028251454708709933ITERATION : 12, loss : 0.028322881674536286ITERATION : 13, loss : 0.028373026091950162ITERATION : 14, loss : 0.028408239068120865ITERATION : 15, loss : 0.028432980302256182ITERATION : 16, loss : 0.02845037491104726ITERATION : 17, loss : 0.028462612106735903ITERATION : 18, loss : 0.02847122618374593ITERATION : 19, loss : 0.0284772931115078ITERATION : 20, loss : 0.02848156818840151ITERATION : 21, loss : 0.028484581911552207ITERATION : 22, loss : 0.028486707243042714ITERATION : 23, loss : 0.028488206625214653ITERATION : 24, loss : 0.02848926470277035ITERATION : 25, loss : 0.02849001164033909ITERATION : 26, loss : 0.028490539045061873ITERATION : 27, loss : 0.02849091150004234ITERATION : 28, loss : 0.0284911746057871ITERATION : 29, loss : 0.028491360479402296ITERATION : 30, loss : 0.02849149179302702ITERATION : 31, loss : 0.0284915846894273ITERATION : 32, loss : 0.028491650353585047ITERATION : 33, loss : 0.028491696778929987ITERATION : 34, loss : 0.028491729569417767ITERATION : 35, loss : 0.028491752807289106ITERATION : 36, loss : 0.02849176922071268ITERATION : 37, loss : 0.028491780810449112ITERATION : 38, loss : 0.02849178899790056ITERATION : 39, loss : 0.02849179480300995ITERATION : 40, loss : 0.02849179890702556ITERATION : 41, loss : 0.028491801806576456ITERATION : 42, loss : 0.028491803865027317ITERATION : 43, loss : 0.028491805313393116ITERATION : 44, loss : 0.02849180640007975ITERATION : 45, loss : 0.028491807112567782ITERATION : 46, loss : 0.028491807567767376ITERATION : 47, loss : 0.02849180790881607ITERATION : 48, loss : 0.02849180814877218ITERATION : 49, loss : 0.028491808339191033ITERATION : 50, loss : 0.028491808471935352ITERATION : 51, loss : 0.028491808523540958ITERATION : 52, loss : 0.028491808552855703ITERATION : 53, loss : 0.028491808693752844ITERATION : 54, loss : 0.028491808693752844ITERATION : 55, loss : 0.028491808693752844ITERATION : 56, loss : 0.028491808693752844ITERATION : 57, loss : 0.028491808693752844ITERATION : 58, loss : 0.028491808693752844ITERATION : 59, loss : 0.028491808693752844ITERATION : 60, loss : 0.028491808693752844ITERATION : 61, loss : 0.028491808693752844ITERATION : 62, loss : 0.028491808693752844ITERATION : 63, loss : 0.028491808693752844ITERATION : 64, loss : 0.028491808693752844ITERATION : 65, loss : 0.028491808693752844ITERATION : 66, loss : 0.028491808693752844ITERATION : 67, loss : 0.028491808693752844ITERATION : 68, loss : 0.028491808693752844ITERATION : 69, loss : 0.028491808693752844ITERATION : 70, loss : 0.028491808693752844ITERATION : 71, loss : 0.028491808693752844ITERATION : 72, loss : 0.028491808693752844ITERATION : 73, loss : 0.028491808693752844ITERATION : 74, loss : 0.028491808693752844ITERATION : 75, loss : 0.028491808693752844ITERATION : 76, loss : 0.028491808693752844ITERATION : 77, loss : 0.028491808693752844ITERATION : 78, loss : 0.028491808693752844ITERATION : 79, loss : 0.028491808693752844ITERATION : 80, loss : 0.028491808693752844ITERATION : 81, loss : 0.028491808693752844ITERATION : 82, loss : 0.028491808693752844ITERATION : 83, loss : 0.028491808693752844ITERATION : 84, loss : 0.028491808693752844ITERATION : 85, loss : 0.028491808693752844ITERATION : 86, loss : 0.028491808693752844ITERATION : 87, loss : 0.028491808693752844ITERATION : 88, loss : 0.028491808693752844ITERATION : 89, loss : 0.028491808693752844ITERATION : 90, loss : 0.028491808693752844ITERATION : 91, loss : 0.028491808693752844ITERATION : 92, loss : 0.028491808693752844ITERATION : 93, loss : 0.028491808693752844ITERATION : 94, loss : 0.028491808693752844ITERATION : 95, loss : 0.028491808693752844ITERATION : 96, loss : 0.028491808693752844ITERATION : 97, loss : 0.028491808693752844ITERATION : 98, loss : 0.028491808693752844ITERATION : 99, loss : 0.028491808693752844ITERATION : 100, loss : 0.028491808693752844
gradient norm in None layer : 0.0005797387377237971
gradient norm in None layer : 3.53214123848332e-05
gradient norm in None layer : 3.7454801541650914e-05
gradient norm in None layer : 0.0004930175176050721
gradient norm in None layer : 4.0899212201536476e-05
gradient norm in None layer : 4.302820163743875e-05
gradient norm in None layer : 0.00021698633602204676
gradient norm in None layer : 9.339972643883667e-06
gradient norm in None layer : 9.046422117560943e-06
gradient norm in None layer : 0.00018712995519010696
gradient norm in None layer : 7.816704572513621e-06
gradient norm in None layer : 7.880030102803165e-06
gradient norm in None layer : 4.685770769275199e-05
gradient norm in None layer : 1.6856182163117465e-06
gradient norm in None layer : 1.2721252593839178e-06
gradient norm in None layer : 4.430508291767752e-05
gradient norm in None layer : 2.1644796055104003e-06
gradient norm in None layer : 1.6928455354050245e-06
gradient norm in None layer : 6.94285395559412e-05
gradient norm in None layer : 6.883501178904223e-07
gradient norm in None layer : 0.00015296375181784628
gradient norm in None layer : 1.314457002786501e-05
gradient norm in None layer : 9.080049643905085e-06
gradient norm in None layer : 0.00017981907276123715
gradient norm in None layer : 1.7450718859029588e-05
gradient norm in None layer : 1.5603498170817677e-05
gradient norm in None layer : 0.00024631633566639535
gradient norm in None layer : 2.0121474637433405e-06
gradient norm in None layer : 0.00044687255362220755
gradient norm in None layer : 3.459262215972483e-05
gradient norm in None layer : 3.7648943399628155e-05
gradient norm in None layer : 0.000519639848044236
gradient norm in None layer : 4.563487858627169e-05
gradient norm in None layer : 5.4307830668676436e-05
gradient norm in None layer : 3.919703720517182e-05
gradient norm in None layer : 7.218793161413419e-06
Total gradient norm: 0.0011282239252324304
invariance loss : 4.21571781247248, avg_den : 0.424652099609375, density loss : 0.324652099609375, mse loss : 0.025072927437365493, solver time : 143.33390069007874 sec , total loss : 0.029613297349447346, running loss : 0.041716124053852235
Epoch 0/10 , batch 30/12500 
ITERATION : 1, loss : 0.02660054802270458ITERATION : 2, loss : 0.021709100940330295ITERATION : 3, loss : 0.025949992529956185ITERATION : 4, loss : 0.02802612431548048ITERATION : 5, loss : 0.02938158429657031ITERATION : 6, loss : 0.03069712563855939ITERATION : 7, loss : 0.03177227690910814ITERATION : 8, loss : 0.03258659887822975ITERATION : 9, loss : 0.033179224358263336ITERATION : 10, loss : 0.033600914002241085ITERATION : 11, loss : 0.033897084188723545ITERATION : 12, loss : 0.03410352538817886ITERATION : 13, loss : 0.0342468010566779ITERATION : 14, loss : 0.0343460052094684ITERATION : 15, loss : 0.03441461609184679ITERATION : 16, loss : 0.03446204948865537ITERATION : 17, loss : 0.03449484365858914ITERATION : 18, loss : 0.034517524165450106ITERATION : 19, loss : 0.03453321749400621ITERATION : 20, loss : 0.034544082152988695ITERATION : 21, loss : 0.03455160844760935ITERATION : 22, loss : 0.034556825428036286ITERATION : 23, loss : 0.03456044399914351ITERATION : 24, loss : 0.03456295531250063ITERATION : 25, loss : 0.034564699367175614ITERATION : 26, loss : 0.03456591122022055ITERATION : 27, loss : 0.034566753769954967ITERATION : 28, loss : 0.03456733994936989ITERATION : 29, loss : 0.03456774801057284ITERATION : 30, loss : 0.034568032117290676ITERATION : 31, loss : 0.03456823013815341ITERATION : 32, loss : 0.03456836820628779ITERATION : 33, loss : 0.034568464566720226ITERATION : 34, loss : 0.034568531813815855ITERATION : 35, loss : 0.0345685787773141ITERATION : 36, loss : 0.03456861159145882ITERATION : 37, loss : 0.034568634531356106ITERATION : 38, loss : 0.034568650548659464ITERATION : 39, loss : 0.03456866178354916ITERATION : 40, loss : 0.034568669662277555ITERATION : 41, loss : 0.03456867515505132ITERATION : 42, loss : 0.034568678997403006ITERATION : 43, loss : 0.03456868174855688ITERATION : 44, loss : 0.03456868360896063ITERATION : 45, loss : 0.0345686848905865ITERATION : 46, loss : 0.03456868582522912ITERATION : 47, loss : 0.03456868640926563ITERATION : 48, loss : 0.03456868685876964ITERATION : 49, loss : 0.03456868714851118ITERATION : 50, loss : 0.034568687443742294ITERATION : 51, loss : 0.03456868756294534ITERATION : 52, loss : 0.03456868767756874ITERATION : 53, loss : 0.034568687732009884ITERATION : 54, loss : 0.03456868778801823ITERATION : 55, loss : 0.0345686877896855ITERATION : 56, loss : 0.0345686877896855ITERATION : 57, loss : 0.0345686877896855ITERATION : 58, loss : 0.0345686877896855ITERATION : 59, loss : 0.0345686877896855ITERATION : 60, loss : 0.0345686877896855ITERATION : 61, loss : 0.0345686877896855ITERATION : 62, loss : 0.0345686877896855ITERATION : 63, loss : 0.0345686877896855ITERATION : 64, loss : 0.0345686877896855ITERATION : 65, loss : 0.0345686877896855ITERATION : 66, loss : 0.0345686877896855ITERATION : 67, loss : 0.0345686877896855ITERATION : 68, loss : 0.0345686877896855ITERATION : 69, loss : 0.0345686877896855ITERATION : 70, loss : 0.0345686877896855ITERATION : 71, loss : 0.0345686877896855ITERATION : 72, loss : 0.0345686877896855ITERATION : 73, loss : 0.0345686877896855ITERATION : 74, loss : 0.0345686877896855ITERATION : 75, loss : 0.0345686877896855ITERATION : 76, loss : 0.0345686877896855ITERATION : 77, loss : 0.0345686877896855ITERATION : 78, loss : 0.0345686877896855ITERATION : 79, loss : 0.0345686877896855ITERATION : 80, loss : 0.0345686877896855ITERATION : 81, loss : 0.0345686877896855ITERATION : 82, loss : 0.0345686877896855ITERATION : 83, loss : 0.0345686877896855ITERATION : 84, loss : 0.0345686877896855ITERATION : 85, loss : 0.0345686877896855ITERATION : 86, loss : 0.0345686877896855ITERATION : 87, loss : 0.0345686877896855ITERATION : 88, loss : 0.0345686877896855ITERATION : 89, loss : 0.0345686877896855ITERATION : 90, loss : 0.0345686877896855ITERATION : 91, loss : 0.0345686877896855ITERATION : 92, loss : 0.0345686877896855ITERATION : 93, loss : 0.0345686877896855ITERATION : 94, loss : 0.0345686877896855ITERATION : 95, loss : 0.0345686877896855ITERATION : 96, loss : 0.0345686877896855ITERATION : 97, loss : 0.0345686877896855ITERATION : 98, loss : 0.0345686877896855ITERATION : 99, loss : 0.0345686877896855ITERATION : 100, loss : 0.0345686877896855
ITERATION : 1, loss : 0.04321079805598443ITERATION : 2, loss : 0.03325073152712919ITERATION : 3, loss : 0.02961007326770165ITERATION : 4, loss : 0.02843899390885085ITERATION : 5, loss : 0.02835541960053904ITERATION : 6, loss : 0.028726450774424577ITERATION : 7, loss : 0.029248554731047614ITERATION : 8, loss : 0.02844475587183644ITERATION : 9, loss : 0.02706266383679911ITERATION : 10, loss : 0.026069624044731038ITERATION : 11, loss : 0.025346476971924572ITERATION : 12, loss : 0.024814517442046144ITERATION : 13, loss : 0.02442019567139841ITERATION : 14, loss : 0.024126202763915906ITERATION : 15, loss : 0.02390604782297422ITERATION : 16, loss : 0.02374063682252859ITERATION : 17, loss : 0.023616043228737473ITERATION : 18, loss : 0.02352201522203401ITERATION : 19, loss : 0.023450951405037983ITERATION : 20, loss : 0.023397184206966064ITERATION : 21, loss : 0.023356469750307654ITERATION : 22, loss : 0.023325619779117842ITERATION : 23, loss : 0.023302233024104557ITERATION : 24, loss : 0.02328449755904571ITERATION : 25, loss : 0.023271043992467603ITERATION : 26, loss : 0.0232608364202613ITERATION : 27, loss : 0.023253090417844496ITERATION : 28, loss : 0.023247211654133626ITERATION : 29, loss : 0.023242749585194056ITERATION : 30, loss : 0.023239362604135293ITERATION : 31, loss : 0.023236791512468217ITERATION : 32, loss : 0.02323483968216532ITERATION : 33, loss : 0.02323335793317236ITERATION : 34, loss : 0.023232233001092905ITERATION : 35, loss : 0.02323137896314797ITERATION : 36, loss : 0.023230730588214687ITERATION : 37, loss : 0.02323023833566147ITERATION : 38, loss : 0.023229864590134472ITERATION : 39, loss : 0.023229580677429515ITERATION : 40, loss : 0.023229365262485743ITERATION : 41, loss : 0.023229201719063668ITERATION : 42, loss : 0.02322907755579881ITERATION : 43, loss : 0.023228983329610393ITERATION : 44, loss : 0.02322891173924517ITERATION : 45, loss : 0.023228857374760287ITERATION : 46, loss : 0.02322881612403519ITERATION : 47, loss : 0.02322878481254758ITERATION : 48, loss : 0.023228761097551325ITERATION : 49, loss : 0.023228743037721357ITERATION : 50, loss : 0.023228729320930973ITERATION : 51, loss : 0.023228718904896875ITERATION : 52, loss : 0.02322871113267724ITERATION : 53, loss : 0.023228705101913556ITERATION : 54, loss : 0.023228700479512986ITERATION : 55, loss : 0.023228697066613527ITERATION : 56, loss : 0.023228694505951983ITERATION : 57, loss : 0.02322869253218183ITERATION : 58, loss : 0.023228690968285223ITERATION : 59, loss : 0.023228689877172713ITERATION : 60, loss : 0.02322868904717216ITERATION : 61, loss : 0.02322868848810542ITERATION : 62, loss : 0.02322868789903538ITERATION : 63, loss : 0.023228687514001797ITERATION : 64, loss : 0.023228687164143495ITERATION : 65, loss : 0.02322868697516347ITERATION : 66, loss : 0.023228686844883807ITERATION : 67, loss : 0.02322868671305407ITERATION : 68, loss : 0.023228686713416612ITERATION : 69, loss : 0.023228686713416612ITERATION : 70, loss : 0.023228686713416612ITERATION : 71, loss : 0.023228686713416612ITERATION : 72, loss : 0.023228686713416612ITERATION : 73, loss : 0.023228686713416612ITERATION : 74, loss : 0.023228686713416612ITERATION : 75, loss : 0.023228686713416612ITERATION : 76, loss : 0.023228686713416612ITERATION : 77, loss : 0.023228686713416612ITERATION : 78, loss : 0.023228686713416612ITERATION : 79, loss : 0.023228686713416612ITERATION : 80, loss : 0.023228686713416612ITERATION : 81, loss : 0.023228686713416612ITERATION : 82, loss : 0.023228686713416612ITERATION : 83, loss : 0.023228686713416612ITERATION : 84, loss : 0.023228686713416612ITERATION : 85, loss : 0.023228686713416612ITERATION : 86, loss : 0.023228686713416612ITERATION : 87, loss : 0.023228686713416612ITERATION : 88, loss : 0.023228686713416612ITERATION : 89, loss : 0.023228686713416612ITERATION : 90, loss : 0.023228686713416612ITERATION : 91, loss : 0.023228686713416612ITERATION : 92, loss : 0.023228686713416612ITERATION : 93, loss : 0.023228686713416612ITERATION : 94, loss : 0.023228686713416612ITERATION : 95, loss : 0.023228686713416612ITERATION : 96, loss : 0.023228686713416612ITERATION : 97, loss : 0.023228686713416612ITERATION : 98, loss : 0.023228686713416612ITERATION : 99, loss : 0.023228686713416612ITERATION : 100, loss : 0.023228686713416612
ITERATION : 1, loss : 0.02037980441577348ITERATION : 2, loss : 0.018653046436823997ITERATION : 3, loss : 0.01800435079578636ITERATION : 4, loss : 0.017631687501678893ITERATION : 5, loss : 0.017404437786817335ITERATION : 6, loss : 0.01725593487840943ITERATION : 7, loss : 0.0171571324714469ITERATION : 8, loss : 0.017090741403316685ITERATION : 9, loss : 0.017045780121755658ITERATION : 10, loss : 0.017015148439539458ITERATION : 11, loss : 0.01699419141363735ITERATION : 12, loss : 0.0169798149009521ITERATION : 13, loss : 0.016969937688576246ITERATION : 14, loss : 0.016963146968677623ITERATION : 15, loss : 0.016958477588026183ITERATION : 16, loss : 0.016955267462298678ITERATION : 17, loss : 0.016953061353706108ITERATION : 18, loss : 0.016951545987092927ITERATION : 19, loss : 0.016950505577562693ITERATION : 20, loss : 0.016949791658114464ITERATION : 21, loss : 0.01694930196693172ITERATION : 22, loss : 0.016948966213268822ITERATION : 23, loss : 0.01694873614538636ITERATION : 24, loss : 0.016948578532097063ITERATION : 25, loss : 0.016948470561030066ITERATION : 26, loss : 0.016948396654523575ITERATION : 27, loss : 0.016948346084251314ITERATION : 28, loss : 0.016948311476904083ITERATION : 29, loss : 0.0169482877991759ITERATION : 30, loss : 0.016948271615897906ITERATION : 31, loss : 0.0169482605380861ITERATION : 32, loss : 0.01694825297300699ITERATION : 33, loss : 0.01694824780233329ITERATION : 34, loss : 0.016948244271013706ITERATION : 35, loss : 0.01694824184061704ITERATION : 36, loss : 0.016948240181478707ITERATION : 37, loss : 0.01694823903819435ITERATION : 38, loss : 0.01694823827616024ITERATION : 39, loss : 0.01694823775437093ITERATION : 40, loss : 0.016948237412686258ITERATION : 41, loss : 0.016948237172664603ITERATION : 42, loss : 0.016948237021292077ITERATION : 43, loss : 0.016948236909287637ITERATION : 44, loss : 0.016948236836896197ITERATION : 45, loss : 0.01694823678914708ITERATION : 46, loss : 0.01694823675975639ITERATION : 47, loss : 0.01694823673728281ITERATION : 48, loss : 0.016948236722474357ITERATION : 49, loss : 0.016948236710159454ITERATION : 50, loss : 0.01694823670790635ITERATION : 51, loss : 0.01694823669869695ITERATION : 52, loss : 0.01694823669895715ITERATION : 53, loss : 0.016948236699704803ITERATION : 54, loss : 0.016948236699704803ITERATION : 55, loss : 0.016948236699704803ITERATION : 56, loss : 0.016948236699704803ITERATION : 57, loss : 0.016948236699704803ITERATION : 58, loss : 0.016948236699704803ITERATION : 59, loss : 0.016948236699704803ITERATION : 60, loss : 0.016948236699704803ITERATION : 61, loss : 0.016948236699704803ITERATION : 62, loss : 0.016948236699704803ITERATION : 63, loss : 0.016948236699704803ITERATION : 64, loss : 0.016948236699704803ITERATION : 65, loss : 0.016948236699704803ITERATION : 66, loss : 0.016948236699704803ITERATION : 67, loss : 0.016948236699704803ITERATION : 68, loss : 0.016948236699704803ITERATION : 69, loss : 0.016948236699704803ITERATION : 70, loss : 0.016948236699704803ITERATION : 71, loss : 0.016948236699704803ITERATION : 72, loss : 0.016948236699704803ITERATION : 73, loss : 0.016948236699704803ITERATION : 74, loss : 0.016948236699704803ITERATION : 75, loss : 0.016948236699704803ITERATION : 76, loss : 0.016948236699704803ITERATION : 77, loss : 0.016948236699704803ITERATION : 78, loss : 0.016948236699704803ITERATION : 79, loss : 0.016948236699704803ITERATION : 80, loss : 0.016948236699704803ITERATION : 81, loss : 0.016948236699704803ITERATION : 82, loss : 0.016948236699704803ITERATION : 83, loss : 0.016948236699704803ITERATION : 84, loss : 0.016948236699704803ITERATION : 85, loss : 0.016948236699704803ITERATION : 86, loss : 0.016948236699704803ITERATION : 87, loss : 0.016948236699704803ITERATION : 88, loss : 0.016948236699704803ITERATION : 89, loss : 0.016948236699704803ITERATION : 90, loss : 0.016948236699704803ITERATION : 91, loss : 0.016948236699704803ITERATION : 92, loss : 0.016948236699704803ITERATION : 93, loss : 0.016948236699704803ITERATION : 94, loss : 0.016948236699704803ITERATION : 95, loss : 0.016948236699704803ITERATION : 96, loss : 0.016948236699704803ITERATION : 97, loss : 0.016948236699704803ITERATION : 98, loss : 0.016948236699704803ITERATION : 99, loss : 0.016948236699704803ITERATION : 100, loss : 0.016948236699704803
ITERATION : 1, loss : 0.01885411296309428ITERATION : 2, loss : 0.018331354710223447ITERATION : 3, loss : 0.01849949265242413ITERATION : 4, loss : 0.017726010272122526ITERATION : 5, loss : 0.016676548059610424ITERATION : 6, loss : 0.01588506615709127ITERATION : 7, loss : 0.015317311377673686ITERATION : 8, loss : 0.014916467188426215ITERATION : 9, loss : 0.014634378550692143ITERATION : 10, loss : 0.014435606790222452ITERATION : 11, loss : 0.014295181363085949ITERATION : 12, loss : 0.014195713670994892ITERATION : 13, loss : 0.014125097848056099ITERATION : 14, loss : 0.014074873800477379ITERATION : 15, loss : 0.014039102842205187ITERATION : 16, loss : 0.014013598622996283ITERATION : 17, loss : 0.013995399748122705ITERATION : 18, loss : 0.013982405542706432ITERATION : 19, loss : 0.013973123379396865ITERATION : 20, loss : 0.01396649039018101ITERATION : 21, loss : 0.01396174897357898ITERATION : 22, loss : 0.013958358894801978ITERATION : 23, loss : 0.013955934556307309ITERATION : 24, loss : 0.013954200485456132ITERATION : 25, loss : 0.013952959990105578ITERATION : 26, loss : 0.013952072462461764ITERATION : 27, loss : 0.013951437348488676ITERATION : 28, loss : 0.013950982817027136ITERATION : 29, loss : 0.013950657464156574ITERATION : 30, loss : 0.013950424568936628ITERATION : 31, loss : 0.013950257823214933ITERATION : 32, loss : 0.01395013845639716ITERATION : 33, loss : 0.01395005296011604ITERATION : 34, loss : 0.013949991726079117ITERATION : 35, loss : 0.013949947873542982ITERATION : 36, loss : 0.01394991644011248ITERATION : 37, loss : 0.013949893958315567ITERATION : 38, loss : 0.013949877840834526ITERATION : 39, loss : 0.013949866290490448ITERATION : 40, loss : 0.013949858008097988ITERATION : 41, loss : 0.013949852087355804ITERATION : 42, loss : 0.013949847861449335ITERATION : 43, loss : 0.013949844822219788ITERATION : 44, loss : 0.013949842659612237ITERATION : 45, loss : 0.01394984112522345ITERATION : 46, loss : 0.01394984003748363ITERATION : 47, loss : 0.013949839220828335ITERATION : 48, loss : 0.013949838605469098ITERATION : 49, loss : 0.01394983824019787ITERATION : 50, loss : 0.013949837925313298ITERATION : 51, loss : 0.013949837766587522ITERATION : 52, loss : 0.013949837599217547ITERATION : 53, loss : 0.013949837517676497ITERATION : 54, loss : 0.01394983746195457ITERATION : 55, loss : 0.013949837433364807ITERATION : 56, loss : 0.013949837429739499ITERATION : 57, loss : 0.013949837429739499ITERATION : 58, loss : 0.013949837429739499ITERATION : 59, loss : 0.013949837429739499ITERATION : 60, loss : 0.013949837429739499ITERATION : 61, loss : 0.013949837429739499ITERATION : 62, loss : 0.013949837429739499ITERATION : 63, loss : 0.013949837429739499ITERATION : 64, loss : 0.013949837429739499ITERATION : 65, loss : 0.013949837429739499ITERATION : 66, loss : 0.013949837429739499ITERATION : 67, loss : 0.013949837429739499ITERATION : 68, loss : 0.013949837429739499ITERATION : 69, loss : 0.013949837429739499ITERATION : 70, loss : 0.013949837429739499ITERATION : 71, loss : 0.013949837429739499ITERATION : 72, loss : 0.013949837429739499ITERATION : 73, loss : 0.013949837429739499ITERATION : 74, loss : 0.013949837429739499ITERATION : 75, loss : 0.013949837429739499ITERATION : 76, loss : 0.013949837429739499ITERATION : 77, loss : 0.013949837429739499ITERATION : 78, loss : 0.013949837429739499ITERATION : 79, loss : 0.013949837429739499ITERATION : 80, loss : 0.013949837429739499ITERATION : 81, loss : 0.013949837429739499ITERATION : 82, loss : 0.013949837429739499ITERATION : 83, loss : 0.013949837429739499ITERATION : 84, loss : 0.013949837429739499ITERATION : 85, loss : 0.013949837429739499ITERATION : 86, loss : 0.013949837429739499ITERATION : 87, loss : 0.013949837429739499ITERATION : 88, loss : 0.013949837429739499ITERATION : 89, loss : 0.013949837429739499ITERATION : 90, loss : 0.013949837429739499ITERATION : 91, loss : 0.013949837429739499ITERATION : 92, loss : 0.013949837429739499ITERATION : 93, loss : 0.013949837429739499ITERATION : 94, loss : 0.013949837429739499ITERATION : 95, loss : 0.013949837429739499ITERATION : 96, loss : 0.013949837429739499ITERATION : 97, loss : 0.013949837429739499ITERATION : 98, loss : 0.013949837429739499ITERATION : 99, loss : 0.013949837429739499ITERATION : 100, loss : 0.013949837429739499
ITERATION : 1, loss : 0.05676303339818199ITERATION : 2, loss : 0.05657306531851966ITERATION : 3, loss : 0.05608688781005139ITERATION : 4, loss : 0.05580842546044666ITERATION : 5, loss : 0.055664437916483844ITERATION : 6, loss : 0.05559184740656371ITERATION : 7, loss : 0.05555566892561087ITERATION : 8, loss : 0.055537649109591546ITERATION : 9, loss : 0.055528516873179036ITERATION : 10, loss : 0.05552367130643122ITERATION : 11, loss : 0.0555208811311858ITERATION : 12, loss : 0.055519089865676254ITERATION : 13, loss : 0.055517809106511024ITERATION : 14, loss : 0.05551681738763101ITERATION : 15, loss : 0.05551601472308022ITERATION : 16, loss : 0.05551535360772067ITERATION : 17, loss : 0.05551480826509739ITERATION : 18, loss : 0.05551436114013339ITERATION : 19, loss : 0.05551399803477718ITERATION : 20, loss : 0.0555137060994033ITERATION : 21, loss : 0.05551347372446457ITERATION : 22, loss : 0.055513290480508834ITERATION : 23, loss : 0.05551314708981454ITERATION : 24, loss : 0.05551303577258136ITERATION : 25, loss : 0.05551294992869684ITERATION : 26, loss : 0.055512884082454146ITERATION : 27, loss : 0.05551283384970922ITERATION : 28, loss : 0.05551279571236009ITERATION : 29, loss : 0.055512766818395044ITERATION : 30, loss : 0.05551274511183691ITERATION : 31, loss : 0.05551272882355898ITERATION : 32, loss : 0.05551271654556725ITERATION : 33, loss : 0.05551270742142076ITERATION : 34, loss : 0.055512700651373724ITERATION : 35, loss : 0.05551269560090939ITERATION : 36, loss : 0.05551269185005952ITERATION : 37, loss : 0.055512689051192436ITERATION : 38, loss : 0.05551268703255275ITERATION : 39, loss : 0.05551268551112134ITERATION : 40, loss : 0.05551268438508353ITERATION : 41, loss : 0.05551268353896287ITERATION : 42, loss : 0.055512682918622505ITERATION : 43, loss : 0.05551268244502736ITERATION : 44, loss : 0.055512682112438975ITERATION : 45, loss : 0.05551268186084036ITERATION : 46, loss : 0.05551268167190515ITERATION : 47, loss : 0.05551268153458037ITERATION : 48, loss : 0.05551268142511134ITERATION : 49, loss : 0.05551268137000077ITERATION : 50, loss : 0.055512681302448234ITERATION : 51, loss : 0.05551268124913001ITERATION : 52, loss : 0.055512681227935054ITERATION : 53, loss : 0.05551268121592886ITERATION : 54, loss : 0.05551268121077893ITERATION : 55, loss : 0.05551268121077893ITERATION : 56, loss : 0.05551268121077893ITERATION : 57, loss : 0.05551268121077893ITERATION : 58, loss : 0.05551268121077893ITERATION : 59, loss : 0.05551268121077893ITERATION : 60, loss : 0.05551268121077893ITERATION : 61, loss : 0.05551268121077893ITERATION : 62, loss : 0.05551268121077893ITERATION : 63, loss : 0.05551268121077893ITERATION : 64, loss : 0.05551268121077893ITERATION : 65, loss : 0.05551268121077893ITERATION : 66, loss : 0.05551268121077893ITERATION : 67, loss : 0.05551268121077893ITERATION : 68, loss : 0.05551268121077893ITERATION : 69, loss : 0.05551268121077893ITERATION : 70, loss : 0.05551268121077893ITERATION : 71, loss : 0.05551268121077893ITERATION : 72, loss : 0.05551268121077893ITERATION : 73, loss : 0.05551268121077893ITERATION : 74, loss : 0.05551268121077893ITERATION : 75, loss : 0.05551268121077893ITERATION : 76, loss : 0.05551268121077893ITERATION : 77, loss : 0.05551268121077893ITERATION : 78, loss : 0.05551268121077893ITERATION : 79, loss : 0.05551268121077893ITERATION : 80, loss : 0.05551268121077893ITERATION : 81, loss : 0.05551268121077893ITERATION : 82, loss : 0.05551268121077893ITERATION : 83, loss : 0.05551268121077893ITERATION : 84, loss : 0.05551268121077893ITERATION : 85, loss : 0.05551268121077893ITERATION : 86, loss : 0.05551268121077893ITERATION : 87, loss : 0.05551268121077893ITERATION : 88, loss : 0.05551268121077893ITERATION : 89, loss : 0.05551268121077893ITERATION : 90, loss : 0.05551268121077893ITERATION : 91, loss : 0.05551268121077893ITERATION : 92, loss : 0.05551268121077893ITERATION : 93, loss : 0.05551268121077893ITERATION : 94, loss : 0.05551268121077893ITERATION : 95, loss : 0.05551268121077893ITERATION : 96, loss : 0.05551268121077893ITERATION : 97, loss : 0.05551268121077893ITERATION : 98, loss : 0.05551268121077893ITERATION : 99, loss : 0.05551268121077893ITERATION : 100, loss : 0.05551268121077893
ITERATION : 1, loss : 0.013288125544704857ITERATION : 2, loss : 0.011691366732937225ITERATION : 3, loss : 0.012002250966933366ITERATION : 4, loss : 0.012482117533716417ITERATION : 5, loss : 0.012903795192512975ITERATION : 6, loss : 0.013239970969977243ITERATION : 7, loss : 0.013498480763641976ITERATION : 8, loss : 0.013693528314850408ITERATION : 9, loss : 0.013838976645241934ITERATION : 10, loss : 0.013946600958619817ITERATION : 11, loss : 0.014025818500027995ITERATION : 12, loss : 0.014083915002548382ITERATION : 13, loss : 0.014126413760655645ITERATION : 14, loss : 0.014157447200444514ITERATION : 15, loss : 0.014180079982367285ITERATION : 16, loss : 0.014196571534841124ITERATION : 17, loss : 0.014208580539873948ITERATION : 18, loss : 0.0142173215837936ITERATION : 19, loss : 0.01422368194304252ITERATION : 20, loss : 0.014228308876606326ITERATION : 21, loss : 0.014231674358808153ITERATION : 22, loss : 0.01423412194581396ITERATION : 23, loss : 0.014235901884674383ITERATION : 24, loss : 0.014237196160032113ITERATION : 25, loss : 0.01423813729045907ITERATION : 26, loss : 0.014238821616880705ITERATION : 27, loss : 0.014239319183480781ITERATION : 28, loss : 0.014239680948741486ITERATION : 29, loss : 0.014239943961024772ITERATION : 30, loss : 0.01424013521862388ITERATION : 31, loss : 0.014240274238071572ITERATION : 32, loss : 0.014240375313487975ITERATION : 33, loss : 0.014240448819950206ITERATION : 34, loss : 0.014240502279584498ITERATION : 35, loss : 0.014240541143683715ITERATION : 36, loss : 0.014240569386576633ITERATION : 37, loss : 0.014240589923253895ITERATION : 38, loss : 0.014240604896203057ITERATION : 39, loss : 0.014240615750570729ITERATION : 40, loss : 0.014240623619385441ITERATION : 41, loss : 0.014240629344675983ITERATION : 42, loss : 0.014240633522441223ITERATION : 43, loss : 0.014240636522778556ITERATION : 44, loss : 0.014240638738967974ITERATION : 45, loss : 0.014240640324047316ITERATION : 46, loss : 0.014240641500004599ITERATION : 47, loss : 0.014240642346013865ITERATION : 48, loss : 0.014240642962965355ITERATION : 49, loss : 0.014240643398166802ITERATION : 50, loss : 0.014240643719737843ITERATION : 51, loss : 0.014240643934783317ITERATION : 52, loss : 0.014240644128858783ITERATION : 53, loss : 0.014240644307397392ITERATION : 54, loss : 0.014240644352068205ITERATION : 55, loss : 0.014240644460327776ITERATION : 56, loss : 0.014240644475368816ITERATION : 57, loss : 0.014240644475443041ITERATION : 58, loss : 0.014240644475443041ITERATION : 59, loss : 0.014240644475443041ITERATION : 60, loss : 0.014240644475443041ITERATION : 61, loss : 0.014240644475443041ITERATION : 62, loss : 0.014240644475443041ITERATION : 63, loss : 0.014240644475443041ITERATION : 64, loss : 0.014240644475443041ITERATION : 65, loss : 0.014240644475443041ITERATION : 66, loss : 0.014240644475443041ITERATION : 67, loss : 0.014240644475443041ITERATION : 68, loss : 0.014240644475443041ITERATION : 69, loss : 0.014240644475443041ITERATION : 70, loss : 0.014240644475443041ITERATION : 71, loss : 0.014240644475443041ITERATION : 72, loss : 0.014240644475443041ITERATION : 73, loss : 0.014240644475443041ITERATION : 74, loss : 0.014240644475443041ITERATION : 75, loss : 0.014240644475443041ITERATION : 76, loss : 0.014240644475443041ITERATION : 77, loss : 0.014240644475443041ITERATION : 78, loss : 0.014240644475443041ITERATION : 79, loss : 0.014240644475443041ITERATION : 80, loss : 0.014240644475443041ITERATION : 81, loss : 0.014240644475443041ITERATION : 82, loss : 0.014240644475443041ITERATION : 83, loss : 0.014240644475443041ITERATION : 84, loss : 0.014240644475443041ITERATION : 85, loss : 0.014240644475443041ITERATION : 86, loss : 0.014240644475443041ITERATION : 87, loss : 0.014240644475443041ITERATION : 88, loss : 0.014240644475443041ITERATION : 89, loss : 0.014240644475443041ITERATION : 90, loss : 0.014240644475443041ITERATION : 91, loss : 0.014240644475443041ITERATION : 92, loss : 0.014240644475443041ITERATION : 93, loss : 0.014240644475443041ITERATION : 94, loss : 0.014240644475443041ITERATION : 95, loss : 0.014240644475443041ITERATION : 96, loss : 0.014240644475443041ITERATION : 97, loss : 0.014240644475443041ITERATION : 98, loss : 0.014240644475443041ITERATION : 99, loss : 0.014240644475443041ITERATION : 100, loss : 0.014240644475443041
ITERATION : 1, loss : 0.014318616550487768ITERATION : 2, loss : 0.019755868361404357ITERATION : 3, loss : 0.023914971689315462ITERATION : 4, loss : 0.023510840347429257ITERATION : 5, loss : 0.023404065681017865ITERATION : 6, loss : 0.023428692052722522ITERATION : 7, loss : 0.023484515875320924ITERATION : 8, loss : 0.023539074723357735ITERATION : 9, loss : 0.023583467168021847ITERATION : 10, loss : 0.023616911416244835ITERATION : 11, loss : 0.02364110283971569ITERATION : 12, loss : 0.023658175247767387ITERATION : 13, loss : 0.023670025262120448ITERATION : 14, loss : 0.023678150454762632ITERATION : 15, loss : 0.023683667103919292ITERATION : 16, loss : 0.023687380828480827ITERATION : 17, loss : 0.023689860941669482ITERATION : 18, loss : 0.023691504085488996ITERATION : 19, loss : 0.02369258341987314ITERATION : 20, loss : 0.023693286016540346ITERATION : 21, loss : 0.023693738432656243ITERATION : 22, loss : 0.023694026107082617ITERATION : 23, loss : 0.023694206106185483ITERATION : 24, loss : 0.023694316591310008ITERATION : 25, loss : 0.023694382514068035ITERATION : 26, loss : 0.023694420232591634ITERATION : 27, loss : 0.023694440576723545ITERATION : 28, loss : 0.023694450393289355ITERATION : 29, loss : 0.0236944541070787ITERATION : 30, loss : 0.023694454371581655ITERATION : 31, loss : 0.023694452924082614ITERATION : 32, loss : 0.023694450684679902ITERATION : 33, loss : 0.023694448142347637ITERATION : 34, loss : 0.023694445793600527ITERATION : 35, loss : 0.023694443721339598ITERATION : 36, loss : 0.023694441900115906ITERATION : 37, loss : 0.02369444036386845ITERATION : 38, loss : 0.023694439086609842ITERATION : 39, loss : 0.023694438093224333ITERATION : 40, loss : 0.02369443731478855ITERATION : 41, loss : 0.023694436628409952ITERATION : 42, loss : 0.023694436140249017ITERATION : 43, loss : 0.023694435696147792ITERATION : 44, loss : 0.023694435414938743ITERATION : 45, loss : 0.023694435139020524ITERATION : 46, loss : 0.023694434963109582ITERATION : 47, loss : 0.02369443478435438ITERATION : 48, loss : 0.023694434695291505ITERATION : 49, loss : 0.023694434593425844ITERATION : 50, loss : 0.023694434549262386ITERATION : 51, loss : 0.023694434486007006ITERATION : 52, loss : 0.023694434492278225ITERATION : 53, loss : 0.023694434457824123ITERATION : 54, loss : 0.023694434456408103ITERATION : 55, loss : 0.023694434456408103ITERATION : 56, loss : 0.023694434456408103ITERATION : 57, loss : 0.023694434456408103ITERATION : 58, loss : 0.023694434456408103ITERATION : 59, loss : 0.023694434456408103ITERATION : 60, loss : 0.023694434456408103ITERATION : 61, loss : 0.023694434456408103ITERATION : 62, loss : 0.023694434456408103ITERATION : 63, loss : 0.023694434456408103ITERATION : 64, loss : 0.023694434456408103ITERATION : 65, loss : 0.023694434456408103ITERATION : 66, loss : 0.023694434456408103ITERATION : 67, loss : 0.023694434456408103ITERATION : 68, loss : 0.023694434456408103ITERATION : 69, loss : 0.023694434456408103ITERATION : 70, loss : 0.023694434456408103ITERATION : 71, loss : 0.023694434456408103ITERATION : 72, loss : 0.023694434456408103ITERATION : 73, loss : 0.023694434456408103ITERATION : 74, loss : 0.023694434456408103ITERATION : 75, loss : 0.023694434456408103ITERATION : 76, loss : 0.023694434456408103ITERATION : 77, loss : 0.023694434456408103ITERATION : 78, loss : 0.023694434456408103ITERATION : 79, loss : 0.023694434456408103ITERATION : 80, loss : 0.023694434456408103ITERATION : 81, loss : 0.023694434456408103ITERATION : 82, loss : 0.023694434456408103ITERATION : 83, loss : 0.023694434456408103ITERATION : 84, loss : 0.023694434456408103ITERATION : 85, loss : 0.023694434456408103ITERATION : 86, loss : 0.023694434456408103ITERATION : 87, loss : 0.023694434456408103ITERATION : 88, loss : 0.023694434456408103ITERATION : 89, loss : 0.023694434456408103ITERATION : 90, loss : 0.023694434456408103ITERATION : 91, loss : 0.023694434456408103ITERATION : 92, loss : 0.023694434456408103ITERATION : 93, loss : 0.023694434456408103ITERATION : 94, loss : 0.023694434456408103ITERATION : 95, loss : 0.023694434456408103ITERATION : 96, loss : 0.023694434456408103ITERATION : 97, loss : 0.023694434456408103ITERATION : 98, loss : 0.023694434456408103ITERATION : 99, loss : 0.023694434456408103ITERATION : 100, loss : 0.023694434456408103
ITERATION : 1, loss : 0.05956841737578598ITERATION : 2, loss : 0.05212046043759026ITERATION : 3, loss : 0.048773693829170434ITERATION : 4, loss : 0.044980098827968724ITERATION : 5, loss : 0.0426269441503263ITERATION : 6, loss : 0.04111292557145826ITERATION : 7, loss : 0.040112147833975785ITERATION : 8, loss : 0.03943836485605224ITERATION : 9, loss : 0.0389792827219178ITERATION : 10, loss : 0.038664091069104706ITERATION : 11, loss : 0.038446636007272286ITERATION : 12, loss : 0.03829614327063523ITERATION : 13, loss : 0.038191782565771513ITERATION : 14, loss : 0.03811931654677685ITERATION : 15, loss : 0.03806895245345227ITERATION : 16, loss : 0.03803392725857803ITERATION : 17, loss : 0.038009558229693166ITERATION : 18, loss : 0.03799259701663446ITERATION : 19, loss : 0.03798078850294381ITERATION : 20, loss : 0.037972565003641635ITERATION : 21, loss : 0.03796683661146551ITERATION : 22, loss : 0.0379628454699041ITERATION : 23, loss : 0.03796006402888921ITERATION : 24, loss : 0.03795812505461065ITERATION : 25, loss : 0.03795677311229048ITERATION : 26, loss : 0.03795583024540942ITERATION : 27, loss : 0.03795517252866263ITERATION : 28, loss : 0.03795471351118666ITERATION : 29, loss : 0.037954393194011594ITERATION : 30, loss : 0.03795416955490647ITERATION : 31, loss : 0.037954013417574615ITERATION : 32, loss : 0.03795390440586437ITERATION : 33, loss : 0.03795382799723042ITERATION : 34, loss : 0.037953774577666494ITERATION : 35, loss : 0.03795373749468392ITERATION : 36, loss : 0.03795371140876372ITERATION : 37, loss : 0.037953693053350133ITERATION : 38, loss : 0.03795368031706262ITERATION : 39, loss : 0.03795367128031626ITERATION : 40, loss : 0.03795366497666542ITERATION : 41, loss : 0.037953660628619365ITERATION : 42, loss : 0.03795365757440376ITERATION : 43, loss : 0.03795365545958314ITERATION : 44, loss : 0.03795365398306439ITERATION : 45, loss : 0.0379536530405204ITERATION : 46, loss : 0.037953652354041696ITERATION : 47, loss : 0.037953651804185494ITERATION : 48, loss : 0.037953651542392ITERATION : 49, loss : 0.03795365129479657ITERATION : 50, loss : 0.03795365117433568ITERATION : 51, loss : 0.03795365102523322ITERATION : 52, loss : 0.037953650988633875ITERATION : 53, loss : 0.03795365094530238ITERATION : 54, loss : 0.03795365092451723ITERATION : 55, loss : 0.037953650889838784ITERATION : 56, loss : 0.03795365088102912ITERATION : 57, loss : 0.03795365088103708ITERATION : 58, loss : 0.03795365088103708ITERATION : 59, loss : 0.03795365088103708ITERATION : 60, loss : 0.03795365088103708ITERATION : 61, loss : 0.03795365088103708ITERATION : 62, loss : 0.03795365088103708ITERATION : 63, loss : 0.03795365088103708ITERATION : 64, loss : 0.03795365088103708ITERATION : 65, loss : 0.03795365088103708ITERATION : 66, loss : 0.03795365088103708ITERATION : 67, loss : 0.03795365088103708ITERATION : 68, loss : 0.03795365088103708ITERATION : 69, loss : 0.03795365088103708ITERATION : 70, loss : 0.03795365088103708ITERATION : 71, loss : 0.03795365088103708ITERATION : 72, loss : 0.03795365088103708ITERATION : 73, loss : 0.03795365088103708ITERATION : 74, loss : 0.03795365088103708ITERATION : 75, loss : 0.03795365088103708ITERATION : 76, loss : 0.03795365088103708ITERATION : 77, loss : 0.03795365088103708ITERATION : 78, loss : 0.03795365088103708ITERATION : 79, loss : 0.03795365088103708ITERATION : 80, loss : 0.03795365088103708ITERATION : 81, loss : 0.03795365088103708ITERATION : 82, loss : 0.03795365088103708ITERATION : 83, loss : 0.03795365088103708ITERATION : 84, loss : 0.03795365088103708ITERATION : 85, loss : 0.03795365088103708ITERATION : 86, loss : 0.03795365088103708ITERATION : 87, loss : 0.03795365088103708ITERATION : 88, loss : 0.03795365088103708ITERATION : 89, loss : 0.03795365088103708ITERATION : 90, loss : 0.03795365088103708ITERATION : 91, loss : 0.03795365088103708ITERATION : 92, loss : 0.03795365088103708ITERATION : 93, loss : 0.03795365088103708ITERATION : 94, loss : 0.03795365088103708ITERATION : 95, loss : 0.03795365088103708ITERATION : 96, loss : 0.03795365088103708ITERATION : 97, loss : 0.03795365088103708ITERATION : 98, loss : 0.03795365088103708ITERATION : 99, loss : 0.03795365088103708ITERATION : 100, loss : 0.03795365088103708
gradient norm in None layer : 0.000657657769429245
gradient norm in None layer : 3.492664428531662e-05
gradient norm in None layer : 3.6182151469684424e-05
gradient norm in None layer : 0.0005421732166744762
gradient norm in None layer : 5.0143746007989254e-05
gradient norm in None layer : 5.1391150823575285e-05
gradient norm in None layer : 0.0003172897616259857
gradient norm in None layer : 1.3582218237331638e-05
gradient norm in None layer : 1.2741701403624203e-05
gradient norm in None layer : 0.00028026648811628675
gradient norm in None layer : 1.2674479024502134e-05
gradient norm in None layer : 1.0254879893478396e-05
gradient norm in None layer : 8.761408298168266e-05
gradient norm in None layer : 3.016733313428665e-06
gradient norm in None layer : 2.4151693638481283e-06
gradient norm in None layer : 7.502465114332625e-05
gradient norm in None layer : 3.2966236342253077e-06
gradient norm in None layer : 2.216742901238986e-06
gradient norm in None layer : 9.597223385883048e-05
gradient norm in None layer : 8.014331155911343e-07
gradient norm in None layer : 0.00023578887710173235
gradient norm in None layer : 1.6112915928489894e-05
gradient norm in None layer : 1.1970790585381412e-05
gradient norm in None layer : 0.0002432190584255612
gradient norm in None layer : 2.1431869791229816e-05
gradient norm in None layer : 1.9767860888733455e-05
gradient norm in None layer : 0.0003260179131282585
gradient norm in None layer : 2.730977862400615e-06
gradient norm in None layer : 0.0005377441005082071
gradient norm in None layer : 4.537141412586479e-05
gradient norm in None layer : 4.585876670315121e-05
gradient norm in None layer : 0.0007068175860802674
gradient norm in None layer : 5.8976837082083465e-05
gradient norm in None layer : 7.163455984666704e-05
gradient norm in None layer : 5.2587758464698304e-05
gradient norm in None layer : 9.759406806709007e-06
Total gradient norm: 0.0014011508316492689
invariance loss : 4.250527116572334, avg_den : 0.44549560546875, density loss : 0.34549560546875, mse loss : 0.027512107457026695, solver time : 130.7824399471283 sec , total loss : 0.03210813017906778, running loss : 0.04139585759135942
saving checkpoint
Epoch 0/10 , batch 31/12500 
ITERATION : 1, loss : 0.05083267576501248ITERATION : 2, loss : 0.04477392166898366ITERATION : 3, loss : 0.04262811074620647ITERATION : 4, loss : 0.04161244748639523ITERATION : 5, loss : 0.04094742752320749ITERATION : 6, loss : 0.04047709164074326ITERATION : 7, loss : 0.04013427735356691ITERATION : 8, loss : 0.03988034344799682ITERATION : 9, loss : 0.03969016863958747ITERATION : 10, loss : 0.03954656690187099ITERATION : 11, loss : 0.039437448714091744ITERATION : 12, loss : 0.03935413745244218ITERATION : 13, loss : 0.039290302671214646ITERATION : 14, loss : 0.03924126325042012ITERATION : 15, loss : 0.03920351887978612ITERATION : 16, loss : 0.03917442941011225ITERATION : 17, loss : 0.03915198969283083ITERATION : 18, loss : 0.03913466935698647ITERATION : 19, loss : 0.03912129537770963ITERATION : 20, loss : 0.0391109664997412ITERATION : 21, loss : 0.03910298867117589ITERATION : 22, loss : 0.03909682675413229ITERATION : 23, loss : 0.039092067727294615ITERATION : 24, loss : 0.03908839251985197ITERATION : 25, loss : 0.039085554608734314ITERATION : 26, loss : 0.03908336365536097ITERATION : 27, loss : 0.03908167238315066ITERATION : 28, loss : 0.03908036702333103ITERATION : 29, loss : 0.0390793597006274ITERATION : 30, loss : 0.0390785824521419ITERATION : 31, loss : 0.039077982833121444ITERATION : 32, loss : 0.03907752031348035ITERATION : 33, loss : 0.03907716359338073ITERATION : 34, loss : 0.039076888519342864ITERATION : 35, loss : 0.039076676397352864ITERATION : 36, loss : 0.039076512870381726ITERATION : 37, loss : 0.03907638681420369ITERATION : 38, loss : 0.03907628963068741ITERATION : 39, loss : 0.039076214752158646ITERATION : 40, loss : 0.03907615704970261ITERATION : 41, loss : 0.039076112569195555ITERATION : 42, loss : 0.03907607830228643ITERATION : 43, loss : 0.039076051870756676ITERATION : 44, loss : 0.03907603153669104ITERATION : 45, loss : 0.039076015841096504ITERATION : 46, loss : 0.0390760037256128ITERATION : 47, loss : 0.03907599441431451ITERATION : 48, loss : 0.039075987250486416ITERATION : 49, loss : 0.03907598173159067ITERATION : 50, loss : 0.03907597748369449ITERATION : 51, loss : 0.03907597424060611ITERATION : 52, loss : 0.039075971690766445ITERATION : 53, loss : 0.03907596972631965ITERATION : 54, loss : 0.03907596825843488ITERATION : 55, loss : 0.039075967126066785ITERATION : 56, loss : 0.039075966276469705ITERATION : 57, loss : 0.03907596559479172ITERATION : 58, loss : 0.03907596507850314ITERATION : 59, loss : 0.03907596465411702ITERATION : 60, loss : 0.03907596434918232ITERATION : 61, loss : 0.03907596406705257ITERATION : 62, loss : 0.03907596386016618ITERATION : 63, loss : 0.03907596376930984ITERATION : 64, loss : 0.039075963676576814ITERATION : 65, loss : 0.03907596363330134ITERATION : 66, loss : 0.03907596354340956ITERATION : 67, loss : 0.039075963509806064ITERATION : 68, loss : 0.03907596346622806ITERATION : 69, loss : 0.039075963466435513ITERATION : 70, loss : 0.039075963466435513ITERATION : 71, loss : 0.039075963466435513ITERATION : 72, loss : 0.039075963466435513ITERATION : 73, loss : 0.039075963466435513ITERATION : 74, loss : 0.039075963466435513ITERATION : 75, loss : 0.039075963466435513ITERATION : 76, loss : 0.039075963466435513ITERATION : 77, loss : 0.039075963466435513ITERATION : 78, loss : 0.039075963466435513ITERATION : 79, loss : 0.039075963466435513ITERATION : 80, loss : 0.039075963466435513ITERATION : 81, loss : 0.039075963466435513ITERATION : 82, loss : 0.039075963466435513ITERATION : 83, loss : 0.039075963466435513ITERATION : 84, loss : 0.039075963466435513ITERATION : 85, loss : 0.039075963466435513ITERATION : 86, loss : 0.039075963466435513ITERATION : 87, loss : 0.039075963466435513ITERATION : 88, loss : 0.039075963466435513ITERATION : 89, loss : 0.039075963466435513ITERATION : 90, loss : 0.039075963466435513ITERATION : 91, loss : 0.039075963466435513ITERATION : 92, loss : 0.039075963466435513ITERATION : 93, loss : 0.039075963466435513ITERATION : 94, loss : 0.039075963466435513ITERATION : 95, loss : 0.039075963466435513ITERATION : 96, loss : 0.039075963466435513ITERATION : 97, loss : 0.039075963466435513ITERATION : 98, loss : 0.039075963466435513ITERATION : 99, loss : 0.039075963466435513ITERATION : 100, loss : 0.039075963466435513
ITERATION : 1, loss : 0.025099807443150615ITERATION : 2, loss : 0.018358342824217066ITERATION : 3, loss : 0.013974531551813154ITERATION : 4, loss : 0.011846189741579971ITERATION : 5, loss : 0.0109794032460177ITERATION : 6, loss : 0.010635061921943308ITERATION : 7, loss : 0.010420449889491904ITERATION : 8, loss : 0.010289625295326258ITERATION : 9, loss : 0.010207716486621422ITERATION : 10, loss : 0.010155199793416165ITERATION : 11, loss : 0.01012081933521071ITERATION : 12, loss : 0.010097907965600518ITERATION : 13, loss : 0.010082412179688971ITERATION : 14, loss : 0.010071805014015195ITERATION : 15, loss : 0.010064474091306626ITERATION : 16, loss : 0.010059368889659434ITERATION : 17, loss : 0.010055792330119898ITERATION : 18, loss : 0.010053274969316862ITERATION : 19, loss : 0.010051496546304797ITERATION : 20, loss : 0.010050236473380358ITERATION : 21, loss : 0.010049341540035103ITERATION : 22, loss : 0.010048704731308667ITERATION : 23, loss : 0.01004825088647741ITERATION : 24, loss : 0.010047926968865854ITERATION : 25, loss : 0.010047695511008333ITERATION : 26, loss : 0.010047529978615428ITERATION : 27, loss : 0.010047411504799316ITERATION : 28, loss : 0.010047326608396953ITERATION : 29, loss : 0.010047265745146056ITERATION : 30, loss : 0.01004722206782069ITERATION : 31, loss : 0.010047190701626898ITERATION : 32, loss : 0.010047168171753138ITERATION : 33, loss : 0.010047151972440225ITERATION : 34, loss : 0.010047140297201735ITERATION : 35, loss : 0.010047131901394947ITERATION : 36, loss : 0.010047125860757348ITERATION : 37, loss : 0.010047121503765473ITERATION : 38, loss : 0.010047118371913572ITERATION : 39, loss : 0.010047116103464261ITERATION : 40, loss : 0.010047114471045058ITERATION : 41, loss : 0.010047113290238894ITERATION : 42, loss : 0.010047112438917448ITERATION : 43, loss : 0.010047111805805618ITERATION : 44, loss : 0.010047111363761825ITERATION : 45, loss : 0.010047111050417149ITERATION : 46, loss : 0.010047110827326936ITERATION : 47, loss : 0.010047110650590664ITERATION : 48, loss : 0.010047110519060149ITERATION : 49, loss : 0.010047110437089316ITERATION : 50, loss : 0.010047110372674282ITERATION : 51, loss : 0.0100471103214493ITERATION : 52, loss : 0.010047110299900444ITERATION : 53, loss : 0.010047110290910996ITERATION : 54, loss : 0.01004711027963018ITERATION : 55, loss : 0.010047110266981546ITERATION : 56, loss : 0.01004711024288656ITERATION : 57, loss : 0.010047110242259202ITERATION : 58, loss : 0.010047110242149986ITERATION : 59, loss : 0.010047110242149986ITERATION : 60, loss : 0.010047110242149986ITERATION : 61, loss : 0.010047110242149986ITERATION : 62, loss : 0.010047110242149986ITERATION : 63, loss : 0.010047110242149986ITERATION : 64, loss : 0.010047110242149986ITERATION : 65, loss : 0.010047110242149986ITERATION : 66, loss : 0.010047110242149986ITERATION : 67, loss : 0.010047110242149986ITERATION : 68, loss : 0.010047110242149986ITERATION : 69, loss : 0.010047110242149986ITERATION : 70, loss : 0.010047110242149986ITERATION : 71, loss : 0.010047110242149986ITERATION : 72, loss : 0.010047110242149986ITERATION : 73, loss : 0.010047110242149986ITERATION : 74, loss : 0.010047110242149986ITERATION : 75, loss : 0.010047110242149986ITERATION : 76, loss : 0.010047110242149986ITERATION : 77, loss : 0.010047110242149986ITERATION : 78, loss : 0.010047110242149986ITERATION : 79, loss : 0.010047110242149986ITERATION : 80, loss : 0.010047110242149986ITERATION : 81, loss : 0.010047110242149986ITERATION : 82, loss : 0.010047110242149986ITERATION : 83, loss : 0.010047110242149986ITERATION : 84, loss : 0.010047110242149986ITERATION : 85, loss : 0.010047110242149986ITERATION : 86, loss : 0.010047110242149986ITERATION : 87, loss : 0.010047110242149986ITERATION : 88, loss : 0.010047110242149986ITERATION : 89, loss : 0.010047110242149986ITERATION : 90, loss : 0.010047110242149986ITERATION : 91, loss : 0.010047110242149986ITERATION : 92, loss : 0.010047110242149986ITERATION : 93, loss : 0.010047110242149986ITERATION : 94, loss : 0.010047110242149986ITERATION : 95, loss : 0.010047110242149986ITERATION : 96, loss : 0.010047110242149986ITERATION : 97, loss : 0.010047110242149986ITERATION : 98, loss : 0.010047110242149986ITERATION : 99, loss : 0.010047110242149986ITERATION : 100, loss : 0.010047110242149986
ITERATION : 1, loss : 0.030658410086472944ITERATION : 2, loss : 0.031282770985231204ITERATION : 3, loss : 0.03036397315375867ITERATION : 4, loss : 0.029168319754553824ITERATION : 5, loss : 0.02817785071078429ITERATION : 6, loss : 0.02745283002047095ITERATION : 7, loss : 0.026947205699898363ITERATION : 8, loss : 0.026601750727394215ITERATION : 9, loss : 0.026367658383005185ITERATION : 10, loss : 0.026209403255841632ITERATION : 11, loss : 0.026102365901011802ITERATION : 12, loss : 0.026029835992645807ITERATION : 13, loss : 0.025980568167979553ITERATION : 14, loss : 0.025947011176495834ITERATION : 15, loss : 0.025924093017688138ITERATION : 16, loss : 0.025908398720094778ITERATION : 17, loss : 0.025897623855479012ITERATION : 18, loss : 0.025890207714239492ITERATION : 19, loss : 0.02588509196078346ITERATION : 20, loss : 0.025881555321127972ITERATION : 21, loss : 0.025879105087414523ITERATION : 22, loss : 0.025877404258673322ITERATION : 23, loss : 0.02587622131866542ITERATION : 24, loss : 0.025875397467941156ITERATION : 25, loss : 0.025874822515032866ITERATION : 26, loss : 0.025874421020505488ITERATION : 27, loss : 0.025874139728946813ITERATION : 28, loss : 0.02587394295420367ITERATION : 29, loss : 0.025873804506230842ITERATION : 30, loss : 0.025873707134620005ITERATION : 31, loss : 0.025873638629921626ITERATION : 32, loss : 0.025873590556783645ITERATION : 33, loss : 0.025873556613656666ITERATION : 34, loss : 0.02587353290515439ITERATION : 35, loss : 0.02587351622833008ITERATION : 36, loss : 0.025873504295826466ITERATION : 37, loss : 0.025873496042885094ITERATION : 38, loss : 0.025873490246705582ITERATION : 39, loss : 0.02587348606188166ITERATION : 40, loss : 0.02587348332498442ITERATION : 41, loss : 0.025873481405590096ITERATION : 42, loss : 0.025873479713152812ITERATION : 43, loss : 0.025873478789148908ITERATION : 44, loss : 0.025873477751388076ITERATION : 45, loss : 0.025873477413653378ITERATION : 46, loss : 0.02587347682992981ITERATION : 47, loss : 0.025873476590245373ITERATION : 48, loss : 0.025873476586905527ITERATION : 49, loss : 0.025873476586905527ITERATION : 50, loss : 0.025873476586905527ITERATION : 51, loss : 0.025873476586905527ITERATION : 52, loss : 0.025873476586905527ITERATION : 53, loss : 0.025873476586905527ITERATION : 54, loss : 0.025873476586905527ITERATION : 55, loss : 0.025873476586905527ITERATION : 56, loss : 0.025873476586905527ITERATION : 57, loss : 0.025873476586905527ITERATION : 58, loss : 0.025873476586905527ITERATION : 59, loss : 0.025873476586905527ITERATION : 60, loss : 0.025873476586905527ITERATION : 61, loss : 0.025873476586905527ITERATION : 62, loss : 0.025873476586905527ITERATION : 63, loss : 0.025873476586905527ITERATION : 64, loss : 0.025873476586905527ITERATION : 65, loss : 0.025873476586905527ITERATION : 66, loss : 0.025873476586905527ITERATION : 67, loss : 0.025873476586905527ITERATION : 68, loss : 0.025873476586905527ITERATION : 69, loss : 0.025873476586905527ITERATION : 70, loss : 0.025873476586905527ITERATION : 71, loss : 0.025873476586905527ITERATION : 72, loss : 0.025873476586905527ITERATION : 73, loss : 0.025873476586905527ITERATION : 74, loss : 0.025873476586905527ITERATION : 75, loss : 0.025873476586905527ITERATION : 76, loss : 0.025873476586905527ITERATION : 77, loss : 0.025873476586905527ITERATION : 78, loss : 0.025873476586905527ITERATION : 79, loss : 0.025873476586905527ITERATION : 80, loss : 0.025873476586905527ITERATION : 81, loss : 0.025873476586905527ITERATION : 82, loss : 0.025873476586905527ITERATION : 83, loss : 0.025873476586905527ITERATION : 84, loss : 0.025873476586905527ITERATION : 85, loss : 0.025873476586905527ITERATION : 86, loss : 0.025873476586905527ITERATION : 87, loss : 0.025873476586905527ITERATION : 88, loss : 0.025873476586905527ITERATION : 89, loss : 0.025873476586905527ITERATION : 90, loss : 0.025873476586905527ITERATION : 91, loss : 0.025873476586905527ITERATION : 92, loss : 0.025873476586905527ITERATION : 93, loss : 0.025873476586905527ITERATION : 94, loss : 0.025873476586905527ITERATION : 95, loss : 0.025873476586905527ITERATION : 96, loss : 0.025873476586905527ITERATION : 97, loss : 0.025873476586905527ITERATION : 98, loss : 0.025873476586905527ITERATION : 99, loss : 0.025873476586905527ITERATION : 100, loss : 0.025873476586905527
ITERATION : 1, loss : 0.04043558510070359ITERATION : 2, loss : 0.03323923622299579ITERATION : 3, loss : 0.02931431372572611ITERATION : 4, loss : 0.02845586005149162ITERATION : 5, loss : 0.0288231824419616ITERATION : 6, loss : 0.029613846021167203ITERATION : 7, loss : 0.03046688898843715ITERATION : 8, loss : 0.031233694574249245ITERATION : 9, loss : 0.031867341843946385ITERATION : 10, loss : 0.032366749989731006ITERATION : 11, loss : 0.03274891360288405ITERATION : 12, loss : 0.033035689210113635ITERATION : 13, loss : 0.033247995234853944ITERATION : 14, loss : 0.03340366648973247ITERATION : 15, loss : 0.03351701722299158ITERATION : 16, loss : 0.03359912856712521ITERATION : 17, loss : 0.03365838036506574ITERATION : 18, loss : 0.033701012056947435ITERATION : 19, loss : 0.03373161702661362ITERATION : 20, loss : 0.03375354973144926ITERATION : 21, loss : 0.0337692465717065ITERATION : 22, loss : 0.033780468214980236ITERATION : 23, loss : 0.033788483859637924ITERATION : 24, loss : 0.033794205850578136ITERATION : 25, loss : 0.03379828799224923ITERATION : 26, loss : 0.03380119910798993ITERATION : 27, loss : 0.033803274306297884ITERATION : 28, loss : 0.03380475314153915ITERATION : 29, loss : 0.033805806788820336ITERATION : 30, loss : 0.033806557397467ITERATION : 31, loss : 0.03380709198861763ITERATION : 32, loss : 0.033807472639812236ITERATION : 33, loss : 0.03380774383390738ITERATION : 34, loss : 0.03380793682542619ITERATION : 35, loss : 0.03380807410163065ITERATION : 36, loss : 0.0338081717687416ITERATION : 37, loss : 0.03380824141970405ITERATION : 38, loss : 0.03380829106846347ITERATION : 39, loss : 0.03380832627104987ITERATION : 40, loss : 0.033808351448601164ITERATION : 41, loss : 0.033808369412794345ITERATION : 42, loss : 0.03380838202107664ITERATION : 43, loss : 0.03380839087060836ITERATION : 44, loss : 0.03380839736875851ITERATION : 45, loss : 0.033808401810660124ITERATION : 46, loss : 0.03380840512696225ITERATION : 47, loss : 0.03380840730050396ITERATION : 48, loss : 0.03380840893652377ITERATION : 49, loss : 0.03380841005326805ITERATION : 50, loss : 0.03380841085586696ITERATION : 51, loss : 0.03380841145735695ITERATION : 52, loss : 0.03380841186988522ITERATION : 53, loss : 0.03380841206373388ITERATION : 54, loss : 0.033808412305979085ITERATION : 55, loss : 0.033808412531124335ITERATION : 56, loss : 0.03380841261511656ITERATION : 57, loss : 0.03380841261905668ITERATION : 58, loss : 0.03380841261905668ITERATION : 59, loss : 0.03380841261905668ITERATION : 60, loss : 0.03380841261905668ITERATION : 61, loss : 0.03380841261905668ITERATION : 62, loss : 0.03380841261905668ITERATION : 63, loss : 0.03380841261905668ITERATION : 64, loss : 0.03380841261905668ITERATION : 65, loss : 0.03380841261905668ITERATION : 66, loss : 0.03380841261905668ITERATION : 67, loss : 0.03380841261905668ITERATION : 68, loss : 0.03380841261905668ITERATION : 69, loss : 0.03380841261905668ITERATION : 70, loss : 0.03380841261905668ITERATION : 71, loss : 0.03380841261905668ITERATION : 72, loss : 0.03380841261905668ITERATION : 73, loss : 0.03380841261905668ITERATION : 74, loss : 0.03380841261905668ITERATION : 75, loss : 0.03380841261905668ITERATION : 76, loss : 0.03380841261905668ITERATION : 77, loss : 0.03380841261905668ITERATION : 78, loss : 0.03380841261905668ITERATION : 79, loss : 0.03380841261905668ITERATION : 80, loss : 0.03380841261905668ITERATION : 81, loss : 0.03380841261905668ITERATION : 82, loss : 0.03380841261905668ITERATION : 83, loss : 0.03380841261905668ITERATION : 84, loss : 0.03380841261905668ITERATION : 85, loss : 0.03380841261905668ITERATION : 86, loss : 0.03380841261905668ITERATION : 87, loss : 0.03380841261905668ITERATION : 88, loss : 0.03380841261905668ITERATION : 89, loss : 0.03380841261905668ITERATION : 90, loss : 0.03380841261905668ITERATION : 91, loss : 0.03380841261905668ITERATION : 92, loss : 0.03380841261905668ITERATION : 93, loss : 0.03380841261905668ITERATION : 94, loss : 0.03380841261905668ITERATION : 95, loss : 0.03380841261905668ITERATION : 96, loss : 0.03380841261905668ITERATION : 97, loss : 0.03380841261905668ITERATION : 98, loss : 0.03380841261905668ITERATION : 99, loss : 0.03380841261905668ITERATION : 100, loss : 0.03380841261905668
ITERATION : 1, loss : 0.04554622704687912ITERATION : 2, loss : 0.04576466664942753ITERATION : 3, loss : 0.04847735010095699ITERATION : 4, loss : 0.0514143332345142ITERATION : 5, loss : 0.05266563534467284ITERATION : 6, loss : 0.051763926952264346ITERATION : 7, loss : 0.05122454019189333ITERATION : 8, loss : 0.050887307061835564ITERATION : 9, loss : 0.05066934850826263ITERATION : 10, loss : 0.050524936580438246ITERATION : 11, loss : 0.05042747229001228ITERATION : 12, loss : 0.050360792117502384ITERATION : 13, loss : 0.0503147158394725ITERATION : 14, loss : 0.05028264529935694ITERATION : 15, loss : 0.05026020562853341ITERATION : 16, loss : 0.05024444501765727ITERATION : 17, loss : 0.05023334538494823ITERATION : 18, loss : 0.050225513058445054ITERATION : 19, loss : 0.050219978661735325ITERATION : 20, loss : 0.05021606418641947ITERATION : 21, loss : 0.05021329355833472ITERATION : 22, loss : 0.05021133173056155ITERATION : 23, loss : 0.05020994210460824ITERATION : 24, loss : 0.050208957564587126ITERATION : 25, loss : 0.05020825999865832ITERATION : 26, loss : 0.05020776566769429ITERATION : 27, loss : 0.050207415347476976ITERATION : 28, loss : 0.05020716709300606ITERATION : 29, loss : 0.05020699115464157ITERATION : 30, loss : 0.050206866515199414ITERATION : 31, loss : 0.05020677818999555ITERATION : 32, loss : 0.05020671561564377ITERATION : 33, loss : 0.05020667128215731ITERATION : 34, loss : 0.05020663988209844ITERATION : 35, loss : 0.0502066176242106ITERATION : 36, loss : 0.050206601879409ITERATION : 37, loss : 0.050206590624759365ITERATION : 38, loss : 0.05020658275631425ITERATION : 39, loss : 0.050206577139716574ITERATION : 40, loss : 0.05020657317973669ITERATION : 41, loss : 0.050206570382914824ITERATION : 42, loss : 0.05020656841844702ITERATION : 43, loss : 0.05020656699538499ITERATION : 44, loss : 0.05020656600174812ITERATION : 45, loss : 0.05020656529033817ITERATION : 46, loss : 0.05020656480041339ITERATION : 47, loss : 0.05020656444800935ITERATION : 48, loss : 0.050206564214576734ITERATION : 49, loss : 0.050206564024437926ITERATION : 50, loss : 0.05020656393837027ITERATION : 51, loss : 0.05020656382691344ITERATION : 52, loss : 0.05020656380056905ITERATION : 53, loss : 0.05020656378388717ITERATION : 54, loss : 0.05020656378388717ITERATION : 55, loss : 0.05020656378388717ITERATION : 56, loss : 0.05020656378388717ITERATION : 57, loss : 0.05020656378388717ITERATION : 58, loss : 0.05020656378388717ITERATION : 59, loss : 0.05020656378388717ITERATION : 60, loss : 0.05020656378388717ITERATION : 61, loss : 0.05020656378388717ITERATION : 62, loss : 0.05020656378388717ITERATION : 63, loss : 0.05020656378388717ITERATION : 64, loss : 0.05020656378388717ITERATION : 65, loss : 0.05020656378388717ITERATION : 66, loss : 0.05020656378388717ITERATION : 67, loss : 0.05020656378388717ITERATION : 68, loss : 0.05020656378388717ITERATION : 69, loss : 0.05020656378388717ITERATION : 70, loss : 0.05020656378388717ITERATION : 71, loss : 0.05020656378388717ITERATION : 72, loss : 0.05020656378388717ITERATION : 73, loss : 0.05020656378388717ITERATION : 74, loss : 0.05020656378388717ITERATION : 75, loss : 0.05020656378388717ITERATION : 76, loss : 0.05020656378388717ITERATION : 77, loss : 0.05020656378388717ITERATION : 78, loss : 0.05020656378388717ITERATION : 79, loss : 0.05020656378388717ITERATION : 80, loss : 0.05020656378388717ITERATION : 81, loss : 0.05020656378388717ITERATION : 82, loss : 0.05020656378388717ITERATION : 83, loss : 0.05020656378388717ITERATION : 84, loss : 0.05020656378388717ITERATION : 85, loss : 0.05020656378388717ITERATION : 86, loss : 0.05020656378388717ITERATION : 87, loss : 0.05020656378388717ITERATION : 88, loss : 0.05020656378388717ITERATION : 89, loss : 0.05020656378388717ITERATION : 90, loss : 0.05020656378388717ITERATION : 91, loss : 0.05020656378388717ITERATION : 92, loss : 0.05020656378388717ITERATION : 93, loss : 0.05020656378388717ITERATION : 94, loss : 0.05020656378388717ITERATION : 95, loss : 0.05020656378388717ITERATION : 96, loss : 0.05020656378388717ITERATION : 97, loss : 0.05020656378388717ITERATION : 98, loss : 0.05020656378388717ITERATION : 99, loss : 0.05020656378388717ITERATION : 100, loss : 0.05020656378388717
ITERATION : 1, loss : 0.04549024749634195ITERATION : 2, loss : 0.035329411843781904ITERATION : 3, loss : 0.03256431707317696ITERATION : 4, loss : 0.031414559489500476ITERATION : 5, loss : 0.03085014732848126ITERATION : 6, loss : 0.030537272694697422ITERATION : 7, loss : 0.030346898432939386ITERATION : 8, loss : 0.030223223234129636ITERATION : 9, loss : 0.030139503859546522ITERATION : 10, loss : 0.03008148291245171ITERATION : 11, loss : 0.030040768402644184ITERATION : 12, loss : 0.030012023179777622ITERATION : 13, loss : 0.029991673466759673ITERATION : 14, loss : 0.029977253533234275ITERATION : 15, loss : 0.0299670341917581ITERATION : 16, loss : 0.02995979369375306ITERATION : 17, loss : 0.02995466558715152ITERATION : 18, loss : 0.02995103509987276ITERATION : 19, loss : 0.02994846578214271ITERATION : 20, loss : 0.029946648046022307ITERATION : 21, loss : 0.029945362395015353ITERATION : 22, loss : 0.02994445329475138ITERATION : 23, loss : 0.029943810565542628ITERATION : 24, loss : 0.029943356278156676ITERATION : 25, loss : 0.029943035146346893ITERATION : 26, loss : 0.029942808214399482ITERATION : 27, loss : 0.02994264783937094ITERATION : 28, loss : 0.0299425345472345ITERATION : 29, loss : 0.029942454483590782ITERATION : 30, loss : 0.02994239790492902ITERATION : 31, loss : 0.029942357950131733ITERATION : 32, loss : 0.029942329709364487ITERATION : 33, loss : 0.02994230976618585ITERATION : 34, loss : 0.0299422956694922ITERATION : 35, loss : 0.029942285733751194ITERATION : 36, loss : 0.02994227860422602ITERATION : 37, loss : 0.029942273672164097ITERATION : 38, loss : 0.02994227017434916ITERATION : 39, loss : 0.029942267724911494ITERATION : 40, loss : 0.02994226600774307ITERATION : 41, loss : 0.02994226478634937ITERATION : 42, loss : 0.029942263999924595ITERATION : 43, loss : 0.029942263340004868ITERATION : 44, loss : 0.02994226290297715ITERATION : 45, loss : 0.029942262629767444ITERATION : 46, loss : 0.029942262381415844ITERATION : 47, loss : 0.02994226222432853ITERATION : 48, loss : 0.029942262110058387ITERATION : 49, loss : 0.029942262094263206ITERATION : 50, loss : 0.029942262048110447ITERATION : 51, loss : 0.02994226204967012ITERATION : 52, loss : 0.02994226204967012ITERATION : 53, loss : 0.02994226204967012ITERATION : 54, loss : 0.02994226204967012ITERATION : 55, loss : 0.02994226204967012ITERATION : 56, loss : 0.02994226204967012ITERATION : 57, loss : 0.02994226204967012ITERATION : 58, loss : 0.02994226204967012ITERATION : 59, loss : 0.02994226204967012ITERATION : 60, loss : 0.02994226204967012ITERATION : 61, loss : 0.02994226204967012ITERATION : 62, loss : 0.02994226204967012ITERATION : 63, loss : 0.02994226204967012ITERATION : 64, loss : 0.02994226204967012ITERATION : 65, loss : 0.02994226204967012ITERATION : 66, loss : 0.02994226204967012ITERATION : 67, loss : 0.02994226204967012ITERATION : 68, loss : 0.02994226204967012ITERATION : 69, loss : 0.02994226204967012ITERATION : 70, loss : 0.02994226204967012ITERATION : 71, loss : 0.02994226204967012ITERATION : 72, loss : 0.02994226204967012ITERATION : 73, loss : 0.02994226204967012ITERATION : 74, loss : 0.02994226204967012ITERATION : 75, loss : 0.02994226204967012ITERATION : 76, loss : 0.02994226204967012ITERATION : 77, loss : 0.02994226204967012ITERATION : 78, loss : 0.02994226204967012ITERATION : 79, loss : 0.02994226204967012ITERATION : 80, loss : 0.02994226204967012ITERATION : 81, loss : 0.02994226204967012ITERATION : 82, loss : 0.02994226204967012ITERATION : 83, loss : 0.02994226204967012ITERATION : 84, loss : 0.02994226204967012ITERATION : 85, loss : 0.02994226204967012ITERATION : 86, loss : 0.02994226204967012ITERATION : 87, loss : 0.02994226204967012ITERATION : 88, loss : 0.02994226204967012ITERATION : 89, loss : 0.02994226204967012ITERATION : 90, loss : 0.02994226204967012ITERATION : 91, loss : 0.02994226204967012ITERATION : 92, loss : 0.02994226204967012ITERATION : 93, loss : 0.02994226204967012ITERATION : 94, loss : 0.02994226204967012ITERATION : 95, loss : 0.02994226204967012ITERATION : 96, loss : 0.02994226204967012ITERATION : 97, loss : 0.02994226204967012ITERATION : 98, loss : 0.02994226204967012ITERATION : 99, loss : 0.02994226204967012ITERATION : 100, loss : 0.02994226204967012
ITERATION : 1, loss : 0.02732335332770598ITERATION : 2, loss : 0.02705967722275919ITERATION : 3, loss : 0.02905614307576536ITERATION : 4, loss : 0.028319913009973708ITERATION : 5, loss : 0.02851358086916361ITERATION : 6, loss : 0.028955800052773633ITERATION : 7, loss : 0.02942557646120567ITERATION : 8, loss : 0.029851974037035885ITERATION : 9, loss : 0.030212250370039377ITERATION : 10, loss : 0.030504567857523243ITERATION : 11, loss : 0.030735745940190157ITERATION : 12, loss : 0.030915453385127342ITERATION : 13, loss : 0.031053491476364575ITERATION : 14, loss : 0.031158630802556236ITERATION : 15, loss : 0.031238232002508033ITERATION : 16, loss : 0.03129823975350927ITERATION : 17, loss : 0.03134333956409973ITERATION : 18, loss : 0.031377162543709834ITERATION : 19, loss : 0.031402490516201205ITERATION : 20, loss : 0.03142143731632999ITERATION : 21, loss : 0.031435602055626954ITERATION : 22, loss : 0.03144618682553035ITERATION : 23, loss : 0.03145409459928994ITERATION : 24, loss : 0.03146000216597797ITERATION : 25, loss : 0.03146441551094179ITERATION : 26, loss : 0.031467712220799915ITERATION : 27, loss : 0.031470175297036856ITERATION : 28, loss : 0.03147201573354711ITERATION : 29, loss : 0.0314733911283249ITERATION : 30, loss : 0.03147441908976242ITERATION : 31, loss : 0.031475187511398606ITERATION : 32, loss : 0.0314757620758766ITERATION : 33, loss : 0.03147619156519969ITERATION : 34, loss : 0.03147651277212564ITERATION : 35, loss : 0.031476753091255326ITERATION : 36, loss : 0.03147693274716932ITERATION : 37, loss : 0.03147706750402623ITERATION : 38, loss : 0.0314771683065064ITERATION : 39, loss : 0.031477243033777184ITERATION : 40, loss : 0.03147729934248926ITERATION : 41, loss : 0.03147734133050681ITERATION : 42, loss : 0.03147737262006105ITERATION : 43, loss : 0.031477396273502184ITERATION : 44, loss : 0.03147741403308795ITERATION : 45, loss : 0.031477427856133416ITERATION : 46, loss : 0.03147743777541769ITERATION : 47, loss : 0.03147744459410642ITERATION : 48, loss : 0.03147744987048051ITERATION : 49, loss : 0.03147745405522998ITERATION : 50, loss : 0.031477457635148795ITERATION : 51, loss : 0.03147745949691331ITERATION : 52, loss : 0.03147746096383468ITERATION : 53, loss : 0.031477461917408266ITERATION : 54, loss : 0.031477462584774574ITERATION : 55, loss : 0.03147746317185524ITERATION : 56, loss : 0.03147746421119189ITERATION : 57, loss : 0.03147746429794294ITERATION : 58, loss : 0.03147746451139724ITERATION : 59, loss : 0.03147746462285937ITERATION : 60, loss : 0.03147746466910784ITERATION : 61, loss : 0.03147746483088792ITERATION : 62, loss : 0.03147746485948864ITERATION : 63, loss : 0.03147746485948864ITERATION : 64, loss : 0.03147746485948864ITERATION : 65, loss : 0.03147746485948864ITERATION : 66, loss : 0.03147746485948864ITERATION : 67, loss : 0.03147746485948864ITERATION : 68, loss : 0.03147746485948864ITERATION : 69, loss : 0.03147746485948864ITERATION : 70, loss : 0.03147746485948864ITERATION : 71, loss : 0.03147746485948864ITERATION : 72, loss : 0.03147746485948864ITERATION : 73, loss : 0.03147746485948864ITERATION : 74, loss : 0.03147746485948864ITERATION : 75, loss : 0.03147746485948864ITERATION : 76, loss : 0.03147746485948864ITERATION : 77, loss : 0.03147746485948864ITERATION : 78, loss : 0.03147746485948864ITERATION : 79, loss : 0.03147746485948864ITERATION : 80, loss : 0.03147746485948864ITERATION : 81, loss : 0.03147746485948864ITERATION : 82, loss : 0.03147746485948864ITERATION : 83, loss : 0.03147746485948864ITERATION : 84, loss : 0.03147746485948864ITERATION : 85, loss : 0.03147746485948864ITERATION : 86, loss : 0.03147746485948864ITERATION : 87, loss : 0.03147746485948864ITERATION : 88, loss : 0.03147746485948864ITERATION : 89, loss : 0.03147746485948864ITERATION : 90, loss : 0.03147746485948864ITERATION : 91, loss : 0.03147746485948864ITERATION : 92, loss : 0.03147746485948864ITERATION : 93, loss : 0.03147746485948864ITERATION : 94, loss : 0.03147746485948864ITERATION : 95, loss : 0.03147746485948864ITERATION : 96, loss : 0.03147746485948864ITERATION : 97, loss : 0.03147746485948864ITERATION : 98, loss : 0.03147746485948864ITERATION : 99, loss : 0.03147746485948864ITERATION : 100, loss : 0.03147746485948864
ITERATION : 1, loss : 0.006645547723512351ITERATION : 2, loss : 0.0077255998903388075ITERATION : 3, loss : 0.00808945423026293ITERATION : 4, loss : 0.008277865653256063ITERATION : 5, loss : 0.008398818121325547ITERATION : 6, loss : 0.008484543848751386ITERATION : 7, loss : 0.00854757088850842ITERATION : 8, loss : 0.008594314918696286ITERATION : 9, loss : 0.00862891675888785ITERATION : 10, loss : 0.008654399119289771ITERATION : 11, loss : 0.008673062118210707ITERATION : 12, loss : 0.008686663766236946ITERATION : 13, loss : 0.008696536647298586ITERATION : 14, loss : 0.00870367974816343ITERATION : 15, loss : 0.008708834852663792ITERATION : 16, loss : 0.008712548036649208ITERATION : 17, loss : 0.008715218470765767ITERATION : 18, loss : 0.008717136792793418ITERATION : 19, loss : 0.008718513599014288ITERATION : 20, loss : 0.008719501034615ITERATION : 21, loss : 0.008720208860694679ITERATION : 22, loss : 0.008720716024220056ITERATION : 23, loss : 0.00872107929569136ITERATION : 24, loss : 0.00872133941283965ITERATION : 25, loss : 0.008721525658026445ITERATION : 26, loss : 0.008721658962374725ITERATION : 27, loss : 0.008721754355319307ITERATION : 28, loss : 0.008721822605065807ITERATION : 29, loss : 0.008721871465194007ITERATION : 30, loss : 0.008721906422190565ITERATION : 31, loss : 0.008721931414929995ITERATION : 32, loss : 0.008721949304824258ITERATION : 33, loss : 0.008721962064955572ITERATION : 34, loss : 0.008721971202817999ITERATION : 35, loss : 0.008721977752595199ITERATION : 36, loss : 0.008721982429980473ITERATION : 37, loss : 0.008721985787356397ITERATION : 38, loss : 0.008721988148215528ITERATION : 39, loss : 0.008721989872428677ITERATION : 40, loss : 0.008721991135756868ITERATION : 41, loss : 0.00872199200714489ITERATION : 42, loss : 0.00872199264716998ITERATION : 43, loss : 0.008721993135633125ITERATION : 44, loss : 0.008721993453023732ITERATION : 45, loss : 0.008721993671048946ITERATION : 46, loss : 0.008721993807169873ITERATION : 47, loss : 0.00872199390025756ITERATION : 48, loss : 0.008721993977262957ITERATION : 49, loss : 0.008721993977229297ITERATION : 50, loss : 0.008721993977229297ITERATION : 51, loss : 0.008721993977229297ITERATION : 52, loss : 0.008721993977229297ITERATION : 53, loss : 0.008721993977229297ITERATION : 54, loss : 0.008721993977229297ITERATION : 55, loss : 0.008721993977229297ITERATION : 56, loss : 0.008721993977229297ITERATION : 57, loss : 0.008721993977229297ITERATION : 58, loss : 0.008721993977229297ITERATION : 59, loss : 0.008721993977229297ITERATION : 60, loss : 0.008721993977229297ITERATION : 61, loss : 0.008721993977229297ITERATION : 62, loss : 0.008721993977229297ITERATION : 63, loss : 0.008721993977229297ITERATION : 64, loss : 0.008721993977229297ITERATION : 65, loss : 0.008721993977229297ITERATION : 66, loss : 0.008721993977229297ITERATION : 67, loss : 0.008721993977229297ITERATION : 68, loss : 0.008721993977229297ITERATION : 69, loss : 0.008721993977229297ITERATION : 70, loss : 0.008721993977229297ITERATION : 71, loss : 0.008721993977229297ITERATION : 72, loss : 0.008721993977229297ITERATION : 73, loss : 0.008721993977229297ITERATION : 74, loss : 0.008721993977229297ITERATION : 75, loss : 0.008721993977229297ITERATION : 76, loss : 0.008721993977229297ITERATION : 77, loss : 0.008721993977229297ITERATION : 78, loss : 0.008721993977229297ITERATION : 79, loss : 0.008721993977229297ITERATION : 80, loss : 0.008721993977229297ITERATION : 81, loss : 0.008721993977229297ITERATION : 82, loss : 0.008721993977229297ITERATION : 83, loss : 0.008721993977229297ITERATION : 84, loss : 0.008721993977229297ITERATION : 85, loss : 0.008721993977229297ITERATION : 86, loss : 0.008721993977229297ITERATION : 87, loss : 0.008721993977229297ITERATION : 88, loss : 0.008721993977229297ITERATION : 89, loss : 0.008721993977229297ITERATION : 90, loss : 0.008721993977229297ITERATION : 91, loss : 0.008721993977229297ITERATION : 92, loss : 0.008721993977229297ITERATION : 93, loss : 0.008721993977229297ITERATION : 94, loss : 0.008721993977229297ITERATION : 95, loss : 0.008721993977229297ITERATION : 96, loss : 0.008721993977229297ITERATION : 97, loss : 0.008721993977229297ITERATION : 98, loss : 0.008721993977229297ITERATION : 99, loss : 0.008721993977229297ITERATION : 100, loss : 0.008721993977229297
gradient norm in None layer : 0.001633891759307384
gradient norm in None layer : 9.364107572871724e-05
gradient norm in None layer : 9.479766039489743e-05
gradient norm in None layer : 0.0012388113808806593
gradient norm in None layer : 8.456051708460627e-05
gradient norm in None layer : 9.169780342135841e-05
gradient norm in None layer : 0.0005027903117352366
gradient norm in None layer : 1.8566367717412612e-05
gradient norm in None layer : 1.9677413034204452e-05
gradient norm in None layer : 0.00036732535188950045
gradient norm in None layer : 1.6405003159289548e-05
gradient norm in None layer : 1.5841723040375842e-05
gradient norm in None layer : 0.00011276033127850281
gradient norm in None layer : 4.2528443994941395e-06
gradient norm in None layer : 3.608129365707821e-06
gradient norm in None layer : 0.00010590480365106813
gradient norm in None layer : 4.882775030174307e-06
gradient norm in None layer : 3.6047359450785507e-06
gradient norm in None layer : 0.00014276656541378224
gradient norm in None layer : 2.0814730112662757e-06
gradient norm in None layer : 0.0003035406023455693
gradient norm in None layer : 2.366365347976699e-05
gradient norm in None layer : 1.659423904782472e-05
gradient norm in None layer : 0.0003368758700197175
gradient norm in None layer : 3.5802841811831086e-05
gradient norm in None layer : 4.00731801838155e-05
gradient norm in None layer : 0.0005243929099276149
gradient norm in None layer : 5.238436637569609e-06
gradient norm in None layer : 0.0009620160332883764
gradient norm in None layer : 7.341333055440557e-05
gradient norm in None layer : 8.419898227628215e-05
gradient norm in None layer : 0.001069211020922171
gradient norm in None layer : 8.811649327287119e-05
gradient norm in None layer : 0.00011478020268560573
gradient norm in None layer : 7.464092027718844e-05
gradient norm in None layer : 1.5487159316707385e-05
Total gradient norm: 0.002695034369481986
invariance loss : 4.170078046173414, avg_den : 0.4429779052734375, density loss : 0.3429779052734375, mse loss : 0.028644155948102865, solver time : 145.90249967575073 sec , total loss : 0.03315721189954972, running loss : 0.04113009482710749
Epoch 0/10 , batch 32/12500 
ITERATION : 1, loss : 0.045435711819034864ITERATION : 2, loss : 0.046468126607640534ITERATION : 3, loss : 0.048999149069144904ITERATION : 4, loss : 0.052099254028797726ITERATION : 5, loss : 0.054928894009545995ITERATION : 6, loss : 0.05726768593619381ITERATION : 7, loss : 0.05910171117292239ITERATION : 8, loss : 0.06049748258688187ITERATION : 9, loss : 0.06154075935280156ITERATION : 10, loss : 0.06122150496240371ITERATION : 11, loss : 0.060975773540400076ITERATION : 12, loss : 0.060800757494731746ITERATION : 13, loss : 0.06067572194513372ITERATION : 14, loss : 0.060586175067391106ITERATION : 15, loss : 0.06052192169787115ITERATION : 16, loss : 0.060475750207875684ITERATION : 17, loss : 0.06044253647169542ITERATION : 18, loss : 0.06041862401102114ITERATION : 19, loss : 0.06040139748630296ITERATION : 20, loss : 0.06038898194662004ITERATION : 21, loss : 0.060380031301877ITERATION : 22, loss : 0.06037357679654706ITERATION : 23, loss : 0.06036892136971461ITERATION : 24, loss : 0.06036556318844538ITERATION : 25, loss : 0.06036314053071391ITERATION : 26, loss : 0.06036139247602584ITERATION : 27, loss : 0.0603601317553646ITERATION : 28, loss : 0.060359222094531775ITERATION : 29, loss : 0.060358565796866596ITERATION : 30, loss : 0.06035809223014285ITERATION : 31, loss : 0.060357750339000364ITERATION : 32, loss : 0.06035750347602512ITERATION : 33, loss : 0.060357325463465096ITERATION : 34, loss : 0.06035719700047759ITERATION : 35, loss : 0.06035710441527886ITERATION : 36, loss : 0.060357037522150804ITERATION : 37, loss : 0.06035698941685022ITERATION : 38, loss : 0.060356954514571175ITERATION : 39, loss : 0.06035692949074073ITERATION : 40, loss : 0.06035691131611897ITERATION : 41, loss : 0.060356898311693274ITERATION : 42, loss : 0.06035688886773304ITERATION : 43, loss : 0.0603568821019617ITERATION : 44, loss : 0.060356877208973106ITERATION : 45, loss : 0.06035687361419934ITERATION : 46, loss : 0.06035687115639176ITERATION : 47, loss : 0.06035686940841648ITERATION : 48, loss : 0.06035686794752001ITERATION : 49, loss : 0.06035686715197455ITERATION : 50, loss : 0.060356866317517494ITERATION : 51, loss : 0.060356865788672776ITERATION : 52, loss : 0.06035686538695864ITERATION : 53, loss : 0.060356865136476305ITERATION : 54, loss : 0.060356865135600346ITERATION : 55, loss : 0.060356865135600346ITERATION : 56, loss : 0.060356865135600346ITERATION : 57, loss : 0.060356865135600346ITERATION : 58, loss : 0.060356865135600346ITERATION : 59, loss : 0.060356865135600346ITERATION : 60, loss : 0.060356865135600346ITERATION : 61, loss : 0.060356865135600346ITERATION : 62, loss : 0.060356865135600346ITERATION : 63, loss : 0.060356865135600346ITERATION : 64, loss : 0.060356865135600346ITERATION : 65, loss : 0.060356865135600346ITERATION : 66, loss : 0.060356865135600346ITERATION : 67, loss : 0.060356865135600346ITERATION : 68, loss : 0.060356865135600346ITERATION : 69, loss : 0.060356865135600346ITERATION : 70, loss : 0.060356865135600346ITERATION : 71, loss : 0.060356865135600346ITERATION : 72, loss : 0.060356865135600346ITERATION : 73, loss : 0.060356865135600346ITERATION : 74, loss : 0.060356865135600346ITERATION : 75, loss : 0.060356865135600346ITERATION : 76, loss : 0.060356865135600346ITERATION : 77, loss : 0.060356865135600346ITERATION : 78, loss : 0.060356865135600346ITERATION : 79, loss : 0.060356865135600346ITERATION : 80, loss : 0.060356865135600346ITERATION : 81, loss : 0.060356865135600346ITERATION : 82, loss : 0.060356865135600346ITERATION : 83, loss : 0.060356865135600346ITERATION : 84, loss : 0.060356865135600346ITERATION : 85, loss : 0.060356865135600346ITERATION : 86, loss : 0.060356865135600346ITERATION : 87, loss : 0.060356865135600346ITERATION : 88, loss : 0.060356865135600346ITERATION : 89, loss : 0.060356865135600346ITERATION : 90, loss : 0.060356865135600346ITERATION : 91, loss : 0.060356865135600346ITERATION : 92, loss : 0.060356865135600346ITERATION : 93, loss : 0.060356865135600346ITERATION : 94, loss : 0.060356865135600346ITERATION : 95, loss : 0.060356865135600346ITERATION : 96, loss : 0.060356865135600346ITERATION : 97, loss : 0.060356865135600346ITERATION : 98, loss : 0.060356865135600346ITERATION : 99, loss : 0.060356865135600346ITERATION : 100, loss : 0.060356865135600346
ITERATION : 1, loss : 0.03966856984415443ITERATION : 2, loss : 0.030832330266432024ITERATION : 3, loss : 0.027206417163884687ITERATION : 4, loss : 0.02668747509931263ITERATION : 5, loss : 0.027530683421811013ITERATION : 6, loss : 0.028708128435566787ITERATION : 7, loss : 0.02962303313111885ITERATION : 8, loss : 0.029702483669100117ITERATION : 9, loss : 0.029751010712758832ITERATION : 10, loss : 0.02977151106003489ITERATION : 11, loss : 0.029773360964287322ITERATION : 12, loss : 0.029764968790157754ITERATION : 13, loss : 0.029752147361647226ITERATION : 14, loss : 0.029738382719102646ITERATION : 15, loss : 0.029725515344440546ITERATION : 16, loss : 0.029714352560827294ITERATION : 17, loss : 0.0297051106102107ITERATION : 18, loss : 0.02969769807569583ITERATION : 19, loss : 0.029691887380201523ITERATION : 20, loss : 0.029687409851222223ITERATION : 21, loss : 0.02968400493577622ITERATION : 22, loss : 0.02968144252547054ITERATION : 23, loss : 0.02967953038607399ITERATION : 24, loss : 0.029678113492256374ITERATION : 25, loss : 0.029677069551799783ITERATION : 26, loss : 0.029676303938668526ITERATION : 27, loss : 0.029675744775272798ITERATION : 28, loss : 0.029675337941927237ITERATION : 29, loss : 0.029675042703018264ITERATION : 30, loss : 0.02967482896036985ITERATION : 31, loss : 0.02967467450026416ITERATION : 32, loss : 0.029674563181360112ITERATION : 33, loss : 0.02967448303682021ITERATION : 34, loss : 0.029674425500770027ITERATION : 35, loss : 0.02967438413179172ITERATION : 36, loss : 0.029674354518971317ITERATION : 37, loss : 0.029674333367911688ITERATION : 38, loss : 0.029674318195379364ITERATION : 39, loss : 0.02967430736807776ITERATION : 40, loss : 0.029674299644124977ITERATION : 41, loss : 0.02967429410682691ITERATION : 42, loss : 0.029674290202971423ITERATION : 43, loss : 0.029674287412721493ITERATION : 44, loss : 0.029674285427064278ITERATION : 45, loss : 0.02967428397056256ITERATION : 46, loss : 0.029674282929200286ITERATION : 47, loss : 0.029674282194534978ITERATION : 48, loss : 0.0296742815776171ITERATION : 49, loss : 0.02967428128727734ITERATION : 50, loss : 0.02967428110695881ITERATION : 51, loss : 0.02967428076428073ITERATION : 52, loss : 0.02967428076428073ITERATION : 53, loss : 0.02967428076428073ITERATION : 54, loss : 0.02967428076428073ITERATION : 55, loss : 0.02967428076428073ITERATION : 56, loss : 0.02967428076428073ITERATION : 57, loss : 0.02967428076428073ITERATION : 58, loss : 0.02967428076428073ITERATION : 59, loss : 0.02967428076428073ITERATION : 60, loss : 0.02967428076428073ITERATION : 61, loss : 0.02967428076428073ITERATION : 62, loss : 0.02967428076428073ITERATION : 63, loss : 0.02967428076428073ITERATION : 64, loss : 0.02967428076428073ITERATION : 65, loss : 0.02967428076428073ITERATION : 66, loss : 0.02967428076428073ITERATION : 67, loss : 0.02967428076428073ITERATION : 68, loss : 0.02967428076428073ITERATION : 69, loss : 0.02967428076428073ITERATION : 70, loss : 0.02967428076428073ITERATION : 71, loss : 0.02967428076428073ITERATION : 72, loss : 0.02967428076428073ITERATION : 73, loss : 0.02967428076428073ITERATION : 74, loss : 0.02967428076428073ITERATION : 75, loss : 0.02967428076428073ITERATION : 76, loss : 0.02967428076428073ITERATION : 77, loss : 0.02967428076428073ITERATION : 78, loss : 0.02967428076428073ITERATION : 79, loss : 0.02967428076428073ITERATION : 80, loss : 0.02967428076428073ITERATION : 81, loss : 0.02967428076428073ITERATION : 82, loss : 0.02967428076428073ITERATION : 83, loss : 0.02967428076428073ITERATION : 84, loss : 0.02967428076428073ITERATION : 85, loss : 0.02967428076428073ITERATION : 86, loss : 0.02967428076428073ITERATION : 87, loss : 0.02967428076428073ITERATION : 88, loss : 0.02967428076428073ITERATION : 89, loss : 0.02967428076428073ITERATION : 90, loss : 0.02967428076428073ITERATION : 91, loss : 0.02967428076428073ITERATION : 92, loss : 0.02967428076428073ITERATION : 93, loss : 0.02967428076428073ITERATION : 94, loss : 0.02967428076428073ITERATION : 95, loss : 0.02967428076428073ITERATION : 96, loss : 0.02967428076428073ITERATION : 97, loss : 0.02967428076428073ITERATION : 98, loss : 0.02967428076428073ITERATION : 99, loss : 0.02967428076428073ITERATION : 100, loss : 0.02967428076428073
ITERATION : 1, loss : 0.07494336859668828ITERATION : 2, loss : 0.061305748165156715ITERATION : 3, loss : 0.057946371265916764ITERATION : 4, loss : 0.057171520993064454ITERATION : 5, loss : 0.05587040666591285ITERATION : 6, loss : 0.05502658380043948ITERATION : 7, loss : 0.054487757043496766ITERATION : 8, loss : 0.05412875599546493ITERATION : 9, loss : 0.05388408950214411ITERATION : 10, loss : 0.053715288008432026ITERATION : 11, loss : 0.05359802875438873ITERATION : 12, loss : 0.053516251361580856ITERATION : 13, loss : 0.05345908466072904ITERATION : 14, loss : 0.05341906418213908ITERATION : 15, loss : 0.05339102149479406ITERATION : 16, loss : 0.05337136014016763ITERATION : 17, loss : 0.05335756986029534ITERATION : 18, loss : 0.05334789505812783ITERATION : 19, loss : 0.05334110634701636ITERATION : 20, loss : 0.05333634228296406ITERATION : 21, loss : 0.05333299876421109ITERATION : 22, loss : 0.053330652111692835ITERATION : 23, loss : 0.053329005087212306ITERATION : 24, loss : 0.053327848940037254ITERATION : 25, loss : 0.05332703745283796ITERATION : 26, loss : 0.05332646792054455ITERATION : 27, loss : 0.05332606807156862ITERATION : 28, loss : 0.05332578741267142ITERATION : 29, loss : 0.053325590469845224ITERATION : 30, loss : 0.053325452145067155ITERATION : 31, loss : 0.05332535506952951ITERATION : 32, loss : 0.05332528692494778ITERATION : 33, loss : 0.053325239140524054ITERATION : 34, loss : 0.053325205610568915ITERATION : 35, loss : 0.05332518215925467ITERATION : 36, loss : 0.05332516567342693ITERATION : 37, loss : 0.053325154145091036ITERATION : 38, loss : 0.053325146041904324ITERATION : 39, loss : 0.05332514039908568ITERATION : 40, loss : 0.05332513652307751ITERATION : 41, loss : 0.05332513369392173ITERATION : 42, loss : 0.053325131751662876ITERATION : 43, loss : 0.05332513044225567ITERATION : 44, loss : 0.05332512944982925ITERATION : 45, loss : 0.053325128681002615ITERATION : 46, loss : 0.05332512835567471ITERATION : 47, loss : 0.05332512806998569ITERATION : 48, loss : 0.05332512780696637ITERATION : 49, loss : 0.05332512756331366ITERATION : 50, loss : 0.05332512755301354ITERATION : 51, loss : 0.05332512755174982ITERATION : 52, loss : 0.05332512755174982ITERATION : 53, loss : 0.05332512755174982ITERATION : 54, loss : 0.05332512755174982ITERATION : 55, loss : 0.05332512755174982ITERATION : 56, loss : 0.05332512755174982ITERATION : 57, loss : 0.05332512755174982ITERATION : 58, loss : 0.05332512755174982ITERATION : 59, loss : 0.05332512755174982ITERATION : 60, loss : 0.05332512755174982ITERATION : 61, loss : 0.05332512755174982ITERATION : 62, loss : 0.05332512755174982ITERATION : 63, loss : 0.05332512755174982ITERATION : 64, loss : 0.05332512755174982ITERATION : 65, loss : 0.05332512755174982ITERATION : 66, loss : 0.05332512755174982ITERATION : 67, loss : 0.05332512755174982ITERATION : 68, loss : 0.05332512755174982ITERATION : 69, loss : 0.05332512755174982ITERATION : 70, loss : 0.05332512755174982ITERATION : 71, loss : 0.05332512755174982ITERATION : 72, loss : 0.05332512755174982ITERATION : 73, loss : 0.05332512755174982ITERATION : 74, loss : 0.05332512755174982ITERATION : 75, loss : 0.05332512755174982ITERATION : 76, loss : 0.05332512755174982ITERATION : 77, loss : 0.05332512755174982ITERATION : 78, loss : 0.05332512755174982ITERATION : 79, loss : 0.05332512755174982ITERATION : 80, loss : 0.05332512755174982ITERATION : 81, loss : 0.05332512755174982ITERATION : 82, loss : 0.05332512755174982ITERATION : 83, loss : 0.05332512755174982ITERATION : 84, loss : 0.05332512755174982ITERATION : 85, loss : 0.05332512755174982ITERATION : 86, loss : 0.05332512755174982ITERATION : 87, loss : 0.05332512755174982ITERATION : 88, loss : 0.05332512755174982ITERATION : 89, loss : 0.05332512755174982ITERATION : 90, loss : 0.05332512755174982ITERATION : 91, loss : 0.05332512755174982ITERATION : 92, loss : 0.05332512755174982ITERATION : 93, loss : 0.05332512755174982ITERATION : 94, loss : 0.05332512755174982ITERATION : 95, loss : 0.05332512755174982ITERATION : 96, loss : 0.05332512755174982ITERATION : 97, loss : 0.05332512755174982ITERATION : 98, loss : 0.05332512755174982ITERATION : 99, loss : 0.05332512755174982ITERATION : 100, loss : 0.05332512755174982
ITERATION : 1, loss : 0.03258631123332198ITERATION : 2, loss : 0.033629074886005535ITERATION : 3, loss : 0.037120008702694274ITERATION : 4, loss : 0.03992113794749542ITERATION : 5, loss : 0.04190602002848344ITERATION : 6, loss : 0.043259757710531976ITERATION : 7, loss : 0.0441678255912889ITERATION : 8, loss : 0.04477187113552688ITERATION : 9, loss : 0.04517184971348794ITERATION : 10, loss : 0.04543599591540239ITERATION : 11, loss : 0.04561014575891345ITERATION : 12, loss : 0.045724830593871876ITERATION : 13, loss : 0.04580029146940921ITERATION : 14, loss : 0.04584991021604622ITERATION : 15, loss : 0.04588251808653343ITERATION : 16, loss : 0.04590393623135279ITERATION : 17, loss : 0.04591799814244541ITERATION : 18, loss : 0.04592722658621894ITERATION : 19, loss : 0.045933280611864795ITERATION : 20, loss : 0.04593725078000727ITERATION : 21, loss : 0.045939853462697924ITERATION : 22, loss : 0.04594155911680322ITERATION : 23, loss : 0.045942676604068006ITERATION : 24, loss : 0.04594340857401405ITERATION : 25, loss : 0.04594388784915059ITERATION : 26, loss : 0.04594420165169005ITERATION : 27, loss : 0.045944407039908236ITERATION : 28, loss : 0.0459445414082641ITERATION : 29, loss : 0.04594462932588698ITERATION : 30, loss : 0.04594468680373217ITERATION : 31, loss : 0.04594472443711179ITERATION : 32, loss : 0.045944749037255414ITERATION : 33, loss : 0.045944765173678265ITERATION : 34, loss : 0.04594477567955641ITERATION : 35, loss : 0.045944782556995814ITERATION : 36, loss : 0.045944787033704415ITERATION : 37, loss : 0.045944789975125785ITERATION : 38, loss : 0.045944791839912534ITERATION : 39, loss : 0.045944793108401244ITERATION : 40, loss : 0.04594479389642944ITERATION : 41, loss : 0.04594479441422544ITERATION : 42, loss : 0.04594479478316503ITERATION : 43, loss : 0.045944795012100824ITERATION : 44, loss : 0.04594479517813247ITERATION : 45, loss : 0.045944795262045465ITERATION : 46, loss : 0.045944795313671134ITERATION : 47, loss : 0.045944795314303684ITERATION : 48, loss : 0.045944795314303684ITERATION : 49, loss : 0.045944795314303684ITERATION : 50, loss : 0.045944795314303684ITERATION : 51, loss : 0.045944795314303684ITERATION : 52, loss : 0.045944795314303684ITERATION : 53, loss : 0.045944795314303684ITERATION : 54, loss : 0.045944795314303684ITERATION : 55, loss : 0.045944795314303684ITERATION : 56, loss : 0.045944795314303684ITERATION : 57, loss : 0.045944795314303684ITERATION : 58, loss : 0.045944795314303684ITERATION : 59, loss : 0.045944795314303684ITERATION : 60, loss : 0.045944795314303684ITERATION : 61, loss : 0.045944795314303684ITERATION : 62, loss : 0.045944795314303684ITERATION : 63, loss : 0.045944795314303684ITERATION : 64, loss : 0.045944795314303684ITERATION : 65, loss : 0.045944795314303684ITERATION : 66, loss : 0.045944795314303684ITERATION : 67, loss : 0.045944795314303684ITERATION : 68, loss : 0.045944795314303684ITERATION : 69, loss : 0.045944795314303684ITERATION : 70, loss : 0.045944795314303684ITERATION : 71, loss : 0.045944795314303684ITERATION : 72, loss : 0.045944795314303684ITERATION : 73, loss : 0.045944795314303684ITERATION : 74, loss : 0.045944795314303684ITERATION : 75, loss : 0.045944795314303684ITERATION : 76, loss : 0.045944795314303684ITERATION : 77, loss : 0.045944795314303684ITERATION : 78, loss : 0.045944795314303684ITERATION : 79, loss : 0.045944795314303684ITERATION : 80, loss : 0.045944795314303684ITERATION : 81, loss : 0.045944795314303684ITERATION : 82, loss : 0.045944795314303684ITERATION : 83, loss : 0.045944795314303684ITERATION : 84, loss : 0.045944795314303684ITERATION : 85, loss : 0.045944795314303684ITERATION : 86, loss : 0.045944795314303684ITERATION : 87, loss : 0.045944795314303684ITERATION : 88, loss : 0.045944795314303684ITERATION : 89, loss : 0.045944795314303684ITERATION : 90, loss : 0.045944795314303684ITERATION : 91, loss : 0.045944795314303684ITERATION : 92, loss : 0.045944795314303684ITERATION : 93, loss : 0.045944795314303684ITERATION : 94, loss : 0.045944795314303684ITERATION : 95, loss : 0.045944795314303684ITERATION : 96, loss : 0.045944795314303684ITERATION : 97, loss : 0.045944795314303684ITERATION : 98, loss : 0.045944795314303684ITERATION : 99, loss : 0.045944795314303684ITERATION : 100, loss : 0.045944795314303684
ITERATION : 1, loss : 0.031755721097596744ITERATION : 2, loss : 0.021344141661546866ITERATION : 3, loss : 0.022224976701020492ITERATION : 4, loss : 0.023694199998371118ITERATION : 5, loss : 0.025547861725337052ITERATION : 6, loss : 0.02328900083480573ITERATION : 7, loss : 0.021673141208252568ITERATION : 8, loss : 0.02065146184225563ITERATION : 9, loss : 0.019994849525581844ITERATION : 10, loss : 0.019567302487360324ITERATION : 11, loss : 0.019286078211623932ITERATION : 12, loss : 0.01909966918790998ITERATION : 13, loss : 0.018975382836692464ITERATION : 14, loss : 0.018892143166907484ITERATION : 15, loss : 0.018836198068886832ITERATION : 16, loss : 0.018798491348023066ITERATION : 17, loss : 0.018773017669856458ITERATION : 18, loss : 0.018755773672955278ITERATION : 19, loss : 0.018744079738331292ITERATION : 20, loss : 0.01873613639928683ITERATION : 21, loss : 0.01873073232930347ITERATION : 22, loss : 0.018727050273959493ITERATION : 23, loss : 0.01872453784619773ITERATION : 24, loss : 0.018722820955988866ITERATION : 25, loss : 0.018721646069754478ITERATION : 26, loss : 0.01872084081555097ITERATION : 27, loss : 0.018720288124088064ITERATION : 28, loss : 0.01871990823103822ITERATION : 29, loss : 0.01871964674324717ITERATION : 30, loss : 0.018719466413396422ITERATION : 31, loss : 0.018719341967204352ITERATION : 32, loss : 0.01871925592869511ITERATION : 33, loss : 0.018719196285260243ITERATION : 34, loss : 0.01871915494235643ITERATION : 35, loss : 0.018719126193183493ITERATION : 36, loss : 0.018719106227554835ITERATION : 37, loss : 0.018719092259344552ITERATION : 38, loss : 0.018719082544319934ITERATION : 39, loss : 0.01871907573408102ITERATION : 40, loss : 0.018719070948815534ITERATION : 41, loss : 0.018719067617331497ITERATION : 42, loss : 0.018719065308734635ITERATION : 43, loss : 0.018719063729068734ITERATION : 44, loss : 0.01871906261245992ITERATION : 45, loss : 0.018719061848998013ITERATION : 46, loss : 0.018719061291970752ITERATION : 47, loss : 0.018719060915629585ITERATION : 48, loss : 0.018719060587091045ITERATION : 49, loss : 0.01871906042407325ITERATION : 50, loss : 0.018719060300192516ITERATION : 51, loss : 0.01871906020453157ITERATION : 52, loss : 0.018719060149095947ITERATION : 53, loss : 0.018719060111821458ITERATION : 54, loss : 0.01871906005936496ITERATION : 55, loss : 0.01871906005936496ITERATION : 56, loss : 0.01871906005936496ITERATION : 57, loss : 0.01871906005936496ITERATION : 58, loss : 0.01871906005936496ITERATION : 59, loss : 0.01871906005936496ITERATION : 60, loss : 0.01871906005936496ITERATION : 61, loss : 0.01871906005936496ITERATION : 62, loss : 0.01871906005936496ITERATION : 63, loss : 0.01871906005936496ITERATION : 64, loss : 0.01871906005936496ITERATION : 65, loss : 0.01871906005936496ITERATION : 66, loss : 0.01871906005936496ITERATION : 67, loss : 0.01871906005936496ITERATION : 68, loss : 0.01871906005936496ITERATION : 69, loss : 0.01871906005936496ITERATION : 70, loss : 0.01871906005936496ITERATION : 71, loss : 0.01871906005936496ITERATION : 72, loss : 0.01871906005936496ITERATION : 73, loss : 0.01871906005936496ITERATION : 74, loss : 0.01871906005936496ITERATION : 75, loss : 0.01871906005936496ITERATION : 76, loss : 0.01871906005936496ITERATION : 77, loss : 0.01871906005936496ITERATION : 78, loss : 0.01871906005936496ITERATION : 79, loss : 0.01871906005936496ITERATION : 80, loss : 0.01871906005936496ITERATION : 81, loss : 0.01871906005936496ITERATION : 82, loss : 0.01871906005936496ITERATION : 83, loss : 0.01871906005936496ITERATION : 84, loss : 0.01871906005936496ITERATION : 85, loss : 0.01871906005936496ITERATION : 86, loss : 0.01871906005936496ITERATION : 87, loss : 0.01871906005936496ITERATION : 88, loss : 0.01871906005936496ITERATION : 89, loss : 0.01871906005936496ITERATION : 90, loss : 0.01871906005936496ITERATION : 91, loss : 0.01871906005936496ITERATION : 92, loss : 0.01871906005936496ITERATION : 93, loss : 0.01871906005936496ITERATION : 94, loss : 0.01871906005936496ITERATION : 95, loss : 0.01871906005936496ITERATION : 96, loss : 0.01871906005936496ITERATION : 97, loss : 0.01871906005936496ITERATION : 98, loss : 0.01871906005936496ITERATION : 99, loss : 0.01871906005936496ITERATION : 100, loss : 0.01871906005936496
ITERATION : 1, loss : 0.04239389485072421ITERATION : 2, loss : 0.03235877182512877ITERATION : 3, loss : 0.02846531682175964ITERATION : 4, loss : 0.02680914409782804ITERATION : 5, loss : 0.026295744141254025ITERATION : 6, loss : 0.026022997389837082ITERATION : 7, loss : 0.02586783360429593ITERATION : 8, loss : 0.025774170272506787ITERATION : 9, loss : 0.025714740685069508ITERATION : 10, loss : 0.0256754905257085ITERATION : 11, loss : 0.025648754400105854ITERATION : 12, loss : 0.025630117643038217ITERATION : 13, loss : 0.02561690652620574ITERATION : 14, loss : 0.025607428162124527ITERATION : 15, loss : 0.025600569855810634ITERATION : 16, loss : 0.025595577790415512ITERATION : 17, loss : 0.02559192919960732ITERATION : 18, loss : 0.025589255077196216ITERATION : 19, loss : 0.025587291493790396ITERATION : 20, loss : 0.025585847875483854ITERATION : 21, loss : 0.025584785735792055ITERATION : 22, loss : 0.02558400392188809ITERATION : 23, loss : 0.025583428328596024ITERATION : 24, loss : 0.02558300452477986ITERATION : 25, loss : 0.02558269250423611ITERATION : 26, loss : 0.02558246280809867ITERATION : 27, loss : 0.025582293735660528ITERATION : 28, loss : 0.025582169312693047ITERATION : 29, loss : 0.025582077774501832ITERATION : 30, loss : 0.025582010427567067ITERATION : 31, loss : 0.025581960913996813ITERATION : 32, loss : 0.025581924494097956ITERATION : 33, loss : 0.025581897734727584ITERATION : 34, loss : 0.025581878053266373ITERATION : 35, loss : 0.02558186360896233ITERATION : 36, loss : 0.02558185299500485ITERATION : 37, loss : 0.025581845193169306ITERATION : 38, loss : 0.025581839458310583ITERATION : 39, loss : 0.025581835265394655ITERATION : 40, loss : 0.02558183219721981ITERATION : 41, loss : 0.02558182992810931ITERATION : 42, loss : 0.02558182826710551ITERATION : 43, loss : 0.025581827030858106ITERATION : 44, loss : 0.02558182613607698ITERATION : 45, loss : 0.025581825465651707ITERATION : 46, loss : 0.025581824988905114ITERATION : 47, loss : 0.025581824624939093ITERATION : 48, loss : 0.02558182437050169ITERATION : 49, loss : 0.02558182416608614ITERATION : 50, loss : 0.02558182403801732ITERATION : 51, loss : 0.025581823915332372ITERATION : 52, loss : 0.02558182384977802ITERATION : 53, loss : 0.025581823796165436ITERATION : 54, loss : 0.025581823756751675ITERATION : 55, loss : 0.02558182372803059ITERATION : 56, loss : 0.025581823712621195ITERATION : 57, loss : 0.025581823691104864ITERATION : 58, loss : 0.025581823690825418ITERATION : 59, loss : 0.02558182368908863ITERATION : 60, loss : 0.02558182368908863ITERATION : 61, loss : 0.02558182368908863ITERATION : 62, loss : 0.02558182368908863ITERATION : 63, loss : 0.02558182368908863ITERATION : 64, loss : 0.02558182368908863ITERATION : 65, loss : 0.02558182368908863ITERATION : 66, loss : 0.02558182368908863ITERATION : 67, loss : 0.02558182368908863ITERATION : 68, loss : 0.02558182368908863ITERATION : 69, loss : 0.02558182368908863ITERATION : 70, loss : 0.02558182368908863ITERATION : 71, loss : 0.02558182368908863ITERATION : 72, loss : 0.02558182368908863ITERATION : 73, loss : 0.02558182368908863ITERATION : 74, loss : 0.02558182368908863ITERATION : 75, loss : 0.02558182368908863ITERATION : 76, loss : 0.02558182368908863ITERATION : 77, loss : 0.02558182368908863ITERATION : 78, loss : 0.02558182368908863ITERATION : 79, loss : 0.02558182368908863ITERATION : 80, loss : 0.02558182368908863ITERATION : 81, loss : 0.02558182368908863ITERATION : 82, loss : 0.02558182368908863ITERATION : 83, loss : 0.02558182368908863ITERATION : 84, loss : 0.02558182368908863ITERATION : 85, loss : 0.02558182368908863ITERATION : 86, loss : 0.02558182368908863ITERATION : 87, loss : 0.02558182368908863ITERATION : 88, loss : 0.02558182368908863ITERATION : 89, loss : 0.02558182368908863ITERATION : 90, loss : 0.02558182368908863ITERATION : 91, loss : 0.02558182368908863ITERATION : 92, loss : 0.02558182368908863ITERATION : 93, loss : 0.02558182368908863ITERATION : 94, loss : 0.02558182368908863ITERATION : 95, loss : 0.02558182368908863ITERATION : 96, loss : 0.02558182368908863ITERATION : 97, loss : 0.02558182368908863ITERATION : 98, loss : 0.02558182368908863ITERATION : 99, loss : 0.02558182368908863ITERATION : 100, loss : 0.02558182368908863
ITERATION : 1, loss : 0.010168106200395495ITERATION : 2, loss : 0.009887233754249934ITERATION : 3, loss : 0.009914856998804492ITERATION : 4, loss : 0.010086494770281137ITERATION : 5, loss : 0.01028994575198314ITERATION : 6, loss : 0.010472951732978858ITERATION : 7, loss : 0.0106199604638498ITERATION : 8, loss : 0.0107314407196627ITERATION : 9, loss : 0.01081327476070036ITERATION : 10, loss : 0.010872189111845008ITERATION : 11, loss : 0.01091409332709494ITERATION : 12, loss : 0.01094366985884634ITERATION : 13, loss : 0.010964441027386189ITERATION : 14, loss : 0.010978980336431876ITERATION : 15, loss : 0.010989135095695416ITERATION : 16, loss : 0.010996216957614572ITERATION : 17, loss : 0.011001150770317464ITERATION : 18, loss : 0.011004585579456733ITERATION : 19, loss : 0.011006975534228464ITERATION : 20, loss : 0.011008637845789147ITERATION : 21, loss : 0.011009793708443698ITERATION : 22, loss : 0.011010597095279486ITERATION : 23, loss : 0.011011155413707152ITERATION : 24, loss : 0.011011543321453234ITERATION : 25, loss : 0.01101181273746771ITERATION : 26, loss : 0.011011999872591214ITERATION : 27, loss : 0.011012129777435207ITERATION : 28, loss : 0.011012219928789377ITERATION : 29, loss : 0.011012282548146375ITERATION : 30, loss : 0.011012325928813235ITERATION : 31, loss : 0.011012356033341034ITERATION : 32, loss : 0.011012376923076724ITERATION : 33, loss : 0.011012391394502281ITERATION : 34, loss : 0.011012401415697575ITERATION : 35, loss : 0.011012408387674498ITERATION : 36, loss : 0.01101241320811056ITERATION : 37, loss : 0.011012416531908207ITERATION : 38, loss : 0.011012418844814038ITERATION : 39, loss : 0.011012420430347104ITERATION : 40, loss : 0.01101242153145643ITERATION : 41, loss : 0.01101242227653458ITERATION : 42, loss : 0.011012422789567414ITERATION : 43, loss : 0.011012423127727619ITERATION : 44, loss : 0.011012423380268322ITERATION : 45, loss : 0.011012423536827813ITERATION : 46, loss : 0.011012423663248222ITERATION : 47, loss : 0.01101242372379967ITERATION : 48, loss : 0.011012423779176937ITERATION : 49, loss : 0.011012423807065302ITERATION : 50, loss : 0.011012423838814168ITERATION : 51, loss : 0.011012423842385545ITERATION : 52, loss : 0.011012423858850377ITERATION : 53, loss : 0.011012423858850377ITERATION : 54, loss : 0.011012423858850377ITERATION : 55, loss : 0.011012423858850377ITERATION : 56, loss : 0.011012423858850377ITERATION : 57, loss : 0.011012423858850377ITERATION : 58, loss : 0.011012423858850377ITERATION : 59, loss : 0.011012423858850377ITERATION : 60, loss : 0.011012423858850377ITERATION : 61, loss : 0.011012423858850377ITERATION : 62, loss : 0.011012423858850377ITERATION : 63, loss : 0.011012423858850377ITERATION : 64, loss : 0.011012423858850377ITERATION : 65, loss : 0.011012423858850377ITERATION : 66, loss : 0.011012423858850377ITERATION : 67, loss : 0.011012423858850377ITERATION : 68, loss : 0.011012423858850377ITERATION : 69, loss : 0.011012423858850377ITERATION : 70, loss : 0.011012423858850377ITERATION : 71, loss : 0.011012423858850377ITERATION : 72, loss : 0.011012423858850377ITERATION : 73, loss : 0.011012423858850377ITERATION : 74, loss : 0.011012423858850377ITERATION : 75, loss : 0.011012423858850377ITERATION : 76, loss : 0.011012423858850377ITERATION : 77, loss : 0.011012423858850377ITERATION : 78, loss : 0.011012423858850377ITERATION : 79, loss : 0.011012423858850377ITERATION : 80, loss : 0.011012423858850377ITERATION : 81, loss : 0.011012423858850377ITERATION : 82, loss : 0.011012423858850377ITERATION : 83, loss : 0.011012423858850377ITERATION : 84, loss : 0.011012423858850377ITERATION : 85, loss : 0.011012423858850377ITERATION : 86, loss : 0.011012423858850377ITERATION : 87, loss : 0.011012423858850377ITERATION : 88, loss : 0.011012423858850377ITERATION : 89, loss : 0.011012423858850377ITERATION : 90, loss : 0.011012423858850377ITERATION : 91, loss : 0.011012423858850377ITERATION : 92, loss : 0.011012423858850377ITERATION : 93, loss : 0.011012423858850377ITERATION : 94, loss : 0.011012423858850377ITERATION : 95, loss : 0.011012423858850377ITERATION : 96, loss : 0.011012423858850377ITERATION : 97, loss : 0.011012423858850377ITERATION : 98, loss : 0.011012423858850377ITERATION : 99, loss : 0.011012423858850377ITERATION : 100, loss : 0.011012423858850377
ITERATION : 1, loss : 0.015115913833219629ITERATION : 2, loss : 0.013542415725581168ITERATION : 3, loss : 0.013175949870104765ITERATION : 4, loss : 0.013042258057544553ITERATION : 5, loss : 0.012963786421194626ITERATION : 6, loss : 0.012911281524163118ITERATION : 7, loss : 0.01287519951127269ITERATION : 8, loss : 0.012850049274248985ITERATION : 9, loss : 0.012832289171748463ITERATION : 10, loss : 0.012819619137689928ITERATION : 11, loss : 0.012810522516039832ITERATION : 12, loss : 0.012803971767892938ITERATION : 13, loss : 0.01279925114524024ITERATION : 14, loss : 0.012795851952095833ITERATION : 15, loss : 0.012793407840407327ITERATION : 16, loss : 0.01279165345430161ITERATION : 17, loss : 0.012790396459561077ITERATION : 18, loss : 0.012789497249591728ITERATION : 19, loss : 0.012788854997199504ITERATION : 20, loss : 0.012788396851807252ITERATION : 21, loss : 0.012788070395864016ITERATION : 22, loss : 0.01278783798887962ITERATION : 23, loss : 0.012787672676103454ITERATION : 24, loss : 0.012787555173899573ITERATION : 25, loss : 0.012787471689723238ITERATION : 26, loss : 0.012787412468074437ITERATION : 27, loss : 0.012787370431290471ITERATION : 28, loss : 0.012787340614508085ITERATION : 29, loss : 0.0127873194896608ITERATION : 30, loss : 0.012787304515826093ITERATION : 31, loss : 0.012787293879241076ITERATION : 32, loss : 0.012787286382766072ITERATION : 33, loss : 0.012787281054687998ITERATION : 34, loss : 0.012787277276450671ITERATION : 35, loss : 0.012787274592790537ITERATION : 36, loss : 0.012787272692189284ITERATION : 37, loss : 0.012787271349178583ITERATION : 38, loss : 0.012787270399883031ITERATION : 39, loss : 0.012787269724027136ITERATION : 40, loss : 0.012787269248804835ITERATION : 41, loss : 0.012787268909278436ITERATION : 42, loss : 0.012787268673542539ITERATION : 43, loss : 0.012787268509121196ITERATION : 44, loss : 0.01278726838113775ITERATION : 45, loss : 0.012787268299245199ITERATION : 46, loss : 0.012787268239211173ITERATION : 47, loss : 0.012787268191673914ITERATION : 48, loss : 0.012787268163252305ITERATION : 49, loss : 0.012787268148740314ITERATION : 50, loss : 0.012787268137791328ITERATION : 51, loss : 0.012787268130548216ITERATION : 52, loss : 0.012787268130485377ITERATION : 53, loss : 0.012787268130485377ITERATION : 54, loss : 0.012787268130485377ITERATION : 55, loss : 0.012787268130485377ITERATION : 56, loss : 0.012787268130485377ITERATION : 57, loss : 0.012787268130485377ITERATION : 58, loss : 0.012787268130485377ITERATION : 59, loss : 0.012787268130485377ITERATION : 60, loss : 0.012787268130485377ITERATION : 61, loss : 0.012787268130485377ITERATION : 62, loss : 0.012787268130485377ITERATION : 63, loss : 0.012787268130485377ITERATION : 64, loss : 0.012787268130485377ITERATION : 65, loss : 0.012787268130485377ITERATION : 66, loss : 0.012787268130485377ITERATION : 67, loss : 0.012787268130485377ITERATION : 68, loss : 0.012787268130485377ITERATION : 69, loss : 0.012787268130485377ITERATION : 70, loss : 0.012787268130485377ITERATION : 71, loss : 0.012787268130485377ITERATION : 72, loss : 0.012787268130485377ITERATION : 73, loss : 0.012787268130485377ITERATION : 74, loss : 0.012787268130485377ITERATION : 75, loss : 0.012787268130485377ITERATION : 76, loss : 0.012787268130485377ITERATION : 77, loss : 0.012787268130485377ITERATION : 78, loss : 0.012787268130485377ITERATION : 79, loss : 0.012787268130485377ITERATION : 80, loss : 0.012787268130485377ITERATION : 81, loss : 0.012787268130485377ITERATION : 82, loss : 0.012787268130485377ITERATION : 83, loss : 0.012787268130485377ITERATION : 84, loss : 0.012787268130485377ITERATION : 85, loss : 0.012787268130485377ITERATION : 86, loss : 0.012787268130485377ITERATION : 87, loss : 0.012787268130485377ITERATION : 88, loss : 0.012787268130485377ITERATION : 89, loss : 0.012787268130485377ITERATION : 90, loss : 0.012787268130485377ITERATION : 91, loss : 0.012787268130485377ITERATION : 92, loss : 0.012787268130485377ITERATION : 93, loss : 0.012787268130485377ITERATION : 94, loss : 0.012787268130485377ITERATION : 95, loss : 0.012787268130485377ITERATION : 96, loss : 0.012787268130485377ITERATION : 97, loss : 0.012787268130485377ITERATION : 98, loss : 0.012787268130485377ITERATION : 99, loss : 0.012787268130485377ITERATION : 100, loss : 0.012787268130485377
gradient norm in None layer : 0.0007366892727224793
gradient norm in None layer : 4.289578032422153e-05
gradient norm in None layer : 4.6429913492618346e-05
gradient norm in None layer : 0.0005980686625733327
gradient norm in None layer : 4.657755002749659e-05
gradient norm in None layer : 5.1202567059250124e-05
gradient norm in None layer : 0.0002415409854032535
gradient norm in None layer : 9.740162035817123e-06
gradient norm in None layer : 8.92424283938668e-06
gradient norm in None layer : 0.00019640756075457172
gradient norm in None layer : 9.034416882612722e-06
gradient norm in None layer : 7.929001922014e-06
gradient norm in None layer : 5.913604695165943e-05
gradient norm in None layer : 1.7765803999766474e-06
gradient norm in None layer : 1.3747563656407617e-06
gradient norm in None layer : 5.12784633696595e-05
gradient norm in None layer : 2.2537209858438273e-06
gradient norm in None layer : 1.7332520036483322e-06
gradient norm in None layer : 7.543529247551101e-05
gradient norm in None layer : 9.94914051908788e-07
gradient norm in None layer : 0.00017236868181525145
gradient norm in None layer : 1.2855767511586607e-05
gradient norm in None layer : 9.246780227151705e-06
gradient norm in None layer : 0.00019742172106949576
gradient norm in None layer : 2.204180585433244e-05
gradient norm in None layer : 2.1803743744121667e-05
gradient norm in None layer : 0.00029806222129803565
gradient norm in None layer : 2.7882265175288626e-06
gradient norm in None layer : 0.0005601346519138401
gradient norm in None layer : 4.5821290730522285e-05
gradient norm in None layer : 4.7366951761110225e-05
gradient norm in None layer : 0.0006730170952649137
gradient norm in None layer : 5.240142896609078e-05
gradient norm in None layer : 6.377628424477883e-05
gradient norm in None layer : 4.7415966835842864e-05
gradient norm in None layer : 7.65031622771155e-06
Total gradient norm: 0.001398987370565795
invariance loss : 4.2843647087991465, avg_den : 0.4338531494140625, density loss : 0.3338531494140625, mse loss : 0.03217520556296549, solver time : 131.60417294502258 sec , total loss : 0.036793423421178695, running loss : 0.04099457384567222
Epoch 0/10 , batch 33/12500 
ITERATION : 1, loss : 0.040229068691051466ITERATION : 2, loss : 0.020938237534517803ITERATION : 3, loss : 0.01643871496880636ITERATION : 4, loss : 0.015108177994343148ITERATION : 5, loss : 0.014597319628731538ITERATION : 6, loss : 0.014357185136164618ITERATION : 7, loss : 0.014225893687181775ITERATION : 8, loss : 0.014145593053665327ITERATION : 9, loss : 0.014092490448755643ITERATION : 10, loss : 0.014055566318131835ITERATION : 11, loss : 0.014029100900579454ITERATION : 12, loss : 0.014009792819945785ITERATION : 13, loss : 0.013995561674178656ITERATION : 14, loss : 0.01398501009947387ITERATION : 15, loss : 0.013977159328389398ITERATION : 16, loss : 0.013971305640782186ITERATION : 17, loss : 0.013966935271895633ITERATION : 18, loss : 0.013963669555358249ITERATION : 19, loss : 0.013961227917650624ITERATION : 20, loss : 0.013959401695457202ITERATION : 21, loss : 0.013958035402692885ITERATION : 22, loss : 0.013957013032525294ITERATION : 23, loss : 0.013956247866437874ITERATION : 24, loss : 0.01395567517072639ITERATION : 25, loss : 0.013955246479465763ITERATION : 26, loss : 0.013954925574764037ITERATION : 27, loss : 0.013954685342135224ITERATION : 28, loss : 0.013954505493976583ITERATION : 29, loss : 0.013954370837581808ITERATION : 30, loss : 0.01395427003344944ITERATION : 31, loss : 0.013954194551456677ITERATION : 32, loss : 0.013954138043354766ITERATION : 33, loss : 0.013954095735449987ITERATION : 34, loss : 0.013954064056467472ITERATION : 35, loss : 0.013954040346712358ITERATION : 36, loss : 0.013954022588166828ITERATION : 37, loss : 0.013954009287226966ITERATION : 38, loss : 0.013953999336011314ITERATION : 39, loss : 0.01395399187562666ITERATION : 40, loss : 0.013953986289556594ITERATION : 41, loss : 0.013953982112103563ITERATION : 42, loss : 0.01395397897222129ITERATION : 43, loss : 0.013953976636055276ITERATION : 44, loss : 0.013953974874658074ITERATION : 45, loss : 0.013953973568697134ITERATION : 46, loss : 0.013953972584703498ITERATION : 47, loss : 0.013953971857028376ITERATION : 48, loss : 0.013953971320408936ITERATION : 49, loss : 0.01395397090514283ITERATION : 50, loss : 0.013953970590007797ITERATION : 51, loss : 0.013953970350458377ITERATION : 52, loss : 0.013953970166831678ITERATION : 53, loss : 0.013953970043898017ITERATION : 54, loss : 0.013953969940524644ITERATION : 55, loss : 0.013953969875508366ITERATION : 56, loss : 0.013953969819739708ITERATION : 57, loss : 0.013953969791153676ITERATION : 58, loss : 0.013953969771513869ITERATION : 59, loss : 0.01395396975083566ITERATION : 60, loss : 0.013953969749967293ITERATION : 61, loss : 0.013953969749967293ITERATION : 62, loss : 0.013953969749967293ITERATION : 63, loss : 0.013953969749967293ITERATION : 64, loss : 0.013953969749967293ITERATION : 65, loss : 0.013953969749967293ITERATION : 66, loss : 0.013953969749967293ITERATION : 67, loss : 0.013953969749967293ITERATION : 68, loss : 0.013953969749967293ITERATION : 69, loss : 0.013953969749967293ITERATION : 70, loss : 0.013953969749967293ITERATION : 71, loss : 0.013953969749967293ITERATION : 72, loss : 0.013953969749967293ITERATION : 73, loss : 0.013953969749967293ITERATION : 74, loss : 0.013953969749967293ITERATION : 75, loss : 0.013953969749967293ITERATION : 76, loss : 0.013953969749967293ITERATION : 77, loss : 0.013953969749967293ITERATION : 78, loss : 0.013953969749967293ITERATION : 79, loss : 0.013953969749967293ITERATION : 80, loss : 0.013953969749967293ITERATION : 81, loss : 0.013953969749967293ITERATION : 82, loss : 0.013953969749967293ITERATION : 83, loss : 0.013953969749967293ITERATION : 84, loss : 0.013953969749967293ITERATION : 85, loss : 0.013953969749967293ITERATION : 86, loss : 0.013953969749967293ITERATION : 87, loss : 0.013953969749967293ITERATION : 88, loss : 0.013953969749967293ITERATION : 89, loss : 0.013953969749967293ITERATION : 90, loss : 0.013953969749967293ITERATION : 91, loss : 0.013953969749967293ITERATION : 92, loss : 0.013953969749967293ITERATION : 93, loss : 0.013953969749967293ITERATION : 94, loss : 0.013953969749967293ITERATION : 95, loss : 0.013953969749967293ITERATION : 96, loss : 0.013953969749967293ITERATION : 97, loss : 0.013953969749967293ITERATION : 98, loss : 0.013953969749967293ITERATION : 99, loss : 0.013953969749967293ITERATION : 100, loss : 0.013953969749967293
ITERATION : 1, loss : 0.044245117929585964ITERATION : 2, loss : 0.029335794046644496ITERATION : 3, loss : 0.025152281547094136ITERATION : 4, loss : 0.023632499396584957ITERATION : 5, loss : 0.02269681714090583ITERATION : 6, loss : 0.02217085463797577ITERATION : 7, loss : 0.021899537977809636ITERATION : 8, loss : 0.02175139934921004ITERATION : 9, loss : 0.021666327269531195ITERATION : 10, loss : 0.021615317115838036ITERATION : 11, loss : 0.021583645220137712ITERATION : 12, loss : 0.021563448954536355ITERATION : 13, loss : 0.02155031724141828ITERATION : 14, loss : 0.02154166097774644ITERATION : 15, loss : 0.021535900816442393ITERATION : 16, loss : 0.021532043438040407ITERATION : 17, loss : 0.02152944925967864ITERATION : 18, loss : 0.02152769964605404ITERATION : 19, loss : 0.021526517542482616ITERATION : 20, loss : 0.021525717974299446ITERATION : 21, loss : 0.02152517656558056ITERATION : 22, loss : 0.021524809860786417ITERATION : 23, loss : 0.021524561456454018ITERATION : 24, loss : 0.02152439309100411ITERATION : 25, loss : 0.02152427891235897ITERATION : 26, loss : 0.021524201588983688ITERATION : 27, loss : 0.02152414917219507ITERATION : 28, loss : 0.021524113630792898ITERATION : 29, loss : 0.021524089552986427ITERATION : 30, loss : 0.021524073225967555ITERATION : 31, loss : 0.02152406214796712ITERATION : 32, loss : 0.021524054642761626ITERATION : 33, loss : 0.021524049566842493ITERATION : 34, loss : 0.021524046121537743ITERATION : 35, loss : 0.021524043793167445ITERATION : 36, loss : 0.021524042198202746ITERATION : 37, loss : 0.021524041141180776ITERATION : 38, loss : 0.02152404040403134ITERATION : 39, loss : 0.02152403991998485ITERATION : 40, loss : 0.02152403960060895ITERATION : 41, loss : 0.021524039368143465ITERATION : 42, loss : 0.0215240392114113ITERATION : 43, loss : 0.021524039123571398ITERATION : 44, loss : 0.021524039072242797ITERATION : 45, loss : 0.021524039003044712ITERATION : 46, loss : 0.021524038987397163ITERATION : 47, loss : 0.0215240389571127ITERATION : 48, loss : 0.0215240389571127ITERATION : 49, loss : 0.0215240389571127ITERATION : 50, loss : 0.0215240389571127ITERATION : 51, loss : 0.0215240389571127ITERATION : 52, loss : 0.0215240389571127ITERATION : 53, loss : 0.0215240389571127ITERATION : 54, loss : 0.0215240389571127ITERATION : 55, loss : 0.0215240389571127ITERATION : 56, loss : 0.0215240389571127ITERATION : 57, loss : 0.0215240389571127ITERATION : 58, loss : 0.0215240389571127ITERATION : 59, loss : 0.0215240389571127ITERATION : 60, loss : 0.0215240389571127ITERATION : 61, loss : 0.0215240389571127ITERATION : 62, loss : 0.0215240389571127ITERATION : 63, loss : 0.0215240389571127ITERATION : 64, loss : 0.0215240389571127ITERATION : 65, loss : 0.0215240389571127ITERATION : 66, loss : 0.0215240389571127ITERATION : 67, loss : 0.0215240389571127ITERATION : 68, loss : 0.0215240389571127ITERATION : 69, loss : 0.0215240389571127ITERATION : 70, loss : 0.0215240389571127ITERATION : 71, loss : 0.0215240389571127ITERATION : 72, loss : 0.0215240389571127ITERATION : 73, loss : 0.0215240389571127ITERATION : 74, loss : 0.0215240389571127ITERATION : 75, loss : 0.0215240389571127ITERATION : 76, loss : 0.0215240389571127ITERATION : 77, loss : 0.0215240389571127ITERATION : 78, loss : 0.0215240389571127ITERATION : 79, loss : 0.0215240389571127ITERATION : 80, loss : 0.0215240389571127ITERATION : 81, loss : 0.0215240389571127ITERATION : 82, loss : 0.0215240389571127ITERATION : 83, loss : 0.0215240389571127ITERATION : 84, loss : 0.0215240389571127ITERATION : 85, loss : 0.0215240389571127ITERATION : 86, loss : 0.0215240389571127ITERATION : 87, loss : 0.0215240389571127ITERATION : 88, loss : 0.0215240389571127ITERATION : 89, loss : 0.0215240389571127ITERATION : 90, loss : 0.0215240389571127ITERATION : 91, loss : 0.0215240389571127ITERATION : 92, loss : 0.0215240389571127ITERATION : 93, loss : 0.0215240389571127ITERATION : 94, loss : 0.0215240389571127ITERATION : 95, loss : 0.0215240389571127ITERATION : 96, loss : 0.0215240389571127ITERATION : 97, loss : 0.0215240389571127ITERATION : 98, loss : 0.0215240389571127ITERATION : 99, loss : 0.0215240389571127ITERATION : 100, loss : 0.0215240389571127
ITERATION : 1, loss : 0.030031394236811292ITERATION : 2, loss : 0.0271048970582183ITERATION : 3, loss : 0.028718540933832307ITERATION : 4, loss : 0.030247277707685902ITERATION : 5, loss : 0.028899478682414013ITERATION : 6, loss : 0.028023514642007488ITERATION : 7, loss : 0.0274353153970429ITERATION : 8, loss : 0.02703096273593105ITERATION : 9, loss : 0.026748048464197243ITERATION : 10, loss : 0.026547438555195705ITERATION : 11, loss : 0.026403746167603725ITERATION : 12, loss : 0.026300039076773935ITERATION : 13, loss : 0.026224765200212417ITERATION : 14, loss : 0.02616989805636123ITERATION : 15, loss : 0.02612978004228311ITERATION : 16, loss : 0.026100378317517322ITERATION : 17, loss : 0.026078793440980644ITERATION : 18, loss : 0.026062927147115042ITERATION : 19, loss : 0.026051253548929138ITERATION : 20, loss : 0.02604265878635525ITERATION : 21, loss : 0.02603632764404293ITERATION : 22, loss : 0.026031662183879388ITERATION : 23, loss : 0.026028223294875127ITERATION : 24, loss : 0.02602568798550589ITERATION : 25, loss : 0.02602381853245622ITERATION : 26, loss : 0.02602243993057277ITERATION : 27, loss : 0.026021423243654182ITERATION : 28, loss : 0.026020673383272394ITERATION : 29, loss : 0.026020120317466294ITERATION : 30, loss : 0.026019712325124306ITERATION : 31, loss : 0.026019411437893117ITERATION : 32, loss : 0.026019189424916784ITERATION : 33, loss : 0.026019025708548582ITERATION : 34, loss : 0.02601890490981741ITERATION : 35, loss : 0.026018815846562494ITERATION : 36, loss : 0.026018750110710153ITERATION : 37, loss : 0.02601870165093932ITERATION : 38, loss : 0.02601866588797034ITERATION : 39, loss : 0.02601863954877599ITERATION : 40, loss : 0.02601862009710786ITERATION : 41, loss : 0.026018605775430026ITERATION : 42, loss : 0.0260185951799488ITERATION : 43, loss : 0.026018587391770227ITERATION : 44, loss : 0.026018581592622044ITERATION : 45, loss : 0.026018577371150122ITERATION : 46, loss : 0.02601857422562135ITERATION : 47, loss : 0.0260185719134102ITERATION : 48, loss : 0.02601857020012161ITERATION : 49, loss : 0.026018568989528652ITERATION : 50, loss : 0.026018568041854134ITERATION : 51, loss : 0.026018567413294752ITERATION : 52, loss : 0.026018566871928196ITERATION : 53, loss : 0.026018566511157533ITERATION : 54, loss : 0.026018566249168113ITERATION : 55, loss : 0.02601856601511434ITERATION : 56, loss : 0.026018565869768154ITERATION : 57, loss : 0.02601856579342819ITERATION : 58, loss : 0.02601856570201399ITERATION : 59, loss : 0.026018565650837794ITERATION : 60, loss : 0.026018565581982612ITERATION : 61, loss : 0.026018565589397115ITERATION : 62, loss : 0.02601856558564815ITERATION : 63, loss : 0.026018565585202325ITERATION : 64, loss : 0.026018565585202325ITERATION : 65, loss : 0.026018565585202325ITERATION : 66, loss : 0.026018565585202325ITERATION : 67, loss : 0.026018565585202325ITERATION : 68, loss : 0.026018565585202325ITERATION : 69, loss : 0.026018565585202325ITERATION : 70, loss : 0.026018565585202325ITERATION : 71, loss : 0.026018565585202325ITERATION : 72, loss : 0.026018565585202325ITERATION : 73, loss : 0.026018565585202325ITERATION : 74, loss : 0.026018565585202325ITERATION : 75, loss : 0.026018565585202325ITERATION : 76, loss : 0.026018565585202325ITERATION : 77, loss : 0.026018565585202325ITERATION : 78, loss : 0.026018565585202325ITERATION : 79, loss : 0.026018565585202325ITERATION : 80, loss : 0.026018565585202325ITERATION : 81, loss : 0.026018565585202325ITERATION : 82, loss : 0.026018565585202325ITERATION : 83, loss : 0.026018565585202325ITERATION : 84, loss : 0.026018565585202325ITERATION : 85, loss : 0.026018565585202325ITERATION : 86, loss : 0.026018565585202325ITERATION : 87, loss : 0.026018565585202325ITERATION : 88, loss : 0.026018565585202325ITERATION : 89, loss : 0.026018565585202325ITERATION : 90, loss : 0.026018565585202325ITERATION : 91, loss : 0.026018565585202325ITERATION : 92, loss : 0.026018565585202325ITERATION : 93, loss : 0.026018565585202325ITERATION : 94, loss : 0.026018565585202325ITERATION : 95, loss : 0.026018565585202325ITERATION : 96, loss : 0.026018565585202325ITERATION : 97, loss : 0.026018565585202325ITERATION : 98, loss : 0.026018565585202325ITERATION : 99, loss : 0.026018565585202325ITERATION : 100, loss : 0.026018565585202325
ITERATION : 1, loss : 0.019434997060332776ITERATION : 2, loss : 0.019911682098813752ITERATION : 3, loss : 0.017218875707098128ITERATION : 4, loss : 0.015796938851216906ITERATION : 5, loss : 0.015000470363516932ITERATION : 6, loss : 0.014520589642147069ITERATION : 7, loss : 0.014216898847885996ITERATION : 8, loss : 0.014018104138846743ITERATION : 9, loss : 0.013884941885673298ITERATION : 10, loss : 0.013794338964426348ITERATION : 11, loss : 0.013732034361456641ITERATION : 12, loss : 0.013688876076196189ITERATION : 13, loss : 0.01365882921522433ITERATION : 14, loss : 0.013637836501304462ITERATION : 15, loss : 0.01362313315535623ITERATION : 16, loss : 0.013612816508578218ITERATION : 17, loss : 0.01360556880228482ITERATION : 18, loss : 0.013600472490886686ITERATION : 19, loss : 0.013596886566779263ITERATION : 20, loss : 0.013594362140141753ITERATION : 21, loss : 0.013592584618899513ITERATION : 22, loss : 0.013591332614082107ITERATION : 23, loss : 0.013590450471951027ITERATION : 24, loss : 0.01358982897231487ITERATION : 25, loss : 0.013589391035330137ITERATION : 26, loss : 0.01358908243053085ITERATION : 27, loss : 0.013588865004799609ITERATION : 28, loss : 0.013588711773952293ITERATION : 29, loss : 0.013588603752707495ITERATION : 30, loss : 0.013588527653444718ITERATION : 31, loss : 0.013588473994918875ITERATION : 32, loss : 0.013588436229785717ITERATION : 33, loss : 0.013588409594724201ITERATION : 34, loss : 0.013588390816053864ITERATION : 35, loss : 0.013588377534931402ITERATION : 36, loss : 0.013588368190288847ITERATION : 37, loss : 0.01358836157979752ITERATION : 38, loss : 0.01358835691882719ITERATION : 39, loss : 0.013588353644952054ITERATION : 40, loss : 0.013588351324729867ITERATION : 41, loss : 0.013588349742544656ITERATION : 42, loss : 0.013588348574660758ITERATION : 43, loss : 0.01358834777555349ITERATION : 44, loss : 0.013588347178490045ITERATION : 45, loss : 0.013588346816726181ITERATION : 46, loss : 0.013588346520777105ITERATION : 47, loss : 0.013588346326630049ITERATION : 48, loss : 0.013588346220777463ITERATION : 49, loss : 0.013588346139850955ITERATION : 50, loss : 0.013588346091463747ITERATION : 51, loss : 0.013588346027143766ITERATION : 52, loss : 0.013588346022558895ITERATION : 53, loss : 0.013588346022558895ITERATION : 54, loss : 0.013588346022558895ITERATION : 55, loss : 0.013588346022558895ITERATION : 56, loss : 0.013588346022558895ITERATION : 57, loss : 0.013588346022558895ITERATION : 58, loss : 0.013588346022558895ITERATION : 59, loss : 0.013588346022558895ITERATION : 60, loss : 0.013588346022558895ITERATION : 61, loss : 0.013588346022558895ITERATION : 62, loss : 0.013588346022558895ITERATION : 63, loss : 0.013588346022558895ITERATION : 64, loss : 0.013588346022558895ITERATION : 65, loss : 0.013588346022558895ITERATION : 66, loss : 0.013588346022558895ITERATION : 67, loss : 0.013588346022558895ITERATION : 68, loss : 0.013588346022558895ITERATION : 69, loss : 0.013588346022558895ITERATION : 70, loss : 0.013588346022558895ITERATION : 71, loss : 0.013588346022558895ITERATION : 72, loss : 0.013588346022558895ITERATION : 73, loss : 0.013588346022558895ITERATION : 74, loss : 0.013588346022558895ITERATION : 75, loss : 0.013588346022558895ITERATION : 76, loss : 0.013588346022558895ITERATION : 77, loss : 0.013588346022558895ITERATION : 78, loss : 0.013588346022558895ITERATION : 79, loss : 0.013588346022558895ITERATION : 80, loss : 0.013588346022558895ITERATION : 81, loss : 0.013588346022558895ITERATION : 82, loss : 0.013588346022558895ITERATION : 83, loss : 0.013588346022558895ITERATION : 84, loss : 0.013588346022558895ITERATION : 85, loss : 0.013588346022558895ITERATION : 86, loss : 0.013588346022558895ITERATION : 87, loss : 0.013588346022558895ITERATION : 88, loss : 0.013588346022558895ITERATION : 89, loss : 0.013588346022558895ITERATION : 90, loss : 0.013588346022558895ITERATION : 91, loss : 0.013588346022558895ITERATION : 92, loss : 0.013588346022558895ITERATION : 93, loss : 0.013588346022558895ITERATION : 94, loss : 0.013588346022558895ITERATION : 95, loss : 0.013588346022558895ITERATION : 96, loss : 0.013588346022558895ITERATION : 97, loss : 0.013588346022558895ITERATION : 98, loss : 0.013588346022558895ITERATION : 99, loss : 0.013588346022558895ITERATION : 100, loss : 0.013588346022558895
ITERATION : 1, loss : 0.03742745885891329ITERATION : 2, loss : 0.04187950139571622ITERATION : 3, loss : 0.04372236554112657ITERATION : 4, loss : 0.04417484115014169ITERATION : 5, loss : 0.04419243582195281ITERATION : 6, loss : 0.04424785173044193ITERATION : 7, loss : 0.04430072028918529ITERATION : 8, loss : 0.044343004685915066ITERATION : 9, loss : 0.04437489434795418ITERATION : 10, loss : 0.04439836594385964ITERATION : 11, loss : 0.044415447994794854ITERATION : 12, loss : 0.04442781029970177ITERATION : 13, loss : 0.04443672998077383ITERATION : 14, loss : 0.04444315474086096ITERATION : 15, loss : 0.04444777694817569ITERATION : 16, loss : 0.044451099633585615ITERATION : 17, loss : 0.044453486497619224ITERATION : 18, loss : 0.044455200150187175ITERATION : 19, loss : 0.04445642983542687ITERATION : 20, loss : 0.04445731155870534ITERATION : 21, loss : 0.044457943793979805ITERATION : 22, loss : 0.04445839682458475ITERATION : 23, loss : 0.04445872127753731ITERATION : 24, loss : 0.04445895339116081ITERATION : 25, loss : 0.04445911951143037ITERATION : 26, loss : 0.04445923850840642ITERATION : 27, loss : 0.04445932357373484ITERATION : 28, loss : 0.04445938439893356ITERATION : 29, loss : 0.044459427697628215ITERATION : 30, loss : 0.04445945866273313ITERATION : 31, loss : 0.044459480911360906ITERATION : 32, loss : 0.044459496732457ITERATION : 33, loss : 0.04445950810145032ITERATION : 34, loss : 0.04445951621212126ITERATION : 35, loss : 0.04445952207072789ITERATION : 36, loss : 0.04445952614677017ITERATION : 37, loss : 0.044459529107098175ITERATION : 38, loss : 0.04445953115176318ITERATION : 39, loss : 0.04445953266394565ITERATION : 40, loss : 0.044459533735977275ITERATION : 41, loss : 0.04445953440950167ITERATION : 42, loss : 0.044459534935360756ITERATION : 43, loss : 0.044459535233709255ITERATION : 44, loss : 0.0444595355657385ITERATION : 45, loss : 0.044459535746453296ITERATION : 46, loss : 0.04445953592041263ITERATION : 47, loss : 0.044459535971522086ITERATION : 48, loss : 0.044459535971522086ITERATION : 49, loss : 0.044459535971522086ITERATION : 50, loss : 0.044459535971522086ITERATION : 51, loss : 0.044459535971522086ITERATION : 52, loss : 0.044459535971522086ITERATION : 53, loss : 0.044459535971522086ITERATION : 54, loss : 0.044459535971522086ITERATION : 55, loss : 0.044459535971522086ITERATION : 56, loss : 0.044459535971522086ITERATION : 57, loss : 0.044459535971522086ITERATION : 58, loss : 0.044459535971522086ITERATION : 59, loss : 0.044459535971522086ITERATION : 60, loss : 0.044459535971522086ITERATION : 61, loss : 0.044459535971522086ITERATION : 62, loss : 0.044459535971522086ITERATION : 63, loss : 0.044459535971522086ITERATION : 64, loss : 0.044459535971522086ITERATION : 65, loss : 0.044459535971522086ITERATION : 66, loss : 0.044459535971522086ITERATION : 67, loss : 0.044459535971522086ITERATION : 68, loss : 0.044459535971522086ITERATION : 69, loss : 0.044459535971522086ITERATION : 70, loss : 0.044459535971522086ITERATION : 71, loss : 0.044459535971522086ITERATION : 72, loss : 0.044459535971522086ITERATION : 73, loss : 0.044459535971522086ITERATION : 74, loss : 0.044459535971522086ITERATION : 75, loss : 0.044459535971522086ITERATION : 76, loss : 0.044459535971522086ITERATION : 77, loss : 0.044459535971522086ITERATION : 78, loss : 0.044459535971522086ITERATION : 79, loss : 0.044459535971522086ITERATION : 80, loss : 0.044459535971522086ITERATION : 81, loss : 0.044459535971522086ITERATION : 82, loss : 0.044459535971522086ITERATION : 83, loss : 0.044459535971522086ITERATION : 84, loss : 0.044459535971522086ITERATION : 85, loss : 0.044459535971522086ITERATION : 86, loss : 0.044459535971522086ITERATION : 87, loss : 0.044459535971522086ITERATION : 88, loss : 0.044459535971522086ITERATION : 89, loss : 0.044459535971522086ITERATION : 90, loss : 0.044459535971522086ITERATION : 91, loss : 0.044459535971522086ITERATION : 92, loss : 0.044459535971522086ITERATION : 93, loss : 0.044459535971522086ITERATION : 94, loss : 0.044459535971522086ITERATION : 95, loss : 0.044459535971522086ITERATION : 96, loss : 0.044459535971522086ITERATION : 97, loss : 0.044459535971522086ITERATION : 98, loss : 0.044459535971522086ITERATION : 99, loss : 0.044459535971522086ITERATION : 100, loss : 0.044459535971522086
ITERATION : 1, loss : 0.03992029476734666ITERATION : 2, loss : 0.03479248039631397ITERATION : 3, loss : 0.033716374377108456ITERATION : 4, loss : 0.03391437489051804ITERATION : 5, loss : 0.032945848125185515ITERATION : 6, loss : 0.03226646753744273ITERATION : 7, loss : 0.03182074600059757ITERATION : 8, loss : 0.03151845754505349ITERATION : 9, loss : 0.031309236955924434ITERATION : 10, loss : 0.03116263674060537ITERATION : 11, loss : 0.031059145081191536ITERATION : 12, loss : 0.030985751720891016ITERATION : 13, loss : 0.0309335565251747ITERATION : 14, loss : 0.030896371672426926ITERATION : 15, loss : 0.030869851606583718ITERATION : 16, loss : 0.030850924527135396ITERATION : 17, loss : 0.03083741085882438ITERATION : 18, loss : 0.030827760007966998ITERATION : 19, loss : 0.030820866849204157ITERATION : 20, loss : 0.030815943088095116ITERATION : 21, loss : 0.030812426080984483ITERATION : 22, loss : 0.030809914008540204ITERATION : 23, loss : 0.030808119768882664ITERATION : 24, loss : 0.030806838382624723ITERATION : 25, loss : 0.030805923332429003ITERATION : 26, loss : 0.03080526990126015ITERATION : 27, loss : 0.03080480335271943ITERATION : 28, loss : 0.03080447023635818ITERATION : 29, loss : 0.030804232499220558ITERATION : 30, loss : 0.030804062713776465ITERATION : 31, loss : 0.030803941574140466ITERATION : 32, loss : 0.030803855099070642ITERATION : 33, loss : 0.03080379337386198ITERATION : 34, loss : 0.030803749332803904ITERATION : 35, loss : 0.030803717962306246ITERATION : 36, loss : 0.03080369559488412ITERATION : 37, loss : 0.030803679555723346ITERATION : 38, loss : 0.03080366818042937ITERATION : 39, loss : 0.03080365998405721ITERATION : 40, loss : 0.03080365422068977ITERATION : 41, loss : 0.030803650025408174ITERATION : 42, loss : 0.03080364708656418ITERATION : 43, loss : 0.030803644964673842ITERATION : 44, loss : 0.030803643567864653ITERATION : 45, loss : 0.030803642459180696ITERATION : 46, loss : 0.03080364171244303ITERATION : 47, loss : 0.030803641084868244ITERATION : 48, loss : 0.030803640720577858ITERATION : 49, loss : 0.03080364038844425ITERATION : 50, loss : 0.03080364029532729ITERATION : 51, loss : 0.030803640127435868ITERATION : 52, loss : 0.030803640128332282ITERATION : 53, loss : 0.030803640128332282ITERATION : 54, loss : 0.030803640128332282ITERATION : 55, loss : 0.030803640128332282ITERATION : 56, loss : 0.030803640128332282ITERATION : 57, loss : 0.030803640128332282ITERATION : 58, loss : 0.030803640128332282ITERATION : 59, loss : 0.030803640128332282ITERATION : 60, loss : 0.030803640128332282ITERATION : 61, loss : 0.030803640128332282ITERATION : 62, loss : 0.030803640128332282ITERATION : 63, loss : 0.030803640128332282ITERATION : 64, loss : 0.030803640128332282ITERATION : 65, loss : 0.030803640128332282ITERATION : 66, loss : 0.030803640128332282ITERATION : 67, loss : 0.030803640128332282ITERATION : 68, loss : 0.030803640128332282ITERATION : 69, loss : 0.030803640128332282ITERATION : 70, loss : 0.030803640128332282ITERATION : 71, loss : 0.030803640128332282ITERATION : 72, loss : 0.030803640128332282ITERATION : 73, loss : 0.030803640128332282ITERATION : 74, loss : 0.030803640128332282ITERATION : 75, loss : 0.030803640128332282ITERATION : 76, loss : 0.030803640128332282ITERATION : 77, loss : 0.030803640128332282ITERATION : 78, loss : 0.030803640128332282ITERATION : 79, loss : 0.030803640128332282ITERATION : 80, loss : 0.030803640128332282ITERATION : 81, loss : 0.030803640128332282ITERATION : 82, loss : 0.030803640128332282ITERATION : 83, loss : 0.030803640128332282ITERATION : 84, loss : 0.030803640128332282ITERATION : 85, loss : 0.030803640128332282ITERATION : 86, loss : 0.030803640128332282ITERATION : 87, loss : 0.030803640128332282ITERATION : 88, loss : 0.030803640128332282ITERATION : 89, loss : 0.030803640128332282ITERATION : 90, loss : 0.030803640128332282ITERATION : 91, loss : 0.030803640128332282ITERATION : 92, loss : 0.030803640128332282ITERATION : 93, loss : 0.030803640128332282ITERATION : 94, loss : 0.030803640128332282ITERATION : 95, loss : 0.030803640128332282ITERATION : 96, loss : 0.030803640128332282ITERATION : 97, loss : 0.030803640128332282ITERATION : 98, loss : 0.030803640128332282ITERATION : 99, loss : 0.030803640128332282ITERATION : 100, loss : 0.030803640128332282
ITERATION : 1, loss : 0.013201645555401731ITERATION : 2, loss : 0.01340390590136262ITERATION : 3, loss : 0.014792606655022394ITERATION : 4, loss : 0.014697157707619247ITERATION : 5, loss : 0.01489732845050424ITERATION : 6, loss : 0.015137126414782267ITERATION : 7, loss : 0.01534140009736999ITERATION : 8, loss : 0.01549712793919531ITERATION : 9, loss : 0.015610301849651337ITERATION : 10, loss : 0.015690736376222047ITERATION : 11, loss : 0.0157473155123219ITERATION : 12, loss : 0.015786941148887634ITERATION : 13, loss : 0.01581465644624287ITERATION : 14, loss : 0.015834044749664107ITERATION : 15, loss : 0.015847620417112032ITERATION : 16, loss : 0.015857137739418957ITERATION : 17, loss : 0.015863818818451482ITERATION : 18, loss : 0.01586851519482515ITERATION : 19, loss : 0.015871820736717168ITERATION : 20, loss : 0.015874150103284867ITERATION : 21, loss : 0.015875793489135762ITERATION : 22, loss : 0.01587695412667402ITERATION : 23, loss : 0.015877774695407763ITERATION : 24, loss : 0.01587835540036424ITERATION : 25, loss : 0.01587876668802047ITERATION : 26, loss : 0.015879058186790338ITERATION : 27, loss : 0.015879265045002693ITERATION : 28, loss : 0.015879411963948736ITERATION : 29, loss : 0.015879516358929008ITERATION : 30, loss : 0.0158795905905836ITERATION : 31, loss : 0.015879643404849143ITERATION : 32, loss : 0.01587968099380884ITERATION : 33, loss : 0.015879707822859076ITERATION : 34, loss : 0.01587972691803739ITERATION : 35, loss : 0.015879740550121882ITERATION : 36, loss : 0.015879750286039992ITERATION : 37, loss : 0.015879757236908872ITERATION : 38, loss : 0.015879762151386277ITERATION : 39, loss : 0.01587976569317864ITERATION : 40, loss : 0.015879768193090883ITERATION : 41, loss : 0.01587977000487651ITERATION : 42, loss : 0.015879771289457546ITERATION : 43, loss : 0.015879772223872286ITERATION : 44, loss : 0.015879772850557673ITERATION : 45, loss : 0.015879773324861873ITERATION : 46, loss : 0.0158797736529228ITERATION : 47, loss : 0.015879773821532633ITERATION : 48, loss : 0.015879773968296703ITERATION : 49, loss : 0.015879774105082685ITERATION : 50, loss : 0.015879774207311553ITERATION : 51, loss : 0.015879774261253064ITERATION : 52, loss : 0.015879774314341944ITERATION : 53, loss : 0.01587977431341315ITERATION : 54, loss : 0.01587977431341315ITERATION : 55, loss : 0.01587977431341315ITERATION : 56, loss : 0.01587977431341315ITERATION : 57, loss : 0.01587977431341315ITERATION : 58, loss : 0.01587977431341315ITERATION : 59, loss : 0.01587977431341315ITERATION : 60, loss : 0.01587977431341315ITERATION : 61, loss : 0.01587977431341315ITERATION : 62, loss : 0.01587977431341315ITERATION : 63, loss : 0.01587977431341315ITERATION : 64, loss : 0.01587977431341315ITERATION : 65, loss : 0.01587977431341315ITERATION : 66, loss : 0.01587977431341315ITERATION : 67, loss : 0.01587977431341315ITERATION : 68, loss : 0.01587977431341315ITERATION : 69, loss : 0.01587977431341315ITERATION : 70, loss : 0.01587977431341315ITERATION : 71, loss : 0.01587977431341315ITERATION : 72, loss : 0.01587977431341315ITERATION : 73, loss : 0.01587977431341315ITERATION : 74, loss : 0.01587977431341315ITERATION : 75, loss : 0.01587977431341315ITERATION : 76, loss : 0.01587977431341315ITERATION : 77, loss : 0.01587977431341315ITERATION : 78, loss : 0.01587977431341315ITERATION : 79, loss : 0.01587977431341315ITERATION : 80, loss : 0.01587977431341315ITERATION : 81, loss : 0.01587977431341315ITERATION : 82, loss : 0.01587977431341315ITERATION : 83, loss : 0.01587977431341315ITERATION : 84, loss : 0.01587977431341315ITERATION : 85, loss : 0.01587977431341315ITERATION : 86, loss : 0.01587977431341315ITERATION : 87, loss : 0.01587977431341315ITERATION : 88, loss : 0.01587977431341315ITERATION : 89, loss : 0.01587977431341315ITERATION : 90, loss : 0.01587977431341315ITERATION : 91, loss : 0.01587977431341315ITERATION : 92, loss : 0.01587977431341315ITERATION : 93, loss : 0.01587977431341315ITERATION : 94, loss : 0.01587977431341315ITERATION : 95, loss : 0.01587977431341315ITERATION : 96, loss : 0.01587977431341315ITERATION : 97, loss : 0.01587977431341315ITERATION : 98, loss : 0.01587977431341315ITERATION : 99, loss : 0.01587977431341315ITERATION : 100, loss : 0.01587977431341315
ITERATION : 1, loss : 0.05698339516425343ITERATION : 2, loss : 0.0389712618694488ITERATION : 3, loss : 0.036255172292055673ITERATION : 4, loss : 0.03517713898192347ITERATION : 5, loss : 0.034651559742371316ITERATION : 6, loss : 0.034355870465053985ITERATION : 7, loss : 0.03417296217809502ITERATION : 8, loss : 0.03405287655153298ITERATION : 9, loss : 0.03397111763107006ITERATION : 10, loss : 0.03391420102552421ITERATION : 11, loss : 0.03387402184834446ITERATION : 12, loss : 0.03384539983941627ITERATION : 13, loss : 0.03382488568856588ITERATION : 14, loss : 0.033810119863202065ITERATION : 15, loss : 0.033799459337851244ITERATION : 16, loss : 0.03379174538404673ITERATION : 17, loss : 0.03378615466932535ITERATION : 18, loss : 0.03378209786675499ITERATION : 19, loss : 0.03377915158361001ITERATION : 20, loss : 0.033777010384761934ITERATION : 21, loss : 0.03377545358871705ITERATION : 22, loss : 0.0337743213306775ITERATION : 23, loss : 0.03377349756764421ITERATION : 24, loss : 0.033772898309349514ITERATION : 25, loss : 0.033772462260839534ITERATION : 26, loss : 0.03377214496600766ITERATION : 27, loss : 0.03377191406339926ITERATION : 28, loss : 0.03377174608128268ITERATION : 29, loss : 0.03377162382680931ITERATION : 30, loss : 0.0337715349609926ITERATION : 31, loss : 0.03377147030342059ITERATION : 32, loss : 0.033771423167033125ITERATION : 33, loss : 0.03377138892013217ITERATION : 34, loss : 0.033771364102768285ITERATION : 35, loss : 0.033771346043660916ITERATION : 36, loss : 0.03377133286242978ITERATION : 37, loss : 0.03377132330924434ITERATION : 38, loss : 0.03377131633609644ITERATION : 39, loss : 0.03377131129243061ITERATION : 40, loss : 0.03377130765410151ITERATION : 41, loss : 0.033771305027734344ITERATION : 42, loss : 0.033771303117818965ITERATION : 43, loss : 0.03377130175106188ITERATION : 44, loss : 0.03377130076296755ITERATION : 45, loss : 0.033771300058745526ITERATION : 46, loss : 0.03377129952953767ITERATION : 47, loss : 0.033771299151928995ITERATION : 48, loss : 0.03377129889829118ITERATION : 49, loss : 0.03377129870156822ITERATION : 50, loss : 0.03377129854619372ITERATION : 51, loss : 0.03377129847324881ITERATION : 52, loss : 0.03377129839511761ITERATION : 53, loss : 0.03377129832312143ITERATION : 54, loss : 0.03377129832112531ITERATION : 55, loss : 0.03377129832112531ITERATION : 56, loss : 0.03377129832112531ITERATION : 57, loss : 0.03377129832112531ITERATION : 58, loss : 0.03377129832112531ITERATION : 59, loss : 0.03377129832112531ITERATION : 60, loss : 0.03377129832112531ITERATION : 61, loss : 0.03377129832112531ITERATION : 62, loss : 0.03377129832112531ITERATION : 63, loss : 0.03377129832112531ITERATION : 64, loss : 0.03377129832112531ITERATION : 65, loss : 0.03377129832112531ITERATION : 66, loss : 0.03377129832112531ITERATION : 67, loss : 0.03377129832112531ITERATION : 68, loss : 0.03377129832112531ITERATION : 69, loss : 0.03377129832112531ITERATION : 70, loss : 0.03377129832112531ITERATION : 71, loss : 0.03377129832112531ITERATION : 72, loss : 0.03377129832112531ITERATION : 73, loss : 0.03377129832112531ITERATION : 74, loss : 0.03377129832112531ITERATION : 75, loss : 0.03377129832112531ITERATION : 76, loss : 0.03377129832112531ITERATION : 77, loss : 0.03377129832112531ITERATION : 78, loss : 0.03377129832112531ITERATION : 79, loss : 0.03377129832112531ITERATION : 80, loss : 0.03377129832112531ITERATION : 81, loss : 0.03377129832112531ITERATION : 82, loss : 0.03377129832112531ITERATION : 83, loss : 0.03377129832112531ITERATION : 84, loss : 0.03377129832112531ITERATION : 85, loss : 0.03377129832112531ITERATION : 86, loss : 0.03377129832112531ITERATION : 87, loss : 0.03377129832112531ITERATION : 88, loss : 0.03377129832112531ITERATION : 89, loss : 0.03377129832112531ITERATION : 90, loss : 0.03377129832112531ITERATION : 91, loss : 0.03377129832112531ITERATION : 92, loss : 0.03377129832112531ITERATION : 93, loss : 0.03377129832112531ITERATION : 94, loss : 0.03377129832112531ITERATION : 95, loss : 0.03377129832112531ITERATION : 96, loss : 0.03377129832112531ITERATION : 97, loss : 0.03377129832112531ITERATION : 98, loss : 0.03377129832112531ITERATION : 99, loss : 0.03377129832112531ITERATION : 100, loss : 0.03377129832112531
gradient norm in None layer : 0.0010139897310478246
gradient norm in None layer : 6.263088956961448e-05
gradient norm in None layer : 6.0748644751826644e-05
gradient norm in None layer : 0.0008470038046487395
gradient norm in None layer : 7.088635420887868e-05
gradient norm in None layer : 7.146659027430331e-05
gradient norm in None layer : 0.00035804351499489173
gradient norm in None layer : 1.3646491605520302e-05
gradient norm in None layer : 1.5284862153673152e-05
gradient norm in None layer : 0.00030998960084079497
gradient norm in None layer : 1.2355749470737066e-05
gradient norm in None layer : 1.1280378846993016e-05
gradient norm in None layer : 0.00010872494385110786
gradient norm in None layer : 3.1372571874988772e-06
gradient norm in None layer : 2.265474672311625e-06
gradient norm in None layer : 8.317268864215435e-05
gradient norm in None layer : 3.2122337875996135e-06
gradient norm in None layer : 2.36222358415964e-06
gradient norm in None layer : 0.00010515722600970293
gradient norm in None layer : 1.6346979186024393e-06
gradient norm in None layer : 0.00026043505505109167
gradient norm in None layer : 1.922494636537078e-05
gradient norm in None layer : 1.3826397137469138e-05
gradient norm in None layer : 0.0002678456619591915
gradient norm in None layer : 2.7902026860247636e-05
gradient norm in None layer : 2.586878594808413e-05
gradient norm in None layer : 0.0003795398864089605
gradient norm in None layer : 2.522096420203121e-06
gradient norm in None layer : 0.0007820430839661714
gradient norm in None layer : 6.049447745965616e-05
gradient norm in None layer : 6.214457387408548e-05
gradient norm in None layer : 0.0009088772345705951
gradient norm in None layer : 7.788711257661438e-05
gradient norm in None layer : 8.601304773889437e-05
gradient norm in None layer : 6.604230218225202e-05
gradient norm in None layer : 1.1021393185607758e-05
Total gradient norm: 0.0019408519784772245
invariance loss : 4.44338255610639, avg_den : 0.43418121337890625, density loss : 0.33418121337890627, mse loss : 0.024999896131154255, solver time : 112.5916006565094 sec , total loss : 0.02977745990063955, running loss : 0.04065466130188335
Epoch 0/10 , batch 34/12500 
ITERATION : 1, loss : 0.04262791395435782ITERATION : 2, loss : 0.036231142664454465