CONFIG : 
{'EPOCHS': 10, 'RESUME_CHECKPOINT': None, 'SAVE_EVERY': 10, 'VAL_EVERY': 10, 'OUTPUT_DIR': 'unet-imgnet', 'EXP_NAME': 'test6', 'TRAIN_FILENAME': 'iids_train.txt', 'TEST_FILENAME': 'iids_test.txt', 'ROOT_DIR': 'dataset\\imagenet\\images\\', 'IMG_SIZE': 128, 'INP_CHANNELS': 1, 'OUT_CHANNELS': 1, 'LR': 0.001, 'WEIGHT_DECAY': 0.0, 'MOMENTUM': 0.9, 'OPT': 'Adam', 'SCHEDL': 'lambdaLR', 'TRAIN_BATCH': 8, 'TEST_BATCH': 8, 'ALPHA': 0.001, 'MASK_DEN': 0.1, 'BIN_METH': 'QUANT', 'OFFSET': None, 'TAU': None, 'ITERATIONS': None, 'NOTE': 'scaling prob to density ; alpha removed from dens ; loss(binaried input again)'}

train test dataset loaded
train size : 100000
test  size  : 1000
model loaded
model summary
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
??DoubleConv: 1-1                        --
|    ??Sequential: 2-1                   --
|    |    ??Conv2d: 3-1                  576
|    |    ??BatchNorm2d: 3-2             128
|    |    ??ReLU: 3-3                    --
|    |    ??Conv2d: 3-4                  36,864
|    |    ??BatchNorm2d: 3-5             128
|    |    ??ReLU: 3-6                    --
??Down: 1-2                              --
|    ??Sequential: 2-2                   --
|    |    ??MaxPool2d: 3-7               --
|    |    ??DoubleConv: 3-8              221,696
??Down: 1-3                              --
|    ??Sequential: 2-3                   --
|    |    ??MaxPool2d: 3-9               --
|    |    ??DoubleConv: 3-10             885,760
??Up: 1-4                                --
|    ??ConvTranspose2d: 2-4              131,200
|    ??DoubleConv: 2-5                   --
|    |    ??Sequential: 3-11             442,880
??Up: 1-5                                --
|    ??ConvTranspose2d: 2-6              32,832
|    ??DoubleConv: 2-7                   --
|    |    ??Sequential: 3-12             110,848
??OutConv: 1-6                           --
|    ??Conv2d: 2-8                       65
=================================================================
Total params: 1,862,977
Trainable params: 1,862,977
Non-trainable params: 0
=================================================================
device : cuda
trainer configurations set
train and test dataloaders created
total train batches  : 12500
total test  batches  : 125
optimizer : Adam, scheduler : lambdaLR loaded
optimizer : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
scheduler : <torch.optim.lr_scheduler.LambdaLR object at 0x0000021D2DB38850>
initializing weights using Kaiming/He Initialization

cleaning torch mem and cache

beginning training ...
Epoch 0/10 , batch 1/12500 
gradient norm in None layer : 0.8663048303718694
gradient norm in None layer : 0.038451195142683664
gradient norm in None layer : 0.04281677525713261
gradient norm in None layer : 0.7224612371947905
gradient norm in None layer : 0.0305207413162974
gradient norm in None layer : 0.030161021932548077
gradient norm in None layer : 0.11851554971712723
gradient norm in None layer : 0.004700902127074336
gradient norm in None layer : 0.00476487938368665
gradient norm in None layer : 0.10398788647186048
gradient norm in None layer : 0.004028864089093127
gradient norm in None layer : 0.003982990455197433
gradient norm in None layer : 0.02875645422460586
gradient norm in None layer : 0.000821513510894047
gradient norm in None layer : 0.0007955414232401536
gradient norm in None layer : 0.026475569425272082
gradient norm in None layer : 0.0008768538164797004
gradient norm in None layer : 0.001005907018176734
gradient norm in None layer : 0.034683211066053245
gradient norm in None layer : 0.0005278947112436892
gradient norm in None layer : 0.08944806000909102
gradient norm in None layer : 0.004066783026980632
gradient norm in None layer : 0.003734994583905047
gradient norm in None layer : 0.10522309509675683
gradient norm in None layer : 0.005389456735107673
gradient norm in None layer : 0.005698815739603436
gradient norm in None layer : 0.14595459093146126
gradient norm in None layer : 0.0017231900998761344
gradient norm in None layer : 0.5456048208654288
gradient norm in None layer : 0.035283074859870575
gradient norm in None layer : 0.029786056189621393
gradient norm in None layer : 0.6406879346349561
gradient norm in None layer : 0.1331627415746003
gradient norm in None layer : 0.2143723097637928
gradient norm in None layer : 0.11805219967070556
gradient norm in None layer : 0.04766549670279781
Total gradient norm: 1.4614794648130427
invariance loss : 132.36609459464495, avg_den : 0.0, density loss : 0.1, mse loss : 0.2734061214762793, solver time : 0.4984772205352783 sec , total loss : 0.5057722160709243, running loss : 0.5057722160709243
Epoch 0/10 , batch 2/12500 
gradient norm in None layer : 0.23101817551022208
gradient norm in None layer : 0.009385369547031095
gradient norm in None layer : 0.011813222458057422
gradient norm in None layer : 0.19401715490120164
gradient norm in None layer : 0.009439345751053528
gradient norm in None layer : 0.007465208972132811
gradient norm in None layer : 0.03503015876768256
gradient norm in None layer : 0.0013772321181507506
gradient norm in None layer : 0.0012178292612365124
gradient norm in None layer : 0.0332129620316483
gradient norm in None layer : 0.0013436845619865536
gradient norm in None layer : 0.0011210611782526958
gradient norm in None layer : 0.01072734216022927
gradient norm in None layer : 0.000324129003206545
gradient norm in None layer : 0.00028284240621503085
gradient norm in None layer : 0.009828625081051232
gradient norm in None layer : 0.00038661500238920167
gradient norm in None layer : 0.0004166932092233998
gradient norm in None layer : 0.013488417086006337
gradient norm in None layer : 0.00020050478899988862
gradient norm in None layer : 0.03154210471938784
gradient norm in None layer : 0.00161453327382077
gradient norm in None layer : 0.0012267101702692503
gradient norm in None layer : 0.03496304219284428
gradient norm in None layer : 0.0024275484117039876
gradient norm in None layer : 0.002143378742831984
gradient norm in None layer : 0.055458404313964295
gradient norm in None layer : 0.0006260021011693514
gradient norm in None layer : 0.14058986571298743
gradient norm in None layer : 0.009382687036230323
gradient norm in None layer : 0.008981968159752886
gradient norm in None layer : 0.16441364594328473
gradient norm in None layer : 0.035334107067184706
gradient norm in None layer : 0.055060551688592634
gradient norm in None layer : 0.030944071142069242
gradient norm in None layer : 0.011435688992528243
Total gradient norm: 0.38956106833333726
invariance loss : 76.01676796271744, avg_den : 0.0, density loss : 0.1, mse loss : 0.3116703964229718, solver time : 0.49547481536865234 sec , total loss : 0.4876871643856892, running loss : 0.4967296902283067
Epoch 0/10 , batch 3/12500 
gradient norm in None layer : 0.10317938059540055
gradient norm in None layer : 0.006553873659200483
gradient norm in None layer : 0.005846097813928152
gradient norm in None layer : 0.11534119251509665
gradient norm in None layer : 0.007003216062325452
gradient norm in None layer : 0.007564313629807693
gradient norm in None layer : 0.039496418283837355
gradient norm in None layer : 0.0014292652843183365
gradient norm in None layer : 0.0012507652960479653
gradient norm in None layer : 0.03450528080782886
gradient norm in None layer : 0.0013659543726762763
gradient norm in None layer : 0.0011507546616239934
gradient norm in None layer : 0.011811258541457535
gradient norm in None layer : 0.0003250287143421157
gradient norm in None layer : 0.0002406656788401819
gradient norm in None layer : 0.010375142267158388
gradient norm in None layer : 0.00040653652712139285
gradient norm in None layer : 0.00030971346209524204
gradient norm in None layer : 0.0145144721207407
gradient norm in None layer : 0.00025593768358651267
gradient norm in None layer : 0.0315420046262952
gradient norm in None layer : 0.00155563415525924
gradient norm in None layer : 0.0012765547023634507
gradient norm in None layer : 0.035896886693291816
gradient norm in None layer : 0.0030321883471823813
gradient norm in None layer : 0.0025776293413071498
gradient norm in None layer : 0.06448632338793506
gradient norm in None layer : 0.001204851405278602
gradient norm in None layer : 0.11702135420953624
gradient norm in None layer : 0.00706496765692453
gradient norm in None layer : 0.007805962324922291
gradient norm in None layer : 0.1350676351803125
gradient norm in None layer : 0.029472415694411436
gradient norm in None layer : 0.04051656096015272
gradient norm in None layer : 0.025976912639700235
gradient norm in None layer : 0.008566243066579791
Total gradient norm: 0.2629039241733736
invariance loss : 79.48350627071545, avg_den : 0.04268646240234375, density loss : 0.06862335205078125, mse loss : 0.19001323601044443, solver time : 28.497679710388184 sec , total loss : 0.3381200943319411, running loss : 0.44385982492951814
Epoch 0/10 , batch 4/12500 
gradient norm in None layer : 0.08233976321754419
gradient norm in None layer : 0.004147457748177587
gradient norm in None layer : 0.002373374688670522
gradient norm in None layer : 0.06661348136911843
gradient norm in None layer : 0.003805626545851705
gradient norm in None layer : 0.0037228901979658928
gradient norm in None layer : 0.019100836025312692
gradient norm in None layer : 0.0008504547292892672
gradient norm in None layer : 0.0004686785994588534
gradient norm in None layer : 0.014927807616091545
gradient norm in None layer : 0.000926545224055904
gradient norm in None layer : 0.0004725345206319993
gradient norm in None layer : 0.005604008146873739
gradient norm in None layer : 0.000169558080604849
gradient norm in None layer : 0.00010245010146094672
gradient norm in None layer : 0.0043281851080936625
gradient norm in None layer : 0.0001790102024814352
gradient norm in None layer : 0.00014378523451608774
gradient norm in None layer : 0.006255793189580006
gradient norm in None layer : 6.711256228791968e-05
gradient norm in None layer : 0.013572376156688042
gradient norm in None layer : 0.0009202912775275737
gradient norm in None layer : 0.0006178610953177145
gradient norm in None layer : 0.01820351673641871
gradient norm in None layer : 0.0017186255011570957
gradient norm in None layer : 0.0011950669409363942
gradient norm in None layer : 0.03399610442986375
gradient norm in None layer : 0.0003205984365672228
gradient norm in None layer : 0.06024451776971679
gradient norm in None layer : 0.004438071483179647
gradient norm in None layer : 0.004707893304013858
gradient norm in None layer : 0.08095262402008258
gradient norm in None layer : 0.020471371119588088
gradient norm in None layer : 0.028610089044272095
gradient norm in None layer : 0.018087161717944857
gradient norm in None layer : 0.006091145373376329
Total gradient norm: 0.15953345316915793
invariance loss : 46.26083030806541, avg_den : 0.0213623046875, density loss : 0.0786376953125, mse loss : 0.19955402150186197, solver time : 19.307854175567627 sec , total loss : 0.3244525471224273, running loss : 0.41400800547774547
Epoch 0/10 , batch 5/12500 
gradient norm in None layer : 0.06364143889241917
gradient norm in None layer : 0.0032911372650690674
gradient norm in None layer : 0.003283168826221495
gradient norm in None layer : 0.052699667659407166
gradient norm in None layer : 0.00430036790022443
gradient norm in None layer : 0.004534550924470243
gradient norm in None layer : 0.02416302470804312
gradient norm in None layer : 0.0008428387326329462
gradient norm in None layer : 0.0009985519852120435
gradient norm in None layer : 0.019314498199265596
gradient norm in None layer : 0.0007954220320278522
gradient norm in None layer : 0.0008551564983637306
gradient norm in None layer : 0.006499344926537164
gradient norm in None layer : 0.00016616566095471914
gradient norm in None layer : 0.0001722915775759969
gradient norm in None layer : 0.005698732415669172
gradient norm in None layer : 0.00027261644680835994
gradient norm in None layer : 0.0002588777688281276
gradient norm in None layer : 0.009954175278263458
gradient norm in None layer : 8.149511922939577e-05
gradient norm in None layer : 0.017286190244360847
gradient norm in None layer : 0.0009655174288028073
gradient norm in None layer : 0.0009002646674190869
gradient norm in None layer : 0.02372232286736526
gradient norm in None layer : 0.002431985284979082
gradient norm in None layer : 0.0016028208246936303
gradient norm in None layer : 0.04765652140520745
gradient norm in None layer : 0.0002733851919062274
gradient norm in None layer : 0.06465552570287868
gradient norm in None layer : 0.004347391504593297
gradient norm in None layer : 0.005203438537937931
gradient norm in None layer : 0.09297544652719981
gradient norm in None layer : 0.01502676798808119
gradient norm in None layer : 0.018640009679662655
gradient norm in None layer : 0.012512082265382835
gradient norm in None layer : 0.0037836521658522596
Total gradient norm: 0.15741667328932857
invariance loss : 35.313580800649646, avg_den : 0.066436767578125, density loss : 0.05701904296875, mse loss : 0.16592081714859652, solver time : 48.96334791183472 sec , total loss : 0.2582534409179962, running loss : 0.38285709256579564
Epoch 0/10 , batch 6/12500 
gradient norm in None layer : 0.05263892197047047
gradient norm in None layer : 0.0015122285721038023
gradient norm in None layer : 0.0014567952196263842
gradient norm in None layer : 0.0235093231894608
gradient norm in None layer : 0.0017493577414504228
gradient norm in None layer : 0.0017956285290026033
gradient norm in None layer : 0.006922776907831801
gradient norm in None layer : 0.0002853315283223381
gradient norm in None layer : 0.00032908425999848384
gradient norm in None layer : 0.005543889874521624
gradient norm in None layer : 0.00023290945648189675
gradient norm in None layer : 0.00030132134221730404
gradient norm in None layer : 0.001949985423035585
gradient norm in None layer : 5.234799772946496e-05
gradient norm in None layer : 5.932818442373167e-05
gradient norm in None layer : 0.001627021872871136
gradient norm in None layer : 7.275674935514618e-05
gradient norm in None layer : 7.209486908960453e-05
gradient norm in None layer : 0.0025240803641084185
gradient norm in None layer : 3.851908898600295e-05
gradient norm in None layer : 0.004972368303751967
gradient norm in None layer : 0.0003412515794831562
gradient norm in None layer : 0.0003083792015748773
gradient norm in None layer : 0.0067957323864415485
gradient norm in None layer : 0.0007903905680392315
gradient norm in None layer : 0.0004291077257426392
gradient norm in None layer : 0.01455786097213327
gradient norm in None layer : 0.00011888394457242988
gradient norm in None layer : 0.02508594884386172
gradient norm in None layer : 0.0017368744308804622
gradient norm in None layer : 0.002469718206389261
gradient norm in None layer : 0.03651036861057492
gradient norm in None layer : 0.006770692724894682
gradient norm in None layer : 0.01006194758013231
gradient norm in None layer : 0.005802262051522133
gradient norm in None layer : 0.002091963948822579
Total gradient norm: 0.07659307515154384
invariance loss : 21.908250719808244, avg_den : 0.07160186767578125, density loss : 0.05420379638671875, mse loss : 0.1817563926646321, solver time : 47.56415295600891 sec , total loss : 0.2578684397711591, running loss : 0.36202565043335616
Epoch 0/10 , batch 7/12500 
gradient norm in None layer : 0.034654940855876
gradient norm in None layer : 0.001869185211708701
gradient norm in None layer : 0.0009571453700618412
gradient norm in None layer : 0.02982237941674013
gradient norm in None layer : 0.0020080229863530404
gradient norm in None layer : 0.001766138465466874
gradient norm in None layer : 0.013964106320117919
gradient norm in None layer : 0.0005776450700577409
gradient norm in None layer : 0.0002902182843871167
gradient norm in None layer : 0.01022453217763052
gradient norm in None layer : 0.0004136803086605184
gradient norm in None layer : 0.0003702744639724967
gradient norm in None layer : 0.003537434048794053
gradient norm in None layer : 9.231731874925669e-05
gradient norm in None layer : 8.420197177352762e-05
gradient norm in None layer : 0.0022441203224032044
gradient norm in None layer : 0.00010472535824569492
gradient norm in None layer : 0.00010329119893970488
gradient norm in None layer : 0.003696510847714253
gradient norm in None layer : 4.123848560390313e-05
gradient norm in None layer : 0.007055886948258346
gradient norm in None layer : 0.00044086326793845576
gradient norm in None layer : 0.0003644671950110307
gradient norm in None layer : 0.008549913349635714
gradient norm in None layer : 0.00106509802527113
gradient norm in None layer : 0.0004993078547832577
gradient norm in None layer : 0.0219449365181798
gradient norm in None layer : 0.0001143006359422517
gradient norm in None layer : 0.035815465440193185
gradient norm in None layer : 0.0025350214412865227
gradient norm in None layer : 0.002795392047224195
gradient norm in None layer : 0.04967928854813686
gradient norm in None layer : 0.009471671228861719
gradient norm in None layer : 0.01276387224545314
gradient norm in None layer : 0.008191475758689977
gradient norm in None layer : 0.0027178211195188814
Total gradient norm: 0.08444964879967685
invariance loss : 23.12535984478596, avg_den : 0.0775146484375, density loss : 0.04053955078125, mse loss : 0.10094564252462948, solver time : 63.46726083755493 sec , total loss : 0.16461055315066547, running loss : 0.3338234936786861
Epoch 0/10 , batch 8/12500 
gradient norm in None layer : 0.028530858901472504
gradient norm in None layer : 0.0008602503043295143
gradient norm in None layer : 0.0009705928250422032
gradient norm in None layer : 0.015461782014322271
gradient norm in None layer : 0.001352090527052252
gradient norm in None layer : 0.0012605570240167628
gradient norm in None layer : 0.005271426177076656
gradient norm in None layer : 0.00022210728555356463
gradient norm in None layer : 0.0002763897304229508
gradient norm in None layer : 0.004466680202622895
gradient norm in None layer : 0.00022139623732742912
gradient norm in None layer : 0.00026531342180604646
gradient norm in None layer : 0.0016139850177281736
gradient norm in None layer : 5.301959250203148e-05
gradient norm in None layer : 5.69182203424443e-05
gradient norm in None layer : 0.001443002777876564
gradient norm in None layer : 7.154259978985744e-05
gradient norm in None layer : 6.589777821498222e-05
gradient norm in None layer : 0.002456278814835801
gradient norm in None layer : 2.3209211109987324e-05
gradient norm in None layer : 0.004647832152508925
gradient norm in None layer : 0.00031343220623301714
gradient norm in None layer : 0.00027117227439601055
gradient norm in None layer : 0.007151889922982121
gradient norm in None layer : 0.0007744015792855737
gradient norm in None layer : 0.0003689831462448739
gradient norm in None layer : 0.015557649362798987
gradient norm in None layer : 0.00015628188872476354
gradient norm in None layer : 0.023594472396103128
gradient norm in None layer : 0.001472401266298922
gradient norm in None layer : 0.0017318332125478458
gradient norm in None layer : 0.029760082688739562
gradient norm in None layer : 0.00667652992162971
gradient norm in None layer : 0.011004410997516853
gradient norm in None layer : 0.005746778499216248
gradient norm in None layer : 0.0023734328410160983
Total gradient norm: 0.055538110300570674
invariance loss : 17.437872602843562, avg_den : 0.075775146484375, density loss : 0.04556884765625, mse loss : 0.14933254408420832, solver time : 73.97827911376953 sec , total loss : 0.2123392643433019, running loss : 0.31863796501176306
Epoch 0/10 , batch 9/12500 
