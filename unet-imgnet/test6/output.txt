CONFIG : 
{'EPOCHS': 10, 'RESUME_CHECKPOINT': None, 'SAVE_EVERY': 10, 'VAL_EVERY': 10, 'OUTPUT_DIR': 'unet-imgnet', 'EXP_NAME': 'test6', 'TRAIN_FILENAME': 'iids_train.txt', 'TEST_FILENAME': 'iids_test.txt', 'ROOT_DIR': 'dataset\\imagenet\\images\\', 'IMG_SIZE': 128, 'INP_CHANNELS': 1, 'OUT_CHANNELS': 1, 'LR': 0.001, 'WEIGHT_DECAY': 0.0, 'MOMENTUM': 0.9, 'OPT': 'Adam', 'SCHEDL': 'lambdaLR', 'TRAIN_BATCH': 8, 'TEST_BATCH': 8, 'ALPHA': 0.001, 'MASK_DEN': 0.1, 'BIN_METH': 'QUANT', 'OFFSET': None, 'TAU': None, 'ITERATIONS': None, 'NOTE': 'scaling prob to density ; alpha removed from dens ; loss(binaried input again)'}

train test dataset loaded
train size : 100000
test  size  : 1000
model loaded
model summary
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
??DoubleConv: 1-1                        --
|    ??Sequential: 2-1                   --
|    |    ??Conv2d: 3-1                  576
|    |    ??BatchNorm2d: 3-2             128
|    |    ??ReLU: 3-3                    --
|    |    ??Conv2d: 3-4                  36,864
|    |    ??BatchNorm2d: 3-5             128
|    |    ??ReLU: 3-6                    --
??Down: 1-2                              --
|    ??Sequential: 2-2                   --
|    |    ??MaxPool2d: 3-7               --
|    |    ??DoubleConv: 3-8              221,696
??Down: 1-3                              --
|    ??Sequential: 2-3                   --
|    |    ??MaxPool2d: 3-9               --
|    |    ??DoubleConv: 3-10             885,760
??Up: 1-4                                --
|    ??ConvTranspose2d: 2-4              131,200
|    ??DoubleConv: 2-5                   --
|    |    ??Sequential: 3-11             442,880
??Up: 1-5                                --
|    ??ConvTranspose2d: 2-6              32,832
|    ??DoubleConv: 2-7                   --
|    |    ??Sequential: 3-12             110,848
??OutConv: 1-6                           --
|    ??Conv2d: 2-8                       65
=================================================================
Total params: 1,862,977
Trainable params: 1,862,977
Non-trainable params: 0
=================================================================
device : cuda
trainer configurations set
train and test dataloaders created
total train batches  : 12500
total test  batches  : 125
optimizer : Adam, scheduler : lambdaLR loaded
optimizer : Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)
scheduler : <torch.optim.lr_scheduler.LambdaLR object at 0x000001A118537CA0>
initializing weights using Kaiming/He Initialization

cleaning torch mem and cache

beginning training ...
Epoch 0/10 , batch 1/12500 
ITERATION : 1, loss : 0.30453802780372075 ITERATION : 2, loss : 0.30453802780372075 ITERATION : 3, loss : 0.30453802780372075 ITERATION : 4, loss : 0.30453802780372075 ITERATION : 5, loss : 0.30453802780372075 ITERATION : 6, loss : 0.30453802780372075 
ITERATION : 1, loss : 0.22893052550128967 ITERATION : 2, loss : 0.22893052550128967 ITERATION : 3, loss : 0.22893052550128967 ITERATION : 4, loss : 0.22893052550128967 ITERATION : 5, loss : 0.22893052550128967 ITERATION : 6, loss : 0.22893052550128967 
ITERATION : 1, loss : 0.5108240646038807 ITERATION : 2, loss : 0.5108240646038807 ITERATION : 3, loss : 0.5108240646038807 ITERATION : 4, loss : 0.5108240646038807 ITERATION : 5, loss : 0.5108240646038807 ITERATION : 6, loss : 0.5108240646038807 
ITERATION : 1, loss : 0.39742611583337895 ITERATION : 2, loss : 0.39742611583337895 ITERATION : 3, loss : 0.39742611583337895 ITERATION : 4, loss : 0.39742611583337895 ITERATION : 5, loss : 0.39742611583337895 ITERATION : 6, loss : 0.39742611583337895 
ITERATION : 1, loss : 0.039596973045778645 ITERATION : 2, loss : 0.04044692762021003 ITERATION : 3, loss : 0.04091479917262365 ITERATION : 4, loss : 0.04125125231727824 ITERATION : 5, loss : 0.0415160505857385 ITERATION : 6, loss : 0.041721743287074994 ITERATION : 7, loss : 0.04187722258449284 ITERATION : 8, loss : 0.041992103864402525 ITERATION : 9, loss : 0.0420755953223268 ITERATION : 10, loss : 0.042135571555477536 ITERATION : 11, loss : 0.04217830609903186 ITERATION : 12, loss : 0.04220858222708186 ITERATION : 13, loss : 0.04222994576082851 ITERATION : 14, loss : 0.04224497790528079 ITERATION : 15, loss : 0.04225553356960494 ITERATION : 16, loss : 0.042262935116709935 ITERATION : 17, loss : 0.042268119523347525 ITERATION : 18, loss : 0.042271748219660336 ITERATION : 19, loss : 0.04227428662518638 ITERATION : 20, loss : 0.04227606151847809 
ITERATION : 1, loss : 0.3095629900219467 ITERATION : 2, loss : 0.3095629900219467 ITERATION : 3, loss : 0.3095629900219467 ITERATION : 4, loss : 0.3095629900219467 ITERATION : 5, loss : 0.3095629900219467 ITERATION : 6, loss : 0.3095629900219467 
ITERATION : 1, loss : 0.373634376232745 ITERATION : 2, loss : 0.373634376232745 ITERATION : 3, loss : 0.373634376232745 ITERATION : 4, loss : 0.373634376232745 ITERATION : 5, loss : 0.373634376232745 ITERATION : 6, loss : 0.373634376232745 
ITERATION : 1, loss : 0.10152512982623382 ITERATION : 2, loss : 0.10152512982623382 ITERATION : 3, loss : 0.10152512982623382 ITERATION : 4, loss : 0.10152512982623382 ITERATION : 5, loss : 0.10152512982623382 ITERATION : 6, loss : 0.10152512982623382 
gradient norm in None layer : 0.7167398487592588
gradient norm in None layer : 0.03023618365964184
gradient norm in None layer : 0.021579316442000403
gradient norm in None layer : 0.5870891218583457
gradient norm in None layer : 0.03194637364536347
gradient norm in None layer : 0.02910936116658168
gradient norm in None layer : 0.1133342896502477
gradient norm in None layer : 0.00484802624015911
gradient norm in None layer : 0.004781710924235555
gradient norm in None layer : 0.11060622119053407
gradient norm in None layer : 0.003988288114424531
gradient norm in None layer : 0.0036972264156357555
gradient norm in None layer : 0.03526398169449937
gradient norm in None layer : 0.0009210053169053777
gradient norm in None layer : 0.0008993292695427328
gradient norm in None layer : 0.030877856726972443
gradient norm in None layer : 0.0009459835515626972
gradient norm in None layer : 0.0010224020968759363
gradient norm in None layer : 0.038531303460367344
gradient norm in None layer : 0.0003941618041382327
gradient norm in None layer : 0.09828319333626152
gradient norm in None layer : 0.005196019319636535
gradient norm in None layer : 0.004359264348062381
gradient norm in None layer : 0.10749433039508026
gradient norm in None layer : 0.004415827024888574
gradient norm in None layer : 0.005436776713408063
gradient norm in None layer : 0.1236889109781416
gradient norm in None layer : 0.001139240955643408
gradient norm in None layer : 0.47628584802029483
gradient norm in None layer : 0.025702061016900035
gradient norm in None layer : 0.02584254433946636
gradient norm in None layer : 0.4306839980458993
gradient norm in None layer : 0.08662702960212446
gradient norm in None layer : 0.13868314078304847
gradient norm in None layer : 0.07865550104719649
gradient norm in None layer : 0.0316014643804893
Total gradient norm: 1.1724653850042437
invariance loss : 88.34025389466936, avg_den : 0.00852203369140625, density loss : 0.09147796630859376, mse loss : 0.28358966141770925, solver time : 6.106522798538208 sec , total loss : 0.46340788162097235, running loss : 0.46340788162097235
Epoch 0/10 , batch 2/12500 
ITERATION : 1, loss : 0.23593990342836915 ITERATION : 2, loss : 0.23593990342836915 ITERATION : 3, loss : 0.23593990342836915 ITERATION : 4, loss : 0.23593990342836915 ITERATION : 5, loss : 0.23593990342836915 ITERATION : 6, loss : 0.23593990342836915 
ITERATION : 1, loss : 0.023451819952981907 ITERATION : 2, loss : 0.023506974647180802 ITERATION : 3, loss : 0.025157524769687568 ITERATION : 4, loss : 0.02703659014319339 ITERATION : 5, loss : 0.028711606904741758 ITERATION : 6, loss : 0.030063112506744656 ITERATION : 7, loss : 0.031096838762613186 ITERATION : 8, loss : 0.03186284218388584 ITERATION : 9, loss : 0.032419423022151624 ITERATION : 10, loss : 0.03281883763559998 ITERATION : 11, loss : 0.03310319190623615 ITERATION : 12, loss : 0.033304598138782004 ITERATION : 13, loss : 0.0334467861990895 ITERATION : 14, loss : 0.033546960512939795 ITERATION : 15, loss : 0.03361744574226514 ITERATION : 16, loss : 0.033667004440473755 ITERATION : 17, loss : 0.03370183617971768 ITERATION : 18, loss : 0.033726313681769024 ITERATION : 19, loss : 0.03374351505966659 ITERATION : 20, loss : 0.033755604591800145 ITERATION : 21, loss : 0.03376410304082784 ITERATION : 22, loss : 0.033770078418781095 ITERATION : 23, loss : 0.03377428096081976 ITERATION : 24, loss : 0.03377723745144969 ITERATION : 25, loss : 0.03377931797038492 
ITERATION : 1, loss : 0.20460525542041583 ITERATION : 2, loss : 0.20460525542041583 ITERATION : 3, loss : 0.20460525542041583 ITERATION : 4, loss : 0.20460525542041583 ITERATION : 5, loss : 0.20460525542041583 ITERATION : 6, loss : 0.20460525542041583 
ITERATION : 1, loss : 0.11915042107311105 ITERATION : 2, loss : 0.140845192931008 ITERATION : 3, loss : 0.1536725678645032 ITERATION : 4, loss : 0.16243605767943936 ITERATION : 5, loss : 0.16848209206305872 ITERATION : 6, loss : 0.1726365507797636 ITERATION : 7, loss : 0.17548460668357066 ITERATION : 8, loss : 0.17743667750584455 ITERATION : 9, loss : 0.17877569873244908 ITERATION : 10, loss : 0.17969511244869604 ITERATION : 11, loss : 0.18032691667006293 ITERATION : 12, loss : 0.1807612819974877 ITERATION : 13, loss : 0.18105993840791976 ITERATION : 14, loss : 0.1812652390551912 ITERATION : 15, loss : 0.18140629402782837 ITERATION : 16, loss : 0.18150313624978115 ITERATION : 17, loss : 0.18156956165929128 ITERATION : 18, loss : 0.18161507362043772 ITERATION : 19, loss : 0.18164621816028992 ITERATION : 20, loss : 0.18166750169423013 ITERATION : 21, loss : 0.18168202481566062 ITERATION : 22, loss : 0.18169191922710412 ITERATION : 23, loss : 0.18169864844394149 ITERATION : 24, loss : 0.18170321701497688 ITERATION : 25, loss : 0.18170631233772894 ITERATION : 26, loss : 0.18170840520287726 
ITERATION : 1, loss : 0.2828830849888743 ITERATION : 2, loss : 0.2828830849888743 ITERATION : 3, loss : 0.2828830849888743 ITERATION : 4, loss : 0.2828830849888743 ITERATION : 5, loss : 0.2828830849888743 ITERATION : 6, loss : 0.2828830849888743 
ITERATION : 1, loss : 0.12711642953822355 ITERATION : 2, loss : 0.12711642953822355 ITERATION : 3, loss : 0.12711642953822355 ITERATION : 4, loss : 0.12711642953822355 ITERATION : 5, loss : 0.12711642953822355 ITERATION : 6, loss : 0.12711642953822355 
ITERATION : 1, loss : 0.11646130179302387 ITERATION : 2, loss : 0.11646130179302387 ITERATION : 3, loss : 0.11646130179302387 ITERATION : 4, loss : 0.11646130179302387 ITERATION : 5, loss : 0.11646130179302387 ITERATION : 6, loss : 0.11646130179302387 
ITERATION : 1, loss : 0.8526914321006016 ITERATION : 2, loss : 0.8526914321006016 ITERATION : 3, loss : 0.8526914321006016 ITERATION : 4, loss : 0.8526914321006016 ITERATION : 5, loss : 0.8526914321006016 ITERATION : 6, loss : 0.8526914321006016 
gradient norm in None layer : 0.6657265170267573
gradient norm in None layer : 0.01922275553054446
gradient norm in None layer : 0.015614014305747097
gradient norm in None layer : 0.3616717569528046
gradient norm in None layer : 0.01784888869469561
gradient norm in None layer : 0.016348194539957134
gradient norm in None layer : 0.06828425665261395
gradient norm in None layer : 0.0026812640271183276
gradient norm in None layer : 0.0021766196529566222
gradient norm in None layer : 0.06703352018619647
gradient norm in None layer : 0.0033140286036209303
gradient norm in None layer : 0.0026759429225076442
gradient norm in None layer : 0.02718540936055695
gradient norm in None layer : 0.0006784602452173318
gradient norm in None layer : 0.0005994550008668261
gradient norm in None layer : 0.023570639458378246
gradient norm in None layer : 0.0008913211420188185
gradient norm in None layer : 0.0008962303177414083
gradient norm in None layer : 0.03499485632878017
gradient norm in None layer : 0.0003820666355579689
gradient norm in None layer : 0.08344720218523365
gradient norm in None layer : 0.004431323417044141
gradient norm in None layer : 0.0032975618303652953
gradient norm in None layer : 0.10189029688152866
gradient norm in None layer : 0.005842495501399487
gradient norm in None layer : 0.005254817286940493
gradient norm in None layer : 0.15336715291461903
gradient norm in None layer : 0.0012059667372757191
gradient norm in None layer : 0.3706061212822569
gradient norm in None layer : 0.019519692124778366
gradient norm in None layer : 0.018222950569136687
gradient norm in None layer : 0.36796303258688323
gradient norm in None layer : 0.083900410220674
gradient norm in None layer : 0.10420695988714085
gradient norm in None layer : 0.07152866454746992
gradient norm in None layer : 0.021612335937916588
Total gradient norm: 0.9616685512665187
invariance loss : 103.28689307943344, avg_den : 0.02734375, density loss : 0.07734375, mse loss : 0.2543981413053463, solver time : 53.259801626205444 sec , total loss : 0.43502878438477977, running loss : 0.44921833300287606
Epoch 0/10 , batch 3/12500 
ITERATION : 1, loss : 0.03355485506139998 ITERATION : 2, loss : 0.03177493304269429 ITERATION : 3, loss : 0.031381406712715484 ITERATION : 4, loss : 0.031265326950237385 ITERATION : 5, loss : 0.03122612876809909 ITERATION : 6, loss : 0.031211450811416363 ITERATION : 7, loss : 0.031205318348338982 ITERATION : 8, loss : 0.031202415499045663 ITERATION : 9, loss : 0.031200854785361144 ITERATION : 10, loss : 0.03119992022873162 ITERATION : 11, loss : 0.03119931659210056 
ITERATION : 1, loss : 0.2974267085423027 ITERATION : 2, loss : 0.2974267085423027 ITERATION : 3, loss : 0.2974267085423027 ITERATION : 4, loss : 0.2974267085423027 ITERATION : 5, loss : 0.2974267085423027 ITERATION : 6, loss : 0.2974267085423027 
ITERATION : 1, loss : 0.2754142943307876 ITERATION : 2, loss : 0.2754142943307876 ITERATION : 3, loss : 0.2754142943307876 ITERATION : 4, loss : 0.2754142943307876 ITERATION : 5, loss : 0.2754142943307876 ITERATION : 6, loss : 0.2754142943307876 
ITERATION : 1, loss : 0.6395948716306087 ITERATION : 2, loss : 0.6395948716306087 ITERATION : 3, loss : 0.6395948716306087 ITERATION : 4, loss : 0.6395948716306087 ITERATION : 5, loss : 0.6395948716306087 ITERATION : 6, loss : 0.6395948716306087 
ITERATION : 1, loss : 0.26168169219781884 ITERATION : 2, loss : 0.26168169219781884 ITERATION : 3, loss : 0.26168169219781884 ITERATION : 4, loss : 0.26168169219781884 ITERATION : 5, loss : 0.26168169219781884 ITERATION : 6, loss : 0.26168169219781884 
ITERATION : 1, loss : 0.060271637988961006 ITERATION : 2, loss : 0.07933198557314212 ITERATION : 3, loss : 0.09027218462976833 ITERATION : 4, loss : 0.09714978683321106 ITERATION : 5, loss : 0.1016764560674818 ITERATION : 6, loss : 0.1047388854349636 ITERATION : 7, loss : 0.10684794465589298 ITERATION : 8, loss : 0.10831817004024742 ITERATION : 9, loss : 0.10935191078980501 ITERATION : 10, loss : 0.11008330529167897 ITERATION : 11, loss : 0.11060317524530133 ITERATION : 12, loss : 0.1109739689914241 ITERATION : 13, loss : 0.11123911880232422 ITERATION : 14, loss : 0.11142909300856031 ITERATION : 15, loss : 0.1115654054069355 ITERATION : 16, loss : 0.11166332210130935 ITERATION : 17, loss : 0.11173371742305002 ITERATION : 18, loss : 0.11178435862485353 ITERATION : 19, loss : 0.11182080656443436 ITERATION : 20, loss : 0.11184704886769938 ITERATION : 21, loss : 0.11186594819506267 ITERATION : 22, loss : 0.11187956214867405 ITERATION : 23, loss : 0.11188937044038769 ITERATION : 24, loss : 0.11189643773397712 ITERATION : 25, loss : 0.1119015304666547 ITERATION : 26, loss : 0.11190520054733144 ITERATION : 27, loss : 0.11190784556636843 
ITERATION : 1, loss : 0.02820791916511139 ITERATION : 2, loss : 0.02956525553274076 ITERATION : 3, loss : 0.030232204809291632 ITERATION : 4, loss : 0.030679630531425334 ITERATION : 5, loss : 0.03101528640762044 ITERATION : 6, loss : 0.03127243002185186 ITERATION : 7, loss : 0.03146805453445272 ITERATION : 8, loss : 0.03161506476692294 ITERATION : 9, loss : 0.031724316803212624 ITERATION : 10, loss : 0.03180479783842811 ITERATION : 11, loss : 0.03186369529206495 ITERATION : 12, loss : 0.03190658975719302 ITERATION : 13, loss : 0.03193771997227961 ITERATION : 14, loss : 0.03196025517663529 ITERATION : 15, loss : 0.031976538573682114 ITERATION : 16, loss : 0.03198828910841461 ITERATION : 17, loss : 0.031996760529657525 ITERATION : 18, loss : 0.03200286369331418 ITERATION : 19, loss : 0.03200725847137339 ITERATION : 20, loss : 0.03201042193738322 ITERATION : 21, loss : 0.03201269847287116 
ITERATION : 1, loss : 0.08561439887954952 ITERATION : 2, loss : 0.08561439887954952 ITERATION : 3, loss : 0.08561439887954952 ITERATION : 4, loss : 0.08561439887954952 ITERATION : 5, loss : 0.08561439887954952 ITERATION : 6, loss : 0.08561439887954952 
gradient norm in None layer : 0.39803110911438017
gradient norm in None layer : 0.012776454229557477
gradient norm in None layer : 0.015663102214081057
gradient norm in None layer : 0.2043654007802149
gradient norm in None layer : 0.009936002409712149
gradient norm in None layer : 0.013521490104020125
gradient norm in None layer : 0.03738231030287367
gradient norm in None layer : 0.0015326831141574838
gradient norm in None layer : 0.0012833422308652696
gradient norm in None layer : 0.03286018938615561
gradient norm in None layer : 0.0017273243525322334
gradient norm in None layer : 0.0013272604391686921
gradient norm in None layer : 0.013106632432777082
gradient norm in None layer : 0.00035591744853670764
gradient norm in None layer : 0.00028068908486270994
gradient norm in None layer : 0.011809803230889867
gradient norm in None layer : 0.000485955807811956
gradient norm in None layer : 0.00046576547677632495
gradient norm in None layer : 0.016644161798926013
gradient norm in None layer : 0.00026676355636939743
gradient norm in None layer : 0.03538913522323099
gradient norm in None layer : 0.0024843812789981153
gradient norm in None layer : 0.002017380572404006
gradient norm in None layer : 0.047596549305833485
gradient norm in None layer : 0.003241266436401217
gradient norm in None layer : 0.002793410708908697
gradient norm in None layer : 0.07669459616123651
gradient norm in None layer : 0.0009339523633741149
gradient norm in None layer : 0.19089338371616463
gradient norm in None layer : 0.009768531668627643
gradient norm in None layer : 0.014535570866761384
gradient norm in None layer : 0.1757395147450902
gradient norm in None layer : 0.04299417720096273
gradient norm in None layer : 0.06218060933420752
gradient norm in None layer : 0.03675309936060493
gradient norm in None layer : 0.012027441211656291
Total gradient norm: 0.5368706936290593
invariance loss : 86.67621182826393, avg_den : 0.0411834716796875, density loss : 0.06983642578125, mse loss : 0.21685647827655097, solver time : 21.276305437088013 sec , total loss : 0.3733691158860649, running loss : 0.4239352606306057
Epoch 0/10 , batch 4/12500 
ITERATION : 1, loss : 0.018941436244566925 ITERATION : 2, loss : 0.017510766145876818 ITERATION : 3, loss : 0.017517328202226047 ITERATION : 4, loss : 0.017689154640972263 ITERATION : 5, loss : 0.017858600921036815 ITERATION : 6, loss : 0.017997318701581444 ITERATION : 7, loss : 0.01810391308944175 ITERATION : 8, loss : 0.01818332892333555 ITERATION : 9, loss : 0.018241463632459237 ITERATION : 10, loss : 0.018283563881833262 ITERATION : 11, loss : 0.01831384403999816 ITERATION : 12, loss : 0.018335525878093735 ITERATION : 13, loss : 0.018351005345393877 ITERATION : 14, loss : 0.018362035194705962 ITERATION : 15, loss : 0.01836988428434655 ITERATION : 16, loss : 0.01837546501728905 ITERATION : 17, loss : 0.01837943065501665 ITERATION : 18, loss : 0.018382247549410737 
ITERATION : 1, loss : 0.22998788868315143 ITERATION : 2, loss : 0.22998788868315143 ITERATION : 3, loss : 0.22998788868315143 ITERATION : 4, loss : 0.22998788868315143 ITERATION : 5, loss : 0.22998788868315143 ITERATION : 6, loss : 0.22998788868315143 
ITERATION : 1, loss : 0.1284186547607541 ITERATION : 2, loss : 0.1284186547607541 ITERATION : 3, loss : 0.1284186547607541 ITERATION : 4, loss : 0.1284186547607541 ITERATION : 5, loss : 0.1284186547607541 ITERATION : 6, loss : 0.1284186547607541 
ITERATION : 1, loss : 0.11379195087077436 ITERATION : 2, loss : 0.11206054366760944 ITERATION : 3, loss : 0.11236740975570408 ITERATION : 4, loss : 0.11355337294297096 ITERATION : 5, loss : 0.11481835233845065 ITERATION : 6, loss : 0.11589192228239933 ITERATION : 7, loss : 0.11697124942275416 ITERATION : 8, loss : 0.11780529480210487 ITERATION : 9, loss : 0.11840574604720573 ITERATION : 10, loss : 0.11883533681633368 ITERATION : 11, loss : 0.11914152045935697 ITERATION : 12, loss : 0.11935921670440272 ITERATION : 13, loss : 0.11951374222717136 ITERATION : 14, loss : 0.11962329587008183 ITERATION : 15, loss : 0.11970089336349381 ITERATION : 16, loss : 0.11975581359220683 ITERATION : 17, loss : 0.11979465751638785 ITERATION : 18, loss : 0.11982211404616193 ITERATION : 19, loss : 0.1198415100508539 ITERATION : 20, loss : 0.11985520393104016 ITERATION : 21, loss : 0.11986486652695204 ITERATION : 22, loss : 0.11987168067540167 ITERATION : 23, loss : 0.1198764831444174 ITERATION : 24, loss : 0.1198798658368122 ITERATION : 25, loss : 0.1198822468831095 
ITERATION : 1, loss : 0.036532794448091266 ITERATION : 2, loss : 0.03254150986668195 ITERATION : 3, loss : 0.030489808745542784 ITERATION : 4, loss : 0.02956248612556818 ITERATION : 5, loss : 0.029147329891267625 ITERATION : 6, loss : 0.028958693591661002 ITERATION : 7, loss : 0.028871472508418764 ITERATION : 8, loss : 0.028830486783937497 ITERATION : 9, loss : 0.02881092582893209 ITERATION : 10, loss : 0.02880142802845274 ITERATION : 11, loss : 0.02879671301719718 ITERATION : 12, loss : 0.028794298727104966 ITERATION : 13, loss : 0.02879300771948578 ITERATION : 14, loss : 0.028792277033644786 
ITERATION : 1, loss : 0.7328112329477873 ITERATION : 2, loss : 0.7328112329477873 ITERATION : 3, loss : 0.7328112329477873 ITERATION : 4, loss : 0.7328112329477873 ITERATION : 5, loss : 0.7328112329477873 ITERATION : 6, loss : 0.7328112329477873 
ITERATION : 1, loss : 0.10298682049377013 ITERATION : 2, loss : 0.10711530867527203 ITERATION : 3, loss : 0.11112175032529348 ITERATION : 4, loss : 0.11409002793079712 ITERATION : 5, loss : 0.11622850018436541 ITERATION : 6, loss : 0.1177708139231744 ITERATION : 7, loss : 0.11888728702322036 ITERATION : 8, loss : 0.11969786311430435 ITERATION : 9, loss : 0.12028758126181087 ITERATION : 10, loss : 0.12071726769103876 ITERATION : 11, loss : 0.12103070570292403 ITERATION : 12, loss : 0.1212595442807712 ITERATION : 13, loss : 0.12142672963675796 ITERATION : 14, loss : 0.12154893557629244 ITERATION : 15, loss : 0.12163829910222604 ITERATION : 16, loss : 0.12170366593595881 ITERATION : 17, loss : 0.12175149077118504 ITERATION : 18, loss : 0.12178648700855392 ITERATION : 19, loss : 0.12181209882010427 ITERATION : 20, loss : 0.12183084421559909 ITERATION : 21, loss : 0.12184456481810321 ITERATION : 22, loss : 0.12185460783221755 ITERATION : 23, loss : 0.12186195906299231 ITERATION : 24, loss : 0.12186733993463228 ITERATION : 25, loss : 0.12187127860118464 ITERATION : 26, loss : 0.12187416146981886 ITERATION : 27, loss : 0.12187627154260904 
ITERATION : 1, loss : 0.19242230792046514 ITERATION : 2, loss : 0.17689335546825086 ITERATION : 3, loss : 0.17545123125214693 ITERATION : 4, loss : 0.18061176766535839 ITERATION : 5, loss : 0.1892063628746954 ITERATION : 6, loss : 0.19523059160026165 ITERATION : 7, loss : 0.1995115676166232 ITERATION : 8, loss : 0.20258050671193104 ITERATION : 9, loss : 0.20479377881701682 ITERATION : 10, loss : 0.206396740779105 ITERATION : 11, loss : 0.20756123654709654 ITERATION : 12, loss : 0.208409078659073 ITERATION : 13, loss : 0.2090273678488147 ITERATION : 14, loss : 0.20947878336123021 ITERATION : 15, loss : 0.20980864275779845 ITERATION : 16, loss : 0.21004982481497605 ITERATION : 17, loss : 0.21022624513917426 ITERATION : 18, loss : 0.21035533248687258 ITERATION : 19, loss : 0.21044980559015772 ITERATION : 20, loss : 0.21051895556961336 ITERATION : 21, loss : 0.21056957448208483 ITERATION : 22, loss : 0.21060663006883729 ITERATION : 23, loss : 0.21063375721721228 ITERATION : 24, loss : 0.21065361614067 ITERATION : 25, loss : 0.21066815400614308 ITERATION : 26, loss : 0.21067879635665976 ITERATION : 27, loss : 0.21068658675867874 ITERATION : 28, loss : 0.21069228927684455 ITERATION : 29, loss : 0.21069646337016934 ITERATION : 30, loss : 0.2106995185207201 ITERATION : 31, loss : 0.2107017546674816 
gradient norm in None layer : 0.19674905132943396
gradient norm in None layer : 0.006098703952392488
gradient norm in None layer : 0.004391785705125655
gradient norm in None layer : 0.08535501918475158
gradient norm in None layer : 0.006073187472010695
gradient norm in None layer : 0.005078320490789737
gradient norm in None layer : 0.01597985783866683
gradient norm in None layer : 0.0006453247229876724
gradient norm in None layer : 0.0005414270131728732
gradient norm in None layer : 0.01455165092466467
gradient norm in None layer : 0.0007846998721795159
gradient norm in None layer : 0.0005780826777658361
gradient norm in None layer : 0.005813651237825343
gradient norm in None layer : 0.00018761447307128195
gradient norm in None layer : 0.0001462148848576732
gradient norm in None layer : 0.005376420888925046
gradient norm in None layer : 0.00022907017439358527
gradient norm in None layer : 0.0001907853526919483
gradient norm in None layer : 0.008712203507169563
gradient norm in None layer : 0.000160310502565553
gradient norm in None layer : 0.01672519078136789
gradient norm in None layer : 0.0010278229758632692
gradient norm in None layer : 0.0008293937110369919
gradient norm in None layer : 0.023693083798565388
gradient norm in None layer : 0.0016446695539867743
gradient norm in None layer : 0.0013753164081809193
gradient norm in None layer : 0.038581285407460704
gradient norm in None layer : 0.0006717803764875683
gradient norm in None layer : 0.09746336514939263
gradient norm in None layer : 0.004914988066708668
gradient norm in None layer : 0.005525784202886316
gradient norm in None layer : 0.09316339129273156
gradient norm in None layer : 0.019837656855765197
gradient norm in None layer : 0.023713516718206967
gradient norm in None layer : 0.0169814303177517
gradient norm in None layer : 0.004528269165691368
Total gradient norm: 0.2618333886078001
invariance loss : 64.39101935733349, avg_den : 0.0704803466796875, density loss : 0.05072021484375, mse loss : 0.19885657175849353, solver time : 38.3620331287384 sec , total loss : 0.313967805959577, running loss : 0.3964433969628485
Epoch 0/10 , batch 5/12500 
ITERATION : 1, loss : 0.16102822669322953 ITERATION : 2, loss : 0.16079525305946774 ITERATION : 3, loss : 0.159599770472172 ITERATION : 4, loss : 0.15842519545934636 ITERATION : 5, loss : 0.1574435482355686 ITERATION : 6, loss : 0.15667755239959466 ITERATION : 7, loss : 0.15610017739229065 ITERATION : 8, loss : 0.1556729488437982 ITERATION : 9, loss : 0.15536004610741586 ITERATION : 10, loss : 0.15513222683224465 ITERATION : 11, loss : 0.15496694651529516 ITERATION : 12, loss : 0.15484730910346972 ITERATION : 13, loss : 0.15476084149576402 ITERATION : 14, loss : 0.15469841398740214 ITERATION : 15, loss : 0.15465337854999545 ITERATION : 16, loss : 0.1546209093542556 ITERATION : 17, loss : 0.15459751146461556 ITERATION : 18, loss : 0.15458065701090615 ITERATION : 19, loss : 0.15456851998865156 ITERATION : 20, loss : 0.1545597824894345 ITERATION : 21, loss : 0.15455349378390612 ITERATION : 22, loss : 0.15454896855262895 ITERATION : 23, loss : 0.1545457129273235 ITERATION : 24, loss : 0.15454337107959606 
ITERATION : 1, loss : 0.17427348734901008 ITERATION : 2, loss : 0.17427348734901008 ITERATION : 3, loss : 0.17427348734901008 ITERATION : 4, loss : 0.17427348734901008 ITERATION : 5, loss : 0.17427348734901008 ITERATION : 6, loss : 0.17427348734901008 
ITERATION : 1, loss : 0.1161482795716402 ITERATION : 2, loss : 0.11300639005280078 ITERATION : 3, loss : 0.11316790396017061 ITERATION : 4, loss : 0.11373153610933381 ITERATION : 5, loss : 0.11418854588010863 ITERATION : 6, loss : 0.11450582891828616 ITERATION : 7, loss : 0.11471812592637803 ITERATION : 8, loss : 0.11485954513428612 ITERATION : 9, loss : 0.11495443932169365 ITERATION : 10, loss : 0.11501884653540363 ITERATION : 11, loss : 0.11506309446933863 ITERATION : 12, loss : 0.11509383328038804 ITERATION : 13, loss : 0.11511539109046504 ITERATION : 14, loss : 0.11513062549257566 ITERATION : 15, loss : 0.11514145504159144 ITERATION : 16, loss : 0.11514918671776989 ITERATION : 17, loss : 0.115154724289326 ITERATION : 18, loss : 0.11515869837092252 ITERATION : 19, loss : 0.11516155483496074 ITERATION : 20, loss : 0.11516360926651116 
ITERATION : 1, loss : 0.09116268038474876 ITERATION : 2, loss : 0.10454019265884641 ITERATION : 3, loss : 0.1103760446167571 ITERATION : 4, loss : 0.11391020995980378 ITERATION : 5, loss : 0.116229247964115 ITERATION : 6, loss : 0.11781337588578046 ITERATION : 7, loss : 0.11892052857869864 ITERATION : 8, loss : 0.11970512245377408 ITERATION : 9, loss : 0.12026600522096281 ITERATION : 10, loss : 0.12066922725998523 ITERATION : 11, loss : 0.12096017555665876 ITERATION : 12, loss : 0.12117062014519693 ITERATION : 13, loss : 0.12132307602534274 ITERATION : 14, loss : 0.12143363314542402 ITERATION : 15, loss : 0.1215138554881705 ITERATION : 16, loss : 0.12157208619799262 ITERATION : 17, loss : 0.12161436024955306 ITERATION : 18, loss : 0.12164505068982998 ITERATION : 19, loss : 0.12166733009039872 ITERATION : 20, loss : 0.12168350135969264 ITERATION : 21, loss : 0.12169523708173867 ITERATION : 22, loss : 0.12170375211681893 ITERATION : 23, loss : 0.12170992898542635 ITERATION : 24, loss : 0.12171440869377313 ITERATION : 25, loss : 0.12171765678772839 ITERATION : 26, loss : 0.121720011358206 
ITERATION : 1, loss : 0.07707856481949724 ITERATION : 2, loss : 0.07707856481949724 ITERATION : 3, loss : 0.07707856481949724 ITERATION : 4, loss : 0.07707856481949724 ITERATION : 5, loss : 0.07707856481949724 ITERATION : 6, loss : 0.07707856481949724 
ITERATION : 1, loss : 0.14636144892302527 ITERATION : 2, loss : 0.1326347207383705 ITERATION : 3, loss : 0.12832674376996078 ITERATION : 4, loss : 0.12686865065445022 ITERATION : 5, loss : 0.12648709006884631 ITERATION : 6, loss : 0.12656224291671841 ITERATION : 7, loss : 0.12682031714946962 ITERATION : 8, loss : 0.12713208401226794 ITERATION : 9, loss : 0.12743715074003217 ITERATION : 10, loss : 0.12770979521204984 ITERATION : 11, loss : 0.12794175552505876 ITERATION : 12, loss : 0.12813322409883013 ITERATION : 13, loss : 0.12828812535322373 ITERATION : 14, loss : 0.12841168211610507 ITERATION : 15, loss : 0.12850921450428315 ITERATION : 16, loss : 0.12858559146143625 ITERATION : 17, loss : 0.12864502491126484 ITERATION : 18, loss : 0.1286910365674091 ITERATION : 19, loss : 0.12872650546610073 ITERATION : 20, loss : 0.1287537482420346 ITERATION : 21, loss : 0.12877460722845255 ITERATION : 22, loss : 0.12879053463096296 ITERATION : 23, loss : 0.12880266690031722 ITERATION : 24, loss : 0.12881188837750052 ITERATION : 25, loss : 0.12881888367363728 ITERATION : 26, loss : 0.12882418087901565 ITERATION : 27, loss : 0.1288281857649234 ITERATION : 28, loss : 0.1288312090855153 
ITERATION : 1, loss : 0.13904775531859537 ITERATION : 2, loss : 0.15895997046890584 ITERATION : 3, loss : 0.16717425848064332 ITERATION : 4, loss : 0.17165779216831523 ITERATION : 5, loss : 0.17457530840567226 ITERATION : 6, loss : 0.1766651568124178 ITERATION : 7, loss : 0.1781513022211863 ITERATION : 8, loss : 0.17922202027558498 ITERATION : 9, loss : 0.17999818442333299 ITERATION : 10, loss : 0.18056252105186077 ITERATION : 11, loss : 0.18097346695754318 ITERATION : 12, loss : 0.18127214236426045 ITERATION : 13, loss : 0.1814882921883033 ITERATION : 14, loss : 0.18164591027300958 ITERATION : 15, loss : 0.1817608547495198 ITERATION : 16, loss : 0.18184467863495474 ITERATION : 17, loss : 0.1819058046583896 ITERATION : 18, loss : 0.18195037501596695 ITERATION : 19, loss : 0.18198287023846635 ITERATION : 20, loss : 0.18200655910821054 ITERATION : 21, loss : 0.18202382584857676 ITERATION : 22, loss : 0.18203640989499084 ITERATION : 23, loss : 0.18204557986394443 ITERATION : 24, loss : 0.18205226101689026 ITERATION : 25, loss : 0.18205712818499503 ITERATION : 26, loss : 0.1820606733527486 ITERATION : 27, loss : 0.18206325520420005 
ITERATION : 1, loss : 0.1251703588628179 ITERATION : 2, loss : 0.12408889309491147 ITERATION : 3, loss : 0.12476871587108962 ITERATION : 4, loss : 0.12576932996312917 ITERATION : 5, loss : 0.12669174044729886 ITERATION : 6, loss : 0.12743228493849046 ITERATION : 7, loss : 0.12799204182184998 ITERATION : 8, loss : 0.12840246552921913 ITERATION : 9, loss : 0.12869841224849418 ITERATION : 10, loss : 0.1289097491415128 ITERATION : 11, loss : 0.12905977698257215 ITERATION : 12, loss : 0.12916588373534837 ITERATION : 13, loss : 0.12924074489543297 ITERATION : 14, loss : 0.12929347507999905 ITERATION : 15, loss : 0.12933057458984235 ITERATION : 16, loss : 0.12935665661934395 ITERATION : 17, loss : 0.1293749822099842 ITERATION : 18, loss : 0.12938785245661746 ITERATION : 19, loss : 0.1293968886669229 ITERATION : 20, loss : 0.1294032311723037 ITERATION : 21, loss : 0.12940768212289808 ITERATION : 22, loss : 0.12941080477235054 ITERATION : 23, loss : 0.12941299531367653 
gradient norm in None layer : 0.3056230862160751
gradient norm in None layer : 0.006072179659937886
gradient norm in None layer : 0.0042620626227276946
gradient norm in None layer : 0.106583470942794
gradient norm in None layer : 0.007996859432340438
gradient norm in None layer : 0.006960889598443959
gradient norm in None layer : 0.027171728114392796
gradient norm in None layer : 0.000990969889983687
gradient norm in None layer : 0.0010001983669323251
gradient norm in None layer : 0.027701380338594032
gradient norm in None layer : 0.0013777549234231506
gradient norm in None layer : 0.001290756781397629
gradient norm in None layer : 0.014780688756273367
gradient norm in None layer : 0.0004655292182873715
gradient norm in None layer : 0.0004062103788008226
gradient norm in None layer : 0.01369870160586308
gradient norm in None layer : 0.0006026344136651737
gradient norm in None layer : 0.0005368737711694188
gradient norm in None layer : 0.02134082744757519
gradient norm in None layer : 0.00019837034703306216
gradient norm in None layer : 0.03501883280067653
gradient norm in None layer : 0.0023221985378936243
gradient norm in None layer : 0.0015493179181741738
gradient norm in None layer : 0.05015147605981528
gradient norm in None layer : 0.00447412278188036
gradient norm in None layer : 0.002959267770510337
gradient norm in None layer : 0.0865703136995956
gradient norm in None layer : 0.0006289170283263
gradient norm in None layer : 0.14006405832299745
gradient norm in None layer : 0.010140293518869609
gradient norm in None layer : 0.008353382756586696
gradient norm in None layer : 0.15178867496821632
gradient norm in None layer : 0.023600060306713473
gradient norm in None layer : 0.03066840087162748
gradient norm in None layer : 0.01970399922743323
gradient norm in None layer : 0.0056397870343393415
Total gradient norm: 0.40413806700200305
invariance loss : 59.47858357060355, avg_den : 0.07586669921875, density loss : 0.041546630859375, mse loss : 0.13538581293452656, solver time : 57.12213468551636 sec , total loss : 0.2364110273645051, running loss : 0.3644369230431799
Epoch 0/10 , batch 6/12500 
ITERATION : 1, loss : 0.06468986490766493 ITERATION : 2, loss : 0.059916264700384185 ITERATION : 3, loss : 0.061615725117722404 ITERATION : 4, loss : 0.0636371989346591 ITERATION : 5, loss : 0.0649385815082377 ITERATION : 6, loss : 0.06598202166384709 ITERATION : 7, loss : 0.0667776884062173 ITERATION : 8, loss : 0.06736944116601802 ITERATION : 9, loss : 0.06780356888832495 ITERATION : 10, loss : 0.06811955295733954 ITERATION : 11, loss : 0.06834845643366878 ITERATION : 12, loss : 0.06851379252487083 ITERATION : 13, loss : 0.06863299468298355 ITERATION : 14, loss : 0.06871883594260747 ITERATION : 15, loss : 0.06878060765455171 ITERATION : 16, loss : 0.06882503923766632 ITERATION : 17, loss : 0.06885699050921093 ITERATION : 18, loss : 0.06887996417795787 ITERATION : 19, loss : 0.06889648271002459 ITERATION : 20, loss : 0.06890836018723383 ITERATION : 21, loss : 0.0689169016270473 ITERATION : 22, loss : 0.06892304473420993 ITERATION : 23, loss : 0.0689274637884669 ITERATION : 24, loss : 0.06893064321636762 ITERATION : 25, loss : 0.06893293129806698 
ITERATION : 1, loss : 0.03129494215359888 ITERATION : 2, loss : 0.03196564445089429 ITERATION : 3, loss : 0.03234310629008901 ITERATION : 4, loss : 0.03287693883068696 ITERATION : 5, loss : 0.033363785499170205 ITERATION : 6, loss : 0.03375275389440804 ITERATION : 7, loss : 0.034045929206278566 ITERATION : 8, loss : 0.034260070640799 ITERATION : 9, loss : 0.034413596394506155 ITERATION : 10, loss : 0.034522386041654377 ITERATION : 11, loss : 0.0345988928838316 ITERATION : 12, loss : 0.03465242649595246 ITERATION : 13, loss : 0.03468975835295393 ITERATION : 14, loss : 0.03471573182081553 ITERATION : 15, loss : 0.03473377415365608 ITERATION : 16, loss : 0.03474629350403694 ITERATION : 17, loss : 0.03475497394384403 ITERATION : 18, loss : 0.03476098946310218 ITERATION : 19, loss : 0.03476515665097049 ITERATION : 20, loss : 0.03476804267011195 ITERATION : 21, loss : 0.03477004102074994 
ITERATION : 1, loss : 0.1430730952565162 ITERATION : 2, loss : 0.1430730952565162 ITERATION : 3, loss : 0.1430730952565162 ITERATION : 4, loss : 0.1430730952565162 ITERATION : 5, loss : 0.1430730952565162 ITERATION : 6, loss : 0.1430730952565162 
ITERATION : 1, loss : 0.07391858437890848 ITERATION : 2, loss : 0.07675642577244367 ITERATION : 3, loss : 0.07753822728801898 ITERATION : 4, loss : 0.07786215774393249 ITERATION : 5, loss : 0.07801556601152773 ITERATION : 6, loss : 0.07809061849884885 ITERATION : 7, loss : 0.07812702820722596 ITERATION : 8, loss : 0.0781440617190321 ITERATION : 9, loss : 0.07815147177961425 ITERATION : 10, loss : 0.07815423828051549 ITERATION : 11, loss : 0.07815487986502366 ITERATION : 12, loss : 0.07815464064522883 ITERATION : 13, loss : 0.07815411098754628 
ITERATION : 1, loss : 0.10950049380674952 ITERATION : 2, loss : 0.10609601579926244 ITERATION : 3, loss : 0.104193545374365 ITERATION : 4, loss : 0.10290248150087598 ITERATION : 5, loss : 0.1020105871049399 ITERATION : 6, loss : 0.10139181170216012 ITERATION : 7, loss : 0.10096094147545118 ITERATION : 8, loss : 0.10065993763659883 ITERATION : 9, loss : 0.10044911595734518 ITERATION : 10, loss : 0.10030117494692899 ITERATION : 11, loss : 0.10019721743821158 ITERATION : 12, loss : 0.10012409701019596 ITERATION : 13, loss : 0.10007263263446242 ITERATION : 14, loss : 0.10003639425908631 ITERATION : 15, loss : 0.10001086965074166 ITERATION : 16, loss : 0.09999288791664544 ITERATION : 17, loss : 0.09998021828575646 ITERATION : 18, loss : 0.09997129088943173 ITERATION : 19, loss : 0.09996500012891056 ITERATION : 20, loss : 0.09996056714956536 ITERATION : 21, loss : 0.09995744322975439 ITERATION : 22, loss : 0.09995524193311583 
ITERATION : 1, loss : 0.03290543448965121 ITERATION : 2, loss : 0.03285368898897724 ITERATION : 3, loss : 0.03337715648952127 ITERATION : 4, loss : 0.03388391644653191 ITERATION : 5, loss : 0.03429453070783752 ITERATION : 6, loss : 0.034607320036805465 ITERATION : 7, loss : 0.0348383471655244 ITERATION : 8, loss : 0.03500591962921152 ITERATION : 9, loss : 0.03512607176307388 ITERATION : 10, loss : 0.035211564331384074 ITERATION : 11, loss : 0.03527207817316486 ITERATION : 12, loss : 0.03531475703284885 ITERATION : 13, loss : 0.03534478174480733 ITERATION : 14, loss : 0.03536586714750964 ITERATION : 15, loss : 0.03538065659497124 ITERATION : 16, loss : 0.03539102108428382 ITERATION : 17, loss : 0.03539828024922318 ITERATION : 18, loss : 0.0354033623639386 ITERATION : 19, loss : 0.03540691932219881 ITERATION : 20, loss : 0.03540940834492264 ITERATION : 21, loss : 0.03541114984626332 
ITERATION : 1, loss : 0.0728617288451548 ITERATION : 2, loss : 0.0728617288451548 ITERATION : 3, loss : 0.0728617288451548 ITERATION : 4, loss : 0.0728617288451548 ITERATION : 5, loss : 0.0728617288451548 ITERATION : 6, loss : 0.0728617288451548 
ITERATION : 1, loss : 0.044239741763914875 ITERATION : 2, loss : 0.045488475863731274 ITERATION : 3, loss : 0.046498250009133185 ITERATION : 4, loss : 0.04726462744751942 ITERATION : 5, loss : 0.047823194831413236 ITERATION : 6, loss : 0.04822408831444522 ITERATION : 7, loss : 0.04850988092772071 ITERATION : 8, loss : 0.04871287601312453 ITERATION : 9, loss : 0.04885672953652866 ITERATION : 10, loss : 0.04895851388422826 ITERATION : 11, loss : 0.04903045450096848 ITERATION : 12, loss : 0.04908126368306915 ITERATION : 13, loss : 0.04911712965606049 ITERATION : 14, loss : 0.04914243802636497 ITERATION : 15, loss : 0.04916029205919694 ITERATION : 16, loss : 0.049172885083253695 ITERATION : 17, loss : 0.04918176619181729 ITERATION : 18, loss : 0.04918802899954149 ITERATION : 19, loss : 0.04919244509746023 ITERATION : 20, loss : 0.049195558875869606 ITERATION : 21, loss : 0.04919775433793688 
gradient norm in None layer : 0.08770910319034729
gradient norm in None layer : 0.0026257608629644672
gradient norm in None layer : 0.002352519805303667
gradient norm in None layer : 0.0431785456827684
gradient norm in None layer : 0.0033331375612846814
gradient norm in None layer : 0.003563229585537982
gradient norm in None layer : 0.013677923429068909
gradient norm in None layer : 0.0005980637836495174
gradient norm in None layer : 0.00046301240718997445
gradient norm in None layer : 0.012844887391837916
gradient norm in None layer : 0.0007916938009759375
gradient norm in None layer : 0.0005731486619367502
gradient norm in None layer : 0.005980471712789388
gradient norm in None layer : 0.00019583133810248556
gradient norm in None layer : 0.00017127726559126158
gradient norm in None layer : 0.006652866863651062
gradient norm in None layer : 0.0003430165918362714
gradient norm in None layer : 0.0003008610574107204
gradient norm in None layer : 0.013310534893776777
gradient norm in None layer : 5.534747672427651e-05
gradient norm in None layer : 0.01987517869531816
gradient norm in None layer : 0.001372047361275197
gradient norm in None layer : 0.0008303608595417404
gradient norm in None layer : 0.02801490282313496
gradient norm in None layer : 0.0018763892601330794
gradient norm in None layer : 0.0011270867293433262
gradient norm in None layer : 0.03956387874183435
gradient norm in None layer : 0.0002210542776981777
gradient norm in None layer : 0.06278741832001168
gradient norm in None layer : 0.004648998608199556
gradient norm in None layer : 0.004436672514417365
gradient norm in None layer : 0.0708720469799316
gradient norm in None layer : 0.014227284736048984
gradient norm in None layer : 0.019380115161873233
gradient norm in None layer : 0.011993627502466539
gradient norm in None layer : 0.0037637899470390162
Total gradient norm: 0.15066380099018434
invariance loss : 35.38008295271687, avg_den : 0.06850433349609375, density loss : 0.03706817626953125, mse loss : 0.07279450669066878, solver time : 44.25962233543396 sec , total loss : 0.1452427659129169, running loss : 0.32790456352146935
Epoch 0/10 , batch 7/12500 
ITERATION : 1, loss : 0.1231182170690506 ITERATION : 2, loss : 0.1231182170690506 ITERATION : 3, loss : 0.1231182170690506 ITERATION : 4, loss : 0.1231182170690506 ITERATION : 5, loss : 0.1231182170690506 ITERATION : 6, loss : 0.1231182170690506 
ITERATION : 1, loss : 0.027550876320607882 ITERATION : 2, loss : 0.026855014445091554 ITERATION : 3, loss : 0.02712927158200069 ITERATION : 4, loss : 0.027585961893859534 ITERATION : 5, loss : 0.028017059302739296 ITERATION : 6, loss : 0.028366114987301138 ITERATION : 7, loss : 0.028630003442577736 ITERATION : 8, loss : 0.028822396239762484 ITERATION : 9, loss : 0.028959771268415133 ITERATION : 10, loss : 0.02905663857455388 ITERATION : 11, loss : 0.029124413205120086 ITERATION : 12, loss : 0.029171600179695358 ITERATION : 13, loss : 0.02920435040482646 ITERATION : 14, loss : 0.029227034984489652 ITERATION : 15, loss : 0.029242727307547726 ITERATION : 16, loss : 0.02925357367379904 ITERATION : 17, loss : 0.029261066751586313 ITERATION : 18, loss : 0.029266241438134572 ITERATION : 19, loss : 0.02926981445076492 ITERATION : 20, loss : 0.029272281290072158 ITERATION : 21, loss : 0.029273984321927553 
ITERATION : 1, loss : 0.037704334826347344 ITERATION : 2, loss : 0.038615008306790745 ITERATION : 3, loss : 0.04060765273086294 ITERATION : 4, loss : 0.04390810635255261 ITERATION : 5, loss : 0.04639335283815809 ITERATION : 6, loss : 0.04821324091140261 ITERATION : 7, loss : 0.0495267165288108 ITERATION : 8, loss : 0.05046660759296546 ITERATION : 9, loss : 0.051135520918977974 ITERATION : 10, loss : 0.0516098658117767 ITERATION : 11, loss : 0.05194540696227814 ITERATION : 12, loss : 0.052182350921993845 ITERATION : 13, loss : 0.052349462888812946 ITERATION : 14, loss : 0.05246721742270978 ITERATION : 15, loss : 0.05255013692772143 ITERATION : 16, loss : 0.05260849716958337 ITERATION : 17, loss : 0.05264955624522808 ITERATION : 18, loss : 0.05267843449268031 ITERATION : 19, loss : 0.052698740732342854 ITERATION : 20, loss : 0.05271301664486796 ITERATION : 21, loss : 0.052723051551996504 ITERATION : 22, loss : 0.05273010433706275 ITERATION : 23, loss : 0.05273506076688189 ITERATION : 24, loss : 0.052738543513736726 ITERATION : 25, loss : 0.05274099058699731 ITERATION : 26, loss : 0.05274270986186138 
ITERATION : 1, loss : 0.042213981493301266 ITERATION : 2, loss : 0.027986947540098495 ITERATION : 3, loss : 0.029484175752073603 ITERATION : 4, loss : 0.030359247194550854 ITERATION : 5, loss : 0.030932433505398958 ITERATION : 6, loss : 0.031313886488701106 ITERATION : 7, loss : 0.03156426762914159 ITERATION : 8, loss : 0.03172525056035035 ITERATION : 9, loss : 0.031826420449701895 ITERATION : 10, loss : 0.03188842082860983 ITERATION : 11, loss : 0.03192530578271951 ITERATION : 12, loss : 0.03194642536243264 ITERATION : 13, loss : 0.031957872738721985 ITERATION : 14, loss : 0.03196354318074035 ITERATION : 15, loss : 0.03196587956135144 ITERATION : 16, loss : 0.03196638250595719 ITERATION : 17, loss : 0.031965951738510634 ITERATION : 18, loss : 0.031965110306736516 
ITERATION : 1, loss : 0.12830115259071156 ITERATION : 2, loss : 0.12657866891803823 ITERATION : 3, loss : 0.1250442610544273 ITERATION : 4, loss : 0.12403367554234979 ITERATION : 5, loss : 0.12332846648596876 ITERATION : 6, loss : 0.12282023701857243 ITERATION : 7, loss : 0.12245207633453067 ITERATION : 8, loss : 0.12218692498722743 ITERATION : 9, loss : 0.12199759519438146 ITERATION : 10, loss : 0.12186352882555533 ITERATION : 11, loss : 0.12176927673362455 ITERATION : 12, loss : 0.1217034081969886 ITERATION : 13, loss : 0.12165759806865306 ITERATION : 14, loss : 0.12162586290551389 ITERATION : 15, loss : 0.1216039486121628 ITERATION : 16, loss : 0.12158885571759852 ITERATION : 17, loss : 0.12157848389343343 ITERATION : 18, loss : 0.12157136984478886 ITERATION : 19, loss : 0.12156649821913515 ITERATION : 20, loss : 0.1215631668987063 ITERATION : 21, loss : 0.12156089212234873 ITERATION : 22, loss : 0.12155934055737827 
ITERATION : 1, loss : 0.059543587044559344 ITERATION : 2, loss : 0.05999816475845896 ITERATION : 3, loss : 0.0603849992069447 ITERATION : 4, loss : 0.060691421304441384 ITERATION : 5, loss : 0.0609080145110138 ITERATION : 6, loss : 0.06105349353728067 ITERATION : 7, loss : 0.061149124560972615 ITERATION : 8, loss : 0.06121163741947298 ITERATION : 9, loss : 0.06125263847465549 ITERATION : 10, loss : 0.06127974861598854 ITERATION : 11, loss : 0.061290353555937656 ITERATION : 12, loss : 0.06129261822518785 ITERATION : 13, loss : 0.06129411537126362 ITERATION : 14, loss : 0.06129515372504199 ITERATION : 15, loss : 0.061295902681216284 ITERATION : 16, loss : 0.061296457946948504 
ITERATION : 1, loss : 0.17260906680905183 ITERATION : 2, loss : 0.17260906680905183 ITERATION : 3, loss : 0.17260906680905183 ITERATION : 4, loss : 0.17260906680905183 ITERATION : 5, loss : 0.17260906680905183 ITERATION : 6, loss : 0.17260906680905183 
ITERATION : 1, loss : 0.11885223207116095 ITERATION : 2, loss : 0.11491285526592342 ITERATION : 3, loss : 0.112582012035294 ITERATION : 4, loss : 0.11099642036516884 ITERATION : 5, loss : 0.10990701311240411 ITERATION : 6, loss : 0.10915730063111327 ITERATION : 7, loss : 0.10864088754807018 ITERATION : 8, loss : 0.10828460590778467 ITERATION : 9, loss : 0.10803825600284526 ITERATION : 10, loss : 0.10786749981806253 ITERATION : 11, loss : 0.1077488581998319 ITERATION : 12, loss : 0.10766624897667999 ITERATION : 13, loss : 0.10760862358226446 ITERATION : 14, loss : 0.10756836530891127 ITERATION : 15, loss : 0.10754020623919984 ITERATION : 16, loss : 0.10752049070823487 ITERATION : 17, loss : 0.10750667654273897 ITERATION : 18, loss : 0.10749699216757971 ITERATION : 19, loss : 0.10749019925624191 ITERATION : 20, loss : 0.107485432987496 ITERATION : 21, loss : 0.10748208842178927 ITERATION : 22, loss : 0.10747974046857711 
gradient norm in None layer : 0.061764973153919996
gradient norm in None layer : 0.0028432292342322617
gradient norm in None layer : 0.0020792647411094086
gradient norm in None layer : 0.03864604566680321
gradient norm in None layer : 0.002737945425970684
gradient norm in None layer : 0.0018548492489538627
gradient norm in None layer : 0.00927742099297797
gradient norm in None layer : 0.00043904585846867044
gradient norm in None layer : 0.0003535302668273916
gradient norm in None layer : 0.00838448560746094
gradient norm in None layer : 0.0004885435366672306
gradient norm in None layer : 0.00040485315188505824
gradient norm in None layer : 0.003585451069564018
gradient norm in None layer : 0.0001190135853705824
gradient norm in None layer : 0.00010326027697741443
gradient norm in None layer : 0.0029477555269941317
gradient norm in None layer : 0.00017478789303216103
gradient norm in None layer : 0.00015910603054980398
gradient norm in None layer : 0.005584345725903492
gradient norm in None layer : 3.974506205174755e-05
gradient norm in None layer : 0.009858898595854123
gradient norm in None layer : 0.0007175550093622284
gradient norm in None layer : 0.000425668999385038
gradient norm in None layer : 0.015825449642977732
gradient norm in None layer : 0.0011977182011531523
gradient norm in None layer : 0.0011004154305258587
gradient norm in None layer : 0.030480746014318966
gradient norm in None layer : 0.00013328707276464877
gradient norm in None layer : 0.0433794167530177
gradient norm in None layer : 0.00330587970702532
gradient norm in None layer : 0.0019377652562110833
gradient norm in None layer : 0.044898130522101704
gradient norm in None layer : 0.014245809473407202
gradient norm in None layer : 0.020887015600337414
gradient norm in None layer : 0.012758653116322792
gradient norm in None layer : 0.004500539207175733
Total gradient norm: 0.10750023919481455
invariance loss : 24.93913964497839, avg_den : 0.068878173828125, density loss : 0.031314086914062504, mse loss : 0.08750557841769147, solver time : 46.351598024368286 sec , total loss : 0.14375880497673238, running loss : 0.30159802658650686
Epoch 0/10 , batch 8/12500 
ITERATION : 1, loss : 0.037890260358791304 ITERATION : 2, loss : 0.03716632737250033 ITERATION : 3, loss : 0.0368099332640153 ITERATION : 4, loss : 0.0366219393310611 ITERATION : 5, loss : 0.036517493727420705 ITERATION : 6, loss : 0.036457023537886525 ITERATION : 7, loss : 0.036420657732038404 ITERATION : 8, loss : 0.036397979690511256 ITERATION : 9, loss : 0.03638334826665816 ITERATION : 10, loss : 0.03637361573788725 ITERATION : 11, loss : 0.036366971090134856 ITERATION : 12, loss : 0.03636233790901535 ITERATION : 13, loss : 0.03635905440967188 ITERATION : 14, loss : 0.03635669940906522 
ITERATION : 1, loss : 0.12895976828179204 ITERATION : 2, loss : 0.12895976828179204 ITERATION : 3, loss : 0.12895976828179204 ITERATION : 4, loss : 0.12895976828179204 ITERATION : 5, loss : 0.12895976828179204 ITERATION : 6, loss : 0.12895976828179204 
ITERATION : 1, loss : 0.07601248139163873 ITERATION : 2, loss : 0.07493255216317439 ITERATION : 3, loss : 0.07608452065090043 ITERATION : 4, loss : 0.07748859789415605 ITERATION : 5, loss : 0.07869242031271899 ITERATION : 6, loss : 0.07961996570474654 ITERATION : 7, loss : 0.08030120770413617 ITERATION : 8, loss : 0.08078925737790855 ITERATION : 9, loss : 0.08113408405211836 ITERATION : 10, loss : 0.08137576041141374 ITERATION : 11, loss : 0.08154432780862243 ITERATION : 12, loss : 0.08166155762749175 ITERATION : 13, loss : 0.08174293748185542 ITERATION : 14, loss : 0.0817993671428364 ITERATION : 15, loss : 0.08183846872007486 ITERATION : 16, loss : 0.08186555156258324 ITERATION : 17, loss : 0.08188430482418092 ITERATION : 18, loss : 0.08189728833847816 ITERATION : 19, loss : 0.08190627645146778 ITERATION : 20, loss : 0.08191249836993876 ITERATION : 21, loss : 0.08191680531650686 ITERATION : 22, loss : 0.08191978667566818 ITERATION : 23, loss : 0.08192185048614621 
ITERATION : 1, loss : 0.0356037528538794 ITERATION : 2, loss : 0.038010772317316054 ITERATION : 3, loss : 0.04028911889744574 ITERATION : 4, loss : 0.04221625341284506 ITERATION : 5, loss : 0.04373122621965525 ITERATION : 6, loss : 0.044922034251173984 ITERATION : 7, loss : 0.04582962917714045 ITERATION : 8, loss : 0.046500319715348404 ITERATION : 9, loss : 0.046991231347441646 ITERATION : 10, loss : 0.047348395537026355 ITERATION : 11, loss : 0.047607221846467394 ITERATION : 12, loss : 0.04779428188676672 ITERATION : 13, loss : 0.047929224079383444 ITERATION : 14, loss : 0.0480264430397584 ITERATION : 15, loss : 0.04809642028126651 ITERATION : 16, loss : 0.0481467564898412 ITERATION : 17, loss : 0.04818294753327752 ITERATION : 18, loss : 0.0482089597764563 ITERATION : 19, loss : 0.048227651575459544 ITERATION : 20, loss : 0.04824108067637797 ITERATION : 21, loss : 0.04825072756469798 ITERATION : 22, loss : 0.04825765680247091 ITERATION : 23, loss : 0.04826263361871567 ITERATION : 24, loss : 0.04826620794766324 ITERATION : 25, loss : 0.048268774916172395 
ITERATION : 1, loss : 0.03925036583683911 ITERATION : 2, loss : 0.03976998512504417 ITERATION : 3, loss : 0.0399052352616054 ITERATION : 4, loss : 0.0399593537203939 ITERATION : 5, loss : 0.0399845685999998 ITERATION : 6, loss : 0.039997956456129004 ITERATION : 7, loss : 0.04000585617755491 ITERATION : 8, loss : 0.04001087377994692 ITERATION : 9, loss : 0.0400142081495944 ITERATION : 10, loss : 0.04001648080202847 ITERATION : 11, loss : 0.040018050476948036 
ITERATION : 1, loss : 0.041774813473617105 ITERATION : 2, loss : 0.04722893303853695 ITERATION : 3, loss : 0.05021180603500577 ITERATION : 4, loss : 0.05275088453972001 ITERATION : 5, loss : 0.054807611669225584 ITERATION : 6, loss : 0.05642394474131815 ITERATION : 7, loss : 0.057665144264008185 ITERATION : 8, loss : 0.05860251212035749 ITERATION : 9, loss : 0.059302203008527725 ITERATION : 10, loss : 0.05982025152732984 ITERATION : 11, loss : 0.06020163905734082 ITERATION : 12, loss : 0.06048129665892912 ITERATION : 13, loss : 0.060685780076222585 ITERATION : 14, loss : 0.060834995797097856 ITERATION : 15, loss : 0.06094372493675485 ITERATION : 16, loss : 0.061022871053048336 ITERATION : 17, loss : 0.061080440398073955 ITERATION : 18, loss : 0.061122293006225424 ITERATION : 19, loss : 0.061152708087370646 ITERATION : 20, loss : 0.061174805252669384 ITERATION : 21, loss : 0.061190856245848274 ITERATION : 22, loss : 0.06120251383839879 ITERATION : 23, loss : 0.06121097977112315 ITERATION : 24, loss : 0.06121712746423378 ITERATION : 25, loss : 0.06122159154205168 ITERATION : 26, loss : 0.061224833003489916 ITERATION : 27, loss : 0.06122718669252838 
ITERATION : 1, loss : 0.054260789488197146 ITERATION : 2, loss : 0.055199769219660084 ITERATION : 3, loss : 0.056780418748873844 ITERATION : 4, loss : 0.05778075537484654 ITERATION : 5, loss : 0.05840083658751061 ITERATION : 6, loss : 0.05880374374790105 ITERATION : 7, loss : 0.05907483749949528 ITERATION : 8, loss : 0.05926079612611603 ITERATION : 9, loss : 0.059389635656510405 ITERATION : 10, loss : 0.05932472043118382 ITERATION : 11, loss : 0.05927300942871614 ITERATION : 12, loss : 0.05923735076045063 ITERATION : 13, loss : 0.05921262198745548 ITERATION : 14, loss : 0.05919540241500053 ITERATION : 15, loss : 0.05918337549110604 ITERATION : 16, loss : 0.05917495665904655 ITERATION : 17, loss : 0.059169053454987595 ITERATION : 18, loss : 0.059164909157902006 ITERATION : 19, loss : 0.059161996798203595 ITERATION : 20, loss : 0.05915994860374415 
ITERATION : 1, loss : 0.08959860037559585 ITERATION : 2, loss : 0.08347842836663343 ITERATION : 3, loss : 0.08069983499846607 ITERATION : 4, loss : 0.07929509445443093 ITERATION : 5, loss : 0.07854118097240986 ITERATION : 6, loss : 0.07811326745538012 ITERATION : 7, loss : 0.0778589205177289 ITERATION : 8, loss : 0.07770240964016252 ITERATION : 9, loss : 0.07760367264988752 ITERATION : 10, loss : 0.07754027352655808 ITERATION : 11, loss : 0.0774990505947837 ITERATION : 12, loss : 0.07747200434062926 ITERATION : 13, loss : 0.07745414308568024 ITERATION : 14, loss : 0.0774422913066712 ITERATION : 15, loss : 0.07743439993688539 ITERATION : 16, loss : 0.07742913279296627 ITERATION : 17, loss : 0.07742561127309495 ITERATION : 18, loss : 0.07742325413032362 ITERATION : 19, loss : 0.07742167561329524 
gradient norm in None layer : 0.06032163464596345
gradient norm in None layer : 0.0029104056215579683
gradient norm in None layer : 0.0027640864042283255
gradient norm in None layer : 0.04253834165451836
gradient norm in None layer : 0.0034945646673095734
gradient norm in None layer : 0.0022226902737805566
gradient norm in None layer : 0.009333990147102508
gradient norm in None layer : 0.0004011286453828486
gradient norm in None layer : 0.00031529324679448135
gradient norm in None layer : 0.007991483937026487
gradient norm in None layer : 0.0004678983399699472
gradient norm in None layer : 0.0003332892354880497
gradient norm in None layer : 0.002947785417209409
gradient norm in None layer : 8.880293518789405e-05
gradient norm in None layer : 7.26409352352457e-05
gradient norm in None layer : 0.002646476261498938
gradient norm in None layer : 0.00015290943439100828
gradient norm in None layer : 0.00013297836310447448
gradient norm in None layer : 0.005362624261932627
gradient norm in None layer : 4.14605550343358e-05
gradient norm in None layer : 0.009844313383469232
gradient norm in None layer : 0.0007295688070565905
gradient norm in None layer : 0.00045477470666354644
gradient norm in None layer : 0.01673903681584806
gradient norm in None layer : 0.001301724678296644
gradient norm in None layer : 0.0010187114029579785
gradient norm in None layer : 0.03186826703848076
gradient norm in None layer : 0.0001804130972760498
gradient norm in None layer : 0.058449002255222836
gradient norm in None layer : 0.004694666172424819
gradient norm in None layer : 0.002683352963803344
gradient norm in None layer : 0.06425838752374574
gradient norm in None layer : 0.012924853447638291
gradient norm in None layer : 0.01992590233580879
gradient norm in None layer : 0.01234305044506462
gradient norm in None layer : 0.004712883218780116
Total gradient norm: 0.12404465264693183
invariance loss : 22.921415892305305, avg_den : 0.0733489990234375, density loss : 0.038027954101562504, mse loss : 0.06666674430996146, solver time : 49.147345542907715 sec , total loss : 0.12761611430382927, running loss : 0.2798502875511722
Epoch 0/10 , batch 9/12500 
ITERATION : 1, loss : 0.10356165818959374 ITERATION : 2, loss : 0.10356165818959374 ITERATION : 3, loss : 0.10356165818959374 ITERATION : 4, loss : 0.10356165818959374 ITERATION : 5, loss : 0.10356165818959374 ITERATION : 6, loss : 0.10356165818959374 
ITERATION : 1, loss : 0.027536345382546287 ITERATION : 2, loss : 0.027795972925559412 ITERATION : 3, loss : 0.028438540444442526 ITERATION : 4, loss : 0.028934785917892017 ITERATION : 5, loss : 0.029289256461644667 ITERATION : 6, loss : 0.02954187857576516 ITERATION : 7, loss : 0.029721707840198692 ITERATION : 8, loss : 0.02984914875965046 ITERATION : 9, loss : 0.02993896554933458 ITERATION : 10, loss : 0.03000193981779187 ITERATION : 11, loss : 0.0300459043690887 ITERATION : 12, loss : 0.03007649444057211 ITERATION : 13, loss : 0.030097724463979524 ITERATION : 14, loss : 0.030112430673489326 ITERATION : 15, loss : 0.030122603732201308 ITERATION : 16, loss : 0.030129633963508498 ITERATION : 17, loss : 0.030134488846746618 ITERATION : 18, loss : 0.030137839758664582 ITERATION : 19, loss : 0.03014015180648931 ITERATION : 20, loss : 0.030141746657772785 
ITERATION : 1, loss : 0.08103743660505588 ITERATION : 2, loss : 0.07798874054842148 ITERATION : 3, loss : 0.07886825003239108 ITERATION : 4, loss : 0.08056812880826737 ITERATION : 5, loss : 0.08200006156021591 ITERATION : 6, loss : 0.08310051394303503 ITERATION : 7, loss : 0.08391262313112174 ITERATION : 8, loss : 0.08449944986518482 ITERATION : 9, loss : 0.0849183511427751 ITERATION : 10, loss : 0.08521510910642395 ITERATION : 11, loss : 0.08542427677368614 ITERATION : 12, loss : 0.0855711881087416 ITERATION : 13, loss : 0.08567410971142207 ITERATION : 14, loss : 0.08574607482803695 ITERATION : 15, loss : 0.08579631936901658 ITERATION : 16, loss : 0.08583135714724761 ITERATION : 17, loss : 0.08585576633442094 ITERATION : 18, loss : 0.08587275655874915 ITERATION : 19, loss : 0.08588457375970492 ITERATION : 20, loss : 0.0858927876556936 ITERATION : 21, loss : 0.08589849311146174 ITERATION : 22, loss : 0.08590245396482118 ITERATION : 23, loss : 0.0859052020557795 ITERATION : 24, loss : 0.0859071076017579 
ITERATION : 1, loss : 0.0844465417764963 ITERATION : 2, loss : 0.07973182691635966 ITERATION : 3, loss : 0.07713611519126802 ITERATION : 4, loss : 0.07555604478235736 ITERATION : 5, loss : 0.07454373193511753 ITERATION : 6, loss : 0.07387441061587093 ITERATION : 7, loss : 0.07342275601532006 ITERATION : 8, loss : 0.073113869582421 ITERATION : 9, loss : 0.07290072769496023 ITERATION : 10, loss : 0.07275276153185628 ITERATION : 11, loss : 0.07264961683662567 ITERATION : 12, loss : 0.07257750809382518 ITERATION : 13, loss : 0.07252699500261506 ITERATION : 14, loss : 0.07249155930029864 ITERATION : 15, loss : 0.07246667456663355 ITERATION : 16, loss : 0.07244918624996331 ITERATION : 17, loss : 0.07243688882661191 ITERATION : 18, loss : 0.07242823810272273 ITERATION : 19, loss : 0.07242215096319726 ITERATION : 20, loss : 0.07241786666802466 ITERATION : 21, loss : 0.07241485090005992 ITERATION : 22, loss : 0.0724127277108217 
ITERATION : 1, loss : 0.033596182957362615 ITERATION : 2, loss : 0.0354710664548351 ITERATION : 3, loss : 0.038071767690149236 ITERATION : 4, loss : 0.040467418214520205 ITERATION : 5, loss : 0.04235027922273023 ITERATION : 6, loss : 0.04377514626468004 ITERATION : 7, loss : 0.044832531747109584 ITERATION : 8, loss : 0.04560447029053858 ITERATION : 9, loss : 0.04616201052185779 ITERATION : 10, loss : 0.04656182275630356 ITERATION : 11, loss : 0.04684713581316784 ITERATION : 12, loss : 0.04705006056714334 ITERATION : 13, loss : 0.047194054059806936 ITERATION : 14, loss : 0.04729606582930353 ITERATION : 15, loss : 0.047368254250241014 ITERATION : 16, loss : 0.0474192978852174 ITERATION : 17, loss : 0.04745537018451502 ITERATION : 18, loss : 0.0474808524412624 ITERATION : 19, loss : 0.0474988487406013 ITERATION : 20, loss : 0.047511555835821 ITERATION : 21, loss : 0.04752052707337512 ITERATION : 22, loss : 0.047526860221575595 ITERATION : 23, loss : 0.04753133077537503 ITERATION : 24, loss : 0.047534486394274544 ITERATION : 25, loss : 0.0475367137939749 
ITERATION : 1, loss : 0.40944721380124116 ITERATION : 2, loss : 0.40944721380124116 ITERATION : 3, loss : 0.40944721380124116 ITERATION : 4, loss : 0.40944721380124116 ITERATION : 5, loss : 0.40944721380124116 ITERATION : 6, loss : 0.40944721380124116 
ITERATION : 1, loss : 0.02785140733505571 ITERATION : 2, loss : 0.026232451860263743 ITERATION : 3, loss : 0.0258385270986417 ITERATION : 4, loss : 0.025757736043794992 ITERATION : 5, loss : 0.025752378712287798 ITERATION : 6, loss : 0.0257624367604672 ITERATION : 7, loss : 0.025773113144236134 ITERATION : 8, loss : 0.02578143219915153 ITERATION : 9, loss : 0.02578738312474076 ITERATION : 10, loss : 0.025791531005642142 ITERATION : 11, loss : 0.02579441352908352 
ITERATION : 1, loss : 0.04321948818887701 ITERATION : 2, loss : 0.038896332712882765 ITERATION : 3, loss : 0.037534586550486235 ITERATION : 4, loss : 0.03690909311585125 ITERATION : 5, loss : 0.03659910784840175 ITERATION : 6, loss : 0.03644196606476718 ITERATION : 7, loss : 0.036360752995159076 ITERATION : 8, loss : 0.036317782793297254 ITERATION : 9, loss : 0.036294402658719006 ITERATION : 10, loss : 0.03628127992596849 ITERATION : 11, loss : 0.036273673559032174 ITERATION : 12, loss : 0.036269125786171856 ITERATION : 13, loss : 0.036266330390038626 ITERATION : 14, loss : 0.03626457213944314 ITERATION : 15, loss : 0.036263446639338205 
gradient norm in None layer : 0.10908455588405282
gradient norm in None layer : 0.003792606173544491
gradient norm in None layer : 0.00189860571286026
gradient norm in None layer : 0.056648480147997775
gradient norm in None layer : 0.004428995616629438
gradient norm in None layer : 0.00257918497006139
gradient norm in None layer : 0.008687297516340026
gradient norm in None layer : 0.0004115711252537371
gradient norm in None layer : 0.0003248327022408461
gradient norm in None layer : 0.007680212145530486
gradient norm in None layer : 0.0005017414779001554
gradient norm in None layer : 0.00037854782738658006
gradient norm in None layer : 0.0026944418843577074
gradient norm in None layer : 9.473255158591473e-05
gradient norm in None layer : 8.157355976557722e-05
gradient norm in None layer : 0.0024715467520998987
gradient norm in None layer : 0.0001382990134291083
gradient norm in None layer : 9.553906269220549e-05
gradient norm in None layer : 0.004853818601996621
gradient norm in None layer : 4.746370496180753e-05
gradient norm in None layer : 0.01014173632445761
gradient norm in None layer : 0.0007271891721752082
gradient norm in None layer : 0.0004576872695328576
gradient norm in None layer : 0.015864820386747774
gradient norm in None layer : 0.0013827281327175587
gradient norm in None layer : 0.0009618250190774156
gradient norm in None layer : 0.03401543292915005
gradient norm in None layer : 0.00017119560741009693
gradient norm in None layer : 0.0759794437147107
gradient norm in None layer : 0.0053569287274048425
gradient norm in None layer : 0.003193267848807907
gradient norm in None layer : 0.08220138341100643
gradient norm in None layer : 0.014789449735099974
gradient norm in None layer : 0.018378212030905863
gradient norm in None layer : 0.013607283154300086
gradient norm in None layer : 0.004158640951715001
Total gradient norm: 0.17368887110602066
invariance loss : 21.50333139650401, avg_den : 0.07405853271484375, density loss : 0.03560333251953125, mse loss : 0.10138312849044798, solver time : 40.674718141555786 sec , total loss : 0.15848979240648325, running loss : 0.2663657880906512
Epoch 0/10 , batch 10/12500 
ITERATION : 1, loss : 0.06505313950773857 ITERATION : 2, loss : 0.0688430843126051 ITERATION : 3, loss : 0.0695312540723598 ITERATION : 4, loss : 0.06956096268856848 ITERATION : 5, loss : 0.06947509785334352 ITERATION : 6, loss : 0.06939007781797654 ITERATION : 7, loss : 0.06932567234767889 ITERATION : 8, loss : 0.06928036179110983 ITERATION : 9, loss : 0.06924923272177838 ITERATION : 10, loss : 0.06922796818803431 ITERATION : 11, loss : 0.06921342248489491 ITERATION : 12, loss : 0.06920343379962014 ITERATION : 13, loss : 0.06919654418542162 ITERATION : 14, loss : 0.06919177260954407 ITERATION : 15, loss : 0.0691884568493906 ITERATION : 16, loss : 0.06918614598232327 
ITERATION : 1, loss : 0.06502725511240616 ITERATION : 2, loss : 0.0691728075191885 ITERATION : 3, loss : 0.07205734201567907 ITERATION : 4, loss : 0.0740355217781797 ITERATION : 5, loss : 0.07540801180897325 ITERATION : 6, loss : 0.07636520189362558 ITERATION : 7, loss : 0.07703332709844722 ITERATION : 8, loss : 0.07749946385181311 ITERATION : 9, loss : 0.07782446788742597 ITERATION : 10, loss : 0.0780509521286221 ITERATION : 11, loss : 0.07820872267430819 ITERATION : 12, loss : 0.07831859800165976 ITERATION : 13, loss : 0.07839510262840815 ITERATION : 14, loss : 0.07844836268845906 ITERATION : 15, loss : 0.07848543471109094 ITERATION : 16, loss : 0.07851123487057965 ITERATION : 17, loss : 0.0785291873196814 ITERATION : 18, loss : 0.07854167694691169 ITERATION : 19, loss : 0.07855036436792331 ITERATION : 20, loss : 0.0785564058659774 ITERATION : 21, loss : 0.07856060642860106 ITERATION : 22, loss : 0.07856352632538853 ITERATION : 23, loss : 0.07856555551394392 
ITERATION : 1, loss : 0.1352253963551481 ITERATION : 2, loss : 0.12060209070613769 ITERATION : 3, loss : 0.11355639225667209 ITERATION : 4, loss : 0.10967838493460177 ITERATION : 5, loss : 0.10814914328278731 ITERATION : 6, loss : 0.10825447092657006 ITERATION : 7, loss : 0.10830561366742321 ITERATION : 8, loss : 0.10833395625097524 ITERATION : 9, loss : 0.10835123086053255 ITERATION : 10, loss : 0.10836228439656612 ITERATION : 11, loss : 0.10836946875475915 ITERATION : 12, loss : 0.10837412760055683 ITERATION : 13, loss : 0.10837711579764849 ITERATION : 14, loss : 0.10837900323536002 ITERATION : 15, loss : 0.10838017370670837 
ITERATION : 1, loss : 0.07380135944829033 ITERATION : 2, loss : 0.06714763045822404 ITERATION : 3, loss : 0.0662455155847201 ITERATION : 4, loss : 0.06592265043561689 ITERATION : 5, loss : 0.06582157473881436 ITERATION : 6, loss : 0.06580770458346176 ITERATION : 7, loss : 0.06582571767343044 ITERATION : 8, loss : 0.06585255107741111 ITERATION : 9, loss : 0.065878936288898 ITERATION : 10, loss : 0.06590160693817987 ITERATION : 11, loss : 0.06591986672655849 ITERATION : 12, loss : 0.06593404593338618 ITERATION : 13, loss : 0.06594480960656747 ITERATION : 14, loss : 0.06595285962243111 ITERATION : 15, loss : 0.06595881898134161 ITERATION : 16, loss : 0.06596319920366521 ITERATION : 17, loss : 0.06596640193110971 ITERATION : 18, loss : 0.06596873493384751 
ITERATION : 1, loss : 0.16688471925842102 ITERATION : 2, loss : 0.16688471925842102 ITERATION : 3, loss : 0.16688471925842102 ITERATION : 4, loss : 0.16688471925842102 ITERATION : 5, loss : 0.16688471925842102 ITERATION : 6, loss : 0.16688471925842102 
ITERATION : 1, loss : 0.05055175472304374 ITERATION : 2, loss : 0.05354679648392316 ITERATION : 3, loss : 0.05496370537320759 ITERATION : 4, loss : 0.05601891726792155 ITERATION : 5, loss : 0.05681273551289377 ITERATION : 6, loss : 0.05739802749960503 ITERATION : 7, loss : 0.05772635775291418 ITERATION : 8, loss : 0.05762022211662714 ITERATION : 9, loss : 0.05754238750084362 ITERATION : 10, loss : 0.05748618220540603 ITERATION : 11, loss : 0.05744600669290022 ITERATION : 12, loss : 0.05741748943870494 ITERATION : 13, loss : 0.057397348026245736 ITERATION : 14, loss : 0.05738317505040864 ITERATION : 15, loss : 0.05737322846265011 ITERATION : 16, loss : 0.05736626288613936 ITERATION : 17, loss : 0.05736139353433796 ITERATION : 18, loss : 0.05735799352465174 ITERATION : 19, loss : 0.057355621581462346 
ITERATION : 1, loss : 0.06217042028770385 ITERATION : 2, loss : 0.06123420096766332 ITERATION : 3, loss : 0.06039877673393566 ITERATION : 4, loss : 0.05989982033739023 ITERATION : 5, loss : 0.05959624586820787 ITERATION : 6, loss : 0.05940451732220511 ITERATION : 7, loss : 0.05927962883590473 ITERATION : 8, loss : 0.059196430586453214 ITERATION : 9, loss : 0.05914013386438544 ITERATION : 10, loss : 0.05910163210949891 ITERATION : 11, loss : 0.05907510708292596 ITERATION : 12, loss : 0.0590567409031259 ITERATION : 13, loss : 0.059043978703967964 ITERATION : 14, loss : 0.05903508839173354 ITERATION : 15, loss : 0.05902888390327952 ITERATION : 16, loss : 0.05902454822012835 ITERATION : 17, loss : 0.059021515393601925 ITERATION : 18, loss : 0.05901939245993732 
ITERATION : 1, loss : 0.31318360528355593 ITERATION : 2, loss : 0.3189238661080203 ITERATION : 3, loss : 0.32083697773343706 ITERATION : 4, loss : 0.3218771787680914 ITERATION : 5, loss : 0.32250036104732627 ITERATION : 6, loss : 0.3228889972878085 ITERATION : 7, loss : 0.32313751298386767 ITERATION : 8, loss : 0.3232994786427568 ITERATION : 9, loss : 0.3234066967814962 ITERATION : 10, loss : 0.3234786050958985 ITERATION : 11, loss : 0.3235273589621529 ITERATION : 12, loss : 0.32356071174294465 ITERATION : 13, loss : 0.3235836961053904 ITERATION : 14, loss : 0.32359962951050586 ITERATION : 15, loss : 0.32361072775366595 ITERATION : 16, loss : 0.32361848753203304 ITERATION : 17, loss : 0.3236239295594843 ITERATION : 18, loss : 0.32362775526640997 ITERATION : 19, loss : 0.32363044979522165 ITERATION : 20, loss : 0.32363235056399225 
gradient norm in None layer : 0.056771381978053856
gradient norm in None layer : 0.0018855029601731388
gradient norm in None layer : 0.001858662816259672
gradient norm in None layer : 0.028292606478384493
gradient norm in None layer : 0.002244955993679998
gradient norm in None layer : 0.002012771751438118
gradient norm in None layer : 0.01056701532816041
gradient norm in None layer : 0.0004001386586385599
gradient norm in None layer : 0.0005812204244223107
gradient norm in None layer : 0.008302375122850682
gradient norm in None layer : 0.00043662010756742256
gradient norm in None layer : 0.00046066910828691847
gradient norm in None layer : 0.003941276471908939
gradient norm in None layer : 0.00015116172822315398
gradient norm in None layer : 0.00012097913654287753
gradient norm in None layer : 0.003403852728626452
gradient norm in None layer : 0.00023314295538825718
gradient norm in None layer : 0.0001236109340701472
gradient norm in None layer : 0.0063035675903858725
gradient norm in None layer : 4.0281846330931304e-05
gradient norm in None layer : 0.009458434568349782
gradient norm in None layer : 0.0006936582842542976
gradient norm in None layer : 0.00036293105910721916
gradient norm in None layer : 0.017829285112421628
gradient norm in None layer : 0.0018523219087877134
gradient norm in None layer : 0.0008803733626195717
gradient norm in None layer : 0.04688740003554976
gradient norm in None layer : 0.00035167841070074643
gradient norm in None layer : 0.0469895567163333
gradient norm in None layer : 0.003664098595476094
gradient norm in None layer : 0.00194276379848865
gradient norm in None layer : 0.05951494700900255
gradient norm in None layer : 0.008911617714808773
gradient norm in None layer : 0.012562200527156958
gradient norm in None layer : 0.008414736956184528
gradient norm in None layer : 0.002946480463727315
Total gradient norm: 0.11394163804156748
invariance loss : 16.34075880770895, avg_den : 0.0771636962890625, density loss : 0.029171752929687503, mse loss : 0.11612408675007951, solver time : 49.741925954818726 sec , total loss : 0.16163659848747597, running loss : 0.2558928691303337
saving checkpoint
Epoch 0/10 , batch 11/12500 
ITERATION : 1, loss : 0.15453637048913335 ITERATION : 2, loss : 0.15182995068228824 ITERATION : 3, loss : 0.14914342713716702 ITERATION : 4, loss : 0.14711668605741146 ITERATION : 5, loss : 0.14568185858238283 ITERATION : 6, loss : 0.14468945286099533 ITERATION : 7, loss : 0.1440084700468244 ITERATION : 8, loss : 0.14354185597379723 ITERATION : 9, loss : 0.1432217341832812 ITERATION : 10, loss : 0.14300163618080827 ITERATION : 11, loss : 0.14284995683086002 ITERATION : 12, loss : 0.14274520399813342 ITERATION : 13, loss : 0.14267272657789806 ITERATION : 14, loss : 0.14262250390211817 ITERATION : 15, loss : 0.14258765969724005 ITERATION : 16, loss : 0.14256346145116042 ITERATION : 17, loss : 0.14254664347958917 ITERATION : 18, loss : 0.142534947870713 ITERATION : 19, loss : 0.14252681054110597 ITERATION : 20, loss : 0.1425211468626633 ITERATION : 21, loss : 0.14251720372178608 ITERATION : 22, loss : 0.1425144577671621 ITERATION : 23, loss : 0.14251254516267997 
ITERATION : 1, loss : 0.0646364265592944 ITERATION : 2, loss : 0.061741396547623166 ITERATION : 3, loss : 0.06081180216935996 ITERATION : 4, loss : 0.06057815221364503 ITERATION : 5, loss : 0.060572651663744544 ITERATION : 6, loss : 0.06063398554162246 ITERATION : 7, loss : 0.0607052288521324 ITERATION : 8, loss : 0.06070553484331252 ITERATION : 9, loss : 0.06068743655382259 ITERATION : 10, loss : 0.06067628587619367 ITERATION : 11, loss : 0.0606691827901957 ITERATION : 12, loss : 0.06066452431912297 ITERATION : 13, loss : 0.0606613952551685 
ITERATION : 1, loss : 0.19614259998896805 ITERATION : 2, loss : 0.19571759470494232 ITERATION : 3, loss : 0.19584834134404358 ITERATION : 4, loss : 0.19519979090426212 ITERATION : 5, loss : 0.19452283770743592 ITERATION : 6, loss : 0.19398766765992623 ITERATION : 7, loss : 0.19359996261674753 ITERATION : 8, loss : 0.19332831884852686 ITERATION : 9, loss : 0.19314033439625403 ITERATION : 10, loss : 0.1930106669462475 ITERATION : 11, loss : 0.1929211630364057 ITERATION : 12, loss : 0.19285924512480995 ITERATION : 13, loss : 0.19281629962264699 ITERATION : 14, loss : 0.19278643960125882 ITERATION : 15, loss : 0.1927656336787718 ITERATION : 16, loss : 0.19275111108587167 ITERATION : 17, loss : 0.19274095996187343 ITERATION : 18, loss : 0.19273385682295782 ITERATION : 19, loss : 0.19272888208216493 ITERATION : 20, loss : 0.1927253957432594 ITERATION : 21, loss : 0.19272295135981862 ITERATION : 22, loss : 0.19272123680577782 
ITERATION : 1, loss : 0.07757413437088118 ITERATION : 2, loss : 0.07314189673415872 ITERATION : 3, loss : 0.07314189673415872 ITERATION : 4, loss : 0.07314189673415872 ITERATION : 5, loss : 0.07314189673415872 ITERATION : 6, loss : 0.07314189673415872 ITERATION : 7, loss : 0.07314189673415872 
ITERATION : 1, loss : 0.06021422440855988 ITERATION : 2, loss : 0.06048334072974971 ITERATION : 3, loss : 0.06048334072974971 ITERATION : 4, loss : 0.06048334072974971 ITERATION : 5, loss : 0.06048334072974971 ITERATION : 6, loss : 0.06048334072974971 ITERATION : 7, loss : 0.06048334072974971 
ITERATION : 1, loss : 0.0994749102751048 ITERATION : 2, loss : 0.0994749102751048 ITERATION : 3, loss : 0.0994749102751048 ITERATION : 4, loss : 0.0994749102751048 ITERATION : 5, loss : 0.0994749102751048 ITERATION : 6, loss : 0.0994749102751048 
ITERATION : 1, loss : 0.0853640771856492 ITERATION : 2, loss : 0.0871989328274683 ITERATION : 3, loss : 0.09006289607039017 ITERATION : 4, loss : 0.09255185377364922 ITERATION : 5, loss : 0.09441083488351099 ITERATION : 6, loss : 0.09576693545080925 ITERATION : 7, loss : 0.09674537938830904 ITERATION : 8, loss : 0.09744691500035 ITERATION : 9, loss : 0.09794791286752451 ITERATION : 10, loss : 0.09830475883725899 ITERATION : 11, loss : 0.09855848190920768 ITERATION : 12, loss : 0.09873866821734582 ITERATION : 13, loss : 0.098866528814275 ITERATION : 14, loss : 0.09895721027030592 ITERATION : 15, loss : 0.09902150071215156 ITERATION : 16, loss : 0.09906706983298606 ITERATION : 17, loss : 0.0990993644619968 ITERATION : 18, loss : 0.09912224923511777 ITERATION : 19, loss : 0.0991384650778667 ITERATION : 20, loss : 0.09914995491770254 ITERATION : 21, loss : 0.09915809602642893 ITERATION : 22, loss : 0.09916386423424144 ITERATION : 23, loss : 0.09916795122915212 ITERATION : 24, loss : 0.09917084695224775 ITERATION : 25, loss : 0.09917289867280113 
ITERATION : 1, loss : 0.06869003066624119 ITERATION : 2, loss : 0.06687258020617731 ITERATION : 3, loss : 0.06629708750041485 ITERATION : 4, loss : 0.06615518505200399 ITERATION : 5, loss : 0.06616223119054347 ITERATION : 6, loss : 0.06621383087730848 ITERATION : 7, loss : 0.06627131470602063 ITERATION : 8, loss : 0.06632156870403975 ITERATION : 9, loss : 0.06636148980107205 ITERATION : 10, loss : 0.06639167793026393 ITERATION : 11, loss : 0.06641385627605086 ITERATION : 12, loss : 0.06642985472854165 ITERATION : 13, loss : 0.06644125544772057 ITERATION : 14, loss : 0.06644931178816418 ITERATION : 15, loss : 0.06645497082445528 ITERATION : 16, loss : 0.06645892869399331 ITERATION : 17, loss : 0.06646168798261097 
gradient norm in None layer : 0.02788600903537334
gradient norm in None layer : 0.0010296901721319619
gradient norm in None layer : 0.0009471454115793937
gradient norm in None layer : 0.016441921297937777
gradient norm in None layer : 0.0017061471897386398
gradient norm in None layer : 0.0014063699684608607
gradient norm in None layer : 0.00557449612858695
gradient norm in None layer : 0.00024388945831842605
gradient norm in None layer : 0.0002376137523687721
gradient norm in None layer : 0.0049126048607512586
gradient norm in None layer : 0.00030060797618443303
gradient norm in None layer : 0.00022987623097148467
gradient norm in None layer : 0.002278035989859845
gradient norm in None layer : 9.412481550244016e-05
gradient norm in None layer : 7.129483476459887e-05
gradient norm in None layer : 0.002611061182698453
gradient norm in None layer : 0.0001655952766288183
gradient norm in None layer : 0.00010443915663255947
gradient norm in None layer : 0.00646296519708867
gradient norm in None layer : 2.7277565922804136e-05
gradient norm in None layer : 0.008023889065548894
gradient norm in None layer : 0.0006182244687308202
gradient norm in None layer : 0.000286667500493897
gradient norm in None layer : 0.014021021246688702
gradient norm in None layer : 0.0013226160725921456
gradient norm in None layer : 0.0006085815323128429
gradient norm in None layer : 0.03180673803107097
gradient norm in None layer : 0.0001217039964078024
gradient norm in None layer : 0.03463098574779041
gradient norm in None layer : 0.002492879823146157
gradient norm in None layer : 0.0015119035432237327
gradient norm in None layer : 0.04377386246200555
gradient norm in None layer : 0.007173548490694717
gradient norm in None layer : 0.00934954125895491
gradient norm in None layer : 0.006375388329161096
gradient norm in None layer : 0.002095855548923252
Total gradient norm: 0.07581040835528993
invariance loss : 13.908326961533046, avg_den : 0.0810089111328125, density loss : 0.035540771484375004, mse loss : 0.09932873895225644, solver time : 189.4365270137787 sec , total loss : 0.1487778373981645, running loss : 0.24615513897286379
Epoch 0/10 , batch 12/12500 
ITERATION : 1, loss : 0.04598137070423648 ITERATION : 2, loss : 0.0445892921439649 ITERATION : 3, loss : 0.043908181065144075 ITERATION : 4, loss : 0.04348641431561578 ITERATION : 5, loss : 0.043207804686941124 ITERATION : 6, loss : 0.04301886001298764 ITERATION : 7, loss : 0.042888809693590095 ITERATION : 8, loss : 0.04279838874818931 ITERATION : 9, loss : 0.04273505825635498 ITERATION : 10, loss : 0.04269046181027123 ITERATION : 11, loss : 0.04265893344567919 ITERATION : 12, loss : 0.0426365799570781 ITERATION : 13, loss : 0.0426206986329588 ITERATION : 14, loss : 0.04260939880470837 ITERATION : 15, loss : 0.04260135020203826 ITERATION : 16, loss : 0.04259561297586358 ITERATION : 17, loss : 0.04259152107899083 ITERATION : 18, loss : 0.04258860148545336 ITERATION : 19, loss : 0.04258651772066065 
ITERATION : 1, loss : 0.017772810567047364 ITERATION : 2, loss : 0.017752124218099827 ITERATION : 3, loss : 0.01762506418153516 ITERATION : 4, loss : 0.017493608852922813 ITERATION : 5, loss : 0.0173945459145944 ITERATION : 6, loss : 0.017326138360813956 ITERATION : 7, loss : 0.017279890259134973 ITERATION : 8, loss : 0.017248589619888312 ITERATION : 9, loss : 0.01722723453721921 ITERATION : 10, loss : 0.01721253114008819 ITERATION : 11, loss : 0.017202325278329714 ITERATION : 12, loss : 0.017195195191496895 ITERATION : 13, loss : 0.017190189510655953 ITERATION : 14, loss : 0.01718666275705255 ITERATION : 15, loss : 0.017184171676897897 ITERATION : 16, loss : 0.01718240903814316 
ITERATION : 1, loss : 0.1366812037019761 ITERATION : 2, loss : 0.1366812037019761 ITERATION : 3, loss : 0.1366812037019761 ITERATION : 4, loss : 0.1366812037019761 ITERATION : 5, loss : 0.1366812037019761 ITERATION : 6, loss : 0.1366812037019761 
ITERATION : 1, loss : 0.037075865629501936 ITERATION : 2, loss : 0.038458852264250006 ITERATION : 3, loss : 0.038485982609863606 ITERATION : 4, loss : 0.03852910689077139 ITERATION : 5, loss : 0.03859061064287952 ITERATION : 6, loss : 0.0386464988165103 ITERATION : 7, loss : 0.03869001275663571 ITERATION : 8, loss : 0.03872182868497834 ITERATION : 9, loss : 0.03874446787579226 ITERATION : 10, loss : 0.03876038986956972 ITERATION : 11, loss : 0.03877153538461208 ITERATION : 12, loss : 0.03877932543501215 ITERATION : 13, loss : 0.038784769203642495 ITERATION : 14, loss : 0.038788575183811666 ITERATION : 15, loss : 0.03879123722442482 ITERATION : 16, loss : 0.03879310058475717 
ITERATION : 1, loss : 0.04317029905555868 ITERATION : 2, loss : 0.04194405971965781 ITERATION : 3, loss : 0.04152683371900246 ITERATION : 4, loss : 0.041324846724510804 ITERATION : 5, loss : 0.04122838364825105 ITERATION : 6, loss : 0.041187167927375846 ITERATION : 7, loss : 0.04117280631620335 ITERATION : 8, loss : 0.041170237884201384 ITERATION : 9, loss : 0.041172100308055404 ITERATION : 10, loss : 0.04117509359374456 ITERATION : 11, loss : 0.04117790848181281 ITERATION : 12, loss : 0.04118014262595315 
ITERATION : 1, loss : 0.11348051572863242 ITERATION : 2, loss : 0.11833557662936733 ITERATION : 3, loss : 0.12254522880895206 ITERATION : 4, loss : 0.1257734550417038 ITERATION : 5, loss : 0.12810122668080176 ITERATION : 6, loss : 0.12974280470029667 ITERATION : 7, loss : 0.1308917978514744 ITERATION : 8, loss : 0.13169450655252118 ITERATION : 9, loss : 0.1322555080794981 ITERATION : 10, loss : 0.13264808552072999 ITERATION : 11, loss : 0.1329232416401252 ITERATION : 12, loss : 0.13311641766011925 ITERATION : 13, loss : 0.13325225822263875 ITERATION : 14, loss : 0.13334792840595072 ITERATION : 15, loss : 0.13341540558608936 ITERATION : 16, loss : 0.13346306437531522 ITERATION : 17, loss : 0.13349677004859503 ITERATION : 18, loss : 0.13352063837361117 ITERATION : 19, loss : 0.13353756122367336 ITERATION : 20, loss : 0.13354957441781146 ITERATION : 21, loss : 0.13355811237158455 ITERATION : 22, loss : 0.13356418745525198 ITERATION : 23, loss : 0.1335685152505176 ITERATION : 24, loss : 0.13357160153083422 ITERATION : 25, loss : 0.13357380512871156 
ITERATION : 1, loss : 0.05366665136736946 ITERATION : 2, loss : 0.061750040522725876 ITERATION : 3, loss : 0.06118408265758662 ITERATION : 4, loss : 0.0604070501801786 ITERATION : 5, loss : 0.05998743619310273 ITERATION : 6, loss : 0.05982570702074249 ITERATION : 7, loss : 0.05979443166278838 ITERATION : 8, loss : 0.05981734646109464 ITERATION : 9, loss : 0.05985653462605827 ITERATION : 10, loss : 0.05989541502124021 ITERATION : 11, loss : 0.0599280138904349 ITERATION : 12, loss : 0.05995317403218647 ITERATION : 13, loss : 0.05997166139017208 ITERATION : 14, loss : 0.05998480939413032 ITERATION : 15, loss : 0.059993943359714176 ITERATION : 16, loss : 0.06000017676492775 ITERATION : 17, loss : 0.06000436896157157 ITERATION : 18, loss : 0.06000715336724093 ITERATION : 19, loss : 0.060008981825528454 
ITERATION : 1, loss : 0.05367667829297602 ITERATION : 2, loss : 0.05033236095176798 ITERATION : 3, loss : 0.04895291277894576 ITERATION : 4, loss : 0.04819840982554726 ITERATION : 5, loss : 0.047734936341843365 ITERATION : 6, loss : 0.04743217332723253 ITERATION : 7, loss : 0.047227781148691625 ITERATION : 8, loss : 0.04708737076565064 ITERATION : 9, loss : 0.046990018677765893 ITERATION : 10, loss : 0.04692218919095025 ITERATION : 11, loss : 0.04687480686202379 ITERATION : 12, loss : 0.04684166304723272 ITERATION : 13, loss : 0.04681846299227422 ITERATION : 14, loss : 0.046802217789589914 ITERATION : 15, loss : 0.04679084074594116 ITERATION : 16, loss : 0.046782872555659126 ITERATION : 17, loss : 0.046777291595668014 ITERATION : 18, loss : 0.04677338300018358 ITERATION : 19, loss : 0.04677064546445921 ITERATION : 20, loss : 0.04676872823138462 
gradient norm in None layer : 0.0207133926349504
gradient norm in None layer : 0.0006557628130964619
gradient norm in None layer : 0.0007535497863151245
gradient norm in None layer : 0.010988490888617216
gradient norm in None layer : 0.0007235860248857961
gradient norm in None layer : 0.0009440069293005795
gradient norm in None layer : 0.0037217262095807237
gradient norm in None layer : 0.00015611063337997157
gradient norm in None layer : 0.00018644505107047061
gradient norm in None layer : 0.00307311884798908
gradient norm in None layer : 0.00018102229191845233
gradient norm in None layer : 0.00016859416741080084
gradient norm in None layer : 0.0011918719438848123
gradient norm in None layer : 3.57079720481776e-05
gradient norm in None layer : 4.159762176172593e-05
gradient norm in None layer : 0.0009735117871935422
gradient norm in None layer : 5.090941963175376e-05
gradient norm in None layer : 4.846340975232431e-05
gradient norm in None layer : 0.0017246778871947053
gradient norm in None layer : 2.700502186931497e-05
gradient norm in None layer : 0.0031380679799682416
gradient norm in None layer : 0.00028975357673155424
gradient norm in None layer : 0.00016893392589149435
gradient norm in None layer : 0.005386358126925349
gradient norm in None layer : 0.0004761830928539627
gradient norm in None layer : 0.0003749698045997651
gradient norm in None layer : 0.011265510759250773
gradient norm in None layer : 9.337121346691354e-05
gradient norm in None layer : 0.015630512374939086
gradient norm in None layer : 0.0014326812260182836
gradient norm in None layer : 0.000974946305604302
gradient norm in None layer : 0.021168562951636914
gradient norm in None layer : 0.005426887892759552
gradient norm in None layer : 0.009439065993418363
gradient norm in None layer : 0.004920121223170064
gradient norm in None layer : 0.002249093814693197
Total gradient norm: 0.03988068513936628
invariance loss : 12.415393450910406, avg_den : 0.07621002197265625, density loss : 0.030171203613281253, mse loss : 0.06459686110713936, solver time : 52.20230174064636 sec , total loss : 0.10718345817133101, running loss : 0.23457416557273605
Epoch 0/10 , batch 13/12500 
ITERATION : 1, loss : 0.09319214835735419 ITERATION : 2, loss : 0.09375956249520034 ITERATION : 3, loss : 0.09473247149573298 ITERATION : 4, loss : 0.09560501291189584 ITERATION : 5, loss : 0.09629490860091064 ITERATION : 6, loss : 0.09681235165919862 ITERATION : 7, loss : 0.09718912199434343 ITERATION : 8, loss : 0.09745832288049171 ITERATION : 9, loss : 0.0976481959927062 ITERATION : 10, loss : 0.09778089091474822 ITERATION : 11, loss : 0.09787300214292896 ITERATION : 12, loss : 0.09793661629417424 ITERATION : 13, loss : 0.09798037560930656 ITERATION : 14, loss : 0.09801038085487902 ITERATION : 15, loss : 0.0980309001732134 ITERATION : 16, loss : 0.09804489996975 ITERATION : 17, loss : 0.0980544316915175 ITERATION : 18, loss : 0.09806090864168217 ITERATION : 19, loss : 0.09806530152044228 ITERATION : 20, loss : 0.09806827532600958 ITERATION : 21, loss : 0.09807028451888146 
ITERATION : 1, loss : 0.1028717191547698 ITERATION : 2, loss : 0.10220261562045865 ITERATION : 3, loss : 0.10324491826944351 ITERATION : 4, loss : 0.10430046693476747 ITERATION : 5, loss : 0.10514455004734276 ITERATION : 6, loss : 0.10577637583904363 ITERATION : 7, loss : 0.10616630407026119 ITERATION : 8, loss : 0.10628655126839419 ITERATION : 9, loss : 0.10637373246856736 ITERATION : 10, loss : 0.10643620012270277 ITERATION : 11, loss : 0.10648065087437675 ITERATION : 12, loss : 0.10651214325251837 ITERATION : 13, loss : 0.10653439063440369 ITERATION : 14, loss : 0.10655007565250284 ITERATION : 15, loss : 0.1065611184242031 ITERATION : 16, loss : 0.10656888492799915 ITERATION : 17, loss : 0.10657434306747081 ITERATION : 18, loss : 0.10657817680503666 ITERATION : 19, loss : 0.10658086836291858 ITERATION : 20, loss : 0.10658275738786752 
ITERATION : 1, loss : 0.02927767926105764 ITERATION : 2, loss : 0.029695132435487305 ITERATION : 3, loss : 0.030077251008769287 ITERATION : 4, loss : 0.03032052314797112 ITERATION : 5, loss : 0.030475131808479522 ITERATION : 6, loss : 0.030573389163459513 ITERATION : 7, loss : 0.03063593617429597 ITERATION : 8, loss : 0.030676008528030367 ITERATION : 9, loss : 0.030701945934471613 ITERATION : 10, loss : 0.030718940324921784 ITERATION : 11, loss : 0.030730216772752728 ITERATION : 12, loss : 0.030737789651233943 ITERATION : 13, loss : 0.030742930574827177 ITERATION : 14, loss : 0.030746453097223434 ITERATION : 15, loss : 0.03074888566165427 ITERATION : 16, loss : 0.030750576084141352 
ITERATION : 1, loss : 0.08472944766578845 ITERATION : 2, loss : 0.08403653990045247 ITERATION : 3, loss : 0.08564671478792842 ITERATION : 4, loss : 0.08732208009598733 ITERATION : 5, loss : 0.08871563147024703 ITERATION : 6, loss : 0.08979839493090132 ITERATION : 7, loss : 0.09061259288280431 ITERATION : 8, loss : 0.09121319859384493 ITERATION : 9, loss : 0.09165080461940484 ITERATION : 10, loss : 0.09196700106669922 ITERATION : 11, loss : 0.09219415526263275 ITERATION : 12, loss : 0.09235667850806617 ITERATION : 13, loss : 0.09247262275451783 ITERATION : 14, loss : 0.09255516530905222 ITERATION : 15, loss : 0.0926138404123303 ITERATION : 16, loss : 0.0926555042545192 ITERATION : 17, loss : 0.09268506525871062 ITERATION : 18, loss : 0.09270602733733609 ITERATION : 19, loss : 0.092720885783881 ITERATION : 20, loss : 0.0927314148902681 ITERATION : 21, loss : 0.09273887427189344 ITERATION : 22, loss : 0.0927441581952214 ITERATION : 23, loss : 0.09274790072659153 ITERATION : 24, loss : 0.09275055136456412 ITERATION : 25, loss : 0.09275242856125188 
ITERATION : 1, loss : 0.18505091486162648 ITERATION : 2, loss : 0.18304847059025062 ITERATION : 3, loss : 0.18261087735202627 ITERATION : 4, loss : 0.18241668598993685 ITERATION : 5, loss : 0.18231991348490273 ITERATION : 6, loss : 0.18227413797114772 ITERATION : 7, loss : 0.18225522395884702 ITERATION : 8, loss : 0.18224973218180385 ITERATION : 9, loss : 0.1822503918677639 ITERATION : 10, loss : 0.18225344924809075 ITERATION : 11, loss : 0.18225708212556557 ITERATION : 12, loss : 0.18226048597603423 
ITERATION : 1, loss : 0.1238870317326635 ITERATION : 2, loss : 0.1180727961222094 ITERATION : 3, loss : 0.11660733677258144 ITERATION : 4, loss : 0.11608731944210958 ITERATION : 5, loss : 0.11589238910380013 ITERATION : 6, loss : 0.11583748904978537 ITERATION : 7, loss : 0.11584537663232755 ITERATION : 8, loss : 0.11587902222102238 ITERATION : 9, loss : 0.11592024233471682 ITERATION : 10, loss : 0.11596046316291461 ITERATION : 11, loss : 0.11599608013898849 ITERATION : 12, loss : 0.11602600146234597 ITERATION : 13, loss : 0.11605033773170226 ITERATION : 14, loss : 0.11606971171394838 ITERATION : 15, loss : 0.11608490616997116 ITERATION : 16, loss : 0.11609669378757034 ITERATION : 17, loss : 0.1161057640450866 ITERATION : 18, loss : 0.11611269948219621 ITERATION : 19, loss : 0.11611797621635203 ITERATION : 20, loss : 0.11612197481049952 
ITERATION : 1, loss : 0.23475737994460513 ITERATION : 2, loss : 0.23475737994460513 ITERATION : 3, loss : 0.23475737994460513 ITERATION : 4, loss : 0.23475737994460513 ITERATION : 5, loss : 0.23475737994460513 ITERATION : 6, loss : 0.23475737994460513 
ITERATION : 1, loss : 0.026888953165535256 ITERATION : 2, loss : 0.025510877415565316 ITERATION : 3, loss : 0.02636339230163335 ITERATION : 4, loss : 0.027443775396447427 ITERATION : 5, loss : 0.028366838555498732 ITERATION : 6, loss : 0.029079732843359636 ITERATION : 7, loss : 0.02960562613225217 ITERATION : 8, loss : 0.029983178199974952 ITERATION : 9, loss : 0.030249419258657047 ITERATION : 10, loss : 0.030434878724358277 ITERATION : 11, loss : 0.030562977550629367 ITERATION : 12, loss : 0.03065094009019509 ITERATION : 13, loss : 0.030711098112792407 ITERATION : 14, loss : 0.030752125708727193 ITERATION : 15, loss : 0.030780052425935636 ITERATION : 16, loss : 0.03079903630542241 ITERATION : 17, loss : 0.03081192899330245 ITERATION : 18, loss : 0.03082067913934156 ITERATION : 19, loss : 0.030826614909848834 ITERATION : 20, loss : 0.030830640233736037 ITERATION : 21, loss : 0.030833369242228525 ITERATION : 22, loss : 0.030835219020060332 
gradient norm in None layer : 0.029340870364564797
gradient norm in None layer : 0.0007528945919592631
gradient norm in None layer : 0.000912680951227289
gradient norm in None layer : 0.01265356567596329
gradient norm in None layer : 0.0007595775956788641
gradient norm in None layer : 0.0009471061380913539
gradient norm in None layer : 0.003864975065500382
gradient norm in None layer : 0.00015245886684513732
gradient norm in None layer : 0.00017918201276914827
gradient norm in None layer : 0.003084397702549573
gradient norm in None layer : 0.00019467323380313757
gradient norm in None layer : 0.00016910452972305454
gradient norm in None layer : 0.0011472623446878024
gradient norm in None layer : 3.725716118081565e-05
gradient norm in None layer : 4.119310624911848e-05
gradient norm in None layer : 0.0009570760463511892
gradient norm in None layer : 5.6401540815191456e-05
gradient norm in None layer : 4.954050450295867e-05
gradient norm in None layer : 0.0016023599366111446
gradient norm in None layer : 3.170044136648194e-05
gradient norm in None layer : 0.0031295518711839734
gradient norm in None layer : 0.0002724765096896554
gradient norm in None layer : 0.0001710553200490278
gradient norm in None layer : 0.004765522594555819
gradient norm in None layer : 0.00040581342640412033
gradient norm in None layer : 0.0004146024071631503
gradient norm in None layer : 0.009805214114848523
gradient norm in None layer : 0.00012190719902956741
gradient norm in None layer : 0.01507207292786087
gradient norm in None layer : 0.0012915906156683978
gradient norm in None layer : 0.001083028748145026
gradient norm in None layer : 0.020498248175829438
gradient norm in None layer : 0.0037866580299093875
gradient norm in None layer : 0.006964061791595469
gradient norm in None layer : 0.003284978814107911
gradient norm in None layer : 0.0015695818660111508
Total gradient norm: 0.04368793737595423
invariance loss : 11.782515913616004, avg_den : 0.07891082763671875, density loss : 0.030619812011718754, mse loss : 0.11151638828791768, solver time : 51.59062170982361 sec , total loss : 0.15391871621325245, running loss : 0.22836990023739118
Epoch 0/10 , batch 14/12500 
ITERATION : 1, loss : 0.045831817783085 ITERATION : 2, loss : 0.04657395726227298 ITERATION : 3, loss : 0.04678967186627102 ITERATION : 4, loss : 0.046866838877898866 ITERATION : 5, loss : 0.046891783329389154 ITERATION : 6, loss : 0.04689741484155448 ITERATION : 7, loss : 0.046896415456749364 ITERATION : 8, loss : 0.046893565253154385 ITERATION : 9, loss : 0.046890585055810376 ITERATION : 10, loss : 0.046888021735429 
ITERATION : 1, loss : 0.025938922587534004 ITERATION : 2, loss : 0.026044339999542292 ITERATION : 3, loss : 0.02605237521166228 ITERATION : 4, loss : 0.02605441267573268 ITERATION : 5, loss : 0.02605516852005446 ITERATION : 6, loss : 0.02605518681283582 ITERATION : 7, loss : 0.026054783371348384 
ITERATION : 1, loss : 0.043314763025888374 ITERATION : 2, loss : 0.043314763025888374 ITERATION : 3, loss : 0.043314763025888374 ITERATION : 4, loss : 0.043314763025888374 ITERATION : 5, loss : 0.043314763025888374 ITERATION : 6, loss : 0.043314763025888374 
ITERATION : 1, loss : 0.10852350690350096 ITERATION : 2, loss : 0.10941220600522879 ITERATION : 3, loss : 0.11004594676298785 ITERATION : 4, loss : 0.11040745877579833 ITERATION : 5, loss : 0.11063292839456873 ITERATION : 6, loss : 0.11078644755221358 ITERATION : 7, loss : 0.11089646576359119 ITERATION : 8, loss : 0.11097719579001313 ITERATION : 9, loss : 0.11103695257685903 ITERATION : 10, loss : 0.11108126234994327 ITERATION : 11, loss : 0.11111408142801067 ITERATION : 12, loss : 0.11113833700741309 ITERATION : 13, loss : 0.11115622199517905 ITERATION : 14, loss : 0.11116937968074314 ITERATION : 15, loss : 0.11117903961178334 ITERATION : 16, loss : 0.11118611884567527 ITERATION : 17, loss : 0.11119129861223198 ITERATION : 18, loss : 0.11119508304178735 ITERATION : 19, loss : 0.11119784452362634 
ITERATION : 1, loss : 0.03020168750013833 ITERATION : 2, loss : 0.02635834944007887 ITERATION : 3, loss : 0.025364024501943166 ITERATION : 4, loss : 0.02538066257353866 ITERATION : 5, loss : 0.025554685463797397 ITERATION : 6, loss : 0.02569973521497971 ITERATION : 7, loss : 0.02579795561445416 ITERATION : 8, loss : 0.025860950987203993 ITERATION : 9, loss : 0.025900816553061805 ITERATION : 10, loss : 0.025926046006736904 ITERATION : 11, loss : 0.025942086354204353 ITERATION : 12, loss : 0.025952344516277228 ITERATION : 13, loss : 0.02595894322222227 ITERATION : 14, loss : 0.02596321033111803 ITERATION : 15, loss : 0.025965982048642183 ITERATION : 16, loss : 0.025967788983218793 ITERATION : 17, loss : 0.025968970311840173 
ITERATION : 1, loss : 0.056334543770833766 ITERATION : 2, loss : 0.053964147098612525 ITERATION : 3, loss : 0.05356541691220747 ITERATION : 4, loss : 0.053504208942791474 ITERATION : 5, loss : 0.05352548547575986 ITERATION : 6, loss : 0.05364707458923921 ITERATION : 7, loss : 0.05374276697671271 ITERATION : 8, loss : 0.053814020539229776 ITERATION : 9, loss : 0.05386582035208641 ITERATION : 10, loss : 0.05390301790050666 ITERATION : 11, loss : 0.05392955592592408 ITERATION : 12, loss : 0.0539484221599245 ITERATION : 13, loss : 0.05396180820325525 ITERATION : 14, loss : 0.053971295342240114 ITERATION : 15, loss : 0.05397801490393173 ITERATION : 16, loss : 0.053982772321704654 ITERATION : 17, loss : 0.05398613981118478 ITERATION : 18, loss : 0.05398852296912215 
ITERATION : 1, loss : 0.2205474709871102 ITERATION : 2, loss : 0.21050388485469634 ITERATION : 3, loss : 0.20659223175655073 ITERATION : 4, loss : 0.2049769369174032 ITERATION : 5, loss : 0.20423062096474595 ITERATION : 6, loss : 0.20383378356603704 ITERATION : 7, loss : 0.20359324243219262 ITERATION : 8, loss : 0.2034335546897412 ITERATION : 9, loss : 0.20332217438412967 ITERATION : 10, loss : 0.2032427915859257 ITERATION : 11, loss : 0.20318582464979767 ITERATION : 12, loss : 0.2031449397897219 ITERATION : 13, loss : 0.2031156720793011 ITERATION : 14, loss : 0.20309479076556025 ITERATION : 15, loss : 0.2030799420012419 ITERATION : 16, loss : 0.20306941456641947 ITERATION : 17, loss : 0.20306197024176764 ITERATION : 18, loss : 0.20305671775887232 ITERATION : 19, loss : 0.20305301872369566 ITERATION : 20, loss : 0.2030504177400513 ITERATION : 21, loss : 0.20304859149661064 
ITERATION : 1, loss : 0.08430385489270358 ITERATION : 2, loss : 0.08539559153149233 ITERATION : 3, loss : 0.08584623690772886 ITERATION : 4, loss : 0.08609275685207661 ITERATION : 5, loss : 0.08624608544557015 ITERATION : 6, loss : 0.08634865016465475 ITERATION : 7, loss : 0.08642050829569324 ITERATION : 8, loss : 0.08647246683600164 ITERATION : 9, loss : 0.0865108909889615 ITERATION : 10, loss : 0.08653977934407188 ITERATION : 11, loss : 0.08656176995669239 ITERATION : 12, loss : 0.08657866535501435 ITERATION : 13, loss : 0.08659174215135226 ITERATION : 14, loss : 0.08660192003469051 ITERATION : 15, loss : 0.08660987615898388 ITERATION : 16, loss : 0.08661611670928965 ITERATION : 17, loss : 0.0866210247000231 ITERATION : 18, loss : 0.08662489279359342 ITERATION : 19, loss : 0.08662794637535236 
gradient norm in None layer : 0.027654795649389052
gradient norm in None layer : 0.001100722448718622
gradient norm in None layer : 0.00129272922292867
gradient norm in None layer : 0.016223565655464053
gradient norm in None layer : 0.0009425258450746814
gradient norm in None layer : 0.0011659778103363077
gradient norm in None layer : 0.007579584518231968
gradient norm in None layer : 0.00027845592146597583
gradient norm in None layer : 0.0004077196766675325
gradient norm in None layer : 0.0048922440383662716
gradient norm in None layer : 0.00024181816870631333
gradient norm in None layer : 0.0002851324958881604
gradient norm in None layer : 0.0018892948663905844
gradient norm in None layer : 6.477947655359448e-05
gradient norm in None layer : 7.221798153668967e-05
gradient norm in None layer : 0.0014465696377484502
gradient norm in None layer : 8.047079109128538e-05
gradient norm in None layer : 7.612022455871191e-05
gradient norm in None layer : 0.002251448603792973
gradient norm in None layer : 1.9916436723005833e-05
gradient norm in None layer : 0.004024313222652373
gradient norm in None layer : 0.00031294064054341735
gradient norm in None layer : 0.00023384638963231367
gradient norm in None layer : 0.005090722532603048
gradient norm in None layer : 0.0004221977892548046
gradient norm in None layer : 0.00045783741245460985
gradient norm in None layer : 0.009702188607929686
gradient norm in None layer : 0.00012075026342158151
gradient norm in None layer : 0.014706614219889298
gradient norm in None layer : 0.0012282700034393847
gradient norm in None layer : 0.0011125689432551563
gradient norm in None layer : 0.02162096111776905
gradient norm in None layer : 0.003673983428241386
gradient norm in None layer : 0.006672370345031781
gradient norm in None layer : 0.0029695973592854794
gradient norm in None layer : 0.0014082649021650365
Total gradient norm: 0.04491946794063888
invariance loss : 11.832257351171176, avg_den : 0.079071044921875, density loss : 0.022885131835937503, mse loss : 0.07463618047615218, solver time : 84.30858325958252 sec , total loss : 0.10935356966326086, running loss : 0.2198687337678104
Epoch 0/10 , batch 15/12500 
ITERATION : 1, loss : 0.02090283516754004 ITERATION : 2, loss : 0.020853029289917356 ITERATION : 3, loss : 0.020888746811271383 ITERATION : 4, loss : 0.020912789372652136 ITERATION : 5, loss : 0.02092883948054252 ITERATION : 6, loss : 0.020939782221169486 ITERATION : 7, loss : 0.0209473103766347 ITERATION : 8, loss : 0.020952513944436205 ITERATION : 9, loss : 0.020956122567345924 ITERATION : 10, loss : 0.020958631730606275 ITERATION : 11, loss : 0.020960380182048773 
ITERATION : 1, loss : 0.2987651068692797 ITERATION : 2, loss : 0.305072284415258 ITERATION : 3, loss : 0.3091214387401625 ITERATION : 4, loss : 0.31198896683801813 ITERATION : 5, loss : 0.3140869368823601 ITERATION : 6, loss : 0.3156242281997118 ITERATION : 7, loss : 0.3167469675548356 ITERATION : 8, loss : 0.31756471728253644 ITERATION : 9, loss : 0.3181593053602238 ITERATION : 10, loss : 0.31859119119675106 ITERATION : 11, loss : 0.3189047070052284 ITERATION : 12, loss : 0.31913221147690923 ITERATION : 13, loss : 0.319297262422036 ITERATION : 14, loss : 0.3194169849068209 ITERATION : 15, loss : 0.3195038173094558 ITERATION : 16, loss : 0.31956678951871076 ITERATION : 17, loss : 0.3196124544153713 ITERATION : 18, loss : 0.3196455667339645 ITERATION : 19, loss : 0.31966957561903686 ITERATION : 20, loss : 0.319686982938966 ITERATION : 21, loss : 0.3196996032969963 ITERATION : 22, loss : 0.3197087528311805 ITERATION : 23, loss : 0.3197153856980958 ITERATION : 24, loss : 0.31972019399622015 ITERATION : 25, loss : 0.31972367948923813 ITERATION : 26, loss : 0.31972620614239566 
ITERATION : 1, loss : 0.06204350754833336 ITERATION : 2, loss : 0.05878031000633011 ITERATION : 3, loss : 0.05960730257220181 ITERATION : 4, loss : 0.06091990506858903 ITERATION : 5, loss : 0.06200917408435206 ITERATION : 6, loss : 0.06279240606506216 ITERATION : 7, loss : 0.06332326523693768 ITERATION : 8, loss : 0.06367261875347147 ITERATION : 9, loss : 0.06389893341698635 ITERATION : 10, loss : 0.06404428811867896 ITERATION : 11, loss : 0.0641372060523115 ITERATION : 12, loss : 0.06419644859746516 ITERATION : 13, loss : 0.06423416171073182 ITERATION : 14, loss : 0.0642581435542874 ITERATION : 15, loss : 0.06427337874960978 ITERATION : 16, loss : 0.06428304694410854 ITERATION : 17, loss : 0.0642891743800896 ITERATION : 18, loss : 0.06429305148516143 ITERATION : 19, loss : 0.06429549979862322 ITERATION : 20, loss : 0.06429704208025384 
ITERATION : 1, loss : 0.05877979253382222 ITERATION : 2, loss : 0.06126663290310488 ITERATION : 3, loss : 0.06062627156861819 ITERATION : 4, loss : 0.06034999124925311 ITERATION : 5, loss : 0.06026380486994338 ITERATION : 6, loss : 0.06025378360073234 ITERATION : 7, loss : 0.06026882319940065 ITERATION : 8, loss : 0.06028891016953614 ITERATION : 9, loss : 0.06030718689380369 ITERATION : 10, loss : 0.060321913993618845 ITERATION : 11, loss : 0.06033316283318955 ITERATION : 12, loss : 0.0603415278595076 ITERATION : 13, loss : 0.06034766003993046 ITERATION : 14, loss : 0.06035211990602111 ITERATION : 15, loss : 0.06035534876365759 ITERATION : 16, loss : 0.06035768030688609 
ITERATION : 1, loss : 0.15313690368249813 ITERATION : 2, loss : 0.15540956189220043 ITERATION : 3, loss : 0.15681795443881077 ITERATION : 4, loss : 0.15767933323430372 ITERATION : 5, loss : 0.15821144960810765 ITERATION : 6, loss : 0.15854608870844178 ITERATION : 7, loss : 0.1587604062050302 ITERATION : 8, loss : 0.15889995759588899 ITERATION : 9, loss : 0.15899214825437025 ITERATION : 10, loss : 0.15905380580442569 ITERATION : 11, loss : 0.15909546981750597 ITERATION : 12, loss : 0.15912386429383782 ITERATION : 13, loss : 0.1591433506374405 ITERATION : 14, loss : 0.15915679922814815 ITERATION : 15, loss : 0.1591661226039139 ITERATION : 16, loss : 0.15917260980167067 ITERATION : 17, loss : 0.15917713475172113 ITERATION : 18, loss : 0.15918029906043038 ITERATION : 19, loss : 0.15918251579713455 
ITERATION : 1, loss : 0.059891110027891804 ITERATION : 2, loss : 0.06048334650406286 ITERATION : 3, loss : 0.061008796485900144 ITERATION : 4, loss : 0.06157490763491958 ITERATION : 5, loss : 0.06210117929097851 ITERATION : 6, loss : 0.06253616307616491 ITERATION : 7, loss : 0.06287160598991003 ITERATION : 8, loss : 0.0631198278646314 ITERATION : 9, loss : 0.06329882849487548 ITERATION : 10, loss : 0.06342574403460507 ITERATION : 11, loss : 0.06351469569970895 ITERATION : 12, loss : 0.06357653367798001 ITERATION : 13, loss : 0.06361927043567954 ITERATION : 14, loss : 0.06364867830682935 ITERATION : 15, loss : 0.06366884823133173 ITERATION : 16, loss : 0.06368264756991128 ITERATION : 17, loss : 0.06369206985491643 ITERATION : 18, loss : 0.06369849338499131 ITERATION : 19, loss : 0.0637028668841598 ITERATION : 20, loss : 0.06370584135939296 ITERATION : 21, loss : 0.0637078624094149 
ITERATION : 1, loss : 0.12301653005472457 ITERATION : 2, loss : 0.12276237419783721 ITERATION : 3, loss : 0.12269639870022334 ITERATION : 4, loss : 0.12256418630071779 ITERATION : 5, loss : 0.12243209539304901 ITERATION : 6, loss : 0.12232417799985326 ITERATION : 7, loss : 0.12224251313711113 ITERATION : 8, loss : 0.12218270720600645 ITERATION : 9, loss : 0.12213954484799343 ITERATION : 10, loss : 0.122108596843858 ITERATION : 11, loss : 0.12208647043995115 ITERATION : 12, loss : 0.12207067068268272 ITERATION : 13, loss : 0.1220593946613472 ITERATION : 14, loss : 0.12205134888345222 ITERATION : 15, loss : 0.12204560910674123 ITERATION : 16, loss : 0.1220415149523186 ITERATION : 17, loss : 0.12203859489085124 ITERATION : 18, loss : 0.12203651274115307 
ITERATION : 1, loss : 0.06486766827756288 ITERATION : 2, loss : 0.06148162755863552 ITERATION : 3, loss : 0.060364609852932676 ITERATION : 4, loss : 0.059883085631132864 ITERATION : 5, loss : 0.059638627494309294 ITERATION : 6, loss : 0.05950093319136854 ITERATION : 7, loss : 0.05941800310679561 ITERATION : 8, loss : 0.05936584770226547 ITERATION : 9, loss : 0.05933210921673053 ITERATION : 10, loss : 0.05930987750627948 ITERATION : 11, loss : 0.05929505063916725 ITERATION : 12, loss : 0.059285086107806216 ITERATION : 13, loss : 0.05927835831012365 ITERATION : 14, loss : 0.05927380462965575 ITERATION : 15, loss : 0.059270719804767134 ITERATION : 16, loss : 0.059268630631864536 
gradient norm in None layer : 0.033134244130616185
gradient norm in None layer : 0.0009367092830292739
gradient norm in None layer : 0.0011660115563433065
gradient norm in None layer : 0.01488047772563337
gradient norm in None layer : 0.0009442283907983672
gradient norm in None layer : 0.0012483991480298066
gradient norm in None layer : 0.0044217886411146694
gradient norm in None layer : 0.0001847445374129441
gradient norm in None layer : 0.00021377810203828792
gradient norm in None layer : 0.0040854860833615
gradient norm in None layer : 0.00024604586601927714
gradient norm in None layer : 0.00021181772542391386
gradient norm in None layer : 0.0017943439239297063
gradient norm in None layer : 5.876116184968365e-05
gradient norm in None layer : 5.5427955213071865e-05
gradient norm in None layer : 0.0013650149301426477
gradient norm in None layer : 8.914317472700685e-05
gradient norm in None layer : 6.850578840392878e-05
gradient norm in None layer : 0.0021529333818965574
gradient norm in None layer : 2.0600806654049742e-05
gradient norm in None layer : 0.003928872858971582
gradient norm in None layer : 0.0003085749935231828
gradient norm in None layer : 0.0002034923919223354
gradient norm in None layer : 0.00505268339928218
gradient norm in None layer : 0.0004430223077040678
gradient norm in None layer : 0.00041731390425661766
gradient norm in None layer : 0.009826418452803564
gradient norm in None layer : 9.256868336928653e-05
gradient norm in None layer : 0.016832810848655393
gradient norm in None layer : 0.0010179709326035586
gradient norm in None layer : 0.0015100806632342824
gradient norm in None layer : 0.01936485571430976
gradient norm in None layer : 0.003387332992077499
gradient norm in None layer : 0.006029299163141587
gradient norm in None layer : 0.0028535625277843527
gradient norm in None layer : 0.001254369474043945
Total gradient norm: 0.04719534289016972
invariance loss : 11.542861861204116, avg_den : 0.10863494873046875, density loss : 0.02532196044921875, mse loss : 0.10869210378639393, solver time : 62.03543567657471 sec , total loss : 0.1455569260968168, running loss : 0.21491461325641084
Epoch 0/10 , batch 16/12500 
ITERATION : 1, loss : 0.07475803225787338 ITERATION : 2, loss : 0.07621566673742529 ITERATION : 3, loss : 0.07631866327947918 ITERATION : 4, loss : 0.07616798868289133 ITERATION : 5, loss : 0.07594622495324331 ITERATION : 6, loss : 0.07573868496406404 ITERATION : 7, loss : 0.07557019240490431 ITERATION : 8, loss : 0.07544216764141624 ITERATION : 9, loss : 0.07534839116815882 ITERATION : 10, loss : 0.07528122361257181 ITERATION : 11, loss : 0.07523381473308619 ITERATION : 12, loss : 0.07520068666062396 ITERATION : 13, loss : 0.0751777020263695 ITERATION : 14, loss : 0.07516183738277814 ITERATION : 15, loss : 0.07515092898451091 ITERATION : 16, loss : 0.07514344987777903 ITERATION : 17, loss : 0.07513833295785562 ITERATION : 18, loss : 0.07513483790890005 ITERATION : 19, loss : 0.07513245359404326 ITERATION : 20, loss : 0.07513082846407966 
ITERATION : 1, loss : 0.04532484408941947 ITERATION : 2, loss : 0.044859497496666255 ITERATION : 3, loss : 0.044534876987787866 ITERATION : 4, loss : 0.044308884436385476 ITERATION : 5, loss : 0.04415092867857941 ITERATION : 6, loss : 0.04404041218244099 ITERATION : 7, loss : 0.04396303860301634 ITERATION : 8, loss : 0.043908830619216056 ITERATION : 9, loss : 0.043870826895939366 ITERATION : 10, loss : 0.04384416850814339 ITERATION : 11, loss : 0.04382546043137827 ITERATION : 12, loss : 0.04381232744782346 ITERATION : 13, loss : 0.04380310606833924 ITERATION : 14, loss : 0.043796630201186425 ITERATION : 15, loss : 0.04379208191651765 ITERATION : 16, loss : 0.04378888721742502 ITERATION : 17, loss : 0.04378664316733523 
ITERATION : 1, loss : 0.10670102370749393 ITERATION : 2, loss : 0.10074728257215407 ITERATION : 3, loss : 0.09828297926672527 ITERATION : 4, loss : 0.09727999504565132 ITERATION : 5, loss : 0.09681957638173731 ITERATION : 6, loss : 0.09657862488133821 ITERATION : 7, loss : 0.09643657000305328 ITERATION : 8, loss : 0.09634969162328916 ITERATION : 9, loss : 0.09629191025416985 ITERATION : 10, loss : 0.09625264823617517 ITERATION : 11, loss : 0.09622559870700063 ITERATION : 12, loss : 0.0962068012063165 ITERATION : 13, loss : 0.09619366684073592 ITERATION : 14, loss : 0.09618445683720903 ITERATION : 15, loss : 0.09617798299197851 ITERATION : 16, loss : 0.09617342468282268 ITERATION : 17, loss : 0.09617021109379902 ITERATION : 18, loss : 0.09616794341749937 
ITERATION : 1, loss : 0.06949505625012829 ITERATION : 2, loss : 0.07148632216661814 ITERATION : 3, loss : 0.07218058545610569 ITERATION : 4, loss : 0.07038171732238982 ITERATION : 5, loss : 0.06941669979442638 ITERATION : 6, loss : 0.0688906338831701 ITERATION : 7, loss : 0.06859458234436179 ITERATION : 8, loss : 0.06842195562594969 ITERATION : 9, loss : 0.0683176233596525 ITERATION : 10, loss : 0.06825234653183383 ITERATION : 11, loss : 0.06821018294477404 ITERATION : 12, loss : 0.0681821792022371 ITERATION : 13, loss : 0.06816314618640881 ITERATION : 14, loss : 0.06814997221540406 ITERATION : 15, loss : 0.06814072796591937 ITERATION : 16, loss : 0.06813417645829896 ITERATION : 17, loss : 0.06812950097099181 ITERATION : 18, loss : 0.06812614879618821 ITERATION : 19, loss : 0.06812373831843291 
ITERATION : 1, loss : 0.06473209368215167 ITERATION : 2, loss : 0.06528707869641444 ITERATION : 3, loss : 0.06581883500321987 ITERATION : 4, loss : 0.06622393600351 ITERATION : 5, loss : 0.06651495257355455 ITERATION : 6, loss : 0.06671732409095767 ITERATION : 7, loss : 0.06685515800288022 ITERATION : 8, loss : 0.066947853328208 ITERATION : 9, loss : 0.06700976352652542 ITERATION : 10, loss : 0.06705098584733617 ITERATION : 11, loss : 0.06707841280568183 ITERATION : 12, loss : 0.06709667051387831 ITERATION : 13, loss : 0.06710883766428971 ITERATION : 14, loss : 0.06711695617765619 ITERATION : 15, loss : 0.06712237973495097 ITERATION : 16, loss : 0.06712600668319588 ITERATION : 17, loss : 0.06712843419214812 ITERATION : 18, loss : 0.06713005995476812 
ITERATION : 1, loss : 0.18186705494279992 ITERATION : 2, loss : 0.17753531365253294 ITERATION : 3, loss : 0.17753531365253294 ITERATION : 4, loss : 0.17753531365253294 ITERATION : 5, loss : 0.17753531365253294 ITERATION : 6, loss : 0.17753531365253294 ITERATION : 7, loss : 0.17753531365253294 
ITERATION : 1, loss : 0.25622860119622415 ITERATION : 2, loss : 0.25622860119622415 ITERATION : 3, loss : 0.25622860119622415 ITERATION : 4, loss : 0.25622860119622415 ITERATION : 5, loss : 0.25622860119622415 ITERATION : 6, loss : 0.25622860119622415 
ITERATION : 1, loss : 0.16301768603860364 ITERATION : 2, loss : 0.1649684979999325 ITERATION : 3, loss : 0.1655680032405329 ITERATION : 4, loss : 0.1660981685016362 ITERATION : 5, loss : 0.16655823583630824 ITERATION : 6, loss : 0.16692939546481433 ITERATION : 7, loss : 0.1672149487001271 ITERATION : 8, loss : 0.1674281318900642 ITERATION : 9, loss : 0.16758417342615864 ITERATION : 10, loss : 0.16769686464996522 ITERATION : 11, loss : 0.1677774864874207 ITERATION : 12, loss : 0.16783477727878124 ITERATION : 13, loss : 0.16787528887590342 ITERATION : 14, loss : 0.1679038311327629 ITERATION : 15, loss : 0.16792388556859453 ITERATION : 16, loss : 0.16793794713265428 ITERATION : 17, loss : 0.16794779113716804 ITERATION : 18, loss : 0.16795467428182062 ITERATION : 19, loss : 0.16795948264022292 ITERATION : 20, loss : 0.1679628392587076 ITERATION : 21, loss : 0.16796518112741343 
gradient norm in None layer : 0.01829892003674594
gradient norm in None layer : 0.0005973354708271059
gradient norm in None layer : 0.000676440556884326
gradient norm in None layer : 0.008921831929593832
gradient norm in None layer : 0.0005168656773639163
gradient norm in None layer : 0.0006839024830087063
gradient norm in None layer : 0.0032507481323838703
gradient norm in None layer : 0.00012161954418564007
gradient norm in None layer : 0.00016021751413278158
gradient norm in None layer : 0.0023828596420468213
gradient norm in None layer : 0.00013059379917991035
gradient norm in None layer : 0.00012781614343036754
gradient norm in None layer : 0.0009084042924729651
gradient norm in None layer : 2.9153174549901788e-05
gradient norm in None layer : 3.232742388958225e-05
gradient norm in None layer : 0.0007521172817349058
gradient norm in None layer : 4.472580577242153e-05
gradient norm in None layer : 3.890848521336929e-05
gradient norm in None layer : 0.0011916737091551847
gradient norm in None layer : 1.2962629387416905e-05
gradient norm in None layer : 0.0022270531423900005
gradient norm in None layer : 0.00016034828246348735
gradient norm in None layer : 0.0001347102634446475
gradient norm in None layer : 0.002887619101308208
gradient norm in None layer : 0.00026198565142070097
gradient norm in None layer : 0.0002710849502456694
gradient norm in None layer : 0.006150074002937711
gradient norm in None layer : 6.762162382395883e-05
gradient norm in None layer : 0.009421404338947425
gradient norm in None layer : 0.0007676927304470926
gradient norm in None layer : 0.0007693180730544073
gradient norm in None layer : 0.012960077045949944
gradient norm in None layer : 0.0024124215587317194
gradient norm in None layer : 0.0045100920066414646
gradient norm in None layer : 0.0020916685459868066
gradient norm in None layer : 0.0010154215075371897
Total gradient norm: 0.027855361993921045
invariance loss : 9.673684458846985, avg_den : 0.078033447265625, density loss : 0.028225708007812503, mse loss : 0.11900853866228574, solver time : 218.66152453422546 sec , total loss : 0.15690793112894522, running loss : 0.21128919562344425
Epoch 0/10 , batch 17/12500 
ITERATION : 1, loss : 0.03523645534977627 ITERATION : 2, loss : 0.03417706740697682 ITERATION : 3, loss : 0.03335432634063708 ITERATION : 4, loss : 0.032766691989589425 ITERATION : 5, loss : 0.032358263624612485 ITERATION : 6, loss : 0.03207575784909548 ITERATION : 7, loss : 0.031880091798010796 ITERATION : 8, loss : 0.03174420812421682 ITERATION : 9, loss : 0.03164959260070275 ITERATION : 10, loss : 0.03158356457374412 ITERATION : 11, loss : 0.031537404578576456 ITERATION : 12, loss : 0.03150508942809277 ITERATION : 13, loss : 0.03148244245116737 ITERATION : 14, loss : 0.031466558084079506 ITERATION : 15, loss : 0.0314554103611511 ITERATION : 16, loss : 0.03144758304092518 ITERATION : 17, loss : 0.03144208516396043 ITERATION : 18, loss : 0.03143822240877587 ITERATION : 19, loss : 0.03143550792961895 ITERATION : 20, loss : 0.03143360010586325 
ITERATION : 1, loss : 0.06787441658299329 ITERATION : 2, loss : 0.0691368462148152 ITERATION : 3, loss : 0.06682224142872537 ITERATION : 4, loss : 0.06583098372604551 ITERATION : 5, loss : 0.06538677814557449 ITERATION : 6, loss : 0.0651808695517516 ITERATION : 7, loss : 0.06508306874883282 ITERATION : 8, loss : 0.06503555190407413 ITERATION : 9, loss : 0.06501183365037821 ITERATION : 10, loss : 0.0649995608059257 ITERATION : 11, loss : 0.0649929007406912 ITERATION : 12, loss : 0.06498906920324653 ITERATION : 13, loss : 0.06498671954651743 ITERATION : 14, loss : 0.06498518781770794 ITERATION : 15, loss : 0.06498413719188646 
ITERATION : 1, loss : 0.05178135370363901 ITERATION : 2, loss : 0.05191989853794097 ITERATION : 3, loss : 0.051938750084886284 ITERATION : 4, loss : 0.05192475539047222 ITERATION : 5, loss : 0.05190448165290231 ITERATION : 6, loss : 0.051886229819535556 ITERATION : 7, loss : 0.05187189148139655 ITERATION : 8, loss : 0.051861258366684614 ITERATION : 9, loss : 0.051853590731422754 ITERATION : 10, loss : 0.05184814186569798 ITERATION : 11, loss : 0.05184430086055726 ITERATION : 12, loss : 0.051841605735124474 ITERATION : 13, loss : 0.05183972001467714 
ITERATION : 1, loss : 0.0669815315754185 ITERATION : 2, loss : 0.06828804360941304 ITERATION : 3, loss : 0.06854871622447244 ITERATION : 4, loss : 0.06867376076031975 ITERATION : 5, loss : 0.06874596880690705 ITERATION : 6, loss : 0.06879112147463935 ITERATION : 7, loss : 0.06882073632482014 ITERATION : 8, loss : 0.06884072035628734 ITERATION : 9, loss : 0.06885442835279394 ITERATION : 10, loss : 0.06886391946732162 ITERATION : 11, loss : 0.06887052654027555 ITERATION : 12, loss : 0.0688751405713519 ITERATION : 13, loss : 0.06887836915453929 ITERATION : 14, loss : 0.06888063112738609 
ITERATION : 1, loss : 0.1648604663878983 ITERATION : 2, loss : 0.1648604663878983 ITERATION : 3, loss : 0.1648604663878983 ITERATION : 4, loss : 0.1648604663878983 ITERATION : 5, loss : 0.1648604663878983 ITERATION : 6, loss : 0.1648604663878983 
ITERATION : 1, loss : 0.04865828085275473 ITERATION : 2, loss : 0.048572657589787176 ITERATION : 3, loss : 0.04828038105674428 ITERATION : 4, loss : 0.04799787486724498 ITERATION : 5, loss : 0.047776326662863244 ITERATION : 6, loss : 0.0476137895756635 ITERATION : 7, loss : 0.04749744663894638 ITERATION : 8, loss : 0.04741492145969391 ITERATION : 9, loss : 0.04735655004550674 ITERATION : 10, loss : 0.04731527579265917 ITERATION : 11, loss : 0.04728607229250051 ITERATION : 12, loss : 0.04726539078667103 ITERATION : 13, loss : 0.047250731539217564 ITERATION : 14, loss : 0.04724033308573146 ITERATION : 15, loss : 0.04723295253622612 ITERATION : 16, loss : 0.04722771156694019 ITERATION : 17, loss : 0.04722398861489802 ITERATION : 18, loss : 0.047221343304961375 ITERATION : 19, loss : 0.04721946334473582 
ITERATION : 1, loss : 0.07213479657843946 ITERATION : 2, loss : 0.07494324832908227 ITERATION : 3, loss : 0.07543004501458449 ITERATION : 4, loss : 0.07563295901702233 ITERATION : 5, loss : 0.07578598599189215 ITERATION : 6, loss : 0.07591732799296745 ITERATION : 7, loss : 0.07602770733117233 ITERATION : 8, loss : 0.07611675102601582 ITERATION : 9, loss : 0.0761861177417064 ITERATION : 10, loss : 0.0762387509863393 ITERATION : 11, loss : 0.07627792540103905 ITERATION : 12, loss : 0.07630667730154475 ITERATION : 13, loss : 0.0763275661167796 ITERATION : 14, loss : 0.07634263015079483 ITERATION : 15, loss : 0.07635343506945612 ITERATION : 16, loss : 0.07636115446958422 ITERATION : 17, loss : 0.07636665349391544 ITERATION : 18, loss : 0.07637056247777983 ITERATION : 19, loss : 0.07637333689697337 ITERATION : 20, loss : 0.07637530376924803 
ITERATION : 1, loss : 0.04825144464616538 ITERATION : 2, loss : 0.050635057437555635 ITERATION : 3, loss : 0.05241253062757098 ITERATION : 4, loss : 0.052372739373204275 ITERATION : 5, loss : 0.05215695314342254 ITERATION : 6, loss : 0.052045398107908196 ITERATION : 7, loss : 0.051986339837093144 ITERATION : 8, loss : 0.05195423421439593 ITERATION : 9, loss : 0.05193633770126556 ITERATION : 10, loss : 0.05192614379565621 ITERATION : 11, loss : 0.05192023781463163 ITERATION : 12, loss : 0.05191677624124972 ITERATION : 13, loss : 0.05191473599061359 ITERATION : 14, loss : 0.051913534635087695 ITERATION : 15, loss : 0.05191283333140465 
gradient norm in None layer : 0.018235194079622104
gradient norm in None layer : 0.00043590465172322924
gradient norm in None layer : 0.0006463779507785847
gradient norm in None layer : 0.008021781404300491
gradient norm in None layer : 0.00048099508951364645
gradient norm in None layer : 0.0007280813465523875
gradient norm in None layer : 0.003996102531454008
gradient norm in None layer : 0.0001589665777794291
gradient norm in None layer : 0.00019229695770304965
gradient norm in None layer : 0.0030383417568924207
gradient norm in None layer : 0.0001758861533837372
gradient norm in None layer : 0.00017146961833046901
gradient norm in None layer : 0.0012056960891692705
gradient norm in None layer : 4.126989779887321e-05
gradient norm in None layer : 4.054999790494937e-05
gradient norm in None layer : 0.0010206368644902924
gradient norm in None layer : 5.399423383369455e-05
gradient norm in None layer : 4.8281240136253344e-05
gradient norm in None layer : 0.0014704951726487055
gradient norm in None layer : 1.5241459018801925e-05
gradient norm in None layer : 0.002782666848302794
gradient norm in None layer : 0.00020629843683374195
gradient norm in None layer : 0.00018577304759643322
gradient norm in None layer : 0.0035127422866152547
gradient norm in None layer : 0.00030709270990946484
gradient norm in None layer : 0.0003417453602140546
gradient norm in None layer : 0.007145486288208229
gradient norm in None layer : 9.494647811887457e-05
gradient norm in None layer : 0.009516213905928593
gradient norm in None layer : 0.0007829664215681003
gradient norm in None layer : 0.0008331758229270664
gradient norm in None layer : 0.012526863655887309
gradient norm in None layer : 0.0028321526869502607
gradient norm in None layer : 0.005179491741284506
gradient norm in None layer : 0.0025133189893224764
gradient norm in None layer : 0.0011873019961687768
Total gradient norm: 0.02812526217844969
invariance loss : 10.288026859270143, avg_den : 0.0798492431640625, density loss : 0.0350006103515625, mse loss : 0.06968826940913747, solver time : 304.04395866394043 sec , total loss : 0.11497690661997012, running loss : 0.205623766858534
Epoch 0/10 , batch 18/12500 
ITERATION : 1, loss : 0.10707834343788546 ITERATION : 2, loss : 0.10364399001268106 ITERATION : 3, loss : 0.10291776617505892 ITERATION : 4, loss : 0.10274549315303244 ITERATION : 5, loss : 0.10274205235529266 ITERATION : 6, loss : 0.10279105743231196 ITERATION : 7, loss : 0.10284921711747745 ITERATION : 8, loss : 0.102901152531848 ITERATION : 9, loss : 0.10294279149868095 ITERATION : 10, loss : 0.10297439725775404 ITERATION : 11, loss : 0.10299762852777121 ITERATION : 12, loss : 0.10301435505705137 ITERATION : 13, loss : 0.1030262282839417 ITERATION : 14, loss : 0.10303457044365351 ITERATION : 15, loss : 0.10304038644472219 ITERATION : 16, loss : 0.10304441672533513 ITERATION : 17, loss : 0.10304719602364185 
ITERATION : 1, loss : 0.04017991494312157 ITERATION : 2, loss : 0.040030049259856336 ITERATION : 3, loss : 0.040030049259856336 ITERATION : 4, loss : 0.040030049259856336 ITERATION : 5, loss : 0.040030049259856336 ITERATION : 6, loss : 0.040030049259856336 ITERATION : 7, loss : 0.040030049259856336 
ITERATION : 1, loss : 0.047294764363355224 ITERATION : 2, loss : 0.047294764363355224 ITERATION : 3, loss : 0.047294764363355224 ITERATION : 4, loss : 0.047294764363355224 ITERATION : 5, loss : 0.047294764363355224 ITERATION : 6, loss : 0.047294764363355224 
ITERATION : 1, loss : 0.054378547338182426 ITERATION : 2, loss : 0.05244755487332927 ITERATION : 3, loss : 0.05154227266774984 ITERATION : 4, loss : 0.05157279550240421 ITERATION : 5, loss : 0.05157012721794909 ITERATION : 6, loss : 0.05155378697635923 ITERATION : 7, loss : 0.051534314027215616 ITERATION : 8, loss : 0.05151628703288553 ITERATION : 9, loss : 0.05150126260778783 ITERATION : 10, loss : 0.05148942734758976 ITERATION : 11, loss : 0.051480420756591136 ITERATION : 12, loss : 0.05147372192423457 ITERATION : 13, loss : 0.05146881860578739 ITERATION : 14, loss : 0.05146527077288373 
ITERATION : 1, loss : 0.0699823046246049 ITERATION : 2, loss : 0.06788286677149288 ITERATION : 3, loss : 0.06693389286869282 ITERATION : 4, loss : 0.06645895239127739 ITERATION : 5, loss : 0.06619244024153167 ITERATION : 6, loss : 0.06602961077036285 ITERATION : 7, loss : 0.06592464339536758 ITERATION : 8, loss : 0.06585478681718589 ITERATION : 9, loss : 0.06580741637442894 ITERATION : 10, loss : 0.06577492799576985 ITERATION : 11, loss : 0.06575248735452131 ITERATION : 12, loss : 0.06573691416864741 ITERATION : 13, loss : 0.06572607247711237 ITERATION : 14, loss : 0.06571850747582461 ITERATION : 15, loss : 0.06571322018791853 ITERATION : 16, loss : 0.06570952038420472 ITERATION : 17, loss : 0.06570692900016481 ITERATION : 18, loss : 0.06570511264792549 
ITERATION : 1, loss : 0.029286988478708843 ITERATION : 2, loss : 0.028943344218199833 ITERATION : 3, loss : 0.028797888436899365 ITERATION : 4, loss : 0.028682031912391028 ITERATION : 5, loss : 0.028588853085836773 ITERATION : 6, loss : 0.02851652796927942 ITERATION : 7, loss : 0.028461899029139304 ITERATION : 8, loss : 0.028421425316528588 ITERATION : 9, loss : 0.028391856779355854 ITERATION : 10, loss : 0.028370479988824148 ITERATION : 11, loss : 0.028355147593848374 ITERATION : 12, loss : 0.028344217062155466 ITERATION : 13, loss : 0.028336460934761085 ITERATION : 14, loss : 0.02833097715205955 ITERATION : 15, loss : 0.028327110838534703 ITERATION : 16, loss : 0.028324390836760884 ITERATION : 17, loss : 0.028322480534251484 
ITERATION : 1, loss : 0.028883247305856365 ITERATION : 2, loss : 0.026770013000275927 ITERATION : 3, loss : 0.025992679900876607 ITERATION : 4, loss : 0.02559085803410652 ITERATION : 5, loss : 0.025354901685914907 ITERATION : 6, loss : 0.025207679591353057 ITERATION : 7, loss : 0.02511242979659176 ITERATION : 8, loss : 0.025049274478529778 ITERATION : 9, loss : 0.025006669368791774 ITERATION : 10, loss : 0.024977571790106182 ITERATION : 11, loss : 0.02495752436395427 ITERATION : 12, loss : 0.0249436252154125 ITERATION : 13, loss : 0.024933945080211094 ITERATION : 14, loss : 0.02492718173143277 ITERATION : 15, loss : 0.0249224451112096 ITERATION : 16, loss : 0.024919122097210693 ITERATION : 17, loss : 0.024916788062699164 
ITERATION : 1, loss : 0.09625724368368695 ITERATION : 2, loss : 0.09362435744724111 ITERATION : 3, loss : 0.09207814738049065 ITERATION : 4, loss : 0.09106414388927814 ITERATION : 5, loss : 0.09036792004028364 ITERATION : 6, loss : 0.0898827425616941 ITERATION : 7, loss : 0.08954287620813903 ITERATION : 8, loss : 0.08930423924197135 ITERATION : 9, loss : 0.08913645691749056 ITERATION : 10, loss : 0.08901839479058363 ITERATION : 11, loss : 0.08893527812694849 ITERATION : 12, loss : 0.08887674717408804 ITERATION : 13, loss : 0.0888355250693718 ITERATION : 14, loss : 0.0888064920853324 ITERATION : 15, loss : 0.08878604447981066 ITERATION : 16, loss : 0.0887716447053024 ITERATION : 17, loss : 0.08876150410893388 ITERATION : 18, loss : 0.08875436419320035 ITERATION : 19, loss : 0.088749337123138 ITERATION : 20, loss : 0.08874579778931604 ITERATION : 21, loss : 0.08874330620929466 ITERATION : 22, loss : 0.08874155201002984 
gradient norm in None layer : 0.01828818359520765
gradient norm in None layer : 0.000745880975926383
gradient norm in None layer : 0.0005642848847257723
gradient norm in None layer : 0.012336714036310313
gradient norm in None layer : 0.0007372552775016584
gradient norm in None layer : 0.0007489438558606427
gradient norm in None layer : 0.009067683938395304
gradient norm in None layer : 0.00030563170822298574
gradient norm in None layer : 0.00027372733189614265
gradient norm in None layer : 0.005731552062924563
gradient norm in None layer : 0.00023626054355388614
gradient norm in None layer : 0.00022949525720832102
gradient norm in None layer : 0.0019789180592081203
gradient norm in None layer : 6.203139891733882e-05
gradient norm in None layer : 5.995483233363018e-05
gradient norm in None layer : 0.0015510926442710272
gradient norm in None layer : 6.727948193580903e-05
gradient norm in None layer : 6.31362569264243e-05
gradient norm in None layer : 0.002086239320837679
gradient norm in None layer : 1.9401663179287137e-05
gradient norm in None layer : 0.003993983187184973
gradient norm in None layer : 0.0002597725457503328
gradient norm in None layer : 0.0002616819772610939
gradient norm in None layer : 0.004531610048077607
gradient norm in None layer : 0.0003423904242977774
gradient norm in None layer : 0.00040271676404314327
gradient norm in None layer : 0.008269580586556427
gradient norm in None layer : 9.521985481973486e-05
gradient norm in None layer : 0.01080681609700188
gradient norm in None layer : 0.000896051835320287
gradient norm in None layer : 0.0008467110455266184
gradient norm in None layer : 0.01355242815805729
gradient norm in None layer : 0.003170753423825957
gradient norm in None layer : 0.005156204926485146
gradient norm in None layer : 0.0028279661478792504
gradient norm in None layer : 0.0011944484285803599
Total gradient norm: 0.0326804216231712
invariance loss : 11.921397433956745, avg_den : 0.06940460205078125, density loss : 0.032267761230468754, mse loss : 0.05619040170933039, solver time : 126.32147908210754 sec , total loss : 0.10037956037375588, running loss : 0.19977686649826856
Epoch 0/10 , batch 19/12500 
ITERATION : 1, loss : 0.39834849042408116 ITERATION : 2, loss : 0.4148830347793035 ITERATION : 3, loss : 0.42069481497393685 ITERATION : 4, loss : 0.4230327100363868 ITERATION : 5, loss : 0.4240591686314144 ITERATION : 6, loss : 0.4245334473263737 ITERATION : 7, loss : 0.4247615039239382 ITERATION : 8, loss : 0.42487539922693274 ITERATION : 9, loss : 0.4249350227184343 ITERATION : 10, loss : 0.4249683536948653 ITERATION : 11, loss : 0.42498865171012046 ITERATION : 12, loss : 0.4250022282713682 ITERATION : 13, loss : 0.4250120915952351 ITERATION : 14, loss : 0.4250196879205631 ITERATION : 15, loss : 0.42502573480636946 ITERATION : 16, loss : 0.425030616762022 ITERATION : 17, loss : 0.425034567493407 
ITERATION : 1, loss : 0.3254949367034771 ITERATION : 2, loss : 0.32682980833756514 ITERATION : 3, loss : 0.32712708015301845 ITERATION : 4, loss : 0.3273064380682509 ITERATION : 5, loss : 0.3274251481789219 ITERATION : 6, loss : 0.32750683878207876 ITERATION : 7, loss : 0.3275641206688324 ITERATION : 8, loss : 0.3276046081509635 ITERATION : 9, loss : 0.32763330149804365 ITERATION : 10, loss : 0.3276536404407892 ITERATION : 11, loss : 0.3276680473721232 ITERATION : 12, loss : 0.3276782417859334 ITERATION : 13, loss : 0.3276854480514474 ITERATION : 14, loss : 0.32769053728993014 ITERATION : 15, loss : 0.32769412862822933 ITERATION : 16, loss : 0.3276966613975751 ITERATION : 17, loss : 0.3276984467341372 
ITERATION : 1, loss : 0.1664799071163373 ITERATION : 2, loss : 0.16914572632202113 ITERATION : 3, loss : 0.1705482218754285 ITERATION : 4, loss : 0.17006670103222596 ITERATION : 5, loss : 0.16944741934002974 ITERATION : 6, loss : 0.1690191173228516 ITERATION : 7, loss : 0.1687257286801807 ITERATION : 8, loss : 0.16852476772850566 ITERATION : 9, loss : 0.1683866358121761 ITERATION : 10, loss : 0.16829126692801283 ITERATION : 11, loss : 0.16822513873321956 ITERATION : 12, loss : 0.16817911465564117 ITERATION : 13, loss : 0.16814698435102984 ITERATION : 14, loss : 0.1681244988259099 ITERATION : 15, loss : 0.16810873340097424 ITERATION : 16, loss : 0.16809766371975482 ITERATION : 17, loss : 0.16808988253199933 ITERATION : 18, loss : 0.16808440845599867 ITERATION : 19, loss : 0.16808055496131105 ITERATION : 20, loss : 0.1680778410770394 ITERATION : 21, loss : 0.1680759291153222 
ITERATION : 1, loss : 0.05405338983858312 ITERATION : 2, loss : 0.05405338983858312 ITERATION : 3, loss : 0.05405338983858312 ITERATION : 4, loss : 0.05405338983858312 ITERATION : 5, loss : 0.05405338983858312 ITERATION : 6, loss : 0.05405338983858312 
ITERATION : 1, loss : 0.05536791183132746 ITERATION : 2, loss : 0.05570223758230245 ITERATION : 3, loss : 0.05431629300361766 ITERATION : 4, loss : 0.053742948901596525 ITERATION : 5, loss : 0.05351973756563362 ITERATION : 6, loss : 0.05345439493349853 ITERATION : 7, loss : 0.05345590288902315 ITERATION : 8, loss : 0.053481232854406084 ITERATION : 9, loss : 0.05351114172398444 ITERATION : 10, loss : 0.05353798819737428 ITERATION : 11, loss : 0.053559519773963456 ITERATION : 12, loss : 0.053575773466666685 ITERATION : 13, loss : 0.053587596823026745 ITERATION : 14, loss : 0.053595990335041616 ITERATION : 15, loss : 0.05360184950409309 ITERATION : 16, loss : 0.05360589073897378 ITERATION : 17, loss : 0.05360865349825938 
ITERATION : 1, loss : 0.07252034231405961 ITERATION : 2, loss : 0.07063117896837688 ITERATION : 3, loss : 0.06939363810038549 ITERATION : 4, loss : 0.068586051476082 ITERATION : 5, loss : 0.06804521622590026 ITERATION : 6, loss : 0.06767770840911255 ITERATION : 7, loss : 0.06742565856047868 ITERATION : 8, loss : 0.06725173840484926 ITERATION : 9, loss : 0.06713123441522197 ITERATION : 10, loss : 0.06704750112608407 ITERATION : 11, loss : 0.06698919819359217 ITERATION : 12, loss : 0.06694854056818818 ITERATION : 13, loss : 0.0669201556018481 ITERATION : 14, loss : 0.06690032154305267 ITERATION : 15, loss : 0.06688645320123268 ITERATION : 16, loss : 0.06687675121527441 ITERATION : 17, loss : 0.06686996119319241 ITERATION : 18, loss : 0.06686520772678724 ITERATION : 19, loss : 0.06686187917536818 ITERATION : 20, loss : 0.06685954788957502 
ITERATION : 1, loss : 0.08303873166600582 ITERATION : 2, loss : 0.08303873166600582 ITERATION : 3, loss : 0.08303873166600582 ITERATION : 4, loss : 0.08303873166600582 ITERATION : 5, loss : 0.08303873166600582 ITERATION : 6, loss : 0.08303873166600582 
ITERATION : 1, loss : 0.16384798038000636 ITERATION : 2, loss : 0.15802918697462878 ITERATION : 3, loss : 0.15512497854923746 ITERATION : 4, loss : 0.15330902543001632 ITERATION : 5, loss : 0.152181819247817 ITERATION : 6, loss : 0.15146891474639654 ITERATION : 7, loss : 0.15100703623161274 ITERATION : 8, loss : 0.15070318809466296 ITERATION : 9, loss : 0.15050126561174565 ITERATION : 10, loss : 0.15036613377487984 ITERATION : 11, loss : 0.15027524534319103 ITERATION : 12, loss : 0.15021388979344188 ITERATION : 13, loss : 0.15017235786054578 ITERATION : 14, loss : 0.15014418729816065 ITERATION : 15, loss : 0.15012505046523536 ITERATION : 16, loss : 0.15011203557645098 ITERATION : 17, loss : 0.15010317672543125 ITERATION : 18, loss : 0.1500971429984899 ITERATION : 19, loss : 0.15009303179007305 ITERATION : 20, loss : 0.15009022970458602 ITERATION : 21, loss : 0.15008831959374738 
gradient norm in None layer : 0.13091305832802125
gradient norm in None layer : 0.004574534274626194
gradient norm in None layer : 0.0030205540939649447
gradient norm in None layer : 0.06794907069487277
gradient norm in None layer : 0.003929161029409939
gradient norm in None layer : 0.002467164489468782
gradient norm in None layer : 0.06677432546117125
gradient norm in None layer : 0.002022561243887846
gradient norm in None layer : 0.0016229628702330364
gradient norm in None layer : 0.03331238129563507
gradient norm in None layer : 0.0015067516971738852
gradient norm in None layer : 0.0009653067342036156
gradient norm in None layer : 0.011191649514124526
gradient norm in None layer : 0.0003325790748340172
gradient norm in None layer : 0.00024785964153921396
gradient norm in None layer : 0.006662550922760506
gradient norm in None layer : 0.00025716293693270917
gradient norm in None layer : 0.00023346732418274947
gradient norm in None layer : 0.006911151642317422
gradient norm in None layer : 4.409185672144528e-05
gradient norm in None layer : 0.016555939909131727
gradient norm in None layer : 0.0009944204152335559
gradient norm in None layer : 0.0007991203181521238
gradient norm in None layer : 0.013472416381022378
gradient norm in None layer : 0.0007392491139895037
gradient norm in None layer : 0.0010664389239139207
gradient norm in None layer : 0.01797860081639221
gradient norm in None layer : 0.00018077350068539252
gradient norm in None layer : 0.030598862734960448
gradient norm in None layer : 0.0024815547482705366
gradient norm in None layer : 0.0021008334348034926
gradient norm in None layer : 0.03641349984934829
gradient norm in None layer : 0.005427727339399652
gradient norm in None layer : 0.007552239918451695
gradient norm in None layer : 0.00472883225708464
gradient norm in None layer : 0.0014484949161055337
Total gradient norm: 0.1754108733133614
invariance loss : 9.624071292741634, avg_den : 0.074432373046875, density loss : 0.025567626953125006, mse loss : 0.16605719822862963, solver time : 129.69310235977173 sec , total loss : 0.20124889647449629, running loss : 0.1998543417601753
Epoch 0/10 , batch 20/12500 
ITERATION : 1, loss : 0.30420686574384675 ITERATION : 2, loss : 0.30788073907060565 ITERATION : 3, loss : 0.30979909113766985 ITERATION : 4, loss : 0.31126134932435884 ITERATION : 5, loss : 0.3123644908633888 ITERATION : 6, loss : 0.31317771641670117 ITERATION : 7, loss : 0.3137693002081894 ITERATION : 8, loss : 0.3141967672935764 ITERATION : 9, loss : 0.31450464625335534 ITERATION : 10, loss : 0.31472606013307775 ITERATION : 11, loss : 0.3148851882390144 ITERATION : 12, loss : 0.31499952371728573 ITERATION : 13, loss : 0.31508166952294825 ITERATION : 14, loss : 0.3151406885545189 ITERATION : 15, loss : 0.31518309288890556 ITERATION : 16, loss : 0.31521356052468724 ITERATION : 17, loss : 0.3152354520293939 ITERATION : 18, loss : 0.3152511815842294 ITERATION : 19, loss : 0.3152624836066922 ITERATION : 20, loss : 0.31527060424478665 ITERATION : 21, loss : 0.3152764389612961 ITERATION : 22, loss : 0.3152806310948691 ITERATION : 23, loss : 0.31528364299324146 ITERATION : 24, loss : 0.3152858068864509 
ITERATION : 1, loss : 0.057308718086478985 ITERATION : 2, loss : 0.060219326904762276 ITERATION : 3, loss : 0.06268777885735584 ITERATION : 4, loss : 0.06309303261025505 ITERATION : 5, loss : 0.06342647106315986 ITERATION : 6, loss : 0.0636943138462457 ITERATION : 7, loss : 0.06390284234053828 ITERATION : 8, loss : 0.06406129051922584 ITERATION : 9, loss : 0.06417960821208227 ITERATION : 10, loss : 0.06426688190441995 ITERATION : 11, loss : 0.06433070280641558 ITERATION : 12, loss : 0.06437708979867915 ITERATION : 13, loss : 0.06441066025627853 ITERATION : 14, loss : 0.06443488181121668 ITERATION : 15, loss : 0.06445232063253535 ITERATION : 16, loss : 0.06446485737360089 ITERATION : 17, loss : 0.06447386075604342 ITERATION : 18, loss : 0.06448032202870507 ITERATION : 19, loss : 0.06448495673741232 ITERATION : 20, loss : 0.06448828031222509 ITERATION : 21, loss : 0.06449066308543734 
ITERATION : 1, loss : 0.008225359451139773 ITERATION : 2, loss : 0.008238617923220578 ITERATION : 3, loss : 0.008260382633035268 ITERATION : 4, loss : 0.008279738820865816 ITERATION : 5, loss : 0.008295418501749108 ITERATION : 6, loss : 0.008307441984504864 ITERATION : 7, loss : 0.008316368728152394 ITERATION : 8, loss : 0.008322870539648478 ITERATION : 9, loss : 0.008327551348884738 ITERATION : 10, loss : 0.00833089676443753 ITERATION : 11, loss : 0.008333276654835076 
ITERATION : 1, loss : 0.06314009923697975 ITERATION : 2, loss : 0.04803624688662077 ITERATION : 3, loss : 0.047032333284235436 ITERATION : 4, loss : 0.05202575229713235 ITERATION : 5, loss : 0.053790853702554244 ITERATION : 6, loss : 0.053197912209974114 ITERATION : 7, loss : 0.0529056567512654 ITERATION : 8, loss : 0.05276133169328161 ITERATION : 9, loss : 0.05269077493923261 ITERATION : 10, loss : 0.05265740147735454 ITERATION : 11, loss : 0.05264276019347635 ITERATION : 12, loss : 0.05263738767437618 ITERATION : 13, loss : 0.05263640294204174 ITERATION : 14, loss : 0.05263731066527743 ITERATION : 15, loss : 0.0526388697884869 ITERATION : 16, loss : 0.05264049625542126 
ITERATION : 1, loss : 0.035289129420900914 ITERATION : 2, loss : 0.0349187144738975 ITERATION : 3, loss : 0.03495654982717254 ITERATION : 4, loss : 0.03500837888331616 ITERATION : 5, loss : 0.035046055343341605 ITERATION : 6, loss : 0.03507194230500533 ITERATION : 7, loss : 0.03508970876658789 ITERATION : 8, loss : 0.03510199162743819 ITERATION : 9, loss : 0.035110548034102226 ITERATION : 10, loss : 0.035116546426642546 ITERATION : 11, loss : 0.03512077262685163 ITERATION : 12, loss : 0.035123761686942714 ITERATION : 13, loss : 0.03512588203132817 
ITERATION : 1, loss : 0.11569053505386752 ITERATION : 2, loss : 0.1174898296332746 ITERATION : 3, loss : 0.11830336468041909 ITERATION : 4, loss : 0.11884837605352506 ITERATION : 5, loss : 0.11922609838971215 ITERATION : 6, loss : 0.11948950622255178 ITERATION : 7, loss : 0.11967358627111059 ITERATION : 8, loss : 0.11980234064152126 ITERATION : 9, loss : 0.11989242397771299 ITERATION : 10, loss : 0.11995545311849785 ITERATION : 11, loss : 0.11999955074825691 ITERATION : 12, loss : 0.12003040124874333 ITERATION : 13, loss : 0.12005198336458407 ITERATION : 14, loss : 0.1200670815160086 ITERATION : 15, loss : 0.12007764396767817 ITERATION : 16, loss : 0.12008503364871989 ITERATION : 17, loss : 0.12009020392945957 ITERATION : 18, loss : 0.12009382163779964 ITERATION : 19, loss : 0.12009635319257023 ITERATION : 20, loss : 0.12009812483385762 
ITERATION : 1, loss : 0.16696141124025832 ITERATION : 2, loss : 0.1417372630547727 ITERATION : 3, loss : 0.13449818486011964 ITERATION : 4, loss : 0.13503537564561327 ITERATION : 5, loss : 0.1365538432778591 ITERATION : 6, loss : 0.13781167596852534 ITERATION : 7, loss : 0.13880641425554255 ITERATION : 8, loss : 0.13957997657265891 ITERATION : 9, loss : 0.14017733146745084 ITERATION : 10, loss : 0.14063707697373445 ITERATION : 11, loss : 0.1409902605290094 ITERATION : 12, loss : 0.14126125886551302 ITERATION : 13, loss : 0.14146901495669217 ITERATION : 14, loss : 0.14162817561986282 ITERATION : 15, loss : 0.14175003498532807 ITERATION : 16, loss : 0.14184328647500016 ITERATION : 17, loss : 0.14191461269406846 ITERATION : 18, loss : 0.14196914600673197 ITERATION : 19, loss : 0.1420108243767367 ITERATION : 20, loss : 0.14204266688797976 ITERATION : 21, loss : 0.14206698725806152 ITERATION : 22, loss : 0.1420855567793588 ITERATION : 23, loss : 0.14209973163021064 ITERATION : 24, loss : 0.14211054920942195 ITERATION : 25, loss : 0.1421188027169824 ITERATION : 26, loss : 0.14212509874434562 ITERATION : 27, loss : 0.1421299006035972 ITERATION : 28, loss : 0.14213356216528272 ITERATION : 29, loss : 0.14213635370668917 
ITERATION : 1, loss : 0.07825332238600329 ITERATION : 2, loss : 0.08612007939442731 ITERATION : 3, loss : 0.0912273021185561 ITERATION : 4, loss : 0.09462748207074362 ITERATION : 5, loss : 0.09697206770060474 ITERATION : 6, loss : 0.09862230197027927 ITERATION : 7, loss : 0.09979777127712944 ITERATION : 8, loss : 0.10064120965518945 ITERATION : 9, loss : 0.10124927634951125 ITERATION : 10, loss : 0.10168906478798799 ITERATION : 11, loss : 0.10200786493541916 ITERATION : 12, loss : 0.10223933931620902 ITERATION : 13, loss : 0.10240761072010303 ITERATION : 14, loss : 0.10253004631655527 ITERATION : 15, loss : 0.10261919152375569 ITERATION : 16, loss : 0.10268413108553477 ITERATION : 17, loss : 0.10273145588747058 ITERATION : 18, loss : 0.1027659539670003 ITERATION : 19, loss : 0.10279110745153902 ITERATION : 20, loss : 0.10280945068113016 ITERATION : 21, loss : 0.10282282924434866 ITERATION : 22, loss : 0.102832587794041 ITERATION : 23, loss : 0.10283970638961211 ITERATION : 24, loss : 0.10284489954234029 ITERATION : 25, loss : 0.10284868814131704 ITERATION : 26, loss : 0.10285145219092855 
gradient norm in None layer : 0.025773905009844666
gradient norm in None layer : 0.0007634534235876659
gradient norm in None layer : 0.0007991221912572854
gradient norm in None layer : 0.01216102654657273
gradient norm in None layer : 0.0006686728064055269
gradient norm in None layer : 0.0007136783097960011
gradient norm in None layer : 0.007708153289784822
gradient norm in None layer : 0.00029598978035573465
gradient norm in None layer : 0.0003175983062127332
gradient norm in None layer : 0.004363920403302769
gradient norm in None layer : 0.00021863713649863145
gradient norm in None layer : 0.0001943421147379638
gradient norm in None layer : 0.001543008922015646
gradient norm in None layer : 5.003550241114492e-05
gradient norm in None layer : 5.481650872195656e-05
gradient norm in None layer : 0.0010001673583950534
gradient norm in None layer : 5.9955507262475695e-05
gradient norm in None layer : 4.216679146392094e-05
gradient norm in None layer : 0.001350352773060085
gradient norm in None layer : 1.352834497637954e-05
gradient norm in None layer : 0.002625876579191822
gradient norm in None layer : 0.0001543543994252357
gradient norm in None layer : 0.00016305245733281098
gradient norm in None layer : 0.0024558442835836175
gradient norm in None layer : 0.0002220711048615236
gradient norm in None layer : 0.00025165989129296884
gradient norm in None layer : 0.004719451970446697
gradient norm in None layer : 8.694638331416546e-05
gradient norm in None layer : 0.008309424831716032
gradient norm in None layer : 0.0005211265651060956
gradient norm in None layer : 0.000569443353008911
gradient norm in None layer : 0.009658363206302806
gradient norm in None layer : 0.0017921889169468536
gradient norm in None layer : 0.003328040357977371
gradient norm in None layer : 0.0015443953927526727
gradient norm in None layer : 0.0007506154387906667
Total gradient norm: 0.033374314206987984
invariance loss : 7.872204441377484, avg_den : 0.10770416259765625, density loss : 0.02091217041015625, mse loss : 0.10512025695561851, solver time : 390.83500123023987 sec , total loss : 0.13390463180715223, running loss : 0.19655685626252417
saving checkpoint
Epoch 0/10 , batch 21/12500 
ITERATION : 1, loss : 0.04582361431453098 ITERATION : 2, loss : 0.04328879334657746 ITERATION : 3, loss : 0.04439206593650344 ITERATION : 4, loss : 0.045724985466222605 ITERATION : 5, loss : 0.04669545314612226 ITERATION : 6, loss : 0.047363454424627055 ITERATION : 7, loss : 0.0478181761352099 ITERATION : 8, loss : 0.04812705450688128 ITERATION : 9, loss : 0.048336853446536814 ITERATION : 10, loss : 0.04847944784714757 ITERATION : 11, loss : 0.048576467242862985 ITERATION : 12, loss : 0.048642564328104584 ITERATION : 13, loss : 0.04868766010393636 ITERATION : 14, loss : 0.04871847195539783 ITERATION : 15, loss : 0.04873955429739332 ITERATION : 16, loss : 0.0487539982274848 ITERATION : 17, loss : 0.04876390568705274 ITERATION : 18, loss : 0.048770708380849404 ITERATION : 19, loss : 0.048775383440864746 ITERATION : 20, loss : 0.04877859855308072 ITERATION : 21, loss : 0.048780811299344015 
ITERATION : 1, loss : 0.04066811834549336 ITERATION : 2, loss : 0.03998457722535893 ITERATION : 3, loss : 0.04087661394109622 ITERATION : 4, loss : 0.04197130740728865 ITERATION : 5, loss : 0.04293407836284416 ITERATION : 6, loss : 0.043702436891363806 ITERATION : 7, loss : 0.0442902744988008 ITERATION : 8, loss : 0.04473024936177744 ITERATION : 9, loss : 0.04505544078827503 ITERATION : 10, loss : 0.045293954863921584 ITERATION : 11, loss : 0.045468037885724046 ITERATION : 12, loss : 0.04559468317422579 ITERATION : 13, loss : 0.04568661473016172 ITERATION : 14, loss : 0.0457532457940138 ITERATION : 15, loss : 0.04580148716259721 ITERATION : 16, loss : 0.04583638717344326 ITERATION : 17, loss : 0.04586162113540759 ITERATION : 18, loss : 0.04587985847110817 ITERATION : 19, loss : 0.045893034927945164 ITERATION : 20, loss : 0.04590255258353966 ITERATION : 21, loss : 0.04590942603676214 ITERATION : 22, loss : 0.04591438910709576 ITERATION : 23, loss : 0.045917972310559725 ITERATION : 24, loss : 0.04592055900942918 
ITERATION : 1, loss : 0.09165428031990032 ITERATION : 2, loss : 0.09033079460783928 ITERATION : 3, loss : 0.09020104050194025 ITERATION : 4, loss : 0.09013859328684758 ITERATION : 5, loss : 0.09008687024940716 ITERATION : 6, loss : 0.09004582944078768 ITERATION : 7, loss : 0.09001501833191232 ITERATION : 8, loss : 0.08999256843721913 ITERATION : 9, loss : 0.08997644922462018 ITERATION : 10, loss : 0.0899649565321267 ITERATION : 11, loss : 0.08995678930403044 ITERATION : 12, loss : 0.08995099371370244 ITERATION : 13, loss : 0.08994688376248952 ITERATION : 14, loss : 0.08994396975544951 ITERATION : 15, loss : 0.08994190398900806 
ITERATION : 1, loss : 0.09895300246583742 ITERATION : 2, loss : 0.09772419796065318 ITERATION : 3, loss : 0.09955292560218446 ITERATION : 4, loss : 0.10125383381268925 ITERATION : 5, loss : 0.10263359673102397 ITERATION : 6, loss : 0.10367673770672528 ITERATION : 7, loss : 0.1044486178089868 ITERATION : 8, loss : 0.10501395595073562 ITERATION : 9, loss : 0.10542534467931501 ITERATION : 10, loss : 0.10572334244899116 ITERATION : 11, loss : 0.10593849014686293 ITERATION : 12, loss : 0.10609344935538671 ITERATION : 13, loss : 0.10620486451254332 ITERATION : 14, loss : 0.10628487180193097 ITERATION : 15, loss : 0.10634227406346841 ITERATION : 16, loss : 0.10638343233246826 ITERATION : 17, loss : 0.10641293041377199 ITERATION : 18, loss : 0.10643406533247399 ITERATION : 19, loss : 0.10644920510845789 ITERATION : 20, loss : 0.10646004897880497 ITERATION : 21, loss : 0.10646781527716556 ITERATION : 22, loss : 0.10647337724606955 ITERATION : 23, loss : 0.10647736049607578 ITERATION : 24, loss : 0.10648021314789391 ITERATION : 25, loss : 0.10648225620227328 
ITERATION : 1, loss : 0.03031509691571133 ITERATION : 2, loss : 0.026018006962862977 ITERATION : 3, loss : 0.024536049813426476 ITERATION : 4, loss : 0.023834607946788447 ITERATION : 5, loss : 0.02350533530412526 ITERATION : 6, loss : 0.023345328853463968 ITERATION : 7, loss : 0.023264353282962738 ITERATION : 8, loss : 0.023221511731856496 ITERATION : 9, loss : 0.02319784184430748 ITERATION : 10, loss : 0.023184252480101003 ITERATION : 11, loss : 0.023176200507812314 ITERATION : 12, loss : 0.023171311805454325 ITERATION : 13, loss : 0.023168290007918105 ITERATION : 14, loss : 0.02316639876258656 ITERATION : 15, loss : 0.02316520559067238 
ITERATION : 1, loss : 0.05417722922695409 ITERATION : 2, loss : 0.05417722922695409 ITERATION : 3, loss : 0.05417722922695409 ITERATION : 4, loss : 0.05417722922695409 ITERATION : 5, loss : 0.05417722922695409 ITERATION : 6, loss : 0.05417722922695409 
ITERATION : 1, loss : 0.2675563281899595 ITERATION : 2, loss : 0.2705176757443441 ITERATION : 3, loss : 0.2711490901918445 ITERATION : 4, loss : 0.27070732949069154 ITERATION : 5, loss : 0.27003626357507826 ITERATION : 6, loss : 0.26941776835304887 ITERATION : 7, loss : 0.26892129044064866 ITERATION : 8, loss : 0.2684700257619196 ITERATION : 9, loss : 0.26772324508106765 ITERATION : 10, loss : 0.26720874033232184 ITERATION : 11, loss : 0.2668542391771022 ITERATION : 12, loss : 0.2666099220153199 ITERATION : 13, loss : 0.26644149706044923 ITERATION : 14, loss : 0.266325363342902 ITERATION : 15, loss : 0.26624527152661004 ITERATION : 16, loss : 0.266190028914232 ITERATION : 17, loss : 0.26615192259666326 ITERATION : 18, loss : 0.26612563531220157 ITERATION : 19, loss : 0.2661075003617339 ITERATION : 20, loss : 0.2660949892746362 ITERATION : 21, loss : 0.2660863580221867 ITERATION : 22, loss : 0.2660804033225374 ITERATION : 23, loss : 0.2660762952647106 ITERATION : 24, loss : 0.2660734610250781 ITERATION : 25, loss : 0.266071505635075 
ITERATION : 1, loss : 0.02995086223231525 ITERATION : 2, loss : 0.03057427133687801 ITERATION : 3, loss : 0.03070511778071775 ITERATION : 4, loss : 0.030796632222627348 ITERATION : 5, loss : 0.030872289672602284 ITERATION : 6, loss : 0.030931614850969055 ITERATION : 7, loss : 0.030975970178769522 ITERATION : 8, loss : 0.031008208358238976 ITERATION : 9, loss : 0.031031273206192662 ITERATION : 10, loss : 0.031047632351427525 ITERATION : 11, loss : 0.03105918002765486 ITERATION : 12, loss : 0.031067309985246417 ITERATION : 13, loss : 0.03107302537902519 ITERATION : 14, loss : 0.03107704007881072 ITERATION : 15, loss : 0.03107985886766062 ITERATION : 16, loss : 0.03108183743425608 
gradient norm in None layer : 0.020839727974329446
gradient norm in None layer : 0.0006586543526102951
gradient norm in None layer : 0.0006462711714512191
gradient norm in None layer : 0.0104303707323052
gradient norm in None layer : 0.0005752569432835705
gradient norm in None layer : 0.0006088338750795333
gradient norm in None layer : 0.006094669049633462
gradient norm in None layer : 0.0002631948513272274
gradient norm in None layer : 0.00028363967891627875
gradient norm in None layer : 0.003857962468748389
gradient norm in None layer : 0.00020561643204548645
gradient norm in None layer : 0.00018582591780221788
gradient norm in None layer : 0.001495651201003514
gradient norm in None layer : 5.051581686709321e-05
gradient norm in None layer : 5.280668773239594e-05
gradient norm in None layer : 0.0009406005245649484
gradient norm in None layer : 5.3876055699214654e-05
gradient norm in None layer : 3.8441748499941654e-05
gradient norm in None layer : 0.0011891417863155512
gradient norm in None layer : 1.3808026323229314e-05
gradient norm in None layer : 0.0023011271823927017
gradient norm in None layer : 0.0001374900544538866
gradient norm in None layer : 0.0001499307318035969
gradient norm in None layer : 0.0022545144942439734
gradient norm in None layer : 0.00021076128445101757
gradient norm in None layer : 0.00023864143237392054
gradient norm in None layer : 0.004473525202496214
gradient norm in None layer : 6.071910125736236e-05
gradient norm in None layer : 0.007819386121907128
gradient norm in None layer : 0.0005259287822730504
gradient norm in None layer : 0.0005347927095121713
gradient norm in None layer : 0.009178357107962052
gradient norm in None layer : 0.0019210670821536828
gradient norm in None layer : 0.003533836004964621
gradient norm in None layer : 0.0016967457421997222
gradient norm in None layer : 0.0008341439393901587
Total gradient norm: 0.028243208492938397
invariance loss : 7.567082021617138, avg_den : 0.08458709716796875, density loss : 0.017955017089843754, mse loss : 0.08320266354837651, solver time : 274.3231363296509 sec , total loss : 0.1087247626598374, running loss : 0.19237437561477716
Epoch 0/10 , batch 22/12500 
ITERATION : 1, loss : 0.07889522721486432 ITERATION : 2, loss : 0.07895382118333702 ITERATION : 3, loss : 0.07878975368898106 ITERATION : 4, loss : 0.07866481837101796 ITERATION : 5, loss : 0.0785801440713598 ITERATION : 6, loss : 0.07852384547333359 ITERATION : 7, loss : 0.07848630609530031 ITERATION : 8, loss : 0.07846107548539696 ITERATION : 9, loss : 0.07844397914922363 ITERATION : 10, loss : 0.07843231254042352 ITERATION : 11, loss : 0.0784243048709039 ITERATION : 12, loss : 0.07841878297731791 ITERATION : 13, loss : 0.0784149611019316 ITERATION : 14, loss : 0.078412308141917 ITERATION : 15, loss : 0.07841046237935397 
ITERATION : 1, loss : 0.0982595179713669 ITERATION : 2, loss : 0.0982595179713669 ITERATION : 3, loss : 0.0982595179713669 ITERATION : 4, loss : 0.0982595179713669 ITERATION : 5, loss : 0.0982595179713669 ITERATION : 6, loss : 0.0982595179713669 
ITERATION : 1, loss : 0.019730448217935712 ITERATION : 2, loss : 0.019730448217935712 ITERATION : 3, loss : 0.019730448217935712 ITERATION : 4, loss : 0.019730448217935712 ITERATION : 5, loss : 0.019730448217935712 ITERATION : 6, loss : 0.019730448217935712 
ITERATION : 1, loss : 0.19065229980234882 ITERATION : 2, loss : 0.19065229980234882 ITERATION : 3, loss : 0.19065229980234882 ITERATION : 4, loss : 0.19065229980234882 ITERATION : 5, loss : 0.19065229980234882 ITERATION : 6, loss : 0.19065229980234882 
ITERATION : 1, loss : 0.049546409451947125 ITERATION : 2, loss : 0.053300920566274085 ITERATION : 3, loss : 0.056370105555523145 ITERATION : 4, loss : 0.0584847758175384 ITERATION : 5, loss : 0.059938297062187 ITERATION : 6, loss : 0.06094817049237639 ITERATION : 7, loss : 0.06165551643999653 ITERATION : 8, loss : 0.06215334653372466 ITERATION : 9, loss : 0.06250461701955122 ITERATION : 10, loss : 0.06275278070112121 ITERATION : 11, loss : 0.0629281879816418 ITERATION : 12, loss : 0.06305217965234404 ITERATION : 13, loss : 0.06313981456744713 ITERATION : 14, loss : 0.06320173700506344 ITERATION : 15, loss : 0.06324547682583619 ITERATION : 16, loss : 0.06327636185736495 ITERATION : 17, loss : 0.06329816151401721 ITERATION : 18, loss : 0.06331354232538722 ITERATION : 19, loss : 0.06332438980781574 ITERATION : 20, loss : 0.06333203688857919 ITERATION : 21, loss : 0.06333742560423561 ITERATION : 22, loss : 0.06334122121909073 ITERATION : 23, loss : 0.06334389356293614 ITERATION : 24, loss : 0.06334577422154582 
ITERATION : 1, loss : 0.05117001033610205 ITERATION : 2, loss : 0.049765325165290385 ITERATION : 3, loss : 0.049102626016409745 ITERATION : 4, loss : 0.048804304175968906 ITERATION : 5, loss : 0.04866647553022554 ITERATION : 6, loss : 0.04859994415104183 ITERATION : 7, loss : 0.04856613488005789 ITERATION : 8, loss : 0.04854801293550106 ITERATION : 9, loss : 0.048537790176005235 ITERATION : 10, loss : 0.0485317517188947 ITERATION : 11, loss : 0.048528041342369245 ITERATION : 12, loss : 0.04852568623605465 ITERATION : 13, loss : 0.048524151984295 ITERATION : 14, loss : 0.048523131944774194 
ITERATION : 1, loss : 0.15482991381807396 ITERATION : 2, loss : 0.14982100132147447 ITERATION : 3, loss : 0.14729909108833128 ITERATION : 4, loss : 0.1456986819406462 ITERATION : 5, loss : 0.14459914012689323 ITERATION : 6, loss : 0.14383248893257777 ITERATION : 7, loss : 0.14329761180175424 ITERATION : 8, loss : 0.14292501919168185 ITERATION : 9, loss : 0.14266581478480153 ITERATION : 10, loss : 0.14248563479509935 ITERATION : 11, loss : 0.14236043608931695 ITERATION : 12, loss : 0.14227345405053068 ITERATION : 13, loss : 0.1422130236448406 ITERATION : 14, loss : 0.14217103727562486 ITERATION : 15, loss : 0.14214186284756786 ITERATION : 16, loss : 0.14212158869415592 ITERATION : 17, loss : 0.1421074980904443 ITERATION : 18, loss : 0.14209770404038632 ITERATION : 19, loss : 0.14209089575411488 ITERATION : 20, loss : 0.14208616258981216 ITERATION : 21, loss : 0.14208287176939932 ITERATION : 22, loss : 0.1420805835691679 
ITERATION : 1, loss : 0.09238961612808752 ITERATION : 2, loss : 0.09189418152222327 ITERATION : 3, loss : 0.0918276412995705 ITERATION : 4, loss : 0.0918368348376822 ITERATION : 5, loss : 0.09185031244030847 ITERATION : 6, loss : 0.09185972643963292 ITERATION : 7, loss : 0.09186569668726964 ITERATION : 8, loss : 0.09186944623440332 ITERATION : 9, loss : 0.09187182458063707 
gradient norm in None layer : 0.026127418686014876
gradient norm in None layer : 0.0011968860963498494
gradient norm in None layer : 0.0012596145004216035
gradient norm in None layer : 0.016854229554240194
gradient norm in None layer : 0.000975574583901965
gradient norm in None layer : 0.0010399552273163554
gradient norm in None layer : 0.011190681486733006
gradient norm in None layer : 0.00046758208001970947
gradient norm in None layer : 0.0005465423600109953
gradient norm in None layer : 0.006716235032697054
gradient norm in None layer : 0.0003035117132008284
gradient norm in None layer : 0.00031511582315161494
gradient norm in None layer : 0.0026329591634493546
gradient norm in None layer : 8.646513704265842e-05
gradient norm in None layer : 9.883235989849713e-05
gradient norm in None layer : 0.0016296121255922234
gradient norm in None layer : 9.149618058364365e-05
gradient norm in None layer : 6.119286988308063e-05
gradient norm in None layer : 0.0019395537395412736
gradient norm in None layer : 1.2650595591959834e-05
gradient norm in None layer : 0.0035310802433484428
gradient norm in None layer : 0.0001950129330787067
gradient norm in None layer : 0.0002492708064495706
gradient norm in None layer : 0.0031146639662974
gradient norm in None layer : 0.0002441432266363261
gradient norm in None layer : 0.0003697838734214398
gradient norm in None layer : 0.005674882773737673
gradient norm in None layer : 6.988255869216116e-05
gradient norm in None layer : 0.00880842531192394
gradient norm in None layer : 0.0005583060353576026
gradient norm in None layer : 0.0007695738826249367
gradient norm in None layer : 0.00992684664320423
gradient norm in None layer : 0.0021232735498482383
gradient norm in None layer : 0.0036733046155678164
gradient norm in None layer : 0.0018433717367603423
gradient norm in None layer : 0.0008183188127047647
Total gradient norm: 0.03754951349161028
invariance loss : 8.534797076681585, avg_den : 0.0840606689453125, density loss : 0.026013183593750003, mse loss : 0.09160925533589129, solver time : 239.4102931022644 sec , total loss : 0.12615723600632286, running loss : 0.1893645056325747
Epoch 0/10 , batch 23/12500 
ITERATION : 1, loss : 0.047142189626743124 ITERATION : 2, loss : 0.04983153092968578 ITERATION : 3, loss : 0.052542954040027595 ITERATION : 4, loss : 0.05454466021042099 ITERATION : 5, loss : 0.055886117434656664 ITERATION : 6, loss : 0.056759055861224546 ITERATION : 7, loss : 0.05732394573432585 ITERATION : 8, loss : 0.057690263995051255 ITERATION : 9, loss : 0.05792878593840793 ITERATION : 10, loss : 0.05808474580294714 ITERATION : 11, loss : 0.05818710005245817 ITERATION : 12, loss : 0.058254482309773994 ITERATION : 13, loss : 0.05829895311601963 ITERATION : 14, loss : 0.05832836133114024 ITERATION : 15, loss : 0.05834783849566258 ITERATION : 16, loss : 0.05836075334579126 ITERATION : 17, loss : 0.05836932376151763 ITERATION : 18, loss : 0.05837501463660637 ITERATION : 19, loss : 0.05837879471411866 ITERATION : 20, loss : 0.05838130605980654 ITERATION : 21, loss : 0.05838297453315405 
ITERATION : 1, loss : 0.12846018599693718 ITERATION : 2, loss : 0.12802232571733932 ITERATION : 3, loss : 0.12797310012889979 ITERATION : 4, loss : 0.127947515695063 ITERATION : 5, loss : 0.12792028375941208 ITERATION : 6, loss : 0.12789492629789628 ITERATION : 7, loss : 0.1278736793737433 ITERATION : 8, loss : 0.1278569163902794 ITERATION : 9, loss : 0.12784415860411646 ITERATION : 10, loss : 0.12783466954211167 ITERATION : 11, loss : 0.1278277199087168 ITERATION : 12, loss : 0.12782268470941036 ITERATION : 13, loss : 0.12781906484019376 ITERATION : 14, loss : 0.12781647722512887 
ITERATION : 1, loss : 0.0495446232378554 ITERATION : 2, loss : 0.0495446232378554 ITERATION : 3, loss : 0.0495446232378554 ITERATION : 4, loss : 0.0495446232378554 ITERATION : 5, loss : 0.0495446232378554 ITERATION : 6, loss : 0.0495446232378554 
ITERATION : 1, loss : 0.12150242504130264 ITERATION : 2, loss : 0.12150242504130264 ITERATION : 3, loss : 0.12150242504130264 ITERATION : 4, loss : 0.12150242504130264 ITERATION : 5, loss : 0.12150242504130264 ITERATION : 6, loss : 0.12150242504130264 
ITERATION : 1, loss : 0.0692597744630365 ITERATION : 2, loss : 0.06599542031089117 ITERATION : 3, loss : 0.06411577808135628 ITERATION : 4, loss : 0.0629431822859774 ITERATION : 5, loss : 0.062189754520111475 ITERATION : 6, loss : 0.06169751240489028 ITERATION : 7, loss : 0.06137232167567686 ITERATION : 8, loss : 0.06115592932006116 ITERATION : 9, loss : 0.061011300778951304 ITERATION : 10, loss : 0.06091440653522627 ITERATION : 11, loss : 0.06084942759692148 ITERATION : 12, loss : 0.0608058493484175 ITERATION : 13, loss : 0.06077664066987224 ITERATION : 14, loss : 0.06075708348639407 ITERATION : 15, loss : 0.06074400649913514 ITERATION : 16, loss : 0.060735276721826686 ITERATION : 17, loss : 0.060729459791335844 ITERATION : 18, loss : 0.060725591881335604 ITERATION : 19, loss : 0.06072302589133552 ITERATION : 20, loss : 0.06072132798211288 
ITERATION : 1, loss : 0.05637884083995365 ITERATION : 2, loss : 0.07068714777836889 ITERATION : 3, loss : 0.08017185610787518 ITERATION : 4, loss : 0.08627508443161894 ITERATION : 5, loss : 0.09014531377852562 ITERATION : 6, loss : 0.09263937384296884 ITERATION : 7, loss : 0.09427997859358954 ITERATION : 8, loss : 0.09537556494155318 ITERATION : 9, loss : 0.09611367780063484 ITERATION : 10, loss : 0.09661310880928087 ITERATION : 11, loss : 0.09695157478520126 ITERATION : 12, loss : 0.09718096929251431 ITERATION : 13, loss : 0.09733633203746596 ITERATION : 14, loss : 0.09744144481071518 ITERATION : 15, loss : 0.09751247582388246 ITERATION : 16, loss : 0.09756041762084533 ITERATION : 17, loss : 0.09759273579553579 ITERATION : 18, loss : 0.09761449603003414 ITERATION : 19, loss : 0.0976291298605692 ITERATION : 20, loss : 0.09763895927057963 ITERATION : 21, loss : 0.0976455532008376 ITERATION : 22, loss : 0.09764997061824787 ITERATION : 23, loss : 0.09765292583783068 ITERATION : 24, loss : 0.09765489970398063 
ITERATION : 1, loss : 0.059580994991369665 ITERATION : 2, loss : 0.05964556000222527 ITERATION : 3, loss : 0.05970691193192893 ITERATION : 4, loss : 0.05974661112520238 ITERATION : 5, loss : 0.059771830759350476 ITERATION : 6, loss : 0.059788333546440704 ITERATION : 7, loss : 0.05979943605763488 ITERATION : 8, loss : 0.059807042607939925 ITERATION : 9, loss : 0.05981231058396534 ITERATION : 10, loss : 0.05981598226378014 ITERATION : 11, loss : 0.059818551222712533 ITERATION : 12, loss : 0.05982035321880464 
ITERATION : 1, loss : 0.061194140203434964 ITERATION : 2, loss : 0.061194140203434964 ITERATION : 3, loss : 0.061194140203434964 ITERATION : 4, loss : 0.061194140203434964 ITERATION : 5, loss : 0.061194140203434964 ITERATION : 6, loss : 0.061194140203434964 
gradient norm in None layer : 0.029969276938669492
gradient norm in None layer : 0.0012721138552356712
gradient norm in None layer : 0.001118623066081204
gradient norm in None layer : 0.017427397040590487
gradient norm in None layer : 0.001046675519386694
gradient norm in None layer : 0.0009478694237820053
gradient norm in None layer : 0.011002086371590726
gradient norm in None layer : 0.0004666041870426291
gradient norm in None layer : 0.0005105175439720073
gradient norm in None layer : 0.006265698008219625
gradient norm in None layer : 0.0002914477413938155
gradient norm in None layer : 0.0002904460383409707
gradient norm in None layer : 0.002047195950231128
gradient norm in None layer : 7.261249309859557e-05
gradient norm in None layer : 8.083630637617878e-05
gradient norm in None layer : 0.0013303939064145335
gradient norm in None layer : 8.668723882004168e-05
gradient norm in None layer : 5.8868681013740686e-05
gradient norm in None layer : 0.001703185635525656
gradient norm in None layer : 1.1728224427843826e-05
gradient norm in None layer : 0.003295220150890753
gradient norm in None layer : 0.00018333764779129947
gradient norm in None layer : 0.0002213835056710847
gradient norm in None layer : 0.0027277569555612434
gradient norm in None layer : 0.00023940733827644366
gradient norm in None layer : 0.00034465078825790974
gradient norm in None layer : 0.0049073558953685965
gradient norm in None layer : 7.359781725062201e-05
gradient norm in None layer : 0.008640727440906734
gradient norm in None layer : 0.0005300358937655984
gradient norm in None layer : 0.000675362497997544
gradient norm in None layer : 0.00925423098550538
gradient norm in None layer : 0.0015793232625899993
gradient norm in None layer : 0.00277520256206366
gradient norm in None layer : 0.0013336119784331027
gradient norm in None layer : 0.0005897484582964153
Total gradient norm: 0.03990714623233759
invariance loss : 8.009241408489721, avg_den : 0.07907867431640625, density loss : 0.022389221191406253, mse loss : 0.07957965264322175, solver time : 202.1861412525177 sec , total loss : 0.10997811524311774, running loss : 0.18591292344172877
Epoch 0/10 , batch 24/12500 
ITERATION : 1, loss : 0.04353255516043751 ITERATION : 2, loss : 0.04336023664393472 ITERATION : 3, loss : 0.04328292090328396 ITERATION : 4, loss : 0.04324509739971443 ITERATION : 5, loss : 0.04322530655527388 ITERATION : 6, loss : 0.04321432589918686 ITERATION : 7, loss : 0.04320800487780783 ITERATION : 8, loss : 0.04320427601414006 ITERATION : 9, loss : 0.043202035147356765 ITERATION : 10, loss : 0.043200667445109225 ITERATION : 11, loss : 0.04319982116653903 
ITERATION : 1, loss : 0.021619593988530268 ITERATION : 2, loss : 0.022308327345886936 ITERATION : 3, loss : 0.022596590735342693 ITERATION : 4, loss : 0.022727901062559657 ITERATION : 5, loss : 0.022793173893067425 ITERATION : 6, loss : 0.022827914578371758 ITERATION : 7, loss : 0.022847425433342044 ITERATION : 8, loss : 0.02285885021035366 ITERATION : 9, loss : 0.022865761827199804 ITERATION : 10, loss : 0.022870053284969973 ITERATION : 11, loss : 0.022872775175259315 ITERATION : 12, loss : 0.022874532574592273 ITERATION : 13, loss : 0.022875684481149034 
ITERATION : 1, loss : 0.013521376357486547 ITERATION : 2, loss : 0.013608048899741283 ITERATION : 3, loss : 0.013606005997071714 ITERATION : 4, loss : 0.013603886843894176 ITERATION : 5, loss : 0.013602625997277932 ITERATION : 6, loss : 0.013601641085985371 ITERATION : 7, loss : 0.013600809948822437 
ITERATION : 1, loss : 0.019882325677772567 ITERATION : 2, loss : 0.019882325677772567 ITERATION : 3, loss : 0.019882325677772567 ITERATION : 4, loss : 0.019882325677772567 ITERATION : 5, loss : 0.019882325677772567 ITERATION : 6, loss : 0.019882325677772567 
ITERATION : 1, loss : 0.02706862261875482 ITERATION : 2, loss : 0.024988140291994247 ITERATION : 3, loss : 0.024496132248371372 ITERATION : 4, loss : 0.024348075711289944 ITERATION : 5, loss : 0.024293799842855644 ITERATION : 6, loss : 0.024270498307376567 ITERATION : 7, loss : 0.02425909540307372 ITERATION : 8, loss : 0.024252883528376352 ITERATION : 9, loss : 0.02424920469763864 ITERATION : 10, loss : 0.02424688859515608 ITERATION : 11, loss : 0.024245367163095173 ITERATION : 12, loss : 0.024244338801158193 
ITERATION : 1, loss : 0.15943649157466752 ITERATION : 2, loss : 0.15751298376833228 ITERATION : 3, loss : 0.1555872166032512 ITERATION : 4, loss : 0.15421801457116072 ITERATION : 5, loss : 0.1532709530222586 ITERATION : 6, loss : 0.15261857124784203 ITERATION : 7, loss : 0.15216823273884944 ITERATION : 8, loss : 0.1518562969440131 ITERATION : 9, loss : 0.15163953924367898 ITERATION : 10, loss : 0.15148853061088302 ITERATION : 11, loss : 0.15138311947031022 ITERATION : 12, loss : 0.15130942701251357 ITERATION : 13, loss : 0.15125785146375786 ITERATION : 14, loss : 0.15122172453397303 ITERATION : 15, loss : 0.15119640251279517 ITERATION : 16, loss : 0.15117864547131496 ITERATION : 17, loss : 0.15116618880423638 ITERATION : 18, loss : 0.15115744808952722 ITERATION : 19, loss : 0.15115131301108364 ITERATION : 20, loss : 0.15114700661573924 ITERATION : 21, loss : 0.15114398333311385 ITERATION : 22, loss : 0.15114186022959 
ITERATION : 1, loss : 0.056537418656183384 ITERATION : 2, loss : 0.057035824115252404 ITERATION : 3, loss : 0.05728116530149205 ITERATION : 4, loss : 0.05740367245988774 ITERATION : 5, loss : 0.057467754630683765 ITERATION : 6, loss : 0.0575034825134841 ITERATION : 7, loss : 0.05752448133405832 ITERATION : 8, loss : 0.05753732845559911 ITERATION : 9, loss : 0.05754542848426601 ITERATION : 10, loss : 0.057550653335189335 ITERATION : 11, loss : 0.05755408329342919 ITERATION : 12, loss : 0.05755636611819908 ITERATION : 13, loss : 0.05755790201378958 
ITERATION : 1, loss : 0.05990456479706307 ITERATION : 2, loss : 0.05833731954921843 ITERATION : 3, loss : 0.057574064951559784 ITERATION : 4, loss : 0.05713803320216371 ITERATION : 5, loss : 0.05685964240913405 ITERATION : 6, loss : 0.05667170516385473 ITERATION : 7, loss : 0.056541666010812876 ITERATION : 8, loss : 0.05645073640158511 ITERATION : 9, loss : 0.05638687053932323 ITERATION : 10, loss : 0.05634193242267195 ITERATION : 11, loss : 0.05631029309109768 ITERATION : 12, loss : 0.05628801559815961 ITERATION : 13, loss : 0.05627233253332565 ITERATION : 14, loss : 0.05626129477920781 ITERATION : 15, loss : 0.05625352864641348 ITERATION : 16, loss : 0.056248065897755 ITERATION : 17, loss : 0.05624422426682077 